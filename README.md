# ğŸ§  Ethical Risk Detector (Responsible AI by Design)

A full-featured NLP system to detect **ethical risks** in public sector AI policies and documents using weak supervision and transformer-based models. This project aims to identify indicators of:

- ğŸ” **Bias**
- ğŸ“¹ **Surveillance**
- ğŸ§¾ **Lack of Transparency**

---

## ğŸ“ Project Structure

ethical-risk-detector/
|
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ â† Unprocessed .txt files scraped from gov.uk
â”‚ â”œâ”€â”€ cleaned/ â† JSON files after preprocessing
â”‚ â””â”€â”€ processed/ â† Labeled CSV for training


â”œâ”€â”€ src/

â”‚ â”œâ”€â”€ config.py â† Global path and constants

â”‚ â”œâ”€â”€ scrapping/ â† Web scrapers (e.g., govuk_scraper.py)

â”‚ â”œâ”€â”€ preprocessing/ â† Text cleaning logic

â”‚ â”œâ”€â”€ labeling/ â† Snorkel rules and labeler
â”‚ â”œâ”€â”€ models/ â† Model training, evaluation, inference
â”‚ â””â”€â”€ utils/ â† Utility functions (tokenization, logging, etc.)
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ scrape_data.py â† Run the gov.uk scraper
â”‚ â”œâ”€â”€ clean_data.py â† Run data cleaning pipeline
â”‚ â”œâ”€â”€ label_cleaned_data.py â† Apply Snorkel labeling
â”‚ â”œâ”€â”€ run_inference.py â† Inference CLI
â”‚ â””â”€â”€ train_model.sh â† Shell wrapper for training
â”‚
â”œâ”€â”€ logs/ â† Training logs
â”œâ”€â”€ models/ â† Saved model checkpoints
â”œâ”€â”€ requirements.txt â† All required packages
â”œâ”€â”€ README.md â† This file
â””â”€â”€ .env â† Optional secrets (not versioned)

---

## ğŸ›  Setup

### 1. Clone & Install

```bash
git clone https://github.com/your-org/ethical-risk-detector.git
cd ethical-risk-detector
pip install -r requirements.txt
```

2. Install Torch + Transformers

```bash
pip install torch torchvision torchaudio
pip install transformers datasets snorkel scikit-learn
```

âš™ï¸ Usage
Step 1: Scrape raw data

```bash
python scripts/scrape_data.py
```

Step 2: Clean and preprocess

```bash
python scripts/clean_data.py
```

Step 3: Apply Snorkel labeling functions

```bash
python scripts/label_cleaned_data.py
```

Step 4: Train multi-label classifier (Roberta)

```bash
python -m src.models.trainer

```

Step 5: Run inference

```bash
python scripts/run_inference.py

```

Youâ€™ll be prompted to enter custom text:

```bash
Enter text: AI models trained on historical data may reinforce discrimination.
Predicted Risks: {'bias': 1, 'surveillance': 0, 'transparency': 1}

```

ğŸ“ˆ Model Details
Model: RobertaForSequenceClassification with 3-label output (bias, surveillance, transparency)
Format: Multi-label classification using BCEWithLogitsLoss
Input: Cleaned text from policy documents
Output: Probability (converted to binary via 0.5 threshold)

ğŸ§  Snorkel Labeling Functions
The model is trained using weak supervision via Snorkel. Labeling functions include:
bias_keywords(): Detects fairness, social discrimination, systemic bias
surveillance_keywords(): Flags facial recognition, tracking, real-time monitoring
transparency_keywords(): Detects black-box warnings, lack of explanation or audit
You can find them in src/labeling/snorkel_rules.py.

âœ… Sample Output
| id | title | text | bias | surveillance | transparency |
| ---------- | -------------------------------- | ---- | ---- | ------------ | ------------ |
| govuk_001 | AI Regulation | ... | 1 | 0 | 1 |
| govuk_002 | Facial Recognition in Public Use | ... | 0 | 1 | 0 |

ğŸ“Œ Future Work
SHAP/LIME-based explainability
Human-in-the-loop review dashboard
Fine-tuning on manually annotated corpus
Zero-shot inference with TARS/BART variants

ğŸ‘¤ Author
Muhammad Zain Ul Abideen
ğŸ’¼ Full Stack AI Developer
ğŸ“§ zainm2432003@gmail.com
