ID,Source,Title,Text,Year
govuk_000,govuk,Ai Ethics 0,"Understand how to use artificial intelligence ethically and safely This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. The Office for Artificial Intelligence (OAI) and the Government Digital Service (GDS) have produced the following chapter's guidance in partnership with The Alan Turing Institute'spublic policy programme. This chapter is a summary ofThe Alan Turing Institute's detailed guidance, and readers should refer to the full guidance when implementing these recommendations. AI has the potential to make a substantial impact for individuals, communities, and society. To make sure the impact of your AI project is positive and does not unintentionally harm those affected by it, you and your team should make considerations of AI ethics and safety a high priority. This section introduces AI ethics and provides a high-level overview of the ethical building blocks needed for the responsible delivery of an AI project. The following guidance is designed to complement and supplement theData Ethics Framework. The Data Ethics Framework is a tool that should be used in any project. This guidance is for everyone involved in the design, production, and deployment of an AI project such as: Ethical considerations will arise at every stage of your AI project. You should use the expertise and active cooperation of all your team members to address them. AI ethics is a set of values, principles, and techniques that employ widely accepted standards to guide moral conduct in the development and use of AI systems. The field of AI ethics emerged from the need to address the individual and societal harms AI systems might cause. These harms rarely arise as a result of a deliberate choice - most AI developers do not want to build biased or discriminatory applications or applications which invade users' privacy. The main ways AI systems can cause involuntary harm are: The field of AI ethics mitigates these harms by providing project teams with the values, principles, and techniques needed to produce ethical, fair, and safe AI applications. The guidance summarised in this chapter and presented at length inThe Alan Turing Institute's further guidance on AI ethics and safetyis as comprehensive as possible. However, not all issues discussed will apply equally to each project using AI. An AI model which filters out spam emails, for example, will present fewer ethical challenges than one which identifies vulnerable children. You and your team should formulate governance procedures and protocols for each project using AI, following a careful evaluation of social and ethical impacts. You should establish ethical building blocks for the responsible delivery of your AI project. This involves building a culture of responsible innovation as well as a governance architecture to bring the values and principles of ethical, fair, and safe AI to life. To build and maintain a culture of responsibility you and your team should prioritise 4 goals as you design, develop, and deploy your AI project. In particular, you should make sure your AI project is: Prioritising these goals will help build a culture of responsible innovation. To make sure they are fully incorporated into your project you should establish a governance architecture consisting of a: You should understand the framework of ethical values which support, underwrite, and motivate the responsible design and use of AI. The Alan Turing Institute calls these 'the SUM Values': These values: You can read further guidance on SUM Values inThe Alan Turing Institute's comprehensive guidance on AI ethics and safety. While the SUM Values can help you consider the ethical permissibility of your AI project, they are not specifically catered to the particularities of designing, developing, and implementing an AI system. AI systems increasingly perform tasks previously done by humans. For example, AI systems can screen CVs as part of a recruitment process. However, unlike human recruiters, you cannot hold an AI system directly responsible or accountable for denying applicants a job. This lack of accountability of the AI system itself creates a need for a set of actionable principles tailored to the design and use of AI systems. The Alan Turing Institute calls these the 'FAST Track Principles': Carefully reviewing the FAST Track Principles helps you: If your AI system processes social or demographic data, you should design it to meet a minimum level of discriminatory non-harm. To do this you should: You should design your AI system to be fully answerable and auditable. To do this you should: The technical sustainability of these systems ultimately depends on their safety, including their accuracy, reliability, security, and robustness. You should make sure designers and users remain aware of: Designers and implementers of AI systems should be able to: To assess these criteria in depth,you should consult The Alan Turing Institute's guidance on AI ethics and safety. The final method to make sure you use AI ethically, fairly, and safely is building a process-based governance framework. The Alan Turing Institute calls it a 'PBG Framework'. Its primary purpose is to integrate the SUM Values and the FAST Track Principles across the implementation of AI within a service. Building a good PBG Framework for your AI project will provide your team with an overview of: You may find it useful toconsider further guidance on allocating responsibility and governance for AI projects.",2023
govuk_002,govuk,Ai Ethics 2,"Find guidance for the responsible use and development of data and data technologies developed by and for government and public sector bodies. Use this tool to find data ethics guidance from across government. You can use the filter and search functions to identify the most relevant pieces of information for your organisation's needs. Start now Emaildata.ethics@digital.cabinet-office.gov.ukif you have questions or suggestions about data ethics guidance, or are aware of further relevant guidance that should be included. We encourage anyone working in the government and public sector to refer to the documents below to develop a high-level understanding of key data ethics considerations. TheData Ethics Frameworkexplains how to use data appropriately and responsibly when planning, implementing and evaluating a new policy or service. TheModel for Responsible Innovationis a practical tool to help teams across the public sector and beyond to innovate responsibly with data andAI. When working with public sector data, you have a responsibility to establish whether the data you manage and use is fit for purpose. TheGovernment Data Quality FrameworkandData Sharing Governance Frameworkset out principles and practices to improve the quality and better use of data across government. TheGenerativeAIFramework forHMGexplains how to use generativeAIsafely and responsibly. TheAlgorithmic Transparency Recording Standard (ATRS)provides a standard for public sector organisations to publish information about how and why they are using algorithmic methods in decision-making processes that affect members of the public. TheEthics, Transparency and Accountability Framework for Automated Decision-Makingaims to help government departments with the safe, sustainable and ethical use of automated and algorithmic decision-making systems. AIassurance is vital to ensure the reliability and trustworthiness ofAIsystems. The Responsible Technology Adoption Unit's (RTA)introduction toAIAssuranceidentifies assurance techniques that can support the development of responsibleAI. You can find additional information about tools and processes that support the responsible use ofAIat theRTA'sResponsibleAIToolkitpage. You must use the criteria inThe Technology Code of Practiceto design, build and buy technology in government. When procuringAIsolutions from third parties, refer to theGuidelines forAIprocurement. This includes principles for buyingAItechnology and insights on tackling challenges that may arise during procurement.",2023
govuk_003,govuk,Ai Ethics 3,"Understand how to manage a project which uses artificial intelligence. This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. Once you have planned and prepared for your AI implementation, you will need to make sure you effectively manage risk and governance. This guidance is for people responsible for: The Alan Turing Institute (ATI) has written guidance onhow to use AI ethically and safely. Governance in safety is important to make sure the model shows no signs of bias or discrimination. You can consider whether: Governance in purpose makes sure the model is achieving its purpose/business objectives. You can consider whether: Governance in accountability provides a clear accountability framework for the model. You can consider: Governance in testing and monitoring makes sure a robust testing framework is in place. You can consider: Governance in public narrative protects against reputational risks arising from the application of the model. You can consider whether: Governance in quality assurance makes sure the code has been reviewed and validated. You can consider whether:",2023
govuk_004,govuk,Ai Ethics 4,"Guidance to help you assess if artificial intelligence (AI) is the right technology for your challenge. This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. This guidance will help you assess if artificial intelligence (AI) is theright technology to help you meet user needs. As with all technology projects, you should make sure you can change your mind at a later stage and you can adapt the technology as your understanding of user needs changes. This guidance is relevant for anyone responsible for choosing technology in a public sector organisation. AI is just another tool to help deliver services. Designing any service starts withidentifying user needs. If you think AI may be an appropriate technology choice to help you meet user needs, you will need to consider your data and the specific technology you want to use. Yourdata scientistswill then use your data to build and train an AI model. When assessing if AI could help you meet users' needs, consider if: It's important to remember that AI is not an all-purpose solution. Unlike a human, AI cannot infer, and can only produce an output based on the data a team inputs to the model. When identifying whether AI is the right solution, it's important that you work with: For your AI model to work, it needs access to a large quantity of data. Work with specialists who have the knowledge of your data, such as data scientists, to assess your data state. You can assess whether your data is high enough quality for AI using a combination of: If your problem involves supporting an ongoing business decision process, you will need to plan to establish ongoing, up-to-date access to data. Remember tofollow data protection laws. There is no one 'AI technology'. Currently, widely-available AI technologies are mostly either supervised, unsupervised or reinforcement machine learning. The machine learning techniques that can provide you with the best insight depends on the problem you're trying to solve. There are certain types of problems for which machine learning is commonly used. For some of these you will be able to buy or adapt commercially available products. Because of its experimental and iterative nature, it can be difficult to specify the precise benefits which could come from an AI project. To explore this uncertainty and provide the right level of information around the potential benefits, you can: Once you have secured budget, you'll need to allow enough time and resources toconduct a substantial discoveryto show feasibility. Discovery for projects using AI can often takes longer for similar projects that do not use AI. If your organisation is a central government department, you may have toget approval from the Government Digital Service (GDS) to spend moneyon AI. At this point mostAI projects are classified as 'novel', which requires a high level of scrutiny. You should contact theGDS Standards Assuranceteam for help on the spend controls process. When assessing if AI could help you meet user needs, consider how you will procure the technology. You shoulddefine your purchasing strategyin the same way as you would for any other technology. Whether you build, buy or reuse (or combine these approaches) will depend on a number of considerations, including: It's also important toaddress ethical concernsabout the use of AI from the start of the procurement process. The Office for AI and the World Economic Forum are developing furtherguidance on AI procurement. Your team can build or adapt off-the-shelf AI models or open source algorithms in-house. When making this decision, you should work with data scientists to consider whether: You may be able to buy your AI technology as an off-the-shelf product. This is most suitable if you are looking for a common application of AI, for example optical character recognition. However, buying your AI technology may not always be suitable as the specifics of your data and needs could mean the supplier would have to build from scratch or significantly customise an existing model. Your AI solution will still need to be integrated into an end-to-end service for your users, even if you are able to buy significant components off the shelf. When using AI it's important to understand who is responsible if the system fails, as the problem may lie in a number of areas. For example, failures with the data chosen to train the model, design of the model, coding of the software, or deployment. You should establish a responsibility record which sets out who is responsible for different areas of the AI. It would be useful to consider whether: Depending on your organisation's maturity, it may be useful to set up a dedicated board, committee or forum to handle AI data and model governance. It can be useful to keep a central record of all AI technologies you use, listing:",2023
govuk_005,govuk,Ai Ethics 5,"This page provides details about DSIT's portfolio of AI assurance techniques and how to use it. The portfolio ofAIassurance techniques has been developed by the Responsible Technology Adoption Unit, a directorate within DSIT, initially in collaboration withtechUK. The portfolio is useful for anybody involved in designing, developing, deploying or procuringAI-enabled systems, and showcases examples ofAIassurance techniques being used in the real-world to support the development of trustworthyAI. Search portfolio Please note the inclusion of a case study in the portfolio does not represent a government endorsement of the technique or the organisation, rather we are aiming to demonstrate the range of possible options that currently exist. To learn more about different tools and metrics forAIassurance please refer toOECD's catalogue of tools and metrics for trustworthyAI, a one-stop-shop for tools and metrics designed to helpAIactors develop fair and ethicalAI. We will be developing the portfolio over time, and publishing future iterations with new case studies. If you would like to submit case studies to the portfolio or would like further information please get in touch atai-assurance@dsit.gov.uk. Building and maintaining trust is crucial to realising the benefits ofAI. Organisations designing, developing, and deployingAIneed to be able to check that these systems are trustworthy, and communicate this clearly to their customers, service users, or wider society. AIassurance is about building confidence inAIsystems by measuring, evaluating and communicating whether anAIsystem meets relevant criteria such as: Assurance can also play an important role in identifying and managing the potential risks associated withAI. To assureAIsystems effectively we need a range of assurance techniques for assessing different types ofAIsystems, across a wide variety of contexts, against a range of relevant criteria. To learn more aboutAIassurance, please refer tothe roadmap to anAIassurance ecosystem,AIassurance guide,industry temperature check, and co-developedRTA (formerlyCDEI) and The Alan Turing Institute introduction toAIassurancee-learning module. The Portfolio ofAIassurance techniques was developed by the Responsible Technology Adoption Unit (RTA), in collaboration with techUK, to showcase examples ofAIassurance techniques being used in the real-world. It includes a variety of case studies from across multiple sectors and a range of technical, procedural and educational approaches , illustrating how a combination of different techniques can be used to promote responsibleAI. We have mapped these techniques to the principles set out in the UK government's white paper onAIregulation, to illustrate the potential role of these techniques in supporting widerAIgovernance. To learn more about different tools and metrics forAIassurance, please refer toOECD's catalogue of tools and metrics for trustworthyAI. The portfolio is a helpful resource for anyone involved in designing, developing, deploying or procuringAI-enabled systems. It will help you understand the benefits ofAIassurance for your organisation, if you're someone who is: The portfolio allows you to explore a range of examples ofAIassurance techniques applied across a variety of sectors. You can search for case studies based on multiple features you might be interested in, including the type of technique and the sector you work within. Each case study is also mapped against the most relevant cross-sector regulatory principles published in the government white paper onAIregulation. There are a range of different assurance techniques that can be used to measure, evaluate, and communicate the trustworthiness ofAIsystems. Some of these are listed below: Impact assessment:Used to anticipate the effect of a system on environmental, equality, human rights, data protection, or other outcomes. Impact evaluation:Similar to impact assessments, but are conducted after a system has been implemented in a retrospective manner. Bias audit:Assessing the inputs and outputs of algorithmic systems to determine if there is unfair bias in the input data, the outcome of a decision or classification made by the system. Compliance audit:A review of a company's adherence to internal policies and procedures, or external regulations or legal requirements. Specialised types of compliance audit include system and process audits and regulatory inspection. Certification:A process where an independent body attests that a product, service, organisation or individual has been tested against, and met, objective standards of quality or performance. Conformity assessment:Provides assurance that a product, service or system being supplied meets the expectations specified or claimed, prior to it entering the market. Conformity assessment includes activities such as testing, inspection and certification. Performance testing:Used to assess the performance of a system with respect to predetermined quantitative requirements or benchmarks. Formal verification:Establishes whether a system satisfies some requirements using the formal methods of mathematics. Check with assurance techniques can be used across each stage of theAIlifecycle. TheNationalAIStrategysets out an ambitious plan for how the UK can lead the world as anAIresearch and innovation powerhouse. EffectiveAIregulation is key to realising this vision to unlock the economic and societal benefits ofAIwhile also addressing the complex challenges it presents. In its recentAIregulation white paperthe UK government describes its pro-innovation, proportionate, and adaptable approach toAIregulation that supports responsible innovation across sectors. The white paper outlines five cross-cutting principles forAIregulation: safety, security and robustness; appropriate transparency and explainability; fairness; accountability and governance; and contestability and redress. Due to the unique challenges and opportunities raised byAIin particular contexts the UK will leverage the expertise of existing regulators, who are expected to interpret and implement the principles in their domain and outline what compliance with the principles looks like across different use cases. In addition, the white paper sets out the integral role of tools for trustworthyAI, such as assurance techniques and technical standards, to support the implementation of these regulatory principles in practice, boost international interoperability, and enable the development and deployment of responsibleAI. The RTA has conducted extensive research to investigate current uptake and adoption of tools for trustworthyAI, the findings of which are published in itsindustry temperature check. This report highlights industry appetite for more resources and repositories showcasing what assurance techniques exist, and how these can be applied in practice across different sectors. The UK government is already supporting the development and use of tools for trustworthyAI, through publishing aroadmap to an effectiveAIassurance ecosystemin the UK, having established theUKAIStandards Hubto champion the use of international standards, and now through the publication of the portfolio ofAIassurance techniques. TheAIStandards Hub is a joint initiative led by The Alan Turing Institute in partnership with the British Standards Institution (BSI), the National Physical Laboratory (NPL), and supported by government. The hub's mission is to advance trustworthy and responsibleAIwith a focus on the role that standards can play as governance tools and innovation mechanisms. TheAIStandards Hub aims to help stakeholders navigate and actively participate in internationalAIstandardisation efforts and champion the use of international standards forAI. Dedicated to knowledge sharing, community and capacity building, and strategic research, the hub seeks to bring together industry, government, regulators, consumers, civil society and academia with a view to: To learn more, visit theAIStandards Hubwebsite. The catalogue of tools and metrics for trustworthyAIis a one-stop-shop for tools and metrics designed to helpAIactors develop and useAIsystems that respect human rights and are fair, transparent, explainable, robust, secure and safe. The catalogue gives access to the latest tools and metrics in a user-friendly way but also to use cases that illustrate how those tools and metrics have been used in different contexts. Through the catalogue,AIpractitioners from all over the world can share and compare tools and metrics and build upon each other's efforts to implement trustworthyAI. The OECD catalogue features relevant UK initiatives and works in close collaboration with theAIStandards Hub, showcasing relevant international standards for trustworthyAI. The OECD catalogue will also feature the case studies included in this portfolio. To learn more, visitThe OECD catalogue of tools and metrics for trustworthyAI. Data assurance is a set of processes that increase confidence that data will meet a specific need, and that organisations collecting, accessing, using and sharing data are doing so in trustworthy ways. Data assurance is vital for organisations to build trust, manage risks and maximise opportunities. But how can organisations assess, build and demonstrate trustworthiness with data? Through its data assurance work, theODIis working with partners and collaborators to explore this important and rapidly developing area in managing global data infrastructure.ODIbelieve the adoption of data assurance practices, products and services will reassure organisations and individuals who want to share or reuse data, and support better data governance practices, fostering trust and sustainable behaviour change. To learn more, visit theODIwebsite.",2023
govuk_006,govuk,Ai Ethics 6,"AI is one of the fastest-moving areas of technological advancement in government defence, bringing the capability to counter threats and create opportunities. The Defence Science and Technology Laboratory (Dstl) provides theUKwith world-class capability in applying artificial intelligence (AI), data science and machine learning to defence and security challenges. From putting machine learning on-board Royal Navy ships, to using data science to support intelligence analysis,Dstlis at the heart of such innovation. One ofDstl's missions is tode-mystify the area ofAI. We help theUKMinistry of Defence (MOD) understand how it cansafely, responsibly and ethicallyuseAIto deter and de-escalate conflict, save lives and reduce harm. Our technical experts have their finger on the pulse of new developments inAIand provide critical guidance for theUK. We work in partnership with specialists from academia, industry and allied nations to understand and develop a broad spectrum of techniques for performing tasks and discovering insights from data using automatic processes, We are also an integral part of theDefenceAICentre (DAIC), working collaboratively to connectMODand the widerAIcommunity and enable teams acrossMODto adoptAIin their areas. The wide range of potential applications include streamlining back-office functions, and supporting: AIis critical to theUK's future as a science and technology superpower, and when it comes toAI,MOD's visionis to be ""the world's most effective, efficient, trusted and influential defence organisation for our size."" ...A radical upheaval in defence is underway andAI-related strategic competition is intensifying. Our response must be rapid, ambitious, and comprehensive.""(Defence Artificial Intelligence Strategy (2022) Our cutting edge work covers everything from very early research looking at how machines interact with humans, to applying data science to real-world challenges and operational requirements. Our experts collaborate with international partners to help theUKand our allies advance more quickly. The 3 nations ofAUKUS(Australia - United Kingdom - United States)recently testedAI-enabled uncrewed aerial vehiclesthat allow a human operator to locate, disable and destroy targets on the ground. AUKUS: Autonomy andAIcollaboration Royal Navy crews taking part in a 2021 NATO exercise inUKwaters were able to take advantage ofAIon-board ship for the first time, whenDstland our industry partners brought the latest technology into the command spaces of a Type 45 destroyer and a Type 23 frigate. Tested against a supersonic missile threat, ourAI-based applications were designed to help detect threats earlier and provide a rapid hazard assessment to recommend options the crew can take to counter the threat. This exciting real-world test was the culmination of almost10 years of research and collaboration with industry suppliers. Image taken from HMS Dragon, participating in Exercise Formidable Shield 2021. UK MOD (c) Crown copyright 2021 We work alongside the armed forces to develop and test new technologies in the field to get our research out of the lab and in to operational use as quickly as possible. Dstland theUSAir Force Research Laboratory carried out the firstdeployment of a jointly developed artificial intelligence toolbox in 2 military exercises. The goal was to address the challenge of how to makeAIagile, adaptable, trustworthy and accessible to the warfighter under differentUSandUKmilitary use cases. The trials showed how the toolbox would be deployed ontoUK-USuncrewed ground and aerial vehicles. Members of bothUKandUSarmed forces tested that theAIwas robust and that the intended users understood any limitations of theAI. DuringExercise Spring Storm, soldiers from the 20th Armoured Infantry Brigade used anAIengine developed byDstland the Army in collaboration with industry. This prototype was specifically designed for the way the Army is trained to operate. During the exercise,Dstlstaff analysed how theAIengine was used in practice, to understand the critical human factors including how we build trust in theAI. TheAIengine uses automation and smart analytics along with supervised learning algorithms to save time and effort and help military personnel operate much more effectively. By instantly exploiting information on the surrounding environment and terrain, theAImakes it much quicker to plan and analyse different courses of action. This is one of the first steps towards achieving machine-speed command and control. UK MOD (c) Crown copyright 2021 Dstlis also working with partners on howAIand autonomy pose opportunities and threats to traditional intelligence, surveillance and reconnaissance (ISR). Our research ranges fromAI-enabledISRtasking and collection, to investigating the advantage that could be brought by quantum information processing. This work isn't just done by ourAIspecialists but is a great example of how we work acrossDstl's areas of expertise, such assensingandrobotics and autonomy. We have developed a standard approach forAIand autonomy in networked multi-sensor systems in security and defence which has been evaluated during multinational NATO trials, adopted byMODand industry, and published as British Standards Institute (BSI) Flex 335. Sensing for Asset Protection with Integrated Electronic Networked Technology (SAPIENT) specifies standards and protocols to allowAIalgorithms to work together across a suite of sensors and share data with a mix of technologies. As well as improving efficiency,SAPIENTcan help deliver enhanced situational awareness to support control and command of operations, and givesMODand our allies access to advancedAIsolutions being developed by our innovative supplier base. During a recentAUKUStrial in Australia,Dstland theUKarmed forces collaborated with Australia and theUSin theTrusted Operation of Robotic Vehicles in Contested Environments (TORVICE) trial. A number of missions were conducted, such as route reconnaissance while subject to a range of effects, and these aimed to understand and improve the resilience ofAUKUSAIand autonomy systems when subjected to attack. Understanding and mitigating the impact of threats is a key step towards deploying effective and reliableAIand autonomy systems on future operations. TORVICEAITrial We also work in partnership with theNational Quantum Technology Programme (NQTP), which provides unique insights into quantum information processing, complementing our own research and building expertise to support defence. TheNQTP's work includesfunding researchinto quantum machine learning models, hybrid (traditional/quantum) generative modelling to improve satellite imaging, as well as quantum fingerprinting to protect such models from cyber attacks. Dstlis an outstanding centre for research and we have extensive collaboration underway with leadingAIorganisations. Whenever possible,Dstlworks with external suppliers in academia and industry to meet the needs of theUK's defence and security. We are working together withGoogle Cloudto accelerate the adoption ofAIin theUKdefence sector and withMicrosoftto deliver safe and responsible use ofAI. Dstlis a strategic partner in thedefence and security research programmerun byThe Alan Turing Institutewho conduct world leading research in a large number of areas related to our needs. As another example of our partnership with the Turing,Dstland theDefence and Security Accelerator (DASA)supported a studyidentifying hazardous chemical and biological contamination on surfaces. Working with industry and academic partners, we explored how machine learning and data science could be used to detect substances like anthrax and nerve agents, alongside the development of innovative sensor technologies. This could be safer and more efficient than current methods. We frequently collaborate with universities and other academic institutions, often accelerating the research they have been doing and applying it to solve problems forUKdefence and security - it's a fantastic opportunity for organisations to get to work on leading edge projects with real impact on the defence and security of the country. In 2022, we formed theDefence Data Research Centre(DDRC) - a dedicated centre of excellence focusing on problems related to the use of data forAIapplications with a defence context. We are working closely with staff at the universities of Southampton and Edinburgh, where2 new centres for doctoral traininghave been funded byMODto enable novel research in critical technology areas, such asAIand autonomy. We have close links with other research bodies, including: AIand data science underpin and enable all theUK's defence and security science and technology capabilities. They have the potential to change everything.Dstlplays a pivotal role bringing together our scientific and technical expertise acrossMOD. ""Future conflicts may be won or lost on the speed and efficacy of theAIsolutions employed."" (Defence in a competitive age) Our flagshipAIscience and technology programme delivers core defence-specific research, but also works across our capabilities to build up strong communities of practice to share and expand our expertise. DrivingAIcloser to capability We worked withDefence DigitalandMODto establish theDefenceAICentre(DAIC) to boost research and accelerate the adoption ofAItechnology (announced in theIntegrated Review). The DAIC works collaboratively across defence, united and supported by a core team made up of Defence Digital (as part of Strategic Command),Defence Equipment and Support(Future Capability Group) andDstl. The team champion, enable and innovateAIacrossUKdefence, working with government, industry, academia and our allies for the strategic advantage of our armed forces. This includes working withDASAto help connect with innovators both in theUKand abroad. In 2022, we opened abrand newAIand data science unit based at the National Innovation Centre for Data in Newcastle, opening up new local opportunities for individuals and institutions to work with us. As part of our work in government, we support learning and up-skilling of people working across the defence and security sector. We also want to make it easier for more people to work with us. Our approach to this is broad, from enabling government customers commissioning our services to attracting talent from outside the defence industry to work with us. We run regular events (such asAIFest) to bring together theAIcommunity across government, industry, academia and international partners. At these events, world-leading experts share their experiences including the challenges of military adoption ofAI. We also discuss how to build an effectiveAIcapability. Our series ofBiscuit Booksprovide simple introductions to some of the complex concepts related toAI, data science and machine learning. Topics covered includehuman-centred ways of working withAIin intelligence analysisandassurance ofAIand autonomous systems. We are responsible and ethical in our use ofAIand we are leading the way in helping others (across defence, wider government and internationally) with safe adoption. We helped formMOD's internal guidance on best practice for ethics in developing or using systems which useAI, to achieve theAmbitious, safe, responsiblepolicy statement. This statement sets out the5 ethical principleswe follow: Through our strong links with academia we are contributing toresearch into the ethical use ofAIin defence. Dstl's experts have also collaborated with the Institution of Engineering and Technology to help raise awareness of theeffective use ofAIin safety-related functions, highlighting the importance of underpinning regulation and good practice to embedAIsafety. Our staff, industry and academic partners work across a range of scientific and technology capabilities including: We are always looking for talented individuals to join us. You will work on real-world problems, with the chance to see your science and technology expertise put into practice, including hugely exciting opportunities for overseas travel and working with theUK's allies. Experience in defence is not necessary, and roles vary from apprenticeships to visiting fellowships (for example, we have a Visiting research scheme with the Alan Turing Institute). While we have a high intake of graduates, you can also be mid-career, looking to re-skill and re-train. We work across Defence to partner with academic institutions and build relationships with suppliers. We particularly welcome companies and research bodies who have not worked with defence before, and small and medium-sized enterprises (SMEs). There are several routes toworking with us: Talk to us about potential future partnerships and projects by emailingcentralenquiries@dstl.gov.uk Added video about 'Driving AI closer to capability'. New video of the 3 nations of AUKUS have trialled a futuristic integration of autonomy and AI. Updated with links to the latest case studies and information about Dstl's work on AI. First published.",2023
govuk_007,govuk,Ai Ethics 7,"Guidance on building and using artificial intelligence in the public sector. The Government Digital Service (GDS) and the Office for Artificial Intelligence (OAI) have published joint guidance on how to build and use artificial intelligence (AI) in the public sector. This guidance covers how: OAI, GDS, and The Alan Turing Institute (ATI) have partnered to produce guidance on how to use AI ethically and safely. Emailai-guide@digital.cabinet-office.gov.ukif you: want to talk about using AI in the public sector have any feedback on the AI guidance would like to share an AI case study with us Every day, artificial intelligence (AI) is changing how we experience the world. We already use AI to find the fastest route home, alert us of suspicious activity in our bank accounts and filter out spam emails. The UK government recognises the importance of this technology's development to both business and the public sector. Indeed, Artificial Intelligence and Data was named as one of the four 'Grand Challenges' in the Industrial Strategy White Paper, which are global trends that will transform our future and contribute to the government's long-term plan to boost productivity in the UK. PwC estimates that AI could contribute $15.7tr to the global economy by 2030. The UK is in the top three countries globally in the development of AI technologies and this strength puts us in a prime position to unlock this projected global growth. The same estimates indicate AI could increase our productivity by 14.3% and grow our GDP up to 10.3% by 2030. There are huge opportunities for government to capitalise on this exciting new technology to improve lives. We can deliver more for less, and give a better experience as we do so. For citizens, the application of AI technologies will result in a more personalised and efficient experience. For people working in the public sector it means a reduction in the hours they spend on basic tasks, which will give them more time to spend on innovative ways to improve services. When government and citizens benefit, so does the economy. This year, the UK government ranked second globally in terms of AI readiness, and as the country most prepared within Western Europe to realise the benefits of AI in delivering public services. Putting this readiness into practice and procuring innovative solutions from the UK's thriving tech sector will, in turn, benefit our economy and grow new and innovative markets across sectors. We want the public sector to understand AI and embrace the opportunities here. As part of this work, a review into using AI in the public sector, led by the Government Digital Service and the Office for Artificial Intelligence (a joint DCMS / BEIS unit), was undertaken between November 2018 and April 2019. Its purpose was to show us where AI could have the most impact and where investment could yield the greatest benefit. The findings of the review, published as part of the Government Technology Innovation Strategy, revealed that leaders across the public sector could benefit from better understanding the technology, the opportunities it presents and the limitations of its use. It also found that delivery teams needed more specific guidance on the different considerations for projects with AI components. This guide was produced to meet this need, drawing on best practice from the commercial sector and public sector. We also need to practice what we preach and make sure the public sector is leading from the front in the safe and ethical deployment of AI and other emerging technologies. To reflect this, we have worked with the Alan Turing Institute, the UK's national institute for artificial intelligence, to produce additional guidance on AI ethics and safety in a public sector context. Maximising the benefits of AI is a priority for government, and this guide is an important step forward towards reaching that goal. We encourage you to put this guidance into practice. Minister of State for Digital and the Creative Industries, Margot James MP Minister of State for Universities, Science, Research and Innovation, Chris Skidmore MP Parliamentary Secretary and Minister for Implementation, Oliver Dowden CBE MP Guidance from GDS and OAI on how to assess, plan and manage artificial intelligence. Guidance produced in partnership with The Alan Turing Institute on how to use AI ethically and safely. A collection of examples of how artificial intelligence is being used by the public sector and elsewhere. We have added a new case study to the ""Examples of artificial intelligence use"" section. Included an email address for any feedback on the guidance Addition of a case study Moved Ministerial Foreword from 'A guide to using artificial intelligence in the public sector' guidance page to collection page. First published.",2023
govuk_008,govuk,Ai Ethics 8,"Guidance to help you plan and prepare for implementing artificial intelligence (AI). This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. Once you have assessed whether AI can help your team meet your users' needs, this guidance will explore the steps you should take to plan and prepare before implementing AI. As with all technology projects and programmes,you should follow the Technology Code of Practice. This guidance is for anyone responsible for: As with all projects, you need to make sure you'rehypothesis-ledand can constantly iterate to best help your users and their needs. You should integrate your AI development with your wider project phases. You shouldconsider AI ethics and safetythroughout all phases. Significant time is needed to understand the feasibility of using your data in a new way. This means the discovery phase tends to be longer and more expensive than for services without AI. Your data scientists may be familiar with a lifecycle calledCRISP-DMand may wish to integrate parts of it into your project. Discovery can help you understand the problem that needs to be solved. You should: To prepare for your AI project, you should assess your existing data. Training an AI system on error-strewn data can result in poor results due to: You can use a combination of accuracy, completeness, uniqueness, timeliness, validity, relevancy, representativeness, sufficiency or consistency to see if the data is high enough quality for an AI system to make predictions from. When assessing your AI data, it's useful to collaborate with someone who has deep knowledge of your data, such as adata scientist. They will be familiar with the best practice for measuring, cleaning and maintaining good data standards for ongoing projects.Make your data proportionate to user needsandunderstand the limitations of the datato help you assess your data readiness. Questions for you to consider with data scientists are: If you're unsure about your use of data, consult theData Ethics Framework guidanceto check your project is a safe application and deployment of AI. As with other projects, yourteam should be multidisciplinary, with a diverse combination of roles and skills to reduce bias and make sure your results are as accurate as possible. When working with AI you may need specialist roles such as a: You may not need all of these roles from the very beginning, but this may change as the work progresses. You may want to break up your discovery into smaller phases so you can evaluate what you are learning. It can be useful for your team to have: When preparing for AI implementation, you should identify how you can bestintegrate AI with your existing technology and services. It's useful to consider how you'll manage: When choosing your AI tools, you should bring in specialists, such as data scientists or technical architects to assess what tools you currently have to support AI. Use Cloud Firstwhen setting up your infrastructure. A data science platform is a type of software tool which helps teams connect all of the technology they require across their project workflow, speeding up AI deployment and increasing the transparency and oversight over AI models. When deciding on whether to use a data science platform, it's useful to consider how the platform can: After you've assessed your current data quality, you should prepare your data to make sure it is secure and unbiased. You may find it useful tocreate a data factsheetduring discovery to keep a record of your data quality. In the same way you should havediversity in your team, your data should also be diverse and reflective of the population you are trying to model. This will reduce conscious or unconscious bias. Alongside this, a lack of diverse input could mean certain groups are disadvantaged, as the AI model may not cater for a diverse set of needs. You should read the Data Ethics Framework guidance tounderstand the limitations of your dataand how to recognise any bias present. You should also: Make sure you design your system to keep data secure. To help keep data safe: As with any other software, you should design and build modular, loosely coupled systems which can be easily iterated and adapted. Writing and training algorithms can take a lot of time and computational power. In addition to ongoing cost, you'll need to think about the network and memory resources your team will need to train your model. Most of the data in government available to train our models is within legacy systems which might contain bias and might have poor controls around it. For legacy systems to be compatible with AI technology, you will often need to invest a lot of work tobring your legacy systems up to modern standards. You'll also need to carefully consider the ethical and legal implications of working with historic data and whether you need to seek permission to use this information. When you complete your data preparation phase you should have: During the discovery phase, you should explore the needs of the users of the end to end service. Like other digital services, you'll use this phase to determine whether there's a viable service you could build that would solve user needs, and that it's cost-effective to pursue the problem. You'll be able to check guidance on how toknow when your discovery is finishedbefore moving on to alpha. If you have decided to build your AI model in-house, you should follow these steps. Your team will need to train the models they build on data. Your team should split your data into a: Your team should build a simple baseline version model before they build any more complex models. This provides a benchmark that your team can later compare more complex models against, and will help your team identify problems in your data. Once you have a baseline model, your team can start prototyping more complex models. This is a highly iterative process, requiring substantial amounts of data, and will see your team probably build a number of AI models before deciding on the most effective andappropriate algorithmfor your problem. Keeping your team's first AI model simple and setting up the right end-to-end infrastructure will help smooth the transition from alpha to beta. You can action this by focusing on the infrastructure requirements for your AI pipelines at the same time as your team is developing your model. Your simple model will provide you with baseline metrics and information on the model's behaviour that you can use to test more complex models. Throughout the build, you shouldmake sure your AI model security complieswith advice from the NCSC. Your team will need to test your models throughout the process to mitigate against issues such asoverfitting or underfittingthat could undermine your model's effectiveness once deployed. Your team should only use the test set on your best model. Keep this data separate from your models until this final test. This test will provide you with the most accurate impression of how your model will perform once deployed. Your team will need to evaluate your model to assess how it is performing against unseen data. This will give you an indication of how your model will perform in the real world. The best evaluation metric will depend on the problem you are trying to solve, and your chosen model. While you should select the evaluation metric with data scientists, you should also consider the ethical, economical and societal implications. These considerations make the fine tuning of AI systems relevant to both data scientists and delivery leads. When choosing your final model, you will need to consider: Once you select a final model, your team will need to assess its performance, and refine it to make sure it performs as well as you need it to. When assessing your model's performance consider: If a model does not outperform human performance, it still may be useful. For example, a text classification algorithm might not be as accurate as a human when classifying documents, however they can perform at a far higher scale and speed than a human. When you complete building your AI prototyping phase, you should have: Moving from alpha tobetainvolves integrating the model into the service's decision-making process and using live data for the model to make predictions on. Using your model in your service has 3 stages. You should continue tocollect user needsso your team can use the model's outputs in the real world. When moving from alpha to beta, there are some best-practice guidelines to smooth the transition. After creating a beta version, you team can use automated testing to create some high-level tests before moving to more thorough testing. Working in this way means you can launch new improvements without worrying about functionality once deployed. During alpha, you will have relied mostly on data scientists to assess the opportunity and your data state. Moving to beta needs specialists with a strong knowledge of dev-ops, servers, networking, data stores, data management, data governance, containers, cloud infrastructure and security design. This skillset is likely to be better suited to an engineer rather than a data scientist so maintaining a cross-functional team will help smooth the transition from alpha to beta. When you complete your beta phase, you should have:",2023
govuk_009,govuk,Ai Ethics 9,"The Government Digital Service (GDS) Responsible AI Advisory Panel is seeking applicants for external panel members. TheGDSResponsibleAIAdvisory Panelwill advise and guide the Government Digital Service (GDS) on approaches to the development ofAIacross government. The duties of external panel members of the Government Digital Service (GDS) ResponsibleAIAdvisory Panel (the panel) involve, but are not limited to: External panel members should have: leading expertise in one or more of the following areas: Apply to be an external panel memberto theGDSResponsibleAIAdvisory Panel. The deadline is12pm on Monday 18 August 2025. The following links open in a new tab",2023
govuk_010,govuk,Ai Fairness 0,"Understand how to use artificial intelligence ethically and safely This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. The Office for Artificial Intelligence (OAI) and the Government Digital Service (GDS) have produced the following chapter's guidance in partnership with The Alan Turing Institute'spublic policy programme. This chapter is a summary ofThe Alan Turing Institute's detailed guidance, and readers should refer to the full guidance when implementing these recommendations. AI has the potential to make a substantial impact for individuals, communities, and society. To make sure the impact of your AI project is positive and does not unintentionally harm those affected by it, you and your team should make considerations of AI ethics and safety a high priority. This section introduces AI ethics and provides a high-level overview of the ethical building blocks needed for the responsible delivery of an AI project. The following guidance is designed to complement and supplement theData Ethics Framework. The Data Ethics Framework is a tool that should be used in any project. This guidance is for everyone involved in the design, production, and deployment of an AI project such as: Ethical considerations will arise at every stage of your AI project. You should use the expertise and active cooperation of all your team members to address them. AI ethics is a set of values, principles, and techniques that employ widely accepted standards to guide moral conduct in the development and use of AI systems. The field of AI ethics emerged from the need to address the individual and societal harms AI systems might cause. These harms rarely arise as a result of a deliberate choice - most AI developers do not want to build biased or discriminatory applications or applications which invade users' privacy. The main ways AI systems can cause involuntary harm are: The field of AI ethics mitigates these harms by providing project teams with the values, principles, and techniques needed to produce ethical, fair, and safe AI applications. The guidance summarised in this chapter and presented at length inThe Alan Turing Institute's further guidance on AI ethics and safetyis as comprehensive as possible. However, not all issues discussed will apply equally to each project using AI. An AI model which filters out spam emails, for example, will present fewer ethical challenges than one which identifies vulnerable children. You and your team should formulate governance procedures and protocols for each project using AI, following a careful evaluation of social and ethical impacts. You should establish ethical building blocks for the responsible delivery of your AI project. This involves building a culture of responsible innovation as well as a governance architecture to bring the values and principles of ethical, fair, and safe AI to life. To build and maintain a culture of responsibility you and your team should prioritise 4 goals as you design, develop, and deploy your AI project. In particular, you should make sure your AI project is: Prioritising these goals will help build a culture of responsible innovation. To make sure they are fully incorporated into your project you should establish a governance architecture consisting of a: You should understand the framework of ethical values which support, underwrite, and motivate the responsible design and use of AI. The Alan Turing Institute calls these 'the SUM Values': These values: You can read further guidance on SUM Values inThe Alan Turing Institute's comprehensive guidance on AI ethics and safety. While the SUM Values can help you consider the ethical permissibility of your AI project, they are not specifically catered to the particularities of designing, developing, and implementing an AI system. AI systems increasingly perform tasks previously done by humans. For example, AI systems can screen CVs as part of a recruitment process. However, unlike human recruiters, you cannot hold an AI system directly responsible or accountable for denying applicants a job. This lack of accountability of the AI system itself creates a need for a set of actionable principles tailored to the design and use of AI systems. The Alan Turing Institute calls these the 'FAST Track Principles': Carefully reviewing the FAST Track Principles helps you: If your AI system processes social or demographic data, you should design it to meet a minimum level of discriminatory non-harm. To do this you should: You should design your AI system to be fully answerable and auditable. To do this you should: The technical sustainability of these systems ultimately depends on their safety, including their accuracy, reliability, security, and robustness. You should make sure designers and users remain aware of: Designers and implementers of AI systems should be able to: To assess these criteria in depth,you should consult The Alan Turing Institute's guidance on AI ethics and safety. The final method to make sure you use AI ethically, fairly, and safely is building a process-based governance framework. The Alan Turing Institute calls it a 'PBG Framework'. Its primary purpose is to integrate the SUM Values and the FAST Track Principles across the implementation of AI within a service. Building a good PBG Framework for your AI project will provide your team with an overview of: You may find it useful toconsider further guidance on allocating responsibility and governance for AI projects.",2023
govuk_011,govuk,Ai Fairness 1,"Understand how to manage a project which uses artificial intelligence. This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. Once you have planned and prepared for your AI implementation, you will need to make sure you effectively manage risk and governance. This guidance is for people responsible for: The Alan Turing Institute (ATI) has written guidance onhow to use AI ethically and safely. Governance in safety is important to make sure the model shows no signs of bias or discrimination. You can consider whether: Governance in purpose makes sure the model is achieving its purpose/business objectives. You can consider whether: Governance in accountability provides a clear accountability framework for the model. You can consider: Governance in testing and monitoring makes sure a robust testing framework is in place. You can consider: Governance in public narrative protects against reputational risks arising from the application of the model. You can consider whether: Governance in quality assurance makes sure the code has been reviewed and validated. You can consider whether:",2023
govuk_012,govuk,Ai Fairness 2,"This page provides details about DSIT's portfolio of AI assurance techniques and how to use it. The portfolio ofAIassurance techniques has been developed by the Responsible Technology Adoption Unit, a directorate within DSIT, initially in collaboration withtechUK. The portfolio is useful for anybody involved in designing, developing, deploying or procuringAI-enabled systems, and showcases examples ofAIassurance techniques being used in the real-world to support the development of trustworthyAI. Search portfolio Please note the inclusion of a case study in the portfolio does not represent a government endorsement of the technique or the organisation, rather we are aiming to demonstrate the range of possible options that currently exist. To learn more about different tools and metrics forAIassurance please refer toOECD's catalogue of tools and metrics for trustworthyAI, a one-stop-shop for tools and metrics designed to helpAIactors develop fair and ethicalAI. We will be developing the portfolio over time, and publishing future iterations with new case studies. If you would like to submit case studies to the portfolio or would like further information please get in touch atai-assurance@dsit.gov.uk. Building and maintaining trust is crucial to realising the benefits ofAI. Organisations designing, developing, and deployingAIneed to be able to check that these systems are trustworthy, and communicate this clearly to their customers, service users, or wider society. AIassurance is about building confidence inAIsystems by measuring, evaluating and communicating whether anAIsystem meets relevant criteria such as: Assurance can also play an important role in identifying and managing the potential risks associated withAI. To assureAIsystems effectively we need a range of assurance techniques for assessing different types ofAIsystems, across a wide variety of contexts, against a range of relevant criteria. To learn more aboutAIassurance, please refer tothe roadmap to anAIassurance ecosystem,AIassurance guide,industry temperature check, and co-developedRTA (formerlyCDEI) and The Alan Turing Institute introduction toAIassurancee-learning module. The Portfolio ofAIassurance techniques was developed by the Responsible Technology Adoption Unit (RTA), in collaboration with techUK, to showcase examples ofAIassurance techniques being used in the real-world. It includes a variety of case studies from across multiple sectors and a range of technical, procedural and educational approaches , illustrating how a combination of different techniques can be used to promote responsibleAI. We have mapped these techniques to the principles set out in the UK government's white paper onAIregulation, to illustrate the potential role of these techniques in supporting widerAIgovernance. To learn more about different tools and metrics forAIassurance, please refer toOECD's catalogue of tools and metrics for trustworthyAI. The portfolio is a helpful resource for anyone involved in designing, developing, deploying or procuringAI-enabled systems. It will help you understand the benefits ofAIassurance for your organisation, if you're someone who is: The portfolio allows you to explore a range of examples ofAIassurance techniques applied across a variety of sectors. You can search for case studies based on multiple features you might be interested in, including the type of technique and the sector you work within. Each case study is also mapped against the most relevant cross-sector regulatory principles published in the government white paper onAIregulation. There are a range of different assurance techniques that can be used to measure, evaluate, and communicate the trustworthiness ofAIsystems. Some of these are listed below: Impact assessment:Used to anticipate the effect of a system on environmental, equality, human rights, data protection, or other outcomes. Impact evaluation:Similar to impact assessments, but are conducted after a system has been implemented in a retrospective manner. Bias audit:Assessing the inputs and outputs of algorithmic systems to determine if there is unfair bias in the input data, the outcome of a decision or classification made by the system. Compliance audit:A review of a company's adherence to internal policies and procedures, or external regulations or legal requirements. Specialised types of compliance audit include system and process audits and regulatory inspection. Certification:A process where an independent body attests that a product, service, organisation or individual has been tested against, and met, objective standards of quality or performance. Conformity assessment:Provides assurance that a product, service or system being supplied meets the expectations specified or claimed, prior to it entering the market. Conformity assessment includes activities such as testing, inspection and certification. Performance testing:Used to assess the performance of a system with respect to predetermined quantitative requirements or benchmarks. Formal verification:Establishes whether a system satisfies some requirements using the formal methods of mathematics. Check with assurance techniques can be used across each stage of theAIlifecycle. TheNationalAIStrategysets out an ambitious plan for how the UK can lead the world as anAIresearch and innovation powerhouse. EffectiveAIregulation is key to realising this vision to unlock the economic and societal benefits ofAIwhile also addressing the complex challenges it presents. In its recentAIregulation white paperthe UK government describes its pro-innovation, proportionate, and adaptable approach toAIregulation that supports responsible innovation across sectors. The white paper outlines five cross-cutting principles forAIregulation: safety, security and robustness; appropriate transparency and explainability; fairness; accountability and governance; and contestability and redress. Due to the unique challenges and opportunities raised byAIin particular contexts the UK will leverage the expertise of existing regulators, who are expected to interpret and implement the principles in their domain and outline what compliance with the principles looks like across different use cases. In addition, the white paper sets out the integral role of tools for trustworthyAI, such as assurance techniques and technical standards, to support the implementation of these regulatory principles in practice, boost international interoperability, and enable the development and deployment of responsibleAI. The RTA has conducted extensive research to investigate current uptake and adoption of tools for trustworthyAI, the findings of which are published in itsindustry temperature check. This report highlights industry appetite for more resources and repositories showcasing what assurance techniques exist, and how these can be applied in practice across different sectors. The UK government is already supporting the development and use of tools for trustworthyAI, through publishing aroadmap to an effectiveAIassurance ecosystemin the UK, having established theUKAIStandards Hubto champion the use of international standards, and now through the publication of the portfolio ofAIassurance techniques. TheAIStandards Hub is a joint initiative led by The Alan Turing Institute in partnership with the British Standards Institution (BSI), the National Physical Laboratory (NPL), and supported by government. The hub's mission is to advance trustworthy and responsibleAIwith a focus on the role that standards can play as governance tools and innovation mechanisms. TheAIStandards Hub aims to help stakeholders navigate and actively participate in internationalAIstandardisation efforts and champion the use of international standards forAI. Dedicated to knowledge sharing, community and capacity building, and strategic research, the hub seeks to bring together industry, government, regulators, consumers, civil society and academia with a view to: To learn more, visit theAIStandards Hubwebsite. The catalogue of tools and metrics for trustworthyAIis a one-stop-shop for tools and metrics designed to helpAIactors develop and useAIsystems that respect human rights and are fair, transparent, explainable, robust, secure and safe. The catalogue gives access to the latest tools and metrics in a user-friendly way but also to use cases that illustrate how those tools and metrics have been used in different contexts. Through the catalogue,AIpractitioners from all over the world can share and compare tools and metrics and build upon each other's efforts to implement trustworthyAI. The OECD catalogue features relevant UK initiatives and works in close collaboration with theAIStandards Hub, showcasing relevant international standards for trustworthyAI. The OECD catalogue will also feature the case studies included in this portfolio. To learn more, visitThe OECD catalogue of tools and metrics for trustworthyAI. Data assurance is a set of processes that increase confidence that data will meet a specific need, and that organisations collecting, accessing, using and sharing data are doing so in trustworthy ways. Data assurance is vital for organisations to build trust, manage risks and maximise opportunities. But how can organisations assess, build and demonstrate trustworthiness with data? Through its data assurance work, theODIis working with partners and collaborators to explore this important and rapidly developing area in managing global data infrastructure.ODIbelieve the adoption of data assurance practices, products and services will reassure organisations and individuals who want to share or reuse data, and support better data governance practices, fostering trust and sustainable behaviour change. To learn more, visit theODIwebsite.",2023
govuk_014,govuk,Ai Fairness 4,"Guidance to help you assess if artificial intelligence (AI) is the right technology for your challenge. This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. This guidance will help you assess if artificial intelligence (AI) is theright technology to help you meet user needs. As with all technology projects, you should make sure you can change your mind at a later stage and you can adapt the technology as your understanding of user needs changes. This guidance is relevant for anyone responsible for choosing technology in a public sector organisation. AI is just another tool to help deliver services. Designing any service starts withidentifying user needs. If you think AI may be an appropriate technology choice to help you meet user needs, you will need to consider your data and the specific technology you want to use. Yourdata scientistswill then use your data to build and train an AI model. When assessing if AI could help you meet users' needs, consider if: It's important to remember that AI is not an all-purpose solution. Unlike a human, AI cannot infer, and can only produce an output based on the data a team inputs to the model. When identifying whether AI is the right solution, it's important that you work with: For your AI model to work, it needs access to a large quantity of data. Work with specialists who have the knowledge of your data, such as data scientists, to assess your data state. You can assess whether your data is high enough quality for AI using a combination of: If your problem involves supporting an ongoing business decision process, you will need to plan to establish ongoing, up-to-date access to data. Remember tofollow data protection laws. There is no one 'AI technology'. Currently, widely-available AI technologies are mostly either supervised, unsupervised or reinforcement machine learning. The machine learning techniques that can provide you with the best insight depends on the problem you're trying to solve. There are certain types of problems for which machine learning is commonly used. For some of these you will be able to buy or adapt commercially available products. Because of its experimental and iterative nature, it can be difficult to specify the precise benefits which could come from an AI project. To explore this uncertainty and provide the right level of information around the potential benefits, you can: Once you have secured budget, you'll need to allow enough time and resources toconduct a substantial discoveryto show feasibility. Discovery for projects using AI can often takes longer for similar projects that do not use AI. If your organisation is a central government department, you may have toget approval from the Government Digital Service (GDS) to spend moneyon AI. At this point mostAI projects are classified as 'novel', which requires a high level of scrutiny. You should contact theGDS Standards Assuranceteam for help on the spend controls process. When assessing if AI could help you meet user needs, consider how you will procure the technology. You shoulddefine your purchasing strategyin the same way as you would for any other technology. Whether you build, buy or reuse (or combine these approaches) will depend on a number of considerations, including: It's also important toaddress ethical concernsabout the use of AI from the start of the procurement process. The Office for AI and the World Economic Forum are developing furtherguidance on AI procurement. Your team can build or adapt off-the-shelf AI models or open source algorithms in-house. When making this decision, you should work with data scientists to consider whether: You may be able to buy your AI technology as an off-the-shelf product. This is most suitable if you are looking for a common application of AI, for example optical character recognition. However, buying your AI technology may not always be suitable as the specifics of your data and needs could mean the supplier would have to build from scratch or significantly customise an existing model. Your AI solution will still need to be integrated into an end-to-end service for your users, even if you are able to buy significant components off the shelf. When using AI it's important to understand who is responsible if the system fails, as the problem may lie in a number of areas. For example, failures with the data chosen to train the model, design of the model, coding of the software, or deployment. You should establish a responsibility record which sets out who is responsible for different areas of the AI. It would be useful to consider whether: Depending on your organisation's maturity, it may be useful to set up a dedicated board, committee or forum to handle AI data and model governance. It can be useful to keep a central record of all AI technologies you use, listing:",2023
govuk_015,govuk,Ai Fairness 5,"The Sovereign AI Unit will ensure the government can harness AI's capabilities to unlock economic growth and enhance UK national security. Artificial Intelligence (AI) capabilities are developing at an extraordinary pace. In anAIenabled world, it matters who influences and builds the models, data andAIinfrastructure that are increasingly present in our lives. The Department for Science, Innovation and Technology (DSIT) has set up the SovereignAIUnit to build and harness the UK'sAIcapabilities to unlock economic growth and enhance UK national security. The Unit works closely with the Prime Minister's Adviser onAI. Announced in theAIOpportunities Action Planprepared by Matt Clifford, the SovereignAIUnit has an ambitious mandate to strengthen the UK's capabilities with up to PS500 million of funding. We will do this by: These partnerships are Memorandums of Understandings (MoU) which are non-legally binding, non-commercial agreements.",2023
govuk_016,govuk,Ai Fairness 6,"Representatives of both sectors in newly formed expert working groups on AI and copyright. The Technology and Culture Secretaries kickstart the next phase of work today (Wednesday 16 July) to help deliver a solution which will supportAIinnovation while ensuring robust protection for our creators and vibrant creative industries as part of the Plan for Change. A consultation on the UK's legal framework for copyright which explores how the government can deliver solutions supporting both the creative industries and theAIsector was launched in December last year, attracting 11,500 responses. Close collaboration on the issues raised across the debate has been central to the government's approach - ensuring both sectors not only have the support they need to drive further growth, but that the British public can share in the successes of 2 sectors which are crucial to the Modern Industrial Strategy. Representatives of both theAIsector and creative industries have engaged widely with Ministers throughout the consultation process, and the formal launch of new, expert working groups will continue to ensure both sectors play a vital role in supporting the work which will drive forward practical, workable solutions to foster innovation and growth. Representatives of the creative andAIsectors will now gather in London in the first of a series of regular planned meetings, with the groups made up of key industry figures. They include representatives of: Today's discussions mark the first in a series of planned talks, and will initially focus on the impacts, opportunities, and common ground in theAIand copyright debate, with their work then helping to inform next steps following the conclusion of the government's consultation. Secretary of State for Science, Innovation, and Technology, Peter Kyle said: I am determined to harness expert insights from across the debate as we work together to deliver a solution that brings the legal clarity our creative industries andAIsector badly need in the digital age. Today's meeting and the formation of these expert working groups will continue to ensure all voices can be heard so we can reset and refocus on how we can deliver precisely that. The work we'll be taking forward in the coming months will ensure we can work in partnership to deliver a fresh start for creatives andAIdevelopers alike. Culture Secretary Lisa Nandy said: Our world-class creative industries are a key part of our economy which create jobs and drive growth right across the country. These sectors have been recognised as a priority sector by the government and I am fully focused on supporting them to flourish. We have heard loud and clear the concerns from the creative industries aroundAIand copyright and these roundtables will give us another chance to consider the best way forward. We have committed to ensuring a copyright regime that values and protects human creativity, can be trusted and unlocks new opportunities for innovation across the creative sector and wider economy. Both sectors are a vital part of the government's modern Industrial Strategy, and theAIand Copyright consultation considered a broad range of issues in the copyright debate, including how right holders can have a better understanding of howAIdevelopers are using their material and how it has been obtained. The consultation also explored how access to high-quality data can be improved forAIdevelopers - bolstering their ability to innovate and drive the growth which underpins the government's Plan for Change. Today's talks will also contribute to finalising Terms of Reference for the expert working groups moving forward as they feed into wider discussions with both sectors. DSIT media enquiries Emailpress@dsit.gov.uk Monday to Friday, 8:30am to 6pm 020 7215 3000 The following links open in a new tab",2023
govuk_017,govuk,Ai Fairness 7,"Published 14 June 2023 (c) Crown copyright 2023 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visitnationalarchives.gov.uk/doc/open-government-licence/version/3or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:psi@nationalarchives.gov.uk. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned. This publication is available at https://www.gov.uk/government/publications/enabling-responsible-access-to-demographic-data-to-make-ai-systems-fairer/report-enabling-responsible-access-to-demographic-data-to-make-ai-systems-fairer The use of artificial intelligence (AI), and broader data-driven systems, is becoming increasingly commonplace across a variety of public and commercial services.[footnote 1]With this, therisks associated with biasin these systems have become a growing concern. Organisations deploying such technologies have both legal and ethical obligations to consider these risks. TheWhite PaperonAIRegulation, published in March 2023, reinforced the importance of addressing these risks by including fairness as one of five proposed key regulatory principles to guide and inform the responsible development and use ofAI. Many approaches to detecting and mitigating bias require access to demographic data. This includes characteristics that are protected under the Equality Act 2010, such as age, sex, and race, as well as other socioeconomic attributes.[footnote 2] However, many organisations building or deployingAIsystems struggle to access the demographic data they need. Organisations face a number of practical, ethical, and regulatory challenges when seeking to collect demographic data for bias monitoring themselves, and must ensure that collecting or using such data does not create new risks for the individuals that the data refers to. There is growing interest in the potential of novel approaches to overcome some of these challenges. These include techniques to generate synthetic training data that is more representative of the demographics of the overall population, as well as a variety of governance or technical interventions to enable more responsible data access. Access to demographic data to address bias is important for those working across theAIlifecycle, including organisations developing, deploying and regulatingAI. This report primarily explores approaches with the potential to assist service providers, i.e. those who are deploying data-driven systems (includingAI) to offer a service, to responsibly access data on the demographics of their users to assess for potential bias. This has led us to focus on two contrasting sets of promising data access solutions: data intermediaries and proxies. Of course, these approaches may have relevance to other parties. However, we have not considered in detail techniques such as synthetic generation of training data, which are specifically relevant to developers. Data intermediary is a broad term that covers a range of different activities and governance models for organisations that facilitate greater access to or sharing of data.[footnote 3]TheNational Data Strategyidentified data intermediaries as a promising area to enable greater use and sharing of data, andCDEIhas previously published areportexploring the opportunities they present. There is potential for various forms of data intermediary to help service providers collect, manage and/or use demographic data. Intermediaries could help organisations navigate regulatory complexity, better protect user autonomy and privacy, and improve user experience and data governance standards. However, the overall market for data intermediaries remains nascent, and to our knowledge there are currently no intermediaries offering this type of service in the UK. This gap may reflect the difficulties of being a first mover in this complex area, where demand is unclear and the risks around handling such data require careful management. If gathering demographic data is difficult, another option is to attempt to infer it from other proxy data already held. For example, an individual's forename gives some information about their gender, with the accuracy of the inference highly dependent on context, and the name in question. There are already some examples of service providers using proxies to detect bias in theirAIsystems.[footnote 4] Proxies have the potential to offer an approach to understanding bias where direct collection of demographic data is not feasible. In some circumstances, proxies can enable service providers to infer data that is the source of potential bias under investigation, which is particularly useful for bias detection.[footnote 5]Methods that draw inferences at higher levels of aggregation could enable bias analysis without requiring service providers to process individually-identifiable demographic data. However, significant care is needed. Using proxies does not avoid the need for compliance with data protection law. Inferred demographic data (and in some cases proxy data itself) will likely fall under personal or special categories of data under the UKGDPR. Use of proxies without due care can give rise to damaging inaccuracies and pose risks to service users' privacy and autonomy, and there are some cases in which the use of proxies is likely to be entirely inappropriate. Inferring demographic data for bias detection using proxies should therefore only be considered in certain circumstances, such as when bias can be more accurately identified using a proxy than information about an actual demographic characteristic, where inferences are drawn at a level of aggregation that means no individual is identifiable, or where no realistic better alternative exists. In addition, proxies should only be used with robust safeguards and risk mitigations in place. In the short term, direct collection of demographic data is likely to remain the best option for many service providers seeking to understand bias. It is worth emphasising that, in most circumstances, organisations are able to legally collect most types of demographic data for bias detection provided they take relevant steps to comply with data protection law. Where this is not feasible, use of proxies may be an appropriate alternative, but significant care is needed. However, there is an opportunity for an ecosystem to emerge that offers better options for the responsible collection and use of demographic data to improve the fairness ofAIsystems. In a period where algorithmic bias has been a major focus in academia and industry, approaches to data access have received relatively little attention, despite often being highlighted as a major constraint. This report aims to highlight some of the opportunities for responsible innovation in this area. This kind of ecosystem would be characterised by increased development and deployment of a variety of data access solutions that best meet the needs of service providers and service users, such as data intermediaries. This is one area thatCDEIis keen to explore further through the Fairness Innovation Challenge announced in parallel to this report. However, this is only a partial answer to the genuine challenges in this area. Ongoing efforts by others to develop a robust data assurance ecosystem, ensure regulatory clarity, support research and development, and amplify the voices of marginalised groups are also crucial to enable a better landscape for the responsible use of demographic data. Over the last year,CDEIhas been exploring the challenges around access to demographic datafor detecting and mitigating bias inAIsystems, and the potential of novel solutions to address these challenges. Organisations who useAIsystems should be seeking to ensure that the outcomes of these systems are fair. However, many techniques for detecting and mitigating bias inAIsystems rely on access to data about the demographic traits of service users, and many service providers struggle to access the data they need. Despite this, with a few notable exceptions, the topic has received relatively little attention.[footnote 6] In this report: This report has been informed by the work thatCDEIhas conducted over the last year, including: CDEIis grateful to those who contributed to these workshops, or otherwise contributed to this work. This report has been published alongside the announcement ofCDEI's Fairness Innovation Challenge. The challenge will provide an opportunity to test new ideas for addressingAIfairness challenges in collaboration with government and regulators. We hope that it will generate innovative approaches to addressing some of the data access challenges described here. Disclaimer: The information in this report is not intended to constitute legal advice. If you do require legal advice on any of the topics covered by this report, you should seek out independent legal advice. The use of data-driven systems, includingAI, is becoming increasingly commonplace across a variety of public and commercial services.[footnote 7]In the public sector,AIis being used for tasks ranging fromfraud detection to answering customer queries. Companies in the financial services, technology, and retail sectors also make use ofAIto understand customers' preferences and predict consumer behaviour. When service providers make use ofAIsystems in their services or decision-making processes, they can have direct and significant impacts on the lives of those who use these services. As this becomes increasingly commonplace, the risks associated with bias in these systems are becoming a growing concern. Bias inAIsystems can lead to unfair and potentially discriminatory outcomes for individuals. In 2020,CDEIpublished itsReview into Bias in Algorithmic Decision-Making, which explored this topic in detail. In March 2023, the government published theWhite PaperonAIregulation, which included fairness as one of five key proposed principles that might guide and inform the responsible development and use ofAI. The fairness principle states thatAIsystems should not undermine the legal rights of individuals or organisations, discriminate unfairly against individuals or create unfair market outcomes. The fairness principle considers issues of fairness in a wider sense than exclusively in terms of algorithmic bias, but addressing bias would be a key consideration in implementing it. In some circumstances, bias inAIsystems can lead to discriminatory outcomes. TheEquality Act 2010is the key UK legislation related to discrimination. It protects individuals from discrimination, victimisation and harassment and promotes a fair and more equal society. Age, race, disability, sex, gender reassignment, marriage and civil partnership, pregnancy and maternity, religion or belief and sexual orientation are all protected characteristics under the Equality Act 2010. WhereAIsystems produce unfair outcomes for individuals on the basis of these protected characteristics and are used in a context in scope of the act (e.g. the provision of a service), this might result in discrimination. Even when protected characteristics are not present in the training data,AIsystems still have the potential to discriminate indirectly by identifying patterns or combinations of features in the data, which enable them to infer these protected characteristics from other types of data. As noted inCDEI'sReview into Bias in Algorithmic Decision-Making, 'fairness through unawareness' is often not an effective approach. Service providers must address bias inAIsystems to ensure they are not acting unlawfully. Public sector service providers must also have due regard to advance equality of opportunity and eliminate discrimination under thePublic Sector Equality Duty(PSED). The Equality and Human Rights Commission (EHRC) has publishedguidancefor public bodies about how thePSEDapplies when they are usingAI, whichoutlinesthe need to monitor the impact ofAI-related policies and services. When processing personal data in relation toAI, service providers also have obligations relating tofairness under data protection law. The ICO has producedguidanceon how to operationalise the fairness principle in the context of developing and usingAI, as well as moretargeted guidancefor developers. Many approaches to detecting and mitigating bias inAIsystems require access to demographic data about service users. Demographic data refers to information about socioeconomic attributes. This includes characteristics that are protected under theEquality Act 2010, as well as other socioeconomic attributes such as socioeconomic status, geographic location, or other traits that might put people at risk of abuse, discrimination or disadvantage.[footnote 8] In some cases, service user demographic data might be compared to the datasets used to train a model in order to test whether the training data is representative of the population the model is being deployed on. In other cases, service user demographic data could be used to assess performance or make standardisations to identify where a model is treating individuals from different demographic groups differently. Access to good quality demographic data about a service's users is therefore often a prerequisite to detection, mitigation, and monitoring of bias, and an important first step in the fairness lifecycle. However,research byCDEIandothershas found that service providers currently face a range of legal, ethical and practical challenges in accessing the demographic data they need to effectively detect and mitigate bias in theirAIsystems. Routine collection of demographic data to improve the fairness ofAIis not common practice in either the public or private sectors, except in recruitment.[footnote 9]Without the ability to access demographic data about their users, service providers are severely limited in their ability to detect, mitigate, and monitor for bias, and thereby improve the fairness of theirAIsystems. Service providers are faced with a number of barriers when seeking to collect demographic data themselves. Concerns around public trust CDEI'sreview into bias in algorithmic decision-makingfound that some service providers think that the public do not want their data collected for the purpose of bias monitoring, and may be concerned why they are being asked for it. Evidence from public attitudes research thatCDEIhas conducted suggests that the public's willingness to share their data for bias monitoring varies depending on the organisation collecting it. Our2022 Tracker Surveyfound that 65% of the total respondents would be comfortable providing the government with demographic data about themselves in order to check if services are fair to different groups.Further researchwe conducted found that 77% of the public say they are not concerned with sharing their demographic data when applying for a job. The Tracker Surveyalso found that individuals were most reluctant to share their data with big technology and social media companies. Some companies havehighlighted this as a key challenge, suggesting that commercial organisations may need to provide additional safeguards to demonstrate their trustworthiness. Navigating regulatory compliance Most demographic data is also personal, and often special category, data under UK data protection legislation.[footnote 10]This data must be collected, processed and stored in a lawful, fair and transparent manner for specific, explicit and legitimate purposes only. Service providers must have a lawful basis and meet a separate condition for processing in order to process special category data underthe UKGDPR. In some circumstances, data controllers may be required to meet additional terms and safeguards set out inSchedule 1 of the Data Protection Act 2018. Schedule 1 includes a public interest condition around equality of opportunity or treatment, which is satisfied where processing certain kinds of special category data is ""necessary for the purposes of identifying or keeping under review the existence or absence of equality of opportunity or treatment between groups of people specified in relation to that category with a view to enabling such equality to be promoted or maintained"". This might provide a lawful basis for organisations to process special category data for bias detection and mitigation without requiring direct consent from individual data subjects.[footnote 11] Despite this, navigating the existing legal framework to process demographic data for bias detection and mitigation can be complex for service providers. Uncertainty around how equality and data protection law interact in this context can lead to misperceptions about what is or is not permitted under data protection law. TheCDEI'sReview into Bias in Algorithmic Decision-Makingfound that some service providers were concerned that collecting demographic data is not permitted at all under data protection law, or that it is difficult to justify collecting this data, and then storing and using it in an appropriate way. The ICO recently publishedguidanceto support service providers in navigating data protection law to address bias and discrimination inAIsystems. Data quality When used for bias detection and mitigation, inaccurate or misrepresentative data can be ineffective in identifying bias or even exacerbate existing biases, particularly when marginalised groups are poorly represented in the data. However,collecting good quality demographic data can be challenging in practice. Data collected directly from service users is likely to contain at least a degree of inaccuracy due to some users accidentally or intentionally misreporting their demographic traits. In addition, some users may choose to opt out of providing their data, leading to selection bias that results in a dataset that is not representative of service users. This selection bias may particularly impact individuals from groups who have experienced discrimination and marginalisation, who might be less comfortable sharing their data due to concerns about data privacy and misuse. Data collection expertise Collecting demographic data from service users requires establishing data collection procedures, and there is alack of clarity around how service providers should go about doing this. Setting up effective procedures that enable the collection of good quality data may require in-house expertise, which some service providers deployingAIsystems, particularly smaller organisations, may lack. Collecting and using demographic data for bias detection can also pose risks to the individual service users. Privacy Due to the sensitive and personal nature of demographic data, the collection and use of this data exposes individuals to risks of privacy violations. This is particularly problematic given that detecting and mitigating bias requires data on vulnerable and marginalised groups, who may be less comfortable sharing information on their demographic attributes given their disproportionate experiences of discrimination. This has beendescribed by someas a trade-off between 'group invisibility' and privacy. Misrepresentation When collecting demographic data, service providers have to decide which categories of data to collect and how this data will be disaggregated, and this can be challenging. Demographic categories are not static and tend to evolve over time with societal and cultural change. For example, the Race Disparity Unitrecently announcedthat the government would no longer use the demographic category 'BAME' (black, Asian, and minority ethnic), as it obscures meaningful differences in outcomes across ethnic groups. Ensuring that demographic categories remain up-to-date requires that service providers regularly update the data they collect to reflect such changes. In addition, when demographic categories are imposed on individuals, theyrisk misrepresentingthose who do not identify with them, further disempowering groups who are often already vulnerable and marginalised. There are also'unobserved' demographic characteristics, such as sexual orientation and gender identity, which can be fluid and are challenging to measure. Data theft or misuse The collection of demographic data by service providers increases the risk that this data is either stolen or intentionally misused. Cyberattacks by malicious actors could expose individuals to risks of information theft, which could be used for financial gain. Demographic data could also be intentionally misused by ill-intentioned actors for malicious purposes, such as identity theft, discrimination, or reputational damage. Concerns about data misuse may be particularly acute for individuals from demographic groups that have been historically marginalised or discriminated against. Due to the challenges that organisations face when collecting demographic data themselves, there is growing interest in novel approaches that could address these challenges and enable more widespread and responsible access to demographic data for bias detection and mitigation. The term 'access' is broad, and could involve: We have focused particularly on examples where a service provider is offering a service to users, and wants to understand how the outcomes of that service affect different groups. Though our interest in this area is driven by cases where a service or decision-making process is driven by data orAI, similar approaches to gathering data to monitor for potential bias are also relevant in other non data-driven contexts (e.g. monitoring the fairness of interview processes in recruitment). This has led us to a focus on two groups of potential approaches which seem applicable: data intermediaries and proxies. Data intermediaryis a broad term that covers a range of different activities and governance models for organisations that facilitate greater access to or sharing of data.[footnote 12]This can encompass a wide range of different stewardship activities and governance models. Data intermediaries can reduce risks and practical barriers for organisations looking to access data while promoting data subjects' rights and interests. Proxiesare inferences that are associated with and could be used in place of an actual demographic trait. For example, anindividual's postcode could be used as a proxy for their ethnicityor socio-economic status. Though the presence of proxies in algorithmic decision-making systems can be a source of bias, proxy methods could also be applied to infer the demographic characteristics of a service provider's users to enable bias monitoring. These two solutions offer contrasting approaches to the challenges surrounding data access, with differing opportunities and limitations. There has been significant interest in the concept of data intermediaries for some time, and there are a growing number of pilots and real-world examples of their use.[footnote 13]Despite this, data intermediaries have still not been widely adopted, nor used to enable access to demographic data for bias detection and mitigation in the UK. By contrast, proxies offer a relatively simple and implementable alternative to collecting demographic data, but careful consideration of legal and ethical issues is needed if they are to be used. By focusing on these two contrasting approaches, we will explore the range of possibilities in this space, capturing the scope of potential benefits and challenges that novel solutions have to offer. Some service providers, notably technology companies such as Meta andAirbnb, have started to experiment with these solutions in order to access demographic data to make theirAIsystems fairer. This experimentation with data intermediaries and proxies as a means of accessing demographic data demonstrates that they are perceived to be promising solutions to address the challenges surrounding data access. However, this also demonstrates an urgent need to better understand their potential and limitations. Data intermediary is a broad term that covers a range of different activities and governance models for organisations that facilitate greater access to or sharing of data.[footnote 14]Data intermediaries can perform a range of different administrative functions, including providing legal and quality assurances, managing transfer and usage rights, negotiating sharing arrangements between parties looking to share, access or pool data, and empowering individuals to have greater control over their data. Intermediaries can also provide the technical infrastructure and expertise to support interoperability and data portability, or provide independent analytical services, potentially using privacy-enhancing technologies (PETs). This range of administrative and technical functions is explored in detail inCDEI's 2021report exploring the role of data intermediaries. In simple terms, for the purposes of this report, a (demographic) data intermediary can be understood as an entity that facilitates the sharing of demographic data between those who wish to make their own demographic data available and those who are seeking to access and use demographic data they do not have. Some data intermediaries that collect and share sensitive data for research already operate at scale in the UK. One example isGenomics England, a data custodian that collects sensitive data on the human genome, stores it in a trusted research environment, and grants researchers access to anonymised data for specific research projects. Another prominent example is the Office for National Statistics'Secure Research Service, which provides accredited researchers with secure access to de-identified, unpublished data to work on research projects for the public good. As opposed to facilitating data sharing between service users and providers, these intermediaries provide researchers with access to population-level demographic datasets. Outside the UK, there are also limited examples of intermediaries being used to steward demographic data specifically for bias auditing. The USNational Institute of Standards and Technology (NIST)provides a trustworthy infrastructure for sharing demographic information (including photographs and personally identifying metadata on subjects' age, sex, race, and country of birth) for the purposes of testing the performance of facial recognition algorithms. By providing researchers with access to sensitive personal and demographic data that enables them to quality-assure algorithms for fairness, NIST has significantly expanded the evidence base on algorithmic bias and helped developers improve the performance of their facial recognition algorithms. Here we are focused on a different but related set of use cases. Can intermediaries help service providers to access demographic data about the individuals that interact with their service so that they can understand potential biases and differential impacts? Data intermediaries could play a variety of roles. Here, we primarily consider how they could support the collection, storage, and sharing of demographic data with service providers, though a third-party organisation could also take on an auditing role in certain circumstances. Intermediary models have emerged in other domains where large numbers of service providers have a need to provide common functions, and doing so in a consistent way is beneficial to consumer trust, user experience and/or regulatory compliance. Examples include: Some of the challenges that these intermediaries address are similar in nature to those above, so it is natural to ask the question of whether a similar model could emerge to address the challenges of demographic data access. Intermediaries could offer a number of potential benefits over direct collection of data by service providers, including: Various different types of organisations could act as demographic data intermediaries. For example: The type of organisation acting as an intermediary might have some implications for the type of demographic data intermediary service that is offered; given the sensitivity of the data concerned and the variety of different needs, an ecosystem where multiple options are available to service providers and users seems more desirable than a single intermediary service holding large amounts of data and attempting to meet all needs. There are a variety of different models for the role that intermediaries could play in supporting access to demographic data. To describe these potential roles, we have used the following terms: There are then two different roles specifically related to demographic data which often do not exist today: In the different data intermediary models described below, these roles might or might not be played by the same organisation. Potential model 1 One model could be for a data intermediary to collect and steward demographic data on behalf of a service provider, sharing this with them so they can audit their model for bias. This could potentially operate as follows: Diagram depicting indicative relationships and data flows for a data intermediary collecting and managing data on behalf of a service provider. There are examples of somewhat analogous models being used to enable safe research access to population-level datasets including some demographic data, for example theONS Secure Research ServiceorOpenSAFELY. We are not aware of any similar examples targeted at users of individual services, but it is feasible that a suitably trusted organisation could provide a similar kind of service collecting user or customer demographic data, and sharing this with service providers. Potential model 2 Beyond collection and management of demographic data, there is a growingecosystem ofAIassurance service providersseeking to provide independent bias audits using such data. A potential variant of the intermediary model described above is for such a bias audit provider to also act as an intermediary collecting the data. In this model, the intermediary acts as custodian of users' demographic data, collecting and storing it in a secure environment (as in model 1), but then also auditing service providers' models without ever giving them access to the data itself. This provides an additional layer of separation between the service provider and the demographic data about their customers as compared to the previous model, i.e. they receive only results of an audit, not any demographic data itself. For this model to be feasible, a service provider will typically need to provide additional internal service-related data to the bias audit provider, and therefore is likely to require appropriate legal and technical protection for personal data and intellectual property contained within it. Diagram depicting indicative relationships and data flows for a data. We have identified a couple of real world examples of similar approaches to this in the context of fairness: Potential model 3 As an alternative variant of model 1, there are a variety of intermediary models that seek to give users stronger personal control over how their data is used; variously referred to as personal data stores (PDSs), personal information management systems (PIMS) or data wallets. Each of these offers a form of decentralised store of an individual's personal data controlled by that individual. There are several such platforms already in existence, including commercial companies such asWorld Data Exchange, community interest companies likeMyDex, or cooperatives likeMiData, or open specifications such asSOLID. In this context, such platforms could allow individual data subjects to manage and maintain their own demographic data and share it with service providers for bias audit and mitigation on their own terms. Diagram depicting a service user collecting and managing their own demographic data in a personal data store, and sharing it with a service provider. Common features These contrasting models demonstrate the variety and breadth of data intermediaries that could support access to demographic data for bias detection and mitigation. They are not exhaustive or mutually exclusive, and their features could be changed or adapted. It is unlikely that one solution will suit every sector and group of stakeholders, and an ecosystem offering a combination of different demographic data intermediary types could be the most efficient and effective way to support the responsible use of demographic data for bias monitoring. There are additional legal mechanisms and technical interventions that could be integrated into any of these models to provide additional protections for service users who share their data. Novel data governance mechanisms could provide service users with more autonomy over how their demographic data is used. These includedata trusts(mechanisms for individuals to pool their data rights into a trust in which trustees make decisions about data use on their behalf) anddata cooperatives, in which individuals can voluntarily pool their data and repurpose it in their interests. While such mechanisms have been proposed by academics for some time, there have recently been a number of schemes to pilot them in real-world settings. These include the Data Trusts Initiative'spilot projects, the ODI'sdata trusts pilots, and the Liverpool City Region'sCivic Data Cooperative. Pilots like these indicate a shift towards the development and use of novel data governance mechanisms in practice. Data intermediaries could also integrate the use of technical interventions like PETs to provide stronger privacy and security protections. Large technology companies likeAirbnband Meta have experimented with the use of third parties to access demographic data using privacy-preserving techniques, including secure multi-party computation and p-sensitive k-anonymity, to better protect the privacy of their users. Despite offering a range of potential benefits, such an ecosystem of data intermediaries has not yet emerged. To the best of our knowledge, there are currently no intermediaries providing services specifically designed to support service providers to access demographic data from their users to improve the fairness of theirAIsystems in the UK. Our work suggests that the potential of data intermediaries to enable access to demographic data is constrained by a range of barriers and risks. The absence of organisations offering this type of service suggests that there is not sufficient incentive for such data intermediaries to exist. Incentives might be commercial (i.e. confidence that offering such a service would be a viable commercial proposition), but might also be broader, for example an opportunity for a third sector organisation to support fairness. What drives this absence? Demand among service providers and users for third-party organisations sharing demographic data is unclear. Given the relative immaturity of the market for data intermediaries, there may be a lack of awareness about their potential to enable responsible access to demographic data. In addition, the incentives driving data sharing to monitorAIsystems for bias are primarily legal and ethical as opposed to commercial, meaning demand for demographic data intermediation services relies on service providers' motivation to assess their systems for bias, and service users' willingness to provide their data for this purpose. More broadly, the market for many kinds of data intermediary is still relatively nascent. In the EU, the 2022Data Governance Actintroduced new regulations for the 'providers of data intermediation services', requiring them to demonstrate their compliance with conditions placed on their economic activities. The UK government acknowledged in theNational Data Strategy Mission 1 Policy Frameworkthat there is currently no established market framework for the operation of data intermediaries in the UK and has committed to support the development of a thriving data intermediary ecosystem that enables responsible data sharing. The lack of commercial incentives for data intermediaries sharing demographic data, combined with the challenges of operating in this complex area, has created little impetus for first movers. In addition, in order to use their services to share sensitive data, service providers and users must have confidence that data intermediaries are trustworthy. Ourpublic attitudes researchindicates that one of the most common concerns members of the public have around data intermediaries is that third parties are not sufficiently trustworthy. Non-regulatory approaches, such as data assurance, could help to build confidence in data intermediaries and demonstrate their trustworthiness. TheODIdefines data assurance as ""the process, or set of processes, that increase confidence that data will meet a specific need, and that organisations collecting, accessing, using and sharing data are doing so in trustworthy ways"".Research by Frontier Economicssuggests the data assurance sector in the UK is nascent but growing, with approximately 900 firms currently offering a range of different data assurance products and services in the UK. Standards could provide one way to encourage consistent data governance and management across demographic data intermediaries. This could include adoption of mature and commonplace standards such asISO/IEC 27001, as well as other relevant data standards.[footnote 15]In addition, a number of relevant certification and accreditation schemes already exist, such asCoreTrustSeal,FairDataand theMyData Global Operator Award. These could help data intermediaries sharing demographic data to demonstrate their adherence to data protection and ethical standards. Despite the burgeoning ecosystem for data assurance in the UK, work to understand how assurance products and services could demonstrate the trustworthiness and support the uptake of data intermediaries is in its early stages. No one standard-setting body or certification scheme can cover all the areas required to effectively assure data intermediaries and, given their diversity, a one-size-fits all approach is unlikely to be appropriate. For this reason, a greater understanding of how existing assurance products and services can demonstrate the trustworthiness of data intermediaries, how well these can meet the needs of different stakeholders, and where there may be remaining gaps in the data assurance ecosystem is required. This could support third parties sharing demographic data to demonstrate their trustworthiness, and encourage uptake among service providers and service users. Of course, intermediaries gathering sensitive data of this nature must contend with many of the same challenges that a service provider would have managing the same data. Where data intermediaries are collecting and storing sensitive demographic information about service users, they still need to take steps to minimise the risk that personal data is stolen or intentionally misused; ourpublic attitudes researchfound that data theft and misuse was a common concern among respondents in relation to data intermediaries. In addition, any third party collecting demographic data must ensure the data they collect is good quality. Much like service providers seeking to collect this data themselves, third parties must contend with similar challenges around data accuracy and representativeness. Data intermediaries hold real promise as a means to enable responsible access to demographic data for bias detection and mitigation. They could promote the collection and use of demographic data in ways that supports the regulatory compliance of service providers, protects service user privacy and autonomy, and elicits public trust, while providing better user experience and higher standards than service providers collecting this data themselves. Despite this potential, a market for such services is yet to emerge. We have discussed some of the barriers to this above, but for a service provider that could benefit from this approach, the absence of third parties offering such services in the UK prevents this being a straightforward option at present. Longer term, there remains clear potential for intermediaries to play a useful role. There is a need for piloting potential solutions in this area to support the development of the market for data intermediaries, and demonstrate the opportunities to service providers and users that might use them. This is one area thatCDEIhopes to explore further in the Fairness Innovation Challenge announced in parallel to this report. Many organisations hold a range of data about individuals that they provide services to. However, for the reasons discussed above, they often do not hold some or all of the demographic data that they need to audit their own systems and processes for bias. In contexts where collecting this demographic data directly is hard, an alternative is to infer it from data that you already hold, using proxies for the demographic traits that you are interested in. Proxies can be a source of discrimination inAIsystems where algorithms are able to deduce protected characteristics from relevant data points. For example, most insurance pricing models include postcode as a factor for a variety of valid reasons. However, the mix of ethnic groups varies significantly between different postcode areas, and there is a risk that insurance pricing models indirectly treat individuals from certain ethnicities differently via the proxy of their postcode. This is one reason why monitoring for potential bias is increasingly important asAIsystems become more complex and sophisticated. Conversely, there is potential for proxies to be used to detect and address bias inAIsystems. Using data they already hold as proxies, service providers could infer the demographic traits of their service users, and use this data to detect bias in theirAIsystems. Examples of this include: The inferences you might make about demographic traits from such proxy data will inevitably not be fully accurate, and whether this accuracy is enough to be practically useful for bias monitoring will be dependent on both the proxy data that is available, and the use case. Proxies raise a number of challenging ethical and legal concerns. These are discussed in more detail below. There are a wide range of proxy methods and tools in existence, varying from relatively simple methods to more complex machine learning approaches. These can be used to infer different demographic attributes, although ethnicity and gender have been the most common target variables to date. Many proxy methods and tools involve inferring the demographic traits of identifiable individuals (see Example 2 below). However, some approaches avoid this by using personas (see Example 1 below) or by drawing group inferences in such a way that ensures individuals are not identifiable.[footnote 16] Many of the most popular proxy methods and tools have been developed in the US, although some have been trained on large datasets spanning multiple geographies. These methods and tools vary in their accessibility to service providers, with some available open source and other commercial tools requiring payment for access. Some of these methods and tools were developed specifically to assess bias and discrimination, such as RAND'sBayesian Improved Surname Geocoding(BISG). In recent years, a few prominent technology companies including Meta andAirbnbhave begun to pilot more advanced, privacy-preserving proxy methods with the explicit aim of generating demographic data to improve the fairness of theirAIsystems. The examples below provide three contrasting approaches to using proxies, demonstrating the breadth of possibilities for their use to enable bias monitoring. Example 1: Citizens Advice using name and postcode to infer ethnicity In 2022, Citizens Advice conductedexploratory researchto better understand whether people from ethnic minority backgrounds experience worse outcomes in the car insurance market than white consumers. To measure this, they conducted mystery shopping using 649 personas that varied by name and postcode, comparing the prices paid by shoppers with names that are common among people from different ethnic backgrounds and postcodes with different proportions of ethnic minority communities in the population. They found no significant difference in prices charged to people with different names in the same postcode area. However, average quotes were higher in areas where black or South Asian people make up a large proportion of the population, and this could not be explained by common risk factors such as crime rates, road accidents or levels of deprivation in the area. By using personas, Citizens Advice was able to assess a service for bias without requiring access to the personal data of service users. Although their methodology allowed them to test the outcomes of pricing mechanisms, Citizens Advice acknowledge that it cannot explain exactly why the outcomes they identified occurred. Example 2: Airbnb's Project Lighthouse using first name and photos of faces to infer perceived race Airbnb's Anti-Discrimination product team havedeveloped a privacy-by-design approachto infer the perceived race of their service users using their first name and an image of their face. By measuring inequities on the basis of perceived race, they aimed to account for the fact that discrimination often occurs because of people's perceptions of one another's race as opposed to their actual race. The team sent a k-anonymized version of this service user data to a research partner organisation, who was under a confidentiality agreement and had their systems reviewed by Airbnb security. The research partner assigned perceived race to service users and this data was returned to Airbnb, who perturbed the data to achieve a level of p-sensitivity (i.e. ensuring that each equivalence class in the dataset had at least p distinct values for a sensitive attribute) before storing it. Finally, this p-sensitised k-anonymised dataset was used to measure the acceptance rate gap between different perceived racial groups. By including a research partner and making careful use of privacy techniques, Airbnb's approach enables them to analyse whether hosts exhibit bias on the basis of perceived race while protecting the privacy of service users. Example 3: NamSor using first name and surname to infer gender and ethnicity NamSoris a commercial product which uses machine learning to infer ethnicity and gender from first names and surnames. Namsor SAS, the company who owns the product, suggests it can be used to measure gender or ethnic biases inAI-driven processes, and theyoffer a range of toolsto suit different customers, including API documentation, CSV and Excel files analysis, and developer tools. NamSor has processed over 7.5 billion names and is continually maintained with new training data.The company claimsit is the most accurate ethnicity and gender inference service in the world. Oneindependent, comparative studysupports this claim, suggesting the tool achieves an F1 score of 97.9%. There are a range of reasons why a service provider or technology developer might be motivated to use proxies rather than collect data directly. Despite some potential benefits, the use of proxies presents a number of legal and ethical risks, and practical challenges. Legal risk Most demographic data inferred through the use of proxies is likely to be classified as personal or special category data under the UKGDPR, and must be processed in accordance with data protection legislation. The ICO'sguidance around the legal status of inferred datastates that whether an inference counts as personal data or not depends on whether it relates to an identified or identifiable individual. In addition, it may also be possible to infer or guess details about someone which fall withinspecial categories of data.[footnote 17]Whether or not this counts as special category data will depend on the specific circumstances of how the inference is drawn. Given that the use of proxies to generate demographic data for bias detection involves the intentional inference of relevant information about an individual, proxy methods will likely involve the processing of special category data, regardless of whether these inferences are correct or not. Where proxies are used to infer demographic traits at a higher level of aggregation, such that inferences are drawn only about so-called 'affinity groups' and not specific individuals, theICO states thatthese inferences may also count as personal data depending on how easy it is to identify an individual through group membership. When using proxy methods to draw group inferences, service providers should still comply with the data protection principles, including fairness. The use of proxies may pose additional legal risks for service providers where they are unaware of their legal obligations with respect to inferences or find them difficult to interpret and apply in practice. Accuracy Proxies can generate inaccurate inferences which can obscure or even exacerbate bias inAIsystems when used for bias detection and mitigation. Ourpublic attitudes researchsuggests the accuracy of proxy methods is a key concern for members of the public. There are a number of distinct issues related to the accuracy of proxies. Privacy The use of individual-level proxies may interfere with service users' privacy as they reveal personal information about them. Privacy was a key concern about proxies among participants inour public attitudes study. The inference of some demographic traits may not interfere with privacy much, if at all. However, information relating to more sensitive demographic categories, which form part of the individual's private life, could seriously impede on the privacy of service users. This is supported by evidence from thepublic attitudes study, which found that members of the public are more comfortable with organisations inferring their age than they are other demographic traits, such as disability status or sexuality. The sensitivity of demographic traits may also be compounded by other contextual factors, like the individuals' attributes (e.g. if they are a child or otherwise vulnerable) or their circumstances (e.g. if they live in a homophobic environment). Transparency and user autonomy The use of proxies to infer demographic data is inherently less visible to service users than collecting demographic data directly from them. The low visibility of proxy use raises concerns around transparency and service users' autonomy. When processing personal or special category data for bias monitoring, service providers haveobligations related to transparencyunder the UKGDPR. The ICO has providedguidanceon the right to be informed, which is a key transparency requirement under the UKGDPR. Public trust Proxies are a controversial topic, and the public appear to be less comfortable with their use than with providing their data to a third party.Our public attitudes studyindicated that only 36% of respondents were fairly comfortable with the use of proxies, and 23% were uncomfortable. Levels of public comfort varied depending on the type of proxy, the target demographic trait, and the type of organisation using the proxies. Members of the public were particularly concerned about their privacy, the accuracy of the inferences, and the risks of data misuse. Accessibility The use of proxy methods relies on access to relevant proxy data. The type of proxy required will vary depending on the target variable but could include service user postcodes, names, social media data, or facial photographs. Some of this data may already be held by service providers but some may not. The accessibility of proxy data will place limitations on the applicability of different proxy methods. Data quality When used for bias detection, poor quality data can be ineffective in detecting biases or even introduce new ones, particularly when marginalised groups are poorly represented in the data. The ability to draw inferences that are useful for bias detection purposes therefore relies on access to good quality proxy data. Where service providers do hold data that could be used to infer demographic traits of interest, this data may be incomplete or inaccurate. Where poor quality proxy data is used to infer demographic information about service users, it will produce poor quality inferences. This raises related concerns aroundcompliance with the accuracy principleunder data protection law, which applies to input as well as output data. Proxies offer an alternative approach to accessing demographic data for bias detection and mitigation. Proxies can be a practical approach to bias detection for service providers who already hold relevant data, and can prevent the need for service users to provide their demographic data numerous times to different organisations. In some circumstances, proxies may be the best way for service providers to effectively analyse theirAIsystems for bias, particularly where the proxy is more helpful in identifying bias than the demographic trait itself. Methods that rely on personas or group inferences at a level of aggregation such that individuals are not identifiable may pose few privacy risks to individual service users. Despite this, the use of proxies poses a number of legal and ethical risks, as well as practical challenges. There are some cases in which the use of proxies is likely to be entirely inappropriate and should be avoided. Other methods, although not illegal, will likely involve the processing of special category data, which may entail legal risk for service providers. In addition, proxies can give rise to damaging inaccuracies and pose challenges to the privacy and autonomy of service users, and members of the public appear to be less comfortable with their use than other data access solutions. Proxies are therefore likely to be a viable solution to enable access to demographic data for bias detectiononly in certain circumstances, such as when bias can be more accurately identified using a proxy than information about an actual demographic characteristic, or where inferences are drawn at a level of aggregation that means no individual is identifiable. In addition, proxies shouldonly be used with robust safeguards and risk mitigations in place. Here, we set out the key ethical issues for service providers to consider when seeking to use proxies for bias detection and mitigation. Alongside these ethical considerations, service providers using proxies should consider their legal obligations by referring to the ICO's Guidance onAIand Data Protection, includingAnnex A 'Fairness in theAILifecycle'. Step 1: Establish a strong use case for the use of proxies as opposed to other alternatives This is central to ensuring the ethics of using proxy methods, and helps service providers to exclude the use of proxies where a reasonable, less intrusive alternative exists. There are certain demographic traits for which the use of proxies is not advisable. In particular, where service providers wish to test the system for bias relating to demographic traits that are unobservable, such as sexual orientation, they should seek an alternative approach. However, there are a limited number of scenarios in which the use of proxies to address bias may be justifiable. These include: The strength of these justifications should be weighed up in light of the risk that theAIsystem in question is biased, and the severity of the real-world impact of this bias. To make this assessment, knowledge of the context in which theAIsystem is being deployed is critical, and service providers should engage with civil society organisations and affected groups in determining whether using proxies is appropriate in any given use case. Service providers should also refer to the ICO'sGuidance on Data Protection andAIat this stage to establish that their proposed use of proxies is lawful. Step 2: Select an appropriate method and assess associated risks If a strong case for the use of proxies as opposed to other alternatives has been established, service providers need to select an appropriate proxy method and assess the risks and trade-offs associated with its use. There are a number of commercial tools and open source methods available to service providers. A non-exhaustive list of some methods that are applicable in the UK context can be found in thetechnical report by Frazer Nash. When selecting a method, service providers should consider: Testing the performance of proxy methods by conducting an independent review using a representative dataset to determine which may be most appropriate to use. Looking at historic data and current social and cultural trends to make predictions about likely model drift, and consider its implications for the need for model retraining or continuous learning. Alongside these considerations, service providers need to assess the feasibility of using the method or tool, including factors such as the availability and cost of the method or tool, the availability and quality of proxy data, and available resources and expertise within the organisation. Service providers should also consider conducting a risk assessment to assess the risks and trade-offs associated with the use of this method in the specific context they intend to use it in. They should also carefully consider the limitations of the method or approach they have chosen, and whether there are further actions they can take to overcome these limitations. Step 3: Design and develop robust safeguards and risk mitigations If an appropriate method is chosen and the risks and limitations of this method have been identified, service providers should consider the development of risk mitigations and safeguards, including: Measures to ensure model accuracy, such as regular monitoring of model performance and retraining or revalidation of the model at appropriate intervals. No set of safeguards will entirely eliminate the risks associated with the use of sensitive data and there will always be a degree of residual risk. Service providers should consider and document what that residual risk might look like, and whether it is proportionate compared to the established benefits of using the proxy method. This assessment would again benefit from engagement with civil society and affected groups. If residual risks are deemed acceptable given those benefits, the last step is to implement safeguards and proceed with the use of proxies. Otherwise, service providers may need to consider whether further safeguards might be required, or whether the use of proxies is justifiable at all. Residual risk should also be reviewed on an ongoing basis to ensure new risks associated with changes in context are captured and mitigated. The current landscape of options for accessing demographic data is not ideal, and has significant limitations. Organisations are required to navigate significant legal, ethical, and practical challenges to either collect demographic data or infer it via the use of proxies.Evidence suggeststhat members of the public are likely to feel more comfortable sharing their data when governance mechanisms offer them greater privacy and control over their demographic data, particularly in sectors where levels of public trust in data sharing are lower. In this section, we reflect on what needs to happen to improve this ecosystem, and make it easier for organisations to responsibly use demographic data to address bias. This requires the development and scaling up of ambitious data access solutions that best mitigate ethical risks, are most practical for service providers and users, and are trusted by members of the public. Data intermediaries are one promising area for further development, as well as complimentary governance mechanisms like data trusts and technical interventions such as privacy-enhancing technologies. As government, we have a key role to play in spurring responsible innovation in this area, and a variety of work is underway to support this. Firstly, although demographic data can already be legally processed by service providers for bias detection and mitigation, some organisations may find that the existing data protection framework is complex and difficult to navigate in this area. In September 2021, the government launched aconsultationon reforms to the UK's data protection laws, including seeking views on provisions relating to processing personal data for bias detection and mitigation.Respondents agreedthere should be additional legal clarity on how sensitive data can be lawfully processed for bias detection and correction, and some felt that introducing a new processing condition under Schedule 1 of the Data Protection Act 2018 would be beneficial. As outlined in thegovernment response, the government is introducing a statutory instrument to enable the processing of sensitive personal data for the purpose of monitoring and correcting bias inAIsystems, with appropriate safeguards. This measure fits in the wider approach the government is developing around this issue, as proposed in the White Paper onAIregulation, currently out for consultation. Regulators have already published relevantdata protectionandequalitiesguidance related toAI, as well as some data access solutions, includingproxiesandprivacy-enhancing technologies. However, greater clarity around service providers' equality obligations with respect to detecting and mitigating bias in theirAIsystems would be welcome, and could further incentivise service providers to take action to improve the fairness of their systems. Continued regulatory collaboration between the ICO,EHRCand relevant sectoral regulators will also be critical moving forward to ensure the responsible collection and use of demographic data to improve the fairness ofAIsystems, particularly where novel solutions to generate and share this data are being tested. The government also has an important role to play in incentivising innovation and supporting the development and scaling up of promising solutions to enable responsible access to demographic data.As announced alongside this report, theCDEIplans to run a Fairness Innovation Challenge to support the development of novel solutions to address bias and discrimination across theAIlifecycle. The challenge aims to provide greater clarity about which data access solutions andAIassurance tools and techniques can be applied to address and improve fairness inAIsystems, and encourage the development of holistic approaches to bias detection and mitigation, that move beyond purely technical notions of fairness. In theNational Data Strategy, the government also committed to support the development of a thriving data intermediary ecosystem by considering the role of competition, horizontal governance structures, and strategic investment in intermediary markets. The ongoing work in this area could serve to support the emergence of intermediaries that are able to play a useful role in this area. One specific area of focus for this work relevant here is support for the development of a data assurance ecosystem to ensure that new data access solutions, particularly data intermediaries, are trustworthy. There is a burgeoning ecosystem for data assurance in the UK but work to understand how such services could demonstrate the trustworthiness and support the uptake of data intermediaries is in its early stages. The ODI has publishedresearchexploring the data assurance landscapein support of the government's National Data Strategy. Further research could explore the extent to which the existing data assurance market can engender confidence in new data access solutions and meet the needs of different stakeholders, and identify potential gaps in the ecosystem. As discussed above, service providers should already be taking action to identify and address bias inAIsystems that they deploy. Those seeking to collect demographic data themselves should refer to guidance from the ICO around processing ofpersonal data, includingspecial category data, to ensure their collection and use of demographic data is legally compliant. In addition, the ONS has issuedguidancearound some demographic categories that service providers could use when seeking to collect data for the purposes of measuring equality. Service providers should give consideration to the ways in which they can mitigate the risks associated with demographic data collection, for example, by using more participatory and inclusive approaches to data collection. In some cases, proxies may be a more suitable alternative to collecting demographic data themselves. Service providers should refer to the key ethical considerations in the previous section of this report, as well as the ICO'sGuidance onAIand Data Protectionand other sector-specific guidance, to determine whether such approaches are appropriate and, if so, how they could be used responsibly. Given the growing imperative on many service providers to access demographic data, they should demand solutions from the market that better meet their needs, and the needs of their users, by embedding ethical best practice, supporting them to navigate regulation, and providing more practical services. Novel data governance approaches, such as data intermediaries, alongside complementary governance and technical interventions, could help to meet service providers' needs, and demand for these solutions could stimulate innovation in these areas. There is also an important role for the research community in providing continued research into and piloting of solutions to enable responsible demographic data access. More comparative studies of proxy methods using the same test datasets and performance criteria, particularly filtered or weighted accuracy scores, could help service providers to make better informed decisions as to whether such methods are sufficiently accurate for different demographics and acceptable for use to make assessments about fairness.Data quality is also a persistent challenge whether service providers collect demographic data themselves or access it using a generative method or through a third party. Further research into and piloting of novel approaches to improve data quality, such as participatory approaches to data collection, would be beneficial. Finally, civil society groups have a key role to play in informing and mobilising members of the public and ensuring that solutions and services to enable responsible access to demographic data protect their rights and interests. Demographic data is of vital importance in detecting and correcting bias inAIsystems, yet the collection and use of this data poses risks to individuals, particularly those from marginalised groups. Civil society groups can help to raise awareness among individuals, including members of marginalised communities, about the importance of access to demographic data in tackling bias, whilst simultaneously calling for the development of solutions and services that give people greater autonomy, protect their privacy, and are worthy of their trust. Crucially, civil society groups can also help to amplify the voices of marginalised communities in debates around the design and development of new solutions, ensuring they are consulted and their views accounted for. For example, see Bank of England,'Machine Learning in UK Financial Services', Local Government Association (LGA), 'Using predictive analytics in local public services', and NHS England, 'Artificial Intelligence'.- See theEHRC's 'Five components of data collection and analysis' (pg. 54) in'Measurement Framework for Equality and Human Rights'.- Definition fromCDEI's report,'Unlocking the value of data: Exploring the role of data intermediaries'.- Meta, 'How Meta is working to assess fairness in relation to race in the U.S. across its products and systems', Airbnb, 'Measuring discrepancies in Airbnb guest acceptance rates using anonymized demographic data'.- See Airbnb, 'Measuring discrepancies in Airbnb guest acceptance rates using anonymized demographic data', where Airbnb assessed bias on their platform on the basis of 'perceived race'.- Notable exceptions include thePartnership onAI's Workstreamon Demographic Data, as well as some academic scholarship, including Michael Veale and Reuben Binns,'Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data'(2017).- For example, see Bank of England,'Machine Learning in UK Financial Services', Local Government Association (LGA), 'Using predictive analytics in local public services', and NHS England, 'Artificial Intelligence'.- See theEHRC's 'Five components of data collection and analysis' (pg. 54) in'Measurement Framework for Equality and Human Rights'.- SeeCDEI, 'Review in bias in algorithmic decision-making' and Open Data Institute (ODI),'Monitoring Equality in Digital Public Services'.- Some protected characteristics, including race, ethnicity, disability, and sexual orientation, are also special category data under the UK General Data Protection Regulation (GDPR) and the Data Protection Act 1998.- The equality of opportunity condition (Schedule 1, 8.1(b) of the Data Protection Act 1998) does not cover all special category data (e.g. trade union membership is not included).- Definition fromCDEI's report,'Unlocking the value of data: Exploring the role of data intermediaries'.- Pilots include those by theOpen Data Institute (ODI),Data Trusts Initiative, and theLiverpool City Region Civic Data Cooperative.- Definition fromCDEI's report,'Unlocking the value of data: Exploring the role of data intermediaries'.- Including, for example, theISO/IEC CD 5259-1series, which is currently under development.- The ICO providesguidanceabout when group inferences are personal data.- Special category data includes personal data revealing or concerning data about a data subject's racial or ethnic origin, political opinions, religious and philosophical beliefs, trade union membership, genetic data, biometric data for the purpose of uniquely identifying a natural person, data concerning health, sex life and sexual orientation.-",2023
govuk_018,govuk,Ai Fairness 8,"FairNow's AI governance platform serves as a single source of truth for managing AI governance, risk, and compliance. Many organisations today are leveraging AI in a distributed way across many different teams and departments. FairNow's AI governance platform is an organisation's AI governance command centre, serving as a single source of truth for managing AI governance, risk, and compliance. Risk, legal, data, technology, and business leaders can review their AI risks, track usage, monitor dependencies, and ensure compliance all within a single platform. Core features of FairNow's platform include AI inventory management; management of governance workflows, roles, and accountabilities; risk assessments; testing and ongoing monitoring; documentation and audit trails; vendor AI risk management; and regulatory compliance tracking. Organisations integrate the platform into their day-to-day governance activities, leveraging FairNow's built-in functionality to automate governance tasks, simplify their compliance tracking and reporting, and centralise their oversight of AI risk. Organisations using the platform today leverage a combination of off-the-shelf capabilities and configurable features. For example, many organisations adopt FairNow's risk, regulatory, and compliance intelligence offerings to stay informed of potential risks for each of their AI applications, as well as in-scope laws and regulations; they also add their own incremental risk assessment questions, policies, and controls to ensure that AI usage is adhering to requirements that are specific to their own organisations. More information on the AI White Paper Regulatory Principles By tracking an organisation's AI inventory and usage, the FairNow AI governance platform helps organisations determine the appropriate safety and reliability checks. These checks are logged on the FairNow platform to ensure that these safeguards are applied throughout the AI lifecycle and that a record of compliance is documented and maintained. This is done in two ways. First, through tracking which regulations apply to the organisation's AI and which voluntary standards the organisation follows, we explain which safety, reliability and robustness checks should be followed as part of these frameworks. The platform tracks these various checks. Second, FairNow's proprietary risk assessment logic identifies scenarios in which an AI system poses certain classes of risks related to safety, security, and robustness. The platform explains the risk and recommends potential mitigations to manage the risk. By providing a single source of truth for overall AI governance, organisations can establish a transparent AI governance program. Multiple governance controls help developers and deployers of AI to provide the right level of transparency, notification and explainability to stakeholders; and documentation/approvals stored centrally on the platform ensures organisations maintain robust audit trails. FairNow has built in several model interpretability features to the platform that can help organisations understand the factors that drive their AI. The platform automates multiple types of bias testing in alignment with regulatory expectations and best practices. The first is a disparate impact assessment analysis, which is automated by the platform and reports differences in selection rate or model scoring rate across demographic groups. The second is an explainability analysis which helps organisations understand the drivers behind model decisions, which can help determine the extent to which the model bases its decisions on demographic information versus valid and application-relevant criteria. The third is a chatbot bias assessment, which evaluates chatbots for differences in quality of responses between different demographic groups. Additionally, the platform tracks AI-related laws and regulations to determine which apply to the organisation's AI, so that they can adhere to the appropriate fairness requirements for which they are in scope. Through a set of roles and responsibilities, workflows, and controls, the platform ensures that accountability is established. Organisations can establish multiple roles, set multi-step approval authority, and leverage FairNow's platform to automate notifications and alerts in the event that gaps are identified for a particular AI application or at the organisational level. As a company with decades of combined experience in model risk management and AI governance, we understand how challenging AI governance can be - and how cumbersome it can be without the right tools and automation. Establishing workflows based on industry best practice and in alignment with standards like NIST and ISO while allowing organisations to configure key parts of their AI governance program ensures the platform can serve organisations of all sizes, industries, and maturity levels. With the many different ways that AI can be used across a wide organsation, tracking risk in a centralised manner can be cumbersome and time-consuming. Organisations without the right tools in place can end up tracking their AI inventory and risk across spreadsheets and shared drives. This results in hours spent manually managing processes, increasing the risk of human error and the possibility that something important may fall through the cracks. FairNow's AI governance platform is designed to help organisations track their AI systems and associated risks, whether managing an inventory of five or 500 applications. AI risk assessments allow organisations to evaluate each application's specific components and risk factors, assigning appropriate risk levels. The platform's testing and monitoring features support compliance with global AI laws and regulations, ensuring that AI systems are effective, safe, and fair. Documentation and audit trails provide transparency for internal stakeholders and impacted populations. Vendor AI risk management helps companies ensure that the AI they procure meets international standards. Additionally, regulatory and compliance tracking is essential for those who fall under the jurisdiction of any of the growing number of AI regulations that are in effect across the globe. AI governance platforms and technologies are designed to simplify, streamline, and automate various aspects of the governance process. However, human oversight--both of individual AI applications and the overall governance program--is essential for effectively managing existing and emerging AI risks within an organisation. Platforms like FairNow should be used to facilitate and enhance, rather than replace, human oversight in identifying, managing, and mitigating AI risks in line with internal and external requirements.",2023
govuk_019,govuk,Ai Fairness 9,"FairNow's synthetic bias evaluation technique creates synthetic job resumes that reflect a wide range of jobs, specialisations and job levels so that organisations can conduct a bias assessment with data that reflects their candidate pool. New York City's Local Law 144 has been in effect since July 2023 and was the first law in the US to require bias audits of employers and employment agencies who use AI in hiring or promotion. Under the law, in-scope employers and employment agencies are required to enlist an independent auditor to conduct a disparate impact analysis by race, gender, and intersectional categories thereof. This type of analysis typically requires historical data, but when sufficient historical data is not available (for example: because an AI tool hasn't launched yet or because data is otherwise unavailable), the NYC law allows for test data to be used. FairNow's synthetic bias evaluation technique creates synthetic job resumes that reflect a wide range of jobs, specialisations and job levels so that organisations can conduct a bias assessment with data that reflects their candidate pool. The synthetic resumes are constructed using templates where various attributes are added to connect the resume to a given race and gender. The resumes are otherwise identical in attributes related to the candidate's capability to do the job successfully. Because of this construction, differences in model scores can be attributed to the candidate's demographic attributes. This approach can also be extended to bias testing beyond the NYC LL144 audit requirements. FairNow has leveraged this method to evaluate a leading HR recruitment software provider's AI for bias by disability status and gender identity. More information on the AI White Paper Regulatory Principles With FairNow's synthetic data audit capabilities, organisations that use AI tools to support employment decisions can detect potential bias, even when sufficient real-world data is unavailable or a company does not wish to share certain data with an external third-party auditor. By identifying potential issues early, users can prevent the deployment of biased AI before it can cause harm to job candidates and workers. FairNow's bias testing - including evaluations using synthetic data - provides transparency into areas of potential bias in AI tools used for hiring, promotion, and worker management. Supplemental explainability testing available on FairNow's platform can help organisations quickly pinpoint potential drivers of any differences. FairNow's synthetic bias audit capabilities allow users to detect bias in their AI across dimensions such as gender, race, gender identity, disability status, and more. Leveraging FairNow's platform, users can conduct regular testing and monitoring to proactively address any issues. Assessments for bias are a critical component of AI risk management and mitigation and are referenced in multiple laws globally. On the FairNow platform, bias testing and monitoring can be automated, with key stakeholders in AI governance alerted automatically if findings fall outside threshold ranges. This approach addresses several of the most significant pain points companies face when conducting bias audits on their data. First, companies often lack historical data to conduct a bias audit. This could be because they haven't launched the AI yet to collect data, they have some data but not enough for a statistically significant sample size, or because their demographic data collection is sparse. Second, companies may have thin data on a particular segment or subtype of customers that they'd like to understand better. Our approach can enable organisations to test for potential bias even where actual data is not available. Our approach solves many of the data-related problems that companies face when they look to test for bias. Another benefit is privacy - the organisation does not need to share confidential applicant data with a third party because the data used for this bias audit is synthetic. This saves significant time and effort in procurement and privacy workflows and reduces privacy risks. Because the data used for this audit is synthetically constructed, it may lack some of the nuance and variability seen in real-world job application data. Additionally, while the synthetic data can be customised to the organisation's applicant pool, it may lag real-world shifts in applicant distributions or types.",2023
govuk_021,govuk,Ai Procurement 1,"Guidance to help you assess if artificial intelligence (AI) is the right technology for your challenge. This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. This guidance will help you assess if artificial intelligence (AI) is theright technology to help you meet user needs. As with all technology projects, you should make sure you can change your mind at a later stage and you can adapt the technology as your understanding of user needs changes. This guidance is relevant for anyone responsible for choosing technology in a public sector organisation. AI is just another tool to help deliver services. Designing any service starts withidentifying user needs. If you think AI may be an appropriate technology choice to help you meet user needs, you will need to consider your data and the specific technology you want to use. Yourdata scientistswill then use your data to build and train an AI model. When assessing if AI could help you meet users' needs, consider if: It's important to remember that AI is not an all-purpose solution. Unlike a human, AI cannot infer, and can only produce an output based on the data a team inputs to the model. When identifying whether AI is the right solution, it's important that you work with: For your AI model to work, it needs access to a large quantity of data. Work with specialists who have the knowledge of your data, such as data scientists, to assess your data state. You can assess whether your data is high enough quality for AI using a combination of: If your problem involves supporting an ongoing business decision process, you will need to plan to establish ongoing, up-to-date access to data. Remember tofollow data protection laws. There is no one 'AI technology'. Currently, widely-available AI technologies are mostly either supervised, unsupervised or reinforcement machine learning. The machine learning techniques that can provide you with the best insight depends on the problem you're trying to solve. There are certain types of problems for which machine learning is commonly used. For some of these you will be able to buy or adapt commercially available products. Because of its experimental and iterative nature, it can be difficult to specify the precise benefits which could come from an AI project. To explore this uncertainty and provide the right level of information around the potential benefits, you can: Once you have secured budget, you'll need to allow enough time and resources toconduct a substantial discoveryto show feasibility. Discovery for projects using AI can often takes longer for similar projects that do not use AI. If your organisation is a central government department, you may have toget approval from the Government Digital Service (GDS) to spend moneyon AI. At this point mostAI projects are classified as 'novel', which requires a high level of scrutiny. You should contact theGDS Standards Assuranceteam for help on the spend controls process. When assessing if AI could help you meet user needs, consider how you will procure the technology. You shoulddefine your purchasing strategyin the same way as you would for any other technology. Whether you build, buy or reuse (or combine these approaches) will depend on a number of considerations, including: It's also important toaddress ethical concernsabout the use of AI from the start of the procurement process. The Office for AI and the World Economic Forum are developing furtherguidance on AI procurement. Your team can build or adapt off-the-shelf AI models or open source algorithms in-house. When making this decision, you should work with data scientists to consider whether: You may be able to buy your AI technology as an off-the-shelf product. This is most suitable if you are looking for a common application of AI, for example optical character recognition. However, buying your AI technology may not always be suitable as the specifics of your data and needs could mean the supplier would have to build from scratch or significantly customise an existing model. Your AI solution will still need to be integrated into an end-to-end service for your users, even if you are able to buy significant components off the shelf. When using AI it's important to understand who is responsible if the system fails, as the problem may lie in a number of areas. For example, failures with the data chosen to train the model, design of the model, coding of the software, or deployment. You should establish a responsibility record which sets out who is responsible for different areas of the AI. It would be useful to consider whether: Depending on your organisation's maturity, it may be useful to set up a dedicated board, committee or forum to handle AI data and model governance. It can be useful to keep a central record of all AI technologies you use, listing:",2023
govuk_022,govuk,Ai Procurement 2,"The Sovereign AI Unit will ensure the government can harness AI's capabilities to unlock economic growth and enhance UK national security. Artificial Intelligence (AI) capabilities are developing at an extraordinary pace. In anAIenabled world, it matters who influences and builds the models, data andAIinfrastructure that are increasingly present in our lives. The Department for Science, Innovation and Technology (DSIT) has set up the SovereignAIUnit to build and harness the UK'sAIcapabilities to unlock economic growth and enhance UK national security. The Unit works closely with the Prime Minister's Adviser onAI. Announced in theAIOpportunities Action Planprepared by Matt Clifford, the SovereignAIUnit has an ambitious mandate to strengthen the UK's capabilities with up to PS500 million of funding. We will do this by: These partnerships are Memorandums of Understandings (MoU) which are non-legally binding, non-commercial agreements.",2023
govuk_023,govuk,Ai Procurement 3,"A list of places to find Defence contracts and funding in the technology and innovative research and development (R&D) space. Every year the Ministry of Defence (MOD) works with companies and researchers to develop next-generation innovation, technology and digital solutions to support our frontline troops. You can find some of the places we advertise those opportunities on this page. This is not an exhaustive list. Opportunities outside the technology and innovation space can be found atProcurement atMOD. To give feedback on this page, please contact theCommercial Xteam atdefcomrcl-comrclx@mod.gov.uk. A common way for suppliers to bid for contracts is via online platforms known as frameworks. These have standard terms and processes which make it easier to contract with us. Once you are on a framework you can apply for any advertised opportunities until the framework ends. The frameworks cover a range of goods and services. Some are 'dynamic', which means they can be joined at any time. Others are only open during certain periods, so if you miss the sign up period you will need to wait for it to re-open again. DASA finds and funds exploitable innovation for a safer future. DASA innovation partners support those with innovative ideas or products that could be of use in Defence to get funding, find contracts, write business cases and get introductions to sponsors. Contact your local innovation partner Submit an innovative idea to improve the defence and security of the UK Your idea could be a concept, product or service, at various levels of maturity. No funding limit but generally up to PS250,000 Submit your idea Submit a proposal around a specific area of government interest Viewcurrent themed competitions Get a loan as an SME to develop a mature innovative defence solution into a strong business proposition Loans of up to PS1 million Open for proposals all year round View details of defence innovation loans Future Capability Innovation team: Future Capability Innovationare sponsors of Futures Lab, a place where academia and industry come together to solve complexMODchallenges. There is anintro sessionevery Monday for new joiners.Find out more and register to join. Defence Battle Lab: Battle Labis an online community and physical co-working space in Dorset for business and academia to come together to innovate for Defence. Battle labs can be used to make, test, and trial solutions in collaboration with Defence. Battle labs can also be used to book desks and access meeting rooms. Apply to join on the Battle Lab website. Accelerated Capability Environment (ACE): ACE helps government and the police tackle safety and security challenges arising from our growing use of technology. ACE brings hundreds of suppliers and academics together to solve these challenges. It is open to new joiners and is run by QinetiQ. Apply to join on their website Accessing PhD research funding from the Ministry of Defence Defence and Security Innovation Signposting documentwith details of 5 organisations that also commission innovative technologies in defence, including details of the technology readiness levels (TRLs) they support. Added a framework under 'Current opportunities'. Updated information on frameworks under 'Current opportunities'. Added 'Future pipeline'. First published.",2023
govuk_024,govuk,Ai Procurement 4,"This page provides details about DSIT's portfolio of AI assurance techniques and how to use it. The portfolio ofAIassurance techniques has been developed by the Responsible Technology Adoption Unit, a directorate within DSIT, initially in collaboration withtechUK. The portfolio is useful for anybody involved in designing, developing, deploying or procuringAI-enabled systems, and showcases examples ofAIassurance techniques being used in the real-world to support the development of trustworthyAI. Search portfolio Please note the inclusion of a case study in the portfolio does not represent a government endorsement of the technique or the organisation, rather we are aiming to demonstrate the range of possible options that currently exist. To learn more about different tools and metrics forAIassurance please refer toOECD's catalogue of tools and metrics for trustworthyAI, a one-stop-shop for tools and metrics designed to helpAIactors develop fair and ethicalAI. We will be developing the portfolio over time, and publishing future iterations with new case studies. If you would like to submit case studies to the portfolio or would like further information please get in touch atai-assurance@dsit.gov.uk. Building and maintaining trust is crucial to realising the benefits ofAI. Organisations designing, developing, and deployingAIneed to be able to check that these systems are trustworthy, and communicate this clearly to their customers, service users, or wider society. AIassurance is about building confidence inAIsystems by measuring, evaluating and communicating whether anAIsystem meets relevant criteria such as: Assurance can also play an important role in identifying and managing the potential risks associated withAI. To assureAIsystems effectively we need a range of assurance techniques for assessing different types ofAIsystems, across a wide variety of contexts, against a range of relevant criteria. To learn more aboutAIassurance, please refer tothe roadmap to anAIassurance ecosystem,AIassurance guide,industry temperature check, and co-developedRTA (formerlyCDEI) and The Alan Turing Institute introduction toAIassurancee-learning module. The Portfolio ofAIassurance techniques was developed by the Responsible Technology Adoption Unit (RTA), in collaboration with techUK, to showcase examples ofAIassurance techniques being used in the real-world. It includes a variety of case studies from across multiple sectors and a range of technical, procedural and educational approaches , illustrating how a combination of different techniques can be used to promote responsibleAI. We have mapped these techniques to the principles set out in the UK government's white paper onAIregulation, to illustrate the potential role of these techniques in supporting widerAIgovernance. To learn more about different tools and metrics forAIassurance, please refer toOECD's catalogue of tools and metrics for trustworthyAI. The portfolio is a helpful resource for anyone involved in designing, developing, deploying or procuringAI-enabled systems. It will help you understand the benefits ofAIassurance for your organisation, if you're someone who is: The portfolio allows you to explore a range of examples ofAIassurance techniques applied across a variety of sectors. You can search for case studies based on multiple features you might be interested in, including the type of technique and the sector you work within. Each case study is also mapped against the most relevant cross-sector regulatory principles published in the government white paper onAIregulation. There are a range of different assurance techniques that can be used to measure, evaluate, and communicate the trustworthiness ofAIsystems. Some of these are listed below: Impact assessment:Used to anticipate the effect of a system on environmental, equality, human rights, data protection, or other outcomes. Impact evaluation:Similar to impact assessments, but are conducted after a system has been implemented in a retrospective manner. Bias audit:Assessing the inputs and outputs of algorithmic systems to determine if there is unfair bias in the input data, the outcome of a decision or classification made by the system. Compliance audit:A review of a company's adherence to internal policies and procedures, or external regulations or legal requirements. Specialised types of compliance audit include system and process audits and regulatory inspection. Certification:A process where an independent body attests that a product, service, organisation or individual has been tested against, and met, objective standards of quality or performance. Conformity assessment:Provides assurance that a product, service or system being supplied meets the expectations specified or claimed, prior to it entering the market. Conformity assessment includes activities such as testing, inspection and certification. Performance testing:Used to assess the performance of a system with respect to predetermined quantitative requirements or benchmarks. Formal verification:Establishes whether a system satisfies some requirements using the formal methods of mathematics. Check with assurance techniques can be used across each stage of theAIlifecycle. TheNationalAIStrategysets out an ambitious plan for how the UK can lead the world as anAIresearch and innovation powerhouse. EffectiveAIregulation is key to realising this vision to unlock the economic and societal benefits ofAIwhile also addressing the complex challenges it presents. In its recentAIregulation white paperthe UK government describes its pro-innovation, proportionate, and adaptable approach toAIregulation that supports responsible innovation across sectors. The white paper outlines five cross-cutting principles forAIregulation: safety, security and robustness; appropriate transparency and explainability; fairness; accountability and governance; and contestability and redress. Due to the unique challenges and opportunities raised byAIin particular contexts the UK will leverage the expertise of existing regulators, who are expected to interpret and implement the principles in their domain and outline what compliance with the principles looks like across different use cases. In addition, the white paper sets out the integral role of tools for trustworthyAI, such as assurance techniques and technical standards, to support the implementation of these regulatory principles in practice, boost international interoperability, and enable the development and deployment of responsibleAI. The RTA has conducted extensive research to investigate current uptake and adoption of tools for trustworthyAI, the findings of which are published in itsindustry temperature check. This report highlights industry appetite for more resources and repositories showcasing what assurance techniques exist, and how these can be applied in practice across different sectors. The UK government is already supporting the development and use of tools for trustworthyAI, through publishing aroadmap to an effectiveAIassurance ecosystemin the UK, having established theUKAIStandards Hubto champion the use of international standards, and now through the publication of the portfolio ofAIassurance techniques. TheAIStandards Hub is a joint initiative led by The Alan Turing Institute in partnership with the British Standards Institution (BSI), the National Physical Laboratory (NPL), and supported by government. The hub's mission is to advance trustworthy and responsibleAIwith a focus on the role that standards can play as governance tools and innovation mechanisms. TheAIStandards Hub aims to help stakeholders navigate and actively participate in internationalAIstandardisation efforts and champion the use of international standards forAI. Dedicated to knowledge sharing, community and capacity building, and strategic research, the hub seeks to bring together industry, government, regulators, consumers, civil society and academia with a view to: To learn more, visit theAIStandards Hubwebsite. The catalogue of tools and metrics for trustworthyAIis a one-stop-shop for tools and metrics designed to helpAIactors develop and useAIsystems that respect human rights and are fair, transparent, explainable, robust, secure and safe. The catalogue gives access to the latest tools and metrics in a user-friendly way but also to use cases that illustrate how those tools and metrics have been used in different contexts. Through the catalogue,AIpractitioners from all over the world can share and compare tools and metrics and build upon each other's efforts to implement trustworthyAI. The OECD catalogue features relevant UK initiatives and works in close collaboration with theAIStandards Hub, showcasing relevant international standards for trustworthyAI. The OECD catalogue will also feature the case studies included in this portfolio. To learn more, visitThe OECD catalogue of tools and metrics for trustworthyAI. Data assurance is a set of processes that increase confidence that data will meet a specific need, and that organisations collecting, accessing, using and sharing data are doing so in trustworthy ways. Data assurance is vital for organisations to build trust, manage risks and maximise opportunities. But how can organisations assess, build and demonstrate trustworthiness with data? Through its data assurance work, theODIis working with partners and collaborators to explore this important and rapidly developing area in managing global data infrastructure.ODIbelieve the adoption of data assurance practices, products and services will reassure organisations and individuals who want to share or reuse data, and support better data governance practices, fostering trust and sustainable behaviour change. To learn more, visit theODIwebsite.",2023
govuk_025,govuk,Ai Procurement 5,"AI is one of the fastest-moving areas of technological advancement in government defence, bringing the capability to counter threats and create opportunities. The Defence Science and Technology Laboratory (Dstl) provides theUKwith world-class capability in applying artificial intelligence (AI), data science and machine learning to defence and security challenges. From putting machine learning on-board Royal Navy ships, to using data science to support intelligence analysis,Dstlis at the heart of such innovation. One ofDstl's missions is tode-mystify the area ofAI. We help theUKMinistry of Defence (MOD) understand how it cansafely, responsibly and ethicallyuseAIto deter and de-escalate conflict, save lives and reduce harm. Our technical experts have their finger on the pulse of new developments inAIand provide critical guidance for theUK. We work in partnership with specialists from academia, industry and allied nations to understand and develop a broad spectrum of techniques for performing tasks and discovering insights from data using automatic processes, We are also an integral part of theDefenceAICentre (DAIC), working collaboratively to connectMODand the widerAIcommunity and enable teams acrossMODto adoptAIin their areas. The wide range of potential applications include streamlining back-office functions, and supporting: AIis critical to theUK's future as a science and technology superpower, and when it comes toAI,MOD's visionis to be ""the world's most effective, efficient, trusted and influential defence organisation for our size."" ...A radical upheaval in defence is underway andAI-related strategic competition is intensifying. Our response must be rapid, ambitious, and comprehensive.""(Defence Artificial Intelligence Strategy (2022) Our cutting edge work covers everything from very early research looking at how machines interact with humans, to applying data science to real-world challenges and operational requirements. Our experts collaborate with international partners to help theUKand our allies advance more quickly. The 3 nations ofAUKUS(Australia - United Kingdom - United States)recently testedAI-enabled uncrewed aerial vehiclesthat allow a human operator to locate, disable and destroy targets on the ground. AUKUS: Autonomy andAIcollaboration Royal Navy crews taking part in a 2021 NATO exercise inUKwaters were able to take advantage ofAIon-board ship for the first time, whenDstland our industry partners brought the latest technology into the command spaces of a Type 45 destroyer and a Type 23 frigate. Tested against a supersonic missile threat, ourAI-based applications were designed to help detect threats earlier and provide a rapid hazard assessment to recommend options the crew can take to counter the threat. This exciting real-world test was the culmination of almost10 years of research and collaboration with industry suppliers. Image taken from HMS Dragon, participating in Exercise Formidable Shield 2021. UK MOD (c) Crown copyright 2021 We work alongside the armed forces to develop and test new technologies in the field to get our research out of the lab and in to operational use as quickly as possible. Dstland theUSAir Force Research Laboratory carried out the firstdeployment of a jointly developed artificial intelligence toolbox in 2 military exercises. The goal was to address the challenge of how to makeAIagile, adaptable, trustworthy and accessible to the warfighter under differentUSandUKmilitary use cases. The trials showed how the toolbox would be deployed ontoUK-USuncrewed ground and aerial vehicles. Members of bothUKandUSarmed forces tested that theAIwas robust and that the intended users understood any limitations of theAI. DuringExercise Spring Storm, soldiers from the 20th Armoured Infantry Brigade used anAIengine developed byDstland the Army in collaboration with industry. This prototype was specifically designed for the way the Army is trained to operate. During the exercise,Dstlstaff analysed how theAIengine was used in practice, to understand the critical human factors including how we build trust in theAI. TheAIengine uses automation and smart analytics along with supervised learning algorithms to save time and effort and help military personnel operate much more effectively. By instantly exploiting information on the surrounding environment and terrain, theAImakes it much quicker to plan and analyse different courses of action. This is one of the first steps towards achieving machine-speed command and control. UK MOD (c) Crown copyright 2021 Dstlis also working with partners on howAIand autonomy pose opportunities and threats to traditional intelligence, surveillance and reconnaissance (ISR). Our research ranges fromAI-enabledISRtasking and collection, to investigating the advantage that could be brought by quantum information processing. This work isn't just done by ourAIspecialists but is a great example of how we work acrossDstl's areas of expertise, such assensingandrobotics and autonomy. We have developed a standard approach forAIand autonomy in networked multi-sensor systems in security and defence which has been evaluated during multinational NATO trials, adopted byMODand industry, and published as British Standards Institute (BSI) Flex 335. Sensing for Asset Protection with Integrated Electronic Networked Technology (SAPIENT) specifies standards and protocols to allowAIalgorithms to work together across a suite of sensors and share data with a mix of technologies. As well as improving efficiency,SAPIENTcan help deliver enhanced situational awareness to support control and command of operations, and givesMODand our allies access to advancedAIsolutions being developed by our innovative supplier base. During a recentAUKUStrial in Australia,Dstland theUKarmed forces collaborated with Australia and theUSin theTrusted Operation of Robotic Vehicles in Contested Environments (TORVICE) trial. A number of missions were conducted, such as route reconnaissance while subject to a range of effects, and these aimed to understand and improve the resilience ofAUKUSAIand autonomy systems when subjected to attack. Understanding and mitigating the impact of threats is a key step towards deploying effective and reliableAIand autonomy systems on future operations. TORVICEAITrial We also work in partnership with theNational Quantum Technology Programme (NQTP), which provides unique insights into quantum information processing, complementing our own research and building expertise to support defence. TheNQTP's work includesfunding researchinto quantum machine learning models, hybrid (traditional/quantum) generative modelling to improve satellite imaging, as well as quantum fingerprinting to protect such models from cyber attacks. Dstlis an outstanding centre for research and we have extensive collaboration underway with leadingAIorganisations. Whenever possible,Dstlworks with external suppliers in academia and industry to meet the needs of theUK's defence and security. We are working together withGoogle Cloudto accelerate the adoption ofAIin theUKdefence sector and withMicrosoftto deliver safe and responsible use ofAI. Dstlis a strategic partner in thedefence and security research programmerun byThe Alan Turing Institutewho conduct world leading research in a large number of areas related to our needs. As another example of our partnership with the Turing,Dstland theDefence and Security Accelerator (DASA)supported a studyidentifying hazardous chemical and biological contamination on surfaces. Working with industry and academic partners, we explored how machine learning and data science could be used to detect substances like anthrax and nerve agents, alongside the development of innovative sensor technologies. This could be safer and more efficient than current methods. We frequently collaborate with universities and other academic institutions, often accelerating the research they have been doing and applying it to solve problems forUKdefence and security - it's a fantastic opportunity for organisations to get to work on leading edge projects with real impact on the defence and security of the country. In 2022, we formed theDefence Data Research Centre(DDRC) - a dedicated centre of excellence focusing on problems related to the use of data forAIapplications with a defence context. We are working closely with staff at the universities of Southampton and Edinburgh, where2 new centres for doctoral traininghave been funded byMODto enable novel research in critical technology areas, such asAIand autonomy. We have close links with other research bodies, including: AIand data science underpin and enable all theUK's defence and security science and technology capabilities. They have the potential to change everything.Dstlplays a pivotal role bringing together our scientific and technical expertise acrossMOD. ""Future conflicts may be won or lost on the speed and efficacy of theAIsolutions employed."" (Defence in a competitive age) Our flagshipAIscience and technology programme delivers core defence-specific research, but also works across our capabilities to build up strong communities of practice to share and expand our expertise. DrivingAIcloser to capability We worked withDefence DigitalandMODto establish theDefenceAICentre(DAIC) to boost research and accelerate the adoption ofAItechnology (announced in theIntegrated Review). The DAIC works collaboratively across defence, united and supported by a core team made up of Defence Digital (as part of Strategic Command),Defence Equipment and Support(Future Capability Group) andDstl. The team champion, enable and innovateAIacrossUKdefence, working with government, industry, academia and our allies for the strategic advantage of our armed forces. This includes working withDASAto help connect with innovators both in theUKand abroad. In 2022, we opened abrand newAIand data science unit based at the National Innovation Centre for Data in Newcastle, opening up new local opportunities for individuals and institutions to work with us. As part of our work in government, we support learning and up-skilling of people working across the defence and security sector. We also want to make it easier for more people to work with us. Our approach to this is broad, from enabling government customers commissioning our services to attracting talent from outside the defence industry to work with us. We run regular events (such asAIFest) to bring together theAIcommunity across government, industry, academia and international partners. At these events, world-leading experts share their experiences including the challenges of military adoption ofAI. We also discuss how to build an effectiveAIcapability. Our series ofBiscuit Booksprovide simple introductions to some of the complex concepts related toAI, data science and machine learning. Topics covered includehuman-centred ways of working withAIin intelligence analysisandassurance ofAIand autonomous systems. We are responsible and ethical in our use ofAIand we are leading the way in helping others (across defence, wider government and internationally) with safe adoption. We helped formMOD's internal guidance on best practice for ethics in developing or using systems which useAI, to achieve theAmbitious, safe, responsiblepolicy statement. This statement sets out the5 ethical principleswe follow: Through our strong links with academia we are contributing toresearch into the ethical use ofAIin defence. Dstl's experts have also collaborated with the Institution of Engineering and Technology to help raise awareness of theeffective use ofAIin safety-related functions, highlighting the importance of underpinning regulation and good practice to embedAIsafety. Our staff, industry and academic partners work across a range of scientific and technology capabilities including: We are always looking for talented individuals to join us. You will work on real-world problems, with the chance to see your science and technology expertise put into practice, including hugely exciting opportunities for overseas travel and working with theUK's allies. Experience in defence is not necessary, and roles vary from apprenticeships to visiting fellowships (for example, we have a Visiting research scheme with the Alan Turing Institute). While we have a high intake of graduates, you can also be mid-career, looking to re-skill and re-train. We work across Defence to partner with academic institutions and build relationships with suppliers. We particularly welcome companies and research bodies who have not worked with defence before, and small and medium-sized enterprises (SMEs). There are several routes toworking with us: Talk to us about potential future partnerships and projects by emailingcentralenquiries@dstl.gov.uk Added video about 'Driving AI closer to capability'. New video of the 3 nations of AUKUS have trialled a futuristic integration of autonomy and AI. Updated with links to the latest case studies and information about Dstl's work on AI. First published.",2023
govuk_026,govuk,Ai Procurement 6,"Understand how to use artificial intelligence ethically and safely This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. The Office for Artificial Intelligence (OAI) and the Government Digital Service (GDS) have produced the following chapter's guidance in partnership with The Alan Turing Institute'spublic policy programme. This chapter is a summary ofThe Alan Turing Institute's detailed guidance, and readers should refer to the full guidance when implementing these recommendations. AI has the potential to make a substantial impact for individuals, communities, and society. To make sure the impact of your AI project is positive and does not unintentionally harm those affected by it, you and your team should make considerations of AI ethics and safety a high priority. This section introduces AI ethics and provides a high-level overview of the ethical building blocks needed for the responsible delivery of an AI project. The following guidance is designed to complement and supplement theData Ethics Framework. The Data Ethics Framework is a tool that should be used in any project. This guidance is for everyone involved in the design, production, and deployment of an AI project such as: Ethical considerations will arise at every stage of your AI project. You should use the expertise and active cooperation of all your team members to address them. AI ethics is a set of values, principles, and techniques that employ widely accepted standards to guide moral conduct in the development and use of AI systems. The field of AI ethics emerged from the need to address the individual and societal harms AI systems might cause. These harms rarely arise as a result of a deliberate choice - most AI developers do not want to build biased or discriminatory applications or applications which invade users' privacy. The main ways AI systems can cause involuntary harm are: The field of AI ethics mitigates these harms by providing project teams with the values, principles, and techniques needed to produce ethical, fair, and safe AI applications. The guidance summarised in this chapter and presented at length inThe Alan Turing Institute's further guidance on AI ethics and safetyis as comprehensive as possible. However, not all issues discussed will apply equally to each project using AI. An AI model which filters out spam emails, for example, will present fewer ethical challenges than one which identifies vulnerable children. You and your team should formulate governance procedures and protocols for each project using AI, following a careful evaluation of social and ethical impacts. You should establish ethical building blocks for the responsible delivery of your AI project. This involves building a culture of responsible innovation as well as a governance architecture to bring the values and principles of ethical, fair, and safe AI to life. To build and maintain a culture of responsibility you and your team should prioritise 4 goals as you design, develop, and deploy your AI project. In particular, you should make sure your AI project is: Prioritising these goals will help build a culture of responsible innovation. To make sure they are fully incorporated into your project you should establish a governance architecture consisting of a: You should understand the framework of ethical values which support, underwrite, and motivate the responsible design and use of AI. The Alan Turing Institute calls these 'the SUM Values': These values: You can read further guidance on SUM Values inThe Alan Turing Institute's comprehensive guidance on AI ethics and safety. While the SUM Values can help you consider the ethical permissibility of your AI project, they are not specifically catered to the particularities of designing, developing, and implementing an AI system. AI systems increasingly perform tasks previously done by humans. For example, AI systems can screen CVs as part of a recruitment process. However, unlike human recruiters, you cannot hold an AI system directly responsible or accountable for denying applicants a job. This lack of accountability of the AI system itself creates a need for a set of actionable principles tailored to the design and use of AI systems. The Alan Turing Institute calls these the 'FAST Track Principles': Carefully reviewing the FAST Track Principles helps you: If your AI system processes social or demographic data, you should design it to meet a minimum level of discriminatory non-harm. To do this you should: You should design your AI system to be fully answerable and auditable. To do this you should: The technical sustainability of these systems ultimately depends on their safety, including their accuracy, reliability, security, and robustness. You should make sure designers and users remain aware of: Designers and implementers of AI systems should be able to: To assess these criteria in depth,you should consult The Alan Turing Institute's guidance on AI ethics and safety. The final method to make sure you use AI ethically, fairly, and safely is building a process-based governance framework. The Alan Turing Institute calls it a 'PBG Framework'. Its primary purpose is to integrate the SUM Values and the FAST Track Principles across the implementation of AI within a service. Building a good PBG Framework for your AI project will provide your team with an overview of: You may find it useful toconsider further guidance on allocating responsibility and governance for AI projects.",2023
govuk_027,govuk,Ai Procurement 7,"Guidance on use of artificial intelligence (AI) as part of any appeal, application or examination being dealt with by the Planning Inspectorate. The Inspectorate understands that AI can be used to support our work, and that this can be done positively when it is transparently used. Due to the evolving capability and application of AI we will keep this guidance under review. AI is technology that enables a computer or other machine to exhibit 'intelligence' normally associated with humans. Recent advances mean that AI can now be used to create new content in the form of text, images, videos, audio, computer code and other types of data. It can also be used to alter or enhance existing content. In such cases, AI works by drawing on existing information, usually from a large database or from the internet, to provide a response to a user's prompts or requests. If you use AI to create or alter any part of your documents, information or data, you should tell us that you have done this when you provide the material to us. You should also tell us what systems or tools you have used, the source of the information that the AI system has based its content on, and what information or material the AI has been used to create or alter. In addition, if you have used AI, you should do the following: By following this guidance, you will help us, our Inspectors, and other people involved in the appeal, application or examination to understand the origin, purpose, and accuracy of the information. This will help everyone to interpret it and understand it properly. Any questions should be directed via ourCustomer Services Team.",2023
govuk_028,govuk,Ai Procurement 8,"Find guidance for the responsible use and development of data and data technologies developed by and for government and public sector bodies. Use this tool to find data ethics guidance from across government. You can use the filter and search functions to identify the most relevant pieces of information for your organisation's needs. Start now Emaildata.ethics@digital.cabinet-office.gov.ukif you have questions or suggestions about data ethics guidance, or are aware of further relevant guidance that should be included. We encourage anyone working in the government and public sector to refer to the documents below to develop a high-level understanding of key data ethics considerations. TheData Ethics Frameworkexplains how to use data appropriately and responsibly when planning, implementing and evaluating a new policy or service. TheModel for Responsible Innovationis a practical tool to help teams across the public sector and beyond to innovate responsibly with data andAI. When working with public sector data, you have a responsibility to establish whether the data you manage and use is fit for purpose. TheGovernment Data Quality FrameworkandData Sharing Governance Frameworkset out principles and practices to improve the quality and better use of data across government. TheGenerativeAIFramework forHMGexplains how to use generativeAIsafely and responsibly. TheAlgorithmic Transparency Recording Standard (ATRS)provides a standard for public sector organisations to publish information about how and why they are using algorithmic methods in decision-making processes that affect members of the public. TheEthics, Transparency and Accountability Framework for Automated Decision-Makingaims to help government departments with the safe, sustainable and ethical use of automated and algorithmic decision-making systems. AIassurance is vital to ensure the reliability and trustworthiness ofAIsystems. The Responsible Technology Adoption Unit's (RTA)introduction toAIAssuranceidentifies assurance techniques that can support the development of responsibleAI. You can find additional information about tools and processes that support the responsible use ofAIat theRTA'sResponsibleAIToolkitpage. You must use the criteria inThe Technology Code of Practiceto design, build and buy technology in government. When procuringAIsolutions from third parties, refer to theGuidelines forAIprocurement. This includes principles for buyingAItechnology and insights on tackling challenges that may arise during procurement.",2023
govuk_029,govuk,Ai Procurement 9,"Guidance to help you plan and prepare for implementing artificial intelligence (AI). This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. Once you have assessed whether AI can help your team meet your users' needs, this guidance will explore the steps you should take to plan and prepare before implementing AI. As with all technology projects and programmes,you should follow the Technology Code of Practice. This guidance is for anyone responsible for: As with all projects, you need to make sure you'rehypothesis-ledand can constantly iterate to best help your users and their needs. You should integrate your AI development with your wider project phases. You shouldconsider AI ethics and safetythroughout all phases. Significant time is needed to understand the feasibility of using your data in a new way. This means the discovery phase tends to be longer and more expensive than for services without AI. Your data scientists may be familiar with a lifecycle calledCRISP-DMand may wish to integrate parts of it into your project. Discovery can help you understand the problem that needs to be solved. You should: To prepare for your AI project, you should assess your existing data. Training an AI system on error-strewn data can result in poor results due to: You can use a combination of accuracy, completeness, uniqueness, timeliness, validity, relevancy, representativeness, sufficiency or consistency to see if the data is high enough quality for an AI system to make predictions from. When assessing your AI data, it's useful to collaborate with someone who has deep knowledge of your data, such as adata scientist. They will be familiar with the best practice for measuring, cleaning and maintaining good data standards for ongoing projects.Make your data proportionate to user needsandunderstand the limitations of the datato help you assess your data readiness. Questions for you to consider with data scientists are: If you're unsure about your use of data, consult theData Ethics Framework guidanceto check your project is a safe application and deployment of AI. As with other projects, yourteam should be multidisciplinary, with a diverse combination of roles and skills to reduce bias and make sure your results are as accurate as possible. When working with AI you may need specialist roles such as a: You may not need all of these roles from the very beginning, but this may change as the work progresses. You may want to break up your discovery into smaller phases so you can evaluate what you are learning. It can be useful for your team to have: When preparing for AI implementation, you should identify how you can bestintegrate AI with your existing technology and services. It's useful to consider how you'll manage: When choosing your AI tools, you should bring in specialists, such as data scientists or technical architects to assess what tools you currently have to support AI. Use Cloud Firstwhen setting up your infrastructure. A data science platform is a type of software tool which helps teams connect all of the technology they require across their project workflow, speeding up AI deployment and increasing the transparency and oversight over AI models. When deciding on whether to use a data science platform, it's useful to consider how the platform can: After you've assessed your current data quality, you should prepare your data to make sure it is secure and unbiased. You may find it useful tocreate a data factsheetduring discovery to keep a record of your data quality. In the same way you should havediversity in your team, your data should also be diverse and reflective of the population you are trying to model. This will reduce conscious or unconscious bias. Alongside this, a lack of diverse input could mean certain groups are disadvantaged, as the AI model may not cater for a diverse set of needs. You should read the Data Ethics Framework guidance tounderstand the limitations of your dataand how to recognise any bias present. You should also: Make sure you design your system to keep data secure. To help keep data safe: As with any other software, you should design and build modular, loosely coupled systems which can be easily iterated and adapted. Writing and training algorithms can take a lot of time and computational power. In addition to ongoing cost, you'll need to think about the network and memory resources your team will need to train your model. Most of the data in government available to train our models is within legacy systems which might contain bias and might have poor controls around it. For legacy systems to be compatible with AI technology, you will often need to invest a lot of work tobring your legacy systems up to modern standards. You'll also need to carefully consider the ethical and legal implications of working with historic data and whether you need to seek permission to use this information. When you complete your data preparation phase you should have: During the discovery phase, you should explore the needs of the users of the end to end service. Like other digital services, you'll use this phase to determine whether there's a viable service you could build that would solve user needs, and that it's cost-effective to pursue the problem. You'll be able to check guidance on how toknow when your discovery is finishedbefore moving on to alpha. If you have decided to build your AI model in-house, you should follow these steps. Your team will need to train the models they build on data. Your team should split your data into a: Your team should build a simple baseline version model before they build any more complex models. This provides a benchmark that your team can later compare more complex models against, and will help your team identify problems in your data. Once you have a baseline model, your team can start prototyping more complex models. This is a highly iterative process, requiring substantial amounts of data, and will see your team probably build a number of AI models before deciding on the most effective andappropriate algorithmfor your problem. Keeping your team's first AI model simple and setting up the right end-to-end infrastructure will help smooth the transition from alpha to beta. You can action this by focusing on the infrastructure requirements for your AI pipelines at the same time as your team is developing your model. Your simple model will provide you with baseline metrics and information on the model's behaviour that you can use to test more complex models. Throughout the build, you shouldmake sure your AI model security complieswith advice from the NCSC. Your team will need to test your models throughout the process to mitigate against issues such asoverfitting or underfittingthat could undermine your model's effectiveness once deployed. Your team should only use the test set on your best model. Keep this data separate from your models until this final test. This test will provide you with the most accurate impression of how your model will perform once deployed. Your team will need to evaluate your model to assess how it is performing against unseen data. This will give you an indication of how your model will perform in the real world. The best evaluation metric will depend on the problem you are trying to solve, and your chosen model. While you should select the evaluation metric with data scientists, you should also consider the ethical, economical and societal implications. These considerations make the fine tuning of AI systems relevant to both data scientists and delivery leads. When choosing your final model, you will need to consider: Once you select a final model, your team will need to assess its performance, and refine it to make sure it performs as well as you need it to. When assessing your model's performance consider: If a model does not outperform human performance, it still may be useful. For example, a text classification algorithm might not be as accurate as a human when classifying documents, however they can perform at a far higher scale and speed than a human. When you complete building your AI prototyping phase, you should have: Moving from alpha tobetainvolves integrating the model into the service's decision-making process and using live data for the model to make predictions on. Using your model in your service has 3 stages. You should continue tocollect user needsso your team can use the model's outputs in the real world. When moving from alpha to beta, there are some best-practice guidelines to smooth the transition. After creating a beta version, you team can use automated testing to create some high-level tests before moving to more thorough testing. Working in this way means you can launch new improvements without worrying about functionality once deployed. During alpha, you will have relied mostly on data scientists to assess the opportunity and your data state. Moving to beta needs specialists with a strong knowledge of dev-ops, servers, networking, data stores, data management, data governance, containers, cloud infrastructure and security design. This skillset is likely to be better suited to an engineer rather than a data scientist so maintaining a cross-functional team will help smooth the transition from alpha to beta. When you complete your beta phase, you should have:",2023
govuk_030,govuk,Algorithm Transparency 0,"The Government Digital Service (GDS) is helping public sector organisations provide clear information about how and why they are using algorithmic tools. The Algorithmic Transparency Recording Standard (ATRS) establishes a standardised way for public sector organisations to publish information about how and why they are using algorithmic tools. Algorithmic transparency means being open about how algorithmic tools support decisions. This includes providing information on algorithmic tools and algorithm-assisted decisions in a complete, open, understandable, easily-accessible, and free format. The Hub is made up of The ATRS was developed by teams now situated in the Government Digital Service (GDS). Its design and development was underpinned by extensive collaboration with public sector, industry and academic stakeholders as well as a public engagement study. It was piloted with a variety of public sector organisations. The ATRS is mandatory for all government departments, and for arm's-length bodies (ALBs) which deliver public or frontline services, or directly interact with the general public. Thescope and exemptions policy, published in December 2024, sets out the organisations and algorithmic tools for which the ATRS is a requirement in more detail. The ATRS remains recommended by theData Standards Authorityfor use in the broader public sector. TheBlueprint for Modern Digital Governmentpublished in January 2025 stated that government would 'enhance transparency by building on the Algorithmic Transparency Recording Standard to increase openness about AI usage'. We look forward to continuing to enhance and iterate the ATRS and encourage its use across the public sector. Get in touch with us atalgorithmic-transparency@dsit.gov.uk. Updated the ATRS template and associated guidance for public sector bodies, and updated references to Government Digital Service (GDS). Added the 'Scope and exemptions policy for mandatory use of the Algorithmic Transparency Recording Standard'. This page has been updated with a new version of the ATRS template and a link to a new GOV.UK search page for ATRS published records. First published.",2023
govuk_031,govuk,Algorithm Transparency 1,"Updated 23 September 2022 (c) Crown copyright 2022 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visitnationalarchives.gov.uk/doc/open-government-licence/version/3or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:psi@nationalarchives.gov.uk. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned. This publication is available at https://www.gov.uk/government/publications/findings-from-the-drcf-algorithmic-processing-workstream-spring-2022/the-benefits-and-harms-of-algorithms-a-shared-perspective-from-the-four-digital-regulators Note: This discussion paper is intended to foster debate and discussion among our stakeholders. It should not be taken as an indication of current or future policy by any of the member regulators of the Digital Regulation Cooperation Forum (DRCF). Every day, we use a wide variety of automated systems that collect and process data. Such ""algorithmic processing"" is ubiquitous and often beneficial, underpinning many of the products and services we use in everyday life. From detecting fraudulent activity in financial services to connecting us with friends online or translating languages at the click of a button, these systems have become a core part of modern society. However, algorithmic systems, particularly modern Machine Learning (ML) approaches, pose significant risks if deployed and managed without due care. They can amplify harmful biases that lead to discriminatory decisions or unfair outcomes that reinforce inequalities. They can be used to mislead consumers and distort competition. Further, the opaque and complex nature by which they collect and process large volumes of personal data can put people's privacy rights in jeopardy. It is important for regulators to understand and articulate the nature and severity of these risks. In doing so, they can help empower businesses to develop and deploy algorithmic processing systems in safe and responsible ways that are pro-innovation and pro-consumer. When it comes to addressing these risks, regulators have a variety of options available, such as producing instructive guidance, undertaking enforcement activity and, where necessary, issuing financial penalties for unlawful conduct and mandating new practices. Over the past year, the Digital Regulation Co-operation Forum (DRCF) has enabled our 4 regulatory bodies (CMA, FCA, ICO and Ofcom) to collaborate in defining common areas of interest and concern. From this foundation, we can act more effectively in this space, identifying potential initiatives for individual regulators while recognising areas where joint initiatives and collaboration may have significantly more impact than individual interventions. This paper is one of 2 initial publications by theDRCF on algorithmic processing[footnote 1]. In this paper we set out 6 common areas of focus among the DRCF members: transparency, fairness, access to information, resilience of infrastructure, individual autonomy and healthy competition. These areas were developed by DRCF members in conjunction with stakeholders from academia, civil society, government, industry, public sector and consumer groups. We then outline the current and potential harms and some of the current and future benefits of algorithmic processing that relate to our focus areas. Finally, we explore possible roles for UK regulators, the DRCF in particular, and outline suggestions for future work. The key takeaways from this paper are: As the DRCF continues to evolve, there are opportunities for members to co-ordinate and collaborate in a manner that would enable greater impact than individual regulatory action. These could include: Through the process of researching and writing these papers, we have developed a better mutual understanding of members' capabilities, remits and powers. This includes perceived areas of tension, such as those that between pro-privacy and pro-competition activities. We believe that through continued collaboration and co-ordination we can continue to resolve some of these tensions and have greater positive impact than acting solely as individual regulatory bodies.[footnote 3] In the next financial year, we intend to undertake further activity in the field of algorithmic processing. We are now launching a call for input alongside the publication of these 2 papers to inform the future work of the DRCF, and we welcome and encourage all interested parties to engage with us in helping shape our agenda. This discussion paper examines the benefits and harms posed by algorithmic processing. Here, we understand algorithmic processing as the processing of data (both personal and non-personal) by automated systems. This includes artificial intelligence (AI) applications, such as those powered by machine learning (ML) techniques, but also simpler statistical models. Our interest covers the processing of data, as well as the context in which that processing occurs, such as the means used to collect and store that data, and the ways humans interact with the results of any processing. We are also interested in both the positive and negative impacts on individuals and society that algorithms cause, as well as how different algorithmic systems interact with each other. Algorithmic processing can be used both to produce an output (for example video or text content) and to make or inform decisions that have a direct bearing on individuals. It is already being woven into many digital products and services, resulting in efficiency gains across the public and private sectors. It can and does enable innovation and can unlock significant benefits for individuals, consumers, businesses, public services, and society at large. Examples of the benefits it provides include: However, algorithmic processing can also be a source of harm if not managed responsibly. It may, for example, produce biased outputs/predictions, leading some groups to be treated less favourably than others (for example algorithms used in CV screening software have the potential to unfairly discriminate against job applicants of one gender over another if not deployed or managed with due care).[footnote 4]Algorithmic processing could also lead to society-wide harms (for example by promoting disinformation through social media recommender systems). Algorithmic harms can emerge as an outcome of use, as in the case of the above examples, or through the development of these systems (for example on account of the energy costs of training an AI model[footnote 5]). These examples represent a small fraction of harms that can be caused. Algorithmic processing can pose significant risks for several reasons. It can be used: The CMA, FCA, ICO and Ofcom collectively believe that we can, and should, play a role in identifying and mitigating these risks within the industries we regulate. In terms of algorithmic processing, for example, the data protection legislation the ICO oversees includes provisions that restrict the circumstances in which organisations can make solely automated decisions that have legal or significant effects on individuals. While the remits and powers of members vary[footnote 6], between us we have the ability to produce advice and guidance, set standards in the form of codes of practice, and commend responsible behavior. Our independence means we can provide robust oversight, scrutinising both the public and private sectors. The DRCF is able to provide a coordinated regulatory approach to algorithmic processing. Collaboration is particularly important for addressing issues that cut across our regulatory remits, such as the use of personal data for real-time bidding in the advertising industry, and financial scams on social media.[footnote 7]The DRCF is the first forum in the world where 4 regulators, representing a range of perspectives, can pool insights and expertise on algorithmic processing, as well as conduct joint initiatives on topics of common interest. Working together will allow us to develop consistent messaging, and provide regulatory clarity for those using algorithmic systems. This discussion paper provides an initial assessment of the benefits and harms that can arise from the use of algorithmic processing in the delivery of digital services. Our goal is to better understand how algorithmic processing takes place to help organisations achieve the benefits without causing the harms, laying the groundwork for future action in DRCF's 2022 to 23 workplan. The paper covers the following topics: While we look here at the harms and benefits associated with all types of algorithmic processing, much of the research and stakeholder comments we cite relate specifically to the use of machine learning (ML) algorithms. This reflects the fact that ML-trained algorithms pose novel and sometimes more significant risks, which are only beginning to be understood. For example, they can surface, reward and amplify underlying harmful patterns that did not scale in the past. It is important, however, not to discount the impact of algorithmic systems built using conventional statistical methods, for example the Ofqual algorithm used to decide A-level grades for students in 2020.[footnote 8]In addition, this discussion paper will largely, although not exclusively, focus on the direct harms and benefits caused by the use and development of algorithms as they relate to our regulatory remits. This discussion paper is being published at a time of growing interest - both domestically and internationally - in the effects of algorithmic processing, particularly that powered by AI and ML methods. In 2021 the UK government published a National AI Strategy[footnote 9], setting out its ambition to position the UK as an AI 'superpower'. Among its commitments were to launch a National AI Research and Innovation Programme, an AI Standards Hub, and a new visa regime to attract AI talent to the UK. The government is expected to follow up with a separate AI White Paper later this year that sets out a national position for governing AI, as well as a National AI Strategy for Health and Social Care and a Defence AI Strategy. The government has also put forward a series of proposals to amend the UK data protection framework, including aspects that relate to AI.[footnote 10] Other governments around the world have issued similar policy blueprints, including France, Germany, Canada, the US and China. So too have international and supranational bodies, among them the Ad Hoc Committee on AI at the Council of Europe (CAHAI), which is working on a legal framework for the development, design and application of AI. The European Commission, meanwhile, has proposed its own Artificial Intelligence Act, which - as presently conceived - would introduce new rules such as mandatory ""conformity assessments"" for high-risk applications of algorithmic processing.[footnote 11] Regulators at home and overseas have also begun to examine the impact of algorithmic processing, with some issuing new directives to support the responsible use of algorithms in their sectors: The rest of this paper should be read with this wider context in mind. Indeed, there is much that DRCF members can learn from the research and experiences of other regulators and policymakers, particularly where they have successfully addressed the harms we outline in the following pages. However, this paper is unique in that it captures perspectives from 4 different digital regulators. The production of this discussion paper involved 3 stages. As digital regulation is a complex landscape that cuts across the remit of various regulators, it is important to provide clarity on the priorities that will guide the DRCF's work in this area. To do this the DRCF convened a series of internal workshops which led us to identify the following high-level regulatory priorities: In the second stage, DRCF members grouped a range of algorithmic harms and benefits into several shared areas of focus that are of mutual interest. The identification of these areas helped us to consider if and where a co-regulatory, collaborative approach may be effective in mitigating the potential harms arising from algorithmic processing. It should be noted that we see accountability as an overarching concept that is a key motivator for DRCF members. It was not specifically identified as a shared focus area since it is fundamental to all the areas, particularly transparency. The main areas of focus are: The relationship between the working group priorities and shared areas of focus is shown in the chart on the following page. Each area is explained in detail inCurrent and Potential Harms of Algorithmic Processing In the third stage, DRCF members produced a summary report to share with stakeholders from academia, civil society, government, industry, public sector and consumer groups, as well as a list of questions for their consideration. Stakeholders provided feedback in a series of bilateral engagements with DRCF members. This discussion paper reflects the DRCF's foundational thinking in the first 2 stages and the feedback we received from the list of stakeholders in the third stage. Stakeholders generally agreed that the working group priorities and 6 shared areas of focus were important. In addition, several other potential shared areas of focus were suggested which are discussed later in the paper. Working Group Priorities and Shared areas of focus in algorithmic processing systems In this section we explain each of the shared areas and their importance. We then give examples of harms that can occur within each of these areas, however we acknowledge that some of these harms can affect more than one area. Transparency refers to the act of providing information about how and where algorithmic processing takes place. This information could relate to the technical features of the algorithm, including the data used to train it and the type of outputs it generates. Or the information could relate to the wider context in which the algorithm is deployed, such as the protocols and procedures that govern its use, whether it is overseen by a human operator, and whether there are any mechanisms through which people can seek redress. Transparency serves a number of purposes: The areas in which algorithmic processing should be transparent, include: Algorithmic processing often involves multiple parties, each playing a different role in the journey from the creation of an algorithm through to its deployment. One party may collect data, another may label and clean it, and another still may use it to train an algorithm. There is concern that the number of players involved in algorithmic supply chains is leading to confusion over who is accountable for their proper development and use. A study looking at business-to-business AI services, for example, found that the roles of data ""processor"" and ""controller"" as expressed in data protection legislation are not always clearly identified, meaning those building, selling and using algorithms may not be fulfilling their obligations under the UK GDPR.[footnote 31]This confusion also means that citizens and consumers are left unsure of where they should turn for support in cases where they feel algorithmic processing is being misused. Stakeholders were particularly concerned about the potential for confusion where organisations purchase algorithms ""off the shelf"", and stressed that developers and deployers must be clear on their responsibilities at the point of procurement. Vendors of algorithmic systems should also inform customers of the limitations and risks associated with their products. While there is often a lack of transparency regarding who is accountable for the outcomes of algorithmic processing, on occasion there is also a lack of transparency about the very use of those algorithms. So-called ""invisible processing""[footnote 32]describes circumstances where personal data is obtained and processed without the direct participation or knowledge of individuals. Indeed, there are many reported cases of personal data being collected for one purpose but then being used for another. The ICO, for example, has taken enforcement action against a number of credit reference agencies, which were processing customer data for purposes that were beyond those originally agreed, including to build marketing products that help commercial firms predict people's ability to afford different goods and services.[footnote 33]The ICO has also identified invisible processing in the advertising technology (adtech) ecosystem, where personal data (including behavioural data) has been used to build intricate profiles of internet users, often without their knowledge.[footnote 34] While data subjects may technically provide consent for the reuse of their data, they may not always understand what this means in practice (hence it is not informed consent). As well as being potentially unfair, this lack of transparency makes it more difficult for individuals to exercise their rights in relation to the processing of their personal data. Under UK GDPR, this includes the right to rectify any errors in personal data, the right for personal data to be erased (also known as the right to be ""forgotten""), the right to obtain and reuse personal data for their own purposes, and the right to object to the processing of data under certain circumstances. Without knowing that an algorithm is processing their personal data, individuals are also unable to seek redress for any harms that may have occurred as a result of that processing. They may not even be aware that they are being harmed. For example, those who face unlawful discrimination by a pricing algorithm may not realise they are paying a higher price than someone with similar circumstances in a different demographic group (for example a customer of one ethnicity paying more than a customer of another). Even when individuals are aware that their personal data is being processed, and have made a decision to raise a complaint, they may not know where to turn to begin this process. While the ICO allows people to raise concerns about how an organisation is using their data, the stakeholders we spoke with suggested that public awareness of this option was low. Some stakeholders also believed that stronger measures of redress were required, such as introducing an easier route for people to seek financial compensation where their data protection rights have been infringed, in a manner akin to the small claims court system. Others we spoke with emphasised the need to lower the cost to civil society actors and private individuals of bringing legal action against those misusing algorithmic systems. In general, it is important that regulators work together to simplify the process of raising complaints, enabling people to seek redress without having to navigate separate regulatory systems. In some cases, it is not enough simply to know that an algorithm is present and is processing data. It may also be important to understand how that algorithm has arrived at a particular decision or output (for example to know why someone has received a poor credit score, or why a photo posted on social media has been flagged as inappropriate). Indeed, the ability of individuals to have access to the 'logic' of a system is a requirement under UK data protection law for solely automated decisions that significantly affect them (with certain exceptions). However, the complexity and dynamic nature of some algorithmic systems - particularly those developed using machine learning techniques - can make it difficult to acquire an explanation. By definition, machine learning algorithms are not programmed by human hand but rather learn from data, which can result in models that are difficult to interrogate. This in turn makes it harder for individuals and consumers to understand why an algorithm has made the recommendation or decision it has, and therefore what they should do differently in future to achieve a different result. It also makes it more difficult for those interpreting the results of an algorithm - for example a social media content moderator - to properly act on its outputs, which in turn undermines the quality of decision-making. This is especially the case where operators lack technical expertise. Two of the academic stakeholders we spoke with stressed the importance of ""justifiability"" in the context of an explanation - the idea that those on the receiving end of an algorithmic decision or output should understand the rationale behind that decision, as well as why it was appropriate to use an algorithm in that context. A lack of transparency regarding where and how algorithmic systems operate can also lead to harmful behaviour in a population. Members of the public may not know, for example, that what they are seeing and reading online is in fact produced by, or being recommended by, an algorithmic system, leading them to be less discerning about that content than they should be. One example is the use of algorithms by so-called troll farms to produce fake social media posts during elections - posts which are then shared between real users, facilitating the spread of disinformation. Another example is the use of algorithms to facilitate high-speed trading, which can result in ""herding"" behaviour where individual traders unknowingly mimic the actions of automated trading tools. This can in turn lead to erratic and unstable movements in financial markets. Another issue falling under the banner of algorithmic transparency is the production of ""synthetic media"". Synthetic media describes audio and visual content that either replicates the behaviours and characteristics of real people, or which alters how real people and environments are presented.[footnote 35]This type of content has long been used in the entertainment industry, including to enhance films in post-production, however there are growing concerns that it is now being used to deliberately mislead the public, who are unaware that the content is fabricated by an algorithmic system. A number of stakeholders flagged the risks posed by ""deepfake"" videos on social media, which falsely portray individuals as doing and saying things that are embarrassing, offensive, or in some other way inappropriate. As well as damaging personal reputations[footnote 36], synthetic media also risks undermining user trust in online content of all kinds, making it more difficult for the public to distinguish what is true from what is false.[footnote 37]This in turn could undermine democratic institutions, including news outlets and criminal and civil courts that rely on audio, visual and text-based media as evidence.[footnote 38] For algorithmic systems to win the trust of consumers and citizens, they need to be shown as operating fairly. To some, fairness means that people experience the same outcomes, while to others it means that people are treated in the same way, even if that results in different outcomes for different groups. What counts as ""fair"" in the context of algorithmic processing varies from context to context, and can even vary within a single industry.[footnote 39]However, fairness is not just a subjective ethical value, it is also a legal requirement. The UK GDPR for example mandates that organisations only process personal data fairly and in a transparent manner. Separately, the Equality Act prohibits organisations from discriminating against people on the basis of protected characteristics, including in cases where they are subject to algorithmic processing. The Consumer Rights Act, meanwhile, includes a ""fairness test"", whereby a contract term will be unfair if ""contrary to the requirement of good faith, it causes a significant imbalance in the parties' rights and obligations to the detriment of the consumer"". This applies to contracts between traders and consumers, including those which involve algorithmic processing. With a small number of exceptions, most observers agree it is unfair to discriminate against people on the basis of sensitive characteristics, such as their socio-economic status or accent.[footnote 40]Indeed, discrimination on the basis of protected characteristics (for example age, sexual orientation or race) is prohibited in specific contexts, such as employment or education, under the Equality Act.[footnote 41]It is therefore concerning that a number of algorithmic systems have been shown to produce biased or discriminatory results, from facial recognition technology that is better at recognising male and white faces,[footnote 42]to recruitment screening software that penalises job applications from female candidates.[footnote 43]Researchers have differentiated between 2 main categories of harm caused by biased algorithms: allocative and representational.[footnote 44]Allocative harms are those where particular groups are denied access to important goods and services. Representational harms occur when systems reinforce the subordination of groups through stereotyping, under-representation, and denigration. Few of those who build and use algorithms deliberately set out to unfairly discriminate against people. However, there are many ways that bias can be inadvertently embedded within algorithms. One of these is by using training data that reflects historical bias. For example, if a predictive policing model is trained on the arrest data of police forces that have historically discriminated against black residents, that model is likely to reproduce those same biases in its patrol recommendations. These historical biases can also result in feedback loops, with biased models leading to biased outcomes, which are subsequently fed back into model training exercises. Other sources of bias include model optimisation, human interpretation, and even how a problem has been framed.[footnote 45]It is important to emphasise that those deploying algorithms do not need to intend to discriminate for their conduct to be unlawful. Those building and deploying algorithms often try to address bias by removing information about sensitive characteristics from their data (a technique known as ""fairness through unawareness""). However, other information can act as a proxy for sensitive or protected characteristics, such as postcode acting as a proxy for ethnicity because of the correlation between those 2 variables. That means that depending on the context, simply removing sensitive or protected characteristics may not be the solution. These proxies as correlations are conceptually different to proxies intentionally used in model design when what you want to measure is not observable. For instance, in order to measure individuals' risk of re-offending (unobserved quality), developers building recidivism models often use a score based on past arrests as a proxy since that has been recorded. The validity-reliability of these proxies for unobserved information in model design can affect the fairness of the outcome. A good example of how this can play out unexpectedly comes from the US healthcare system, where an algorithm used to refer patients to specialist healthcare programmes was recently found to systematically discriminate against black people.[footnote 46]The researchers investigating the algorithm found that healthcare costs accrued in a year were being used as a proxy for patient risk scores that would inform referral decisions. However, because healthcare costs were on average lower for black people than for white people with the same chronic conditions, black patients were less likely to be referred to specialist care than white patients, despite having the same support needs. In some situations, however, there may be an operational need to use data points that happen to also correlate with sensitive attributes when building and running an algorithm. To take another example from the insurance industry, car engine size is known to be correlated with gender, yet it is also a material factor in determining the premiums of customers, given that larger engines are more costly to replace and that they result in more serious incidents.[footnote 47]Organisations may benefit from regulatory guidance to understand what counts as a legitimate use of proxy data, particularly in circumstances where they are under an obligation to treat their stakeholders fairly. This is the case, for instance, in the financial services industry, where the FCA has asked firms to demonstrate that ""fair treatment of customers is at the heart of their business model"".[footnote 48] In addition to these issues of demographic discrimination, several stakeholders highlighted how algorithmic processing could be used to discriminate against people on the basis of their purchasing power or willingness to pay. ""Price personalisation"" is not a new activity; many types of business have long attempted to set prices according to what individual customers are willing and able to pay, from cars to holidays, to household goods. However, algorithmic processing could amplify the ability of sellers to predict what people are willing to pay, drawing on data about those individuals which has not previously been available. In theory, this type of practice could be described as fair, since it may lead to lower income customers paying less and higher income customers paying more, possibly resulting in more people being able to access those goods and services. This practice may not be perceived as fair across the board, however. Additionally, others may view this practice as inherently unfair regardless of the outcome, as it would mean sellers are scrutinising the behaviour and characteristics of buyers without their knowledge.[footnote 49] Another reason personalisation might be considered unfair is because it could result in people being penalised for circumstances outside of their control. People living in less affluent areas, for example, may be more likely to be the victim of a burglary, and therefore could face higher premiums for their home insurance - a pricing practice that one of our stakeholders described as a ""poverty premium"".[footnote 50]This example also highlights the difficulty of defining fairness, as it could also be argued that the practice is fair with regards to the insurer in terms of increased premium for increased risk. Less affluent and more vulnerable consumers may also be unaware that some businesses engage in this type of pricing strategy, leaving them more exposed to its effects. Indeed, qualitative research undertaken by Ofcom in 2020 found that participants had very limited awareness and knowledge of personalised pricing, which is consistent with findings in the wider literature, that many consumers are surprised that their online behaviour might influence the prices they are offered for products and services.[footnote 51]The study also found that, with the exception of lower prices for low-income households, consumers were sceptical of the benefits of personalisation, with some saying that the practice was ""disempowering"". By its nature personalised pricing is difficult to spot, and the extent and nature of this practice outside insurance and credit markets is not clearly understood. Another practice that can sometimes result in unfair outcomes is the repurposing of algorithms. While some organisations are able to develop bespoke models that are attuned to their specific needs, others must rely on ""off the shelf"" systems purchased from third party vendors, which may have been trained in a very different context. These models can suffer from lower accuracy levels, and may harm individuals whose data is being analysed.[footnote 52]For example, an algorithm that has been developed to identify hate speech in one region of the world is likely to perform worse when deployed in another region, owing to differences in common parlance and cultural attitudes. The dangers of repurposing algorithms have also been well documented in the world of healthcare, where hospitals (notably in the US) have unsuccessfully sought to export their in-house diagnostic models to other settings.[footnote 53]Those procuring algorithms may be able to work with vendors to retrain systems to suit their own context, however this depends on the buyer having sufficient resources and bargaining power. From targeted job adverts to recommendation systems on social media sites, algorithmic processing is transforming how consumers and citizens access information, products and services online. By enabling a degree of personalisation, they are helping people to both seek out and be exposed to content and opportunities that are more relevant to their interests and appropriate for their needs. However, algorithms also pose several risks in this context, potentially closing people off from alternative viewpoints, as well as depriving some groups from seeing valuable economic opportunities. The use of algorithms to target content online, particularly on social media platforms, could result in internet users being repeatedly exposed to the same type of information. As has been well documented, many of today's platforms deploy sophisticated recommendation algorithms, which adapt as they learn more about the type of content users tend to engage with. In many cases, this results in users being presented with more of the same innocuous content, such as a favourite TV show or a friend's social media posts. However, in other cases this type of targeting can result in people being repeatedly shown content that is misleading or damaging, such as antivaxx conspiracy theories, or even violent content. Toxic online environments, polarisation or online aggressive behaviour may result from exposing internet users to this kind of emotionally charged content, potentially leading to physical harm in the real world.[footnote 54]This phenomenon can affect both individuals (for example if a person acts on misleading health information) and society (for example with so-called online echo chambers fostering political and cultural polarisation). As well as changing the type of content people see online, algorithms can also shape the economic opportunities that internet users are exposed to. Many businesses today use targeted adverts to channel their products and services at desired audiences, saving them time and money, and benefiting consumers who want to see those goods. However, not everyone who has an interest in seeing those adverts is shown them. Researchers from Northeastern University, for example, ran an experiment on Facebook in 2019 which suggested that some online adverts for housing opportunities were being shown to black and white users at differing levels of frequency.[footnote 55]A separate study, also looking at Facebook, found that online job adverts for STEM careers were less frequently displayed to women.[footnote 56]The researchers hypothesised that this was partly because the underlying algorithms were designed to optimise for cost, and women tend to be more costly to advertise to (in part because they are seen as more likely to make a purchase). This is linked to how algorithmic processing can lead to unfair outcomes for some demographic groups, as was explained in [Fairness for individuals affected by algorithmic processing](https://www.gov.uk/government/publications/findings-from-the-drcf-algorithmic-processing-workstream-spring-2022/the-benefits-and-harms-of-algorithms-a-shared-perspective-from-the-four-digital-regulators#Fairness-for-individuals-affected-by-algorithmic-processing] Resilience refers to the capability of algorithmic systems to withstand shocks and perform consistently when exposed to different conditions. This includes being able to cope with adversarial attacks, such as when bad actors seek to ""poison"" datasets or extract personal information from an organisation's training datasets. Algorithms can themselves be weaponised in order to inflict damage, for example by automating spear phishing[footnote 57]activity and scaling up denial of service (DoS) operations. These are issues of concern to all DRCF members. As algorithmic processing has become more important to the functioning of public services and industry, so too has it become an increasingly attractive target for those eager to cause disruption. There are many ways that algorithmic systems can be undermined. One of these is by poisoning training data, resulting in models with lower levels of accuracy. Cyber criminals could, for example, seek to corrupt the training data used to build a bank's fraud detection model, making it less likely that fraudulent activity is noticed. Another way criminals can wrongfoot algorithmic systems is by deploying ""adversarial examples"".[footnote 58]This is where inputs to a model are deliberately manipulated in order to be misclassified or unrecognised, even if that manipulation is imperceptible to the human eye. Terrorist organisations, for instance, could try to evade the content moderation algorithms of social media platforms by making minute changes to the pixel patterns of their images and videos. While these are cases of algorithms being manipulated in order to cause mistakes, cybersecurity experts have also highlighted how algorithms can be manipulated in order to leak sensitive information.[footnote 59]One practice of particular concern is ""model inversion"", where personal information can be inferred about individuals who are featured in training datasets. A report from the US-based Center for Security and Emerging Technology highlights the example of a facial recognition system, where a model's attackers start with a randomly generated image of a face, and make repeated edits to that image until they arrive at a version that the model matches to the name of their target individual.[footnote 60] Stakeholders also raised more general concerns about the ability of organisations to safely manage the data they collect to train and run algorithmic systems. Despite the secure processing of personal data being a key principle under the UK GDPR[footnote 61], a DCMS Cyber Security Breaches Survey produced in 2021 found that 4 in 10 businesses experienced a ""cyber security breach or attack"" in the last 12 months.[footnote 62]The survey also suggested that businesses found it harder to implement cyber security measures during the pandemic, with fewer businesses now deploying security monitoring tools or undertaking any form of user monitoring than was the case a year ago. Just as bad actors can seek to undermine algorithmic systems, so too can they weaponise them for their own purposes. A number of cyber security experts have documented how machine learning algorithms can be used to scale up criminal activity online.[footnote 63]This includes by automating and improving the quality of spear phishing attacks, which are personalised messages designed to extract information or money from their victims. In a recent experiment in Singapore, researchers from the Government Technology Agency used a deep learning natural language model in conjunction with other AI-as-a-service tools to craft bespoke phishing emails tailored to people's backgrounds and personality traits.[footnote 64]Sending these emails to colleagues at the Government Technology Agency as an experiment, the researchers say they were impressed by the quality of the synthetic messages and the rate of click-throughs they were able to generate, when compared to messages that were drafted by humans. As well as scaling up existing threats, algorithms could be used to introduce new ones. In a first of its kind incident, it was reported in 2019 that fraudsters used deepfake technology to mimic the voice of a senior executive from a German energy company, allowing them to request a transfer of several hundred thousand pounds from its UK subsidiary.[footnote 65]A report produced by a consortium of organisations including the Universities of Oxford and Cambridge predicts that novel cyber threats such as these will continue to emerge over the coming years.[footnote 66]This includes the use of algorithms to predict which individuals are most likely to respond to scams, thereby improving target selection.[footnote 67]The report also argues that the advent of machine learning tools is lowering the barriers to entry for cyber criminals, for instance by enabling attackers to perform phishing attacks in multiple languages with little additional effort required. These developments raise questions about the future resilience of traditional cyber security tools used by organisations. Individual autonomy is about citizens and consumers having control over their lives, which includes being able to make informed choices about what they buy, the media they consume, and the people they interact with online. As we have already seen, algorithmic processing is allowing firms to target information, products and services with increasing precision, as well as to build more sophisticated ""choice architectures"" that determine how options are presented to users online. These practices offer tremendous benefits, yet when deployed inappropriately they can also undermine people's autonomy, encouraging them to do things, buy things and believe things that are damaging to themselves and/or wider society. They can also impact on people's freedom to determine their identity, including how they choose to present themselves to the world.[footnote 68]Vulnerable people are especially exposed to these risks. Targeting (for example via the use of recommender systems) is essential in helping people to navigate the large volume of content online; without it, it would be impossible for search engines to function or for music and video streaming sites to serve up the content we want to see and hear. Yet targeting can sometimes err into manipulation, resulting in people making decisions and altering their beliefs in a way they would otherwise not, given time, space and more options at their disposal. The Centre for Data Ethics and Innovation's Online Targeting Review argued that targeting, when left unchecked, could exploit people's impulses and emotions. The CDEI also expressed concern that targeting could be a driver of ""internet addiction"", with recommender systems being designed to maximise endless engagement and clicks. Several stakeholders suggested these risks were greater when people had a ""false sense of control"" over their online interactions, since those who are unaware that targeting is taking place are also less likely to scrutinise what they are seeing and why. Stakeholders also expressed concern that algorithmic targeting may be making consumer preferences more 'sticky'. At any one point in time, consumers will have an affinity for a particular set of brands, products and services, which would typically be expected to change over time as societal tastes evolve, new goods arrive on the market, and brands launch new advertising campaigns. However, algorithmic recommendations (for example those provided via search results or targeted adverts) may serve to limit people's exposure to alternative goods, potentially hardening their preferences. The extent to which people are aware of this practice, consent to it and can escape it in order to access wider options, will determine how much it impacts on individual autonomy. Some groups in society are particularly vulnerable to the effects of targeting. This includes children, older people, people with learning disabilities, and people with addictions. An investigation by the Gambling Commission, for instance, found that 45% of online gamblers were encouraged to spend money on gambling activity due to the adverts they saw.[footnote 69]While there is no formula for determining what types of targeting are harmful, it is clear that user manipulation is a present risk online and one regulators will need to pay close attention to. Given the technical sophistication of some algorithms and the fact they may be deployed behind the scenes in ways that individuals affected may not appreciate, conceptions of who is 'vulnerable' in this context may be broader than when thinking about vulnerability in other regulatory dimensions. A broader grouping of online practices that can undermine the autonomy of citizens and consumers is the use of harmful ""choice architectures"".[footnote 70]User experience and interaction designers, content designers and marketers can be thought of as choice architects, and the design of the environment they create is the choice architecture.[footnote 71]Common examples of choice architecture include how products are ordered in search results, browsing buttons available to users on social media platforms, the number of steps needed to cancel a subscription, or whether an application is selected by default for tasks on mobile devices. Choice architecture is a neutral term. A well-designed website, app or digital service built with users' interests in mind will help consumers choose between suitable products, make transactions faster, and result in suggestions for more relevant products and services. Websites and platforms often spend significant time and resources refining their choice architectures, resulting in a better user experience and reduced friction for users. However, a CMA study identified that firms can design their user interfaces utilising algorithms in a manner that goes against users' interests by exploiting their innate biases, such as loss aversion, inertia and their tendency to choose default options.[footnote 72]Some websites, for example, present consumers with potentially misleading scarcity messages, which aim to convince them that there is only so much time to buy a particular product, or that there is more limited stock than there is in reality.[footnote 73]Furthermore, both search algorithms and personalisation underpinned by algorithms can drive the choice architecture encountered by users.[footnote 74] The CMA has also highlighted the practice of firms using algorithms to predict the likely rating that users would give to their service. Makers of apps, for example, have been shown to use algorithms to determine when users are more likely to leave positive reviews - a tactic that some fear is leading to ""ratings inflation"".[footnote 75] Researchers have coined new phrases to describe particularly harmful forms of choice architecture. These include: dark patterns,[footnote 76]a set of (deliberate) manipulative practices identified by user experience (UX) designers; sludge[footnote 77], which makes it hard for consumers to take action in their interests; and dark nudges[footnote 78], which make it easy for consumers to take action that is not in their interests. Dark patterns have also been observed in the ""consent management platforms"" that are used by websites and apps to acquire consent from internet users to collect, share and sell their personal data. Some of these practices are unlikely to be compliant with data protection regulation. Analysis of choice architecture is already central to some areas of regulatory intervention. For example, qualitative and quantitative analysis of the choice architecture of default applications are key parts of the CMA's recent interim report on mobile eco-systems.[footnote 79]As we go on to discuss in the section below, choice architecture is also highly relevant to the control of personal data. The complexity of algorithmic systems' supply chains (including data collection and annotation) and how they operate across domains may lead to some loss of user control in how and where personal data is shared. In its Online Platforms and Digital Advertising Market Study, the CMA recommended that the new Digital Markets Unit (DMU) has the power to compel platforms to give consumers more control over their data.[footnote 80]Under this arrangement, the DMU would have the ability to introduce a ""choice requirement"", which would require platforms to give consumers the choice to receive non-personalised advertising;[footnote 81]as well as to introduce a ""fairness by design"" duty, which would set out for platforms the choice architecture they should utilise to present effective choices to consumers. Furthermore, the CMA recommended the government consider giving the DMU powers to ask platforms to trial how such choices are presented to consumers given this is a complex area where unintended impacts are possible. The government is currently consulting on empowering the CMA to order trialing of potential remedies in the course of a Market Investigation Reference (MIR),[footnote 82]as well as giving similar powers to the Digital Markets Unit which could support its approach to implementing codes of conduct and pro-competitive interventions.[footnote 83] Strong competition helps to push down costs and prices, drive up service standards and quality, and increase access to products and services. It also creates incentives for innovation, productivity and economic growth. Effective competition means that markets are open to new firms that can offer better deals and products, while firms that cannot keep up either have to change or go out of business. Promoting competition is a priority statutory objective shared by the FCA, Ofcom and the CMA. The ICO is committed to supporting innovation and economic growth which is one aspect of competition.[footnote 84] Recommender systems or ranking systems may be designed to promote a platform's own products, content or services above those of its competitors. Self-preferencing can also occur where companies exploit default effects or saliency, such as where their own products and services are shown as the default option to consumers, rather than in a list of options. The CMA's 2021 report on algorithms outlined this issue and the risks it poses to competition.[footnote 85] Where own-brand content is recommended on video-on-demand services, there is a risk that the diversity of content is reduced for viewers, and public service material, for example, may become less prominent in search results. Algorithms that are used for information retrieval and ranking in search engines may be designed to up-rank certain sponsored links and own-brand content. Some users may be unaware of this preferencing and be unwittingly steered towards products or services that are more profitable to the company. A good example of where this practice has been shown to play out is the online hotel booking industry, where an investigation by the CMA between 2017 to 19 found that search results on some booking sites were affected by the amount of commission a hotel pays to the site.[footnote 86]Such practices may in turn impede competition, for example the ability of challengers to compete in concentrated markets such as search engines, as well as fairness concerns for consumers. Algorithms and the infrastructure around them are evolving and becoming increasingly complex, often with a multitude of interacting components, which could make it hard to explain or reverse engineer the output. Interconnectedness of algorithms developed by multiple organizations can also pose a risk. They could propagate and amplify issues within a system and make it challenging to isolate the root cause(s). For example, the ""Flash Crash""[footnote 87]on 6 May 2010 has highlighted the risks of automated algorithmic trading.[footnote 88] Some developers have also highlighted the challenges of integrating dynamic machine learning models with software that has been programmed using conventional methods. This is because the behaviour of the ML model will change as it is re-trained, which can cause issues with downstream applications. Connected algorithmic processing systems could also facilitate collusion and lead to higher prices for consumers. A firm might develop a certain part of their product or service in-house and source other algorithmic components from third parties, for example to set prices, through which they could exchange information. There are concerns that algorithms could also lead to new forms of tacit collusion - where there is no explicit agreement between businesses to collude, but where pricing algorithms effectively deliver the same result.[footnote 89]At the extreme end, pricing algorithms drawing on ML technology could autonomously learn to collude.[footnote 90]They can be used to automatically detect and respond to price deviations by competitors, which could make explicit collusion between firms more stable, as there is less incentive for those involved to cheat or defect from the cartel. An example of this was the CMA's Trod/GB eye decision in the online posters markets, where the parties agreed that they would not undercut each other's prices for posters and frames sold on Amazon's UK website. They implemented the agreement by using automated repricing software that they each configured to give effect to the illegal cartel.[footnote 91]A possible avenue to address concerns about autonomous learning by algorithms may be increased transparency from businesses, both around pricing behaviour and their rationale for using price matching algorithms. Online platforms and search engines collect individuals' data to train their algorithmic systems, allowing content to be personalised to user interests and needs. This personalisation of content can in turn drive more engagement on those platforms and engines, resulting in the collection of even more personal data with which to further refine their algorithms. This dynamic leads to network effects, with these products or services gaining additional value as more people use them. While this is in one sense a consequence of an effective and attractive service offering, it can also result in barriers to new entrants, who often lack the necessary user data to train comparable algorithmic systems. A study undertaken by the CMA found that this was a particular barrier to new entrants in digital advertising, with Google and Facebook benefiting from rich data sources that are well beyond those available to smaller companies in this market.[footnote 93]For example, a dominant search engine can use its volume and variety of activity to develop a deeper understanding of consumer interests than a competitor with lower market share. This allows the engine to provide more effective search advertising services as well as opportunities for advertisers to target niche search terms. Additionally, mass personal data collection also potentially violates the principle in UK data protection law that requires firms to minimise the amount of personal data they collect. AI development has exacerbated the issue because it creates a heightened demand for data, including personal data. Organisations with data power accumulate granular information on individuals across their online journey that they then use to personalise their offerings, which can exacerbate information asymmetry between consumers and service providers. In addition to the 6 shared areas discussed above, stakeholders suggested the following 3 topics for the DRCF to consider. Stakeholders drew attention to the important role played by human practitioners who operate and interpret the results of algorithmic systems. These practitioners - who range from social media content moderators to the employees of financial services firms - are often seen as providing an additional line of defence against the potential harms that might be caused by algorithms. Applying common sense and more contextual knowledge, they may be able to spot, for example, where a social media post has been mistakenly flagged as containing hate speech, or where a financial transaction has been wrongly interpreted as being fraudulent. However, a growing number of commentators are cautioning against viewing human involvement as a foolproof safeguard. Specialists in human computer interaction (HCI) have highlighted the problem of ""automation bias"", where practitioners uncritically accept the recommended decision of an algorithm, rather than meaningfully engage with that output.[footnote 94]This is a concern the ICO has also identified in its AI guidance, as data protection law prohibits solely automated decisions that significantly impact individuals without a meaningful human review. Practitioners could also become distracted while in command of a system, or be unable to interpret its technical outputs, for example the different types of statistical accuracy indicators that a facial recognition model might produce when it flags a positive match. For these reasons, it is important that users of algorithmic systems implement a wider set of oversight and governance arrangements. This includes establishing effective decision-making procedures for approving new systems, as well as regularly reviewing the accuracy of those systems once they are live. There is intense ongoing debate about the potential impact of algorithmic systems including AI and machine learning on climate change. There are several ways in which AI can help with reducing climate change, however the computational resources required for developing and maintaining this technology can also have a negative impact. Research shows that AI may act as an enabler on 134 targets (79%) across all Sustainable Development Goals developed by United Nations (UN).[footnote 95]For example, machine learning could help optimize energy supply and demand in real time, with increased efficiency. AI can also help retailers reduce their environmental footprint through waste reduction, better optimization of their supply chain to improve how they respond to market demands.[footnote 96]At a consumer level, algorithms can play a positive role by helping users make sustainable choices. The potential of AI to combat climate change is an active topic of research that has been explored by various organisations.[footnote 97]Several agencies are calling upon governments to develop appropriate policies to tap into the full potential of these technologies.[footnote 98] But despite AI's promise, research suggests 35%[footnote 99]of targets across all Sustainable Development Goals may experience a negative impact from its development.[footnote 100]Algorithmic systems, especially advanced ML systems, require very high computational resources, particularly in their training and development phases. For example, research estimated that the carbon footprint of training a single big Natural Language Processing (NLP) model is equal to around 300,000 kg of carbon dioxide emission.[footnote 101] Stakeholders told us that regulators should pay close attention to the way organisations handle data, given that the quality of data is a major determinant in shaping how an algorithmic system performs. Incomplete or outdated training datasets, for example, are likely to result in poorly performing models. Unrepresentative datasets, meanwhile, could result in models that are less accurate when processing the data of particular demographic groups, whether that is for the purposes of screening CVs or targeting consumer adverts. A recent business survey undertaken by Ipsos MORI for the Centre for Data Ethics and Innovation revealed that 23% of UK firms using ""AI and data-driven technology"" saw challenges in accessing quality data.[footnote 102]Of these, 74% cited the problem of collating data from fragmented data sources, while 32% said there was a lack of historical data available on which to train their systems. Even when organisations have access to high quality data, they may not be aware of how to store that data responsibly. Depending on the nature of the data, good data governance may mean storing data in a standardised format, creating metadata to ensure other users understand how it should be used, putting security controls around who has access to that data, and keeping a record of who is using that data and how. Some organisations have taken to creating ""data catalogues"" to monitor their data inventory and ensure its proper use. In the same CDEI-Ipsos MORI survey, 86% of firms who use AI and data-driven technology said they felt able to ""store and manage data responsibly through well-defined governance and data protection protocols"".[footnote 103]While this is reassuring, the survey also identifies room for improvement in several areas. This includes the ability of firms to handle unstructured data (for example videos and images), with only 45% of respondents saying they do this well. The UK government has documented and sought to address a number of these issues in its National Data Strategy, which highlights the recent creation of a government Data Quality Hub to promote best practice methods for maintaining data quality.[footnote 104]Effective algorithmic auditing may also be a way to help address these issues in some settings, with auditors looking not just at how algorithms perform but also how organisations are managing the datasets that underpin them. Auditing of this nature could potentially serve a related purpose of assuring that datasets have been developed responsibly, for example that the data they contain has been labelled by individuals who have been adequately compensated for their time. Algorithmic processing has the potential to bring about huge, positive impacts on people's lives. Some examples include: In this section we provide a discussion about how algorithmic processing can provide benefits within our 6 shared focus areas, both now and in the near future. Algorithms are often discussed as being difficult (or impossible) to interpret or explain, especially when more complex machine learning such as neural networks or deep learning are used. However, algorithms themselves can sometimes be used to assist in creating valuable interpretations and explanations. There is growing interest in 'counterfactual algorithms' that can generate explanations based on what could have happened if the input of a model was different.[footnote 110]For example, an algorithm could inform an individual who had been rejected for a loan that if their income was higher, or their level of debt was lower, that their loan application would have been accepted. Understanding how to achieve a better decision can help foster greater trust. Algorithms can also be used for dimension reduction in models,[footnote 111]removing excessive and irrelevant features from machine learning models and thus making them simpler and potentially more explainable and interpretable. This could lead to better human-computer interactions and making AI accessible to a broader audience. Future benefits could see algorithms assist with better decision-making. New developments in interpretable AI and visualisation of AI are making it easier for human experts to put complex data together to draw actionable insights. For example, in a medical research context, AI-assisted summarisation could one day help clinicians see the most important information and patterns about a patient leading to better treatment.[footnote 112] Algorithms can also be used to detect bias and discrimination. Some suggest that models and so-called 'causal graphs' (graphical representations of the causal relationship between features and outputs) can be used to detect and explain the causal pathways that lead to potential unfairness.[footnote 113]Research in this area is evolving and tools are being developed to assist in detecting unfair bias and discriminatory decision-making. Algorithmic processing can assist in widening access to digital markets. For example, price personalisation can enable certain customers to access goods or services by lowering the price and thereby widening access.[footnote 114]Browser plug-ins enable users to control their browsing data and help them to understand how they are being tracked and what is informing the recommendations being made to them.[footnote 115]This may help empower users and increase user inclusion in online services. In credit underwriting, there may be opportunities to improve the efficiency and inclusiveness of lending if some algorithmic systems can help assess the creditworthiness of customers with limited credit histories ('thin-files').[footnote 116]There is also an opportunity to empower consumers with unique insights into their financial needs, reducing matching frictions and supporting effective decision-making. There may be ways to promote legal inclusion too, such as through automated advice - also known as robo-justice. For example, individuals can receive automated advice on whether they are eligible for legal aid.[footnote 117] As well as creating and exacerbating security risks, algorithmic processing can be used to enhance resilience of infrastructure and users to cyber threats, scams and fraud. For example, algorithms are used for triaging, monitoring and blocking spam/fraudulent activity, which supports consumers, benefits business, and helps to avert data breaches. AI can be used to flag erroneous commercial transactions, and to train systems that detect synthetic media content designed to mimic real individuals. They can be deployed in the financial markets for Anti-Money Laundering (AML) purposes, fraud detection and countering the financing of terrorism purposes. They can also assist in anti-corruption efforts; for example, Microsoft announced its Anti-Corruption Technology and Solutions initiative in late 2020, which will leverage technologies to enhance transparency and to detect and deter corruption. Early applications of this initiative have helped to bring greater transparency to how the use of COVID-19 economic stimulus funds has been spent.[footnote 118] Algorithms can be used to enhance the user experience and enable individuals to make better choices via specific design choices on social media platforms. This can be done at multiple stages of the user journey and allows users to consciously control how and when they share their personal data, or determine what they see (such as filtering results in recommender systems). Context-aware recommender systems[footnote 119]may be used to provide information and services that take into account the users' needs and context. A future trend could see greater personalisation of how individuals interact with algorithmic systems. For example, if a user is partially sighted, an algorithm could adjust the size or font of some text automatically to enable greater autonomy. Algorithmic processing can foster competition by helping customers connect with a greater number of providers, as well as helping firms to access consumers, hence reducing the barrier to entry in some markets. Search engines, for example, are algorithmic systems that allow people to find hundreds if not thousands of products that match their search terms. Price comparison websites use similar techniques to collate information and present consumers with up-to-date prices on a range of goods and services, from flights to car insurance to broadband. Algorithmic processing has also helped to power the growth of the sharing economy, including ride-hailing and home rental services. P2P platforms have opened up more choice for consumers and increased pressure on traditional industries to improve their offerings (for example with Airbnb disrupting the traditional hotel industry). There are strong indications that the increase in use of algorithmic systems will lead to economic growth and efficiency optimisation. It has been predicted that AI, for example, could deliver a 22% boost to the UK economy by 2030.[footnote 120]More economic growth, driven by algorithmic processing, could mean better incentives to invest in the sector experiencing growth. In turn, this leads to greater incentives for new organisations to enter the market, creating greater competition amongst firms. This could produce benefits for consumers as they will have more choices. Implementing algorithmic systems could also reduce the supply costs of goods and services, with savings passed on to customers in the form of lower prices. The DRCF was established to build on the strong working relationships between its members and to enhance this cooperation and the effectiveness of individual regulatory approaches, given the unique challenges posed by the regulation of digital services and products. This discussion paper illustrates some of the benefits and harms that could arise from the current use of algorithmic processing, as well as how these issues might evolve in the near future. We have integrated the views of different agencies to help firms and other stakeholders understand common concerns. Our findings suggest many areas where there is shared interest and therefore opportunities for a greater level of cooperation. We recognise the influential role we can play to shape the algorithmic processing landscape to benefit individuals, consumers, businesses, and society more broadly. The DRCF is pioneering in that it can address issues from 4 different perspectives. We can be inspired by the interventions that individual regulators have made to think of ways of collaborating in the future. Through guidance and thought leadership, we can provide greater clarity for organisations so they can confidently innovate. For example, the ICO and The Alan Turing Institute's co-badged guidance on 'Explaining Decisions Made with AI' guides organisations in ways to make their use of AI systems more transparent. As DRCF members, we may consider ways to build and expand on this to provide further clarity to the organisations we regulate to ensure they are transparent about who is accountable and what the allocation of accountability within the AI pipeline entails. We can also explore ways of clarifying the similarities and differences over the concept of transparency across the different DRCF members. A more hands-on cooperative intervention could be achieved through the increased use of regulatory sandboxes. The FCA's regulatory sandbox allows firms to test products and services in a controlled environment, and to reduce the time-to-market at potentially lower cost. The ICO is an active mentor in the FCA Digital Sandbox, and also runs its own regulatory sandbox programme on a rolling basis. These sandboxes are not exclusively open to organisations developing algorithms, although many of the entrants do use them. We could explore ways of running sandboxes where 2 or more DRCF members can (subject to their particular powers) offer advice and the ability to test products and services that use algorithmic processing in a controlled environment. As well as interventions that are targeted at organisations during the pre-deployment stages, regulators can exercise their powers to take enforcement action against actors who have not complied with the law and caused harm. Appropriate enforcement action can be a powerful tool to deter organisations from ignoring compliance issues. We can explore ways to collaborate in investigations where algorithmic processing is causing harms that span the mandate of more than one regulator. There may also be opportunities for valuable joint work on supporting individuals and consumers in seeking redress over harms they believe they have incurred. The DRCF could also establish greater consistency in the way we engage with citizens about algorithms to enable them to better understand what algorithms are, where they're used, and the choices available to consumers. This includes consistency about the language and terminology we use, as this can easily create or increase confusion. Cooperation can be wider than just between DRCF members, it can include other regulators as well as wider society. For example, engaging with the Equality and Human Rights Commission when we conduct further work on algorithmic processing and fairness. We can also engage with technology providers and professional users (for example media organisations, retail firms, and public services) to better understand how algorithmic processing takes place and how to achieve the benefits while minimising harms. Finally, not every issue that is identified in the context of algorithmic processing will require joint action from DRCF members, and regulatory approaches may well vary in important aspects, reflecting the specific regulatory context and mandate. Many potential benefits and harms related to algorithms are also context dependent and require a tailored approach from an individual regulator that is sensitive to the specifics of a particular sector. Although the 4 regulators within the DRCF have different remits, there are overlapping areas of mutual interest. The DRCF have identified the following 6 cross-cutting focus areas in the context of algorithmic processing: One aim of the DRCF is that future regulatory guidance and thought leadership in these areas is approached in a more joined up way. This approach is important for businesses - particularly in terms of guidance and standard setting. Algorithmic processing systems have the potential to deliver many benefits and harms as identified in this document. We will work together where appropriate to ensure that the harms are mitigated in a proportionate way, and help businesses to innovate so that they can realise the benefits. There was a broad set of answers when stakeholders were asked to identify which area should be prioritised: transparency received the most support with fairness and resilience coming joint second. Some stakeholders also suggested that pursuing some priorities may require balance with others: for example, the pandemic has shown that there can be perceived tensions between protecting individuals from harm and protecting individual rights. There may also be perceived tensions between the aims of competition law and data protection, although these tensions can be resolved. We believe that the ICO and CMA's joint statement provides a blueprint for how tensions or unintended effects across different types of digital regulation can be negotiated between regulators and allow synergies to emerge.[footnote 121]Where firms make ""privacy preserving"" claims in the context of defending their exclusive access to large volumes of data flows, regulators may test those claims as substantial access to data may be a source of market power. Going forward there are a number of potential areas we could focus on, and, of these, transparency and fairness have been shown to be particularly significant. Similarly, actively supporting access to redress is important, as is recognising the role DRCF members can play in helping citizens/users better understand what algorithms are, where they're used and the choices available to them. Many of the issues identified are exacerbated in situations where there are multiple parties involved in the development, deployment and use of systems, for example in AI-as-a-Service tools We have identified the following points as the key takeaways from our work. 1 . Algorithms offer many benefits for individuals and society and these benefits can increase with continued responsible innovation. Companies that innovate responsibly can use algorithms to create benefits for individuals and society in a virtuous cycle. When consumers see evidence of and/or experience benefits they trust and support firms facilitating those benefits. This can create and stimulate markets and drive economic growth. Benefits may include increased productivity; the development of tools for disadvantaged groups; and improved methods of summarising, organising and finding information and content. DRCF members could (where appropriate) work together to identify best practice in different areas of algorithmic design, testing and governance, and disseminate these lessons to help industry innovate responsibly for the benefit of all. There may also be opportunities to help businesses demonstrate compliance where they deploy algorithms, making sure this process is as simple and cost-effective as possible. 2 . Harms can occur both intentionally and inadvertently As explained in this paper, algorithmic processing can be deliberately used to inflict damage, whether that is by automating spear phishing attacks or enabling the creation of subversive deepfake content. Yet much of the harm that results from the use of algorithmic processing may be inadvertent, perhaps caused not by malice but by insufficient understanding on the part of those who deploy these systems. Some users may not appreciate, for example, that harmful bias can be embedded within algorithms, nor that some algorithms may affect vulnerable users differently to the wider population. Thus, it may not be appropriate for DRCF members to assume that organisations understand the risks of algorithmic processing, nor that they are aware of methods to mitigate those risks. DRCF members, as well as producing clear guidance and policies, could therefore look at ways of improving industry's baseline knowledge of the impact algorithms can have on individuals and society. 3 . Those procuring and/or using algorithms often know little about their origins and limitations Those purchasing algorithmic systems often do so with little knowledge of how they have been built and how they perform in different contexts. This makes it more difficult for purchasers to identify and mitigate risks (for example algorithmic bias), and to ascertain whether the systems they are using were developed responsibly (for example built with the support of data labelers who were adequately compensated for their work). DRCF members could support the development of algorithmic auditing practices, and consider appropriate minimum standards in relation to some areas of algorithmic deployment. We could, for example, set standards for third party auditors, or investigate the merits of tools like bias tests. Algorithmic auditing is the subject of another DRCF paper[footnote 122]being published alongside this one. 4 . The lack of visibility in algorithmic processing can undermine accountability Algorithmic processing may take place without the knowledge of those affected by it (for example someone rejected for a credit card may not realise their application was processed by an algorithm, just as those viewing videos on a streaming site may not realise that content has been personalised by an algorithm). In some cases this lack of transparency may make it more difficult for people to exercise their rights - including those under the GDPR. It may also mean algorithmic systems face insufficient scrutiny in some areas (for example from the public, the media and researchers). DRCF members could help organisations communicate more information to consumers about where and how algorithms are being deployed. This could mean issuing new transparency guidelines, such as the ICO's Explaining Decisions Made with AI guidance[footnote 123]or the government's algorithmic transparency standard for the use of high impact algorithms by public bodies. We could also explore the costs and benefits of ""algorithmic registers"", which serve as a public log that anyone can access. 5 . A ""human in the loop"" is not a foolproof safeguard against harms Having a human review the outcomes of an algorithmic system has been suggested by some AI commentators to be an essential safeguard, and indeed data protection law includes specific protections for individuals from being subject to decisions made by solely automated means. Yet research suggests human operators often struggle to interpret the results of algorithmic processing, with some misunderstanding the different ways that accuracy can be measured. Some also place too much faith in the effectiveness of algorithmic processing, insufficiently scrutinising their outputs (for example that of a positive match provided by a content moderation tool used by a social media platform). DRCF members could further investigate the concept a ""human in the loop"" and explore opportunities to help firms understand better the strengths and limitations of this and other approaches to risk mitigation. Appropriate human oversight and accountability will be essential to mitigate potential harms, whatever the technology deployed. DRCF members may find that further engagement with researchers in the field of ""human-computer interaction"" (HCI) is valuable in deepening collective understanding of potential issues related to human oversight and may wish to share emerging insights in this space with the industries they regulate. 6 . There are limitations to DRCF members' current understanding of the risks associated with algorithmic processing Recent years have seen a spate of innovations in algorithmic processing, from the arrival of powerful language models like GPT-3, to the proliferation of facial recognition technology in commercial and consumer apps.[footnote 124]As the number of use cases for algorithmic processing grows, so too will the number of questions concerning the impact of algorithmic processing on society. Already there are many gaps in our knowledge of this technology, with myths and misconceptions commonplace. DRCF members could conduct or commission further research on algorithmic processing where appropriate, and otherwise draw the attention of external researchers to important open questions. There may be additional opportunities to liaise with organisations funding research, like UK Research and Innovation, to help inform their funding priorities. We may also consider using futures methodologies (for example horizon scanning and scenario planning) to identify emerging trends in the development and adoption of algorithms and work through the implications of these. Having presented our view on the most prominent risks and benefits associated with algorithmic processing, we are eager to hear views from a wide range of stakeholders on these matters. The DRCF is therefore launching a call for input on the findings of this report and our related paper on algorithmic auditing. We are particularly interested in hearing the views of stakeholders on the questions set out in Annex A. The call for input will last until Wednesday 8th June. Stakeholders can submit views via email atdrcf.algorithms@cma.gov.uk. We would welcome views from stakeholders on the following questions: DRCF (2022)Auditing algorithms: the existing landscape, role of regulators and future outlook.- Ibid.- See for example ICO and CMA (2021) 'CMA-ICO Joint Statement on Competition and Data Protection Law'.- Reuters (2018), 'Amazon scraps secret AI recruiting tool that showed bias against women'. 11 October.- Taddeo, M., Tsamados, A., Cowls, J., and Floridi, L., (2021) 'Artificial intelligence and the climate emergency: Opportunities, challenges, and recommendation', One Earth, Vol 4, Issue 6, pp.776-779. 18 June.- Such powers may include analysing systems at code-level; interrogating existing policies; interviewing stakeholders; issuing fines and taking other enforcement action where they find unlawful activity involving algorithmic processing.- Real-time bidding is an automated digital auction process that allows advertisers to bid on ad space from publishers.- BBC (2020) 'A-levels: Ofqual's 'cheating' algorithm under review'. 20 August.- Office for AI, DCMS and BEIS (2021) 'National AI Strategy'. 22 September.- Department for Digital, Culture, Media and Sport (2021), 'Data: a new direction'. 10 September. See also: ICO (2021), 'ICO response to DCMS consultation ""Data: a new direction""'. 7 October.- European Commission (2021), 'Laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain union legislative acts'. 21 April ; See the ICO's response: ICO (2021), 'Proposal for a regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence act) and amending certain union legislative acts'. 6 August.- Federal Trade Commission (2020), 'Using Artificial Intelligence and Algorithms'. 8 April.- Federal Trade Commission (2018) 'FTC Hearing #7: The Competition and Consumer Protection Issues of Algorithms, Artificial Intelligence, and Predictive Analytics' 13 to 14 November.- Slaughter, R. K., Kopec, J., and Batal, M., (2021), 'Algorithms and Economic Justice: A Taxonomy of Harms and a Path Forward for the Federal Trade Commission', Yale Journal of Law and Technology, Special Publication. August.- CNIL (2018), 'Algorithms and artificial intelligence: CNIL's report on the ethical issues'. 25 May.- Office of the Australian Information Commissioner (2018), 'Guide to data analytics and the Australian Privacy Principles'. 21 March.- CMA (2021), 'Algorithms: how they can reduce competition and harm consumers'. 19 January. See also CMA (2018) 'Pricing algorithms: Economic working paper on the use of algorithms to facilitate collusion and personalised pricing'. 8 October.- ICO and The Alan Turing Institute (2020), 'Explaining decisions made with AI'. No date.- Ofcom (2019), 'Use of AI in online content moderation'. 18 July.- ICO (2020), 'Guidance on AI and data protection'. No date.- ICO (2021), 'AI and data protection risk toolkit'. No date.- ICO (2021), 'Regulatory Policy Methodology Framework'. 5 May.- The FCA has an outcomes-focused, technology neutral approach to regulation and sets clear expectations around accountability for FSMA authorised firms through the Senior Managers and Certification Regime. Accountability for the outcomes of algorithmic decisions remains with the Senior Manager accountable for the relevant business activity whatever technology is deployed. For instance, where firms use 'robo advisory' services, the Senior Manager accountable for advice would be accountable for the advice given and the end outcomes for consumers. Senior Managers should ensure that there are adequate systems and controls around the use of an algorithm.- Mueller, H., and Ostmann, F., (2020), 'AI transparency in financial services'. 18 February.- Examples include Bracke, P., Croxson K., and Jung, C. (2019) 'Explaining why the computer says no', FCA Insight, and Bono, T., Croxson, K., and Giles, A (2021) `Algorithmic fairness in Credit Scoring', Oxford Review of Economic Policy.- FCA (2018) Algorithmic Trading Compliance in Wholesale Markets.- Bank of England and FCA (2022)The AI Public-Private Forum: Final Report- The Board of the International Organization of Securities Commissions (2020): The use of artificial intelligence and machine learning by market intermediaries and asset managers: Consultation Report- OECD (2022) Public consultation on draft proposed revisions to the Recommendation on G20/OECD High-Level Principles on Financial Consumer Protection- Tsamados, A., Aggarwal, N., Cowls, J., Morley, J., Roberts, H., Taddeo, M., and Floridi, L. (2022) 'The ethics of algorithms: key problems and solutions', AI & Soc.- Cobbe, J. and Singh, J. (2021), 'Artificial Intelligence as a Service: Legal Responsibilities, Liabilities, and Policy Challenges'. Forthcoming in Computer Law & Security Review. 9 June.- ICO, 'When do we need to do a DPIA?'.- ICO (2020), 'ICO takes enforcement action against Experian after data broking investigation'.27 October.- ICO (2019) 'Update report into adtech and real time bidding'.20 June.- The Royal Society (2022), 'The online information environment: Understanding how the internet shapes people's engagement with scientific information'. January.- MIT Technology Review (2021), 'A horrifying new AI app swaps women into porn videos with a click'. 13 September.- The Royal Society (2022), 'The online information environment: Understanding how the internet shapes people's engagement with scientific information'. January. See also, Paris, B. and Donovan, J. (2019), 'Deepfakes and cheap fakes: The manipulation of audio and visual evidence'. Data and Society. 18 September.- Paris, B. and Donovan, J. (2019), 'Deepfakes and cheap fakes: The manipulation of audio and visual evidence'. Data & Society. 18 September.- Binns, R. (2018) 'Fairness in Machine Learning: Lessons from Political Philosophy'. Proceedings of Machine Learning Research. See also CDEI (2020) 'Review into bias in algorithmic decision-making'. 27 November.- For example, while the law deems it fair for insurers to discriminate against people on the basis of their age (with older drivers often paying lower premiums than younger ones), it does not allow discrimination on the basis of gender or ethnicity- UK Government (2010), 'Equality Act 2010'.1 October.- Goode, L. (2018), 'Facial recognition software is biased towards white men, researcher finds'.The Verge. 11 February.- Reuters (2018), 'Amazon scraps secret AI recruiting tool that showed bias against women'. 11 October.- Kiat, L. S. (unknown), 'Machines gone wrong'. No date.- Bias from model optimisation occurs when models are designed to take into account features (e.g. price) which result in some groups being unfavourably treated. For example, MIT and London Business School researchers found in 2018 that online job adverts for STEM careers were less frequently displayed to women, in part because the underlying algorithms were designed to optimise for cost, and women tend to be more costly to advertise to. Bias from model generalisation occurs when organisations fail to use a single model to produce reliable results from multiple groups. In healthcare, for example, symptoms and biomedical markers for some diseases (for example diabetes) can vary by ethnic group, meaning that multiple models may be required to support diagnosis in the population.- Nature (2019), 'Millions of black people affected by racial bias in health-care algorithms'. 26 October.- CDEI (2019), 'Snapshot Paper - AI and Personal Insurance'. 12 September.- FCA (2021), 'Fair treatment of customers'. 24 March. See also: FCA (2021), 'FCA to introduce new Consumer DUty to drive a fundamental shift in industry mindset'. 7 December.- CMA (2021), 'Algorithms: how they can reduce competition and harm consumers'.19 January.- See for example Pandey, A., and Caliskan, A. (2021) 'Disparate Impact of Artificial Intelligence Bias in Ridehailing Economy's Price Discrimination Algorithms'. Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. July.- Ofcom (2020), 'Personalised pricing for communications: Making data work for consumers'. 4 August.- Danks, D., and London, A.J. (2017) 'Algorithmic Bias in Autonomous Systems'. Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI 2017).- Yu, A. C. and Eng, J. (2020), 'One algorithm may not fit all: How selection bias affects machine learning performance'. RadioGraphics, 40 (7). 25 September.- Hao, K. (2021) 'How Facebook got addicted to spreading misinformation MIT Technology Review'. 11 March.- M. Ali et al. (2019), 'Discrimination through optimization: How Facebook's ad delivery can lead to skewed outcomes'. Proceedings of the ACM on Human-Computer Interaction, Volume 3, Issue CSCW, November 2019, Article No.: 199, pp 1.- Lambrecht, A and Tucker, C E (2019) 'Algorithmic Bias? An Empirical Study of Apparent Gender-Based Discrimination in the Display of STEM Career Ads'. Management Science, 65 (7). pp. 2966-2981.- Spear phishing is an email or electronic communications scam targeted towards a specific individual, organisation or business. Although often intended to steal data for malicious purposes, cybercriminals may also intend to install malware on a targeted user's computer.- Belfer Center for Science and International Affairs (2019), 'Attacking Artificial Intelligence: AI's Security Vulnerability and What Policymakers Can Do About It'. August.- ICO (2019), 'Privacy attacks on AI models'. 12 August.- CSET(2020), 'Hacking AI - A Primer for Policymakers on Machine Learning Cybersecurity'. December.- ICO, 'Guide to the UK General Data Protection Regulation (UK GDPR) - Security'- DCMS (2021), 'Cyber Security Breaches Survey 2021'. 24 March.- CSER (2018), 'The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation'. 21 February.- WIRED (2021), 'AI Wrote Better Phishing Emails Than Humans in a Recent Test'. 7 August.- WSJ (2019), 'Fraudsters Used AI to Mimic CEO's Voice in Unusual Cybercrime Case'. 30 August.- CSER (2018), 'The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation'. 21 February.- These algorithms might equally in future be used by anti-fraud agencies to identify those most likely to be targeted by fraudsters, allowing to provide advance warning to these individuals that they are at risk.- ICO (2021), 'Data protection and privacy expectations for online advertising proposals'. 25 November.- CDEI (2020), 'Online targeting: Final report and recommendations'. 4 February.- CMA (2022) 'Online Choice Architecture: How digital design can harm competition and consumers'. 5th April.- Thaler, R. H., Sunstein, C. R., & Balz, J. P. (2013), 'Choice architecture.The behavioral foundations of public policy', Princeton University Press. (pp. 428-439); Johnson, E. (2022). The Elements of Choice: Why the Way We Decide Matters. Oneworld Publications.- Competition and Markets Authority (2021), 'Algorithms: How they can Reduce Competition and Harm Consumers.' 19 January.- For example, the CMA discussed online hotel booking websites which used a combination of partitioned pricing, reference pricing, paid for ranking and scarcity claims to influence customer decision-making. Fung, S. S., Haydock, J., Moore, A., Rutt, J., Ryan, R., Walker, M., & Windle, I. (2019).Recent Developments at the CMA: 2018-2019.Review of Industrial Organization, 55(4), 579-605.- CMA (2021), 'Algorithms: How they can reduce competition and harm consumers'. 19 January.)- FT (2020), 'Apple: how app developers manipulate your mood to boost ranking'. 7 September.- The term ""dark patterns"" was coined by Harry Brignull: for examples of dark patterns, seeWhat are Dark Patterns?.- Sunstein, C. R. (2020), 'Sludge audits'. Behavioural Public Policy, 1-20.- Campione, Chiara (A.A. 2018/2019), 'The dark nudge era: Cambridge analytica, digital manipulation in politics, and the fragmentation of society'. Tesi di Laurea in Nudging: behavioral insights for regulation and public policy, Luiss Guido Carli, , Luiss Guido Carli, relatore Giacomo Sillari, pp. 55. [Bachelor's Degree Thesis] Giacomo Sillari, pp. 55. [Bachelor's Degree Thesis]- Competition and Markets Authority (2021), 'Mobile ecosystems market study'. 15 June.- Competition and Markets Authority (2019), 'Online platforms and digital advertising market study'. 3 July.- Such consumer choice has now been implemented in China. See Vernotti, C. (2022), 'Digital policy experts weigh in on China's new algorithm regulation'. Technode. 5 April.- BEIS (2021), 'Reforming Competition and Consumer Policy'. 1 October.- DCMS (2021), 'A new pro-competition regime for digital markets.' 20 July. Separately, the ICO's Age Appropriate Design Code requires that ""information society services"" set the highest privacy settings as default for child users. See: ICO (2019), 'Age-appropriate design: a code of practice for online services'.- The ICO has a statutory duty under the Deregulation Act 2015 to take into account the desirability of promoting economic growth.- CMA (2021) 'Algorithms: How they can reduce competition and harm consumers'. 19 January.- CMA (2019). 'Online hotel booking'. 13 September.- The flash crash was a United States trillion-dollar stock market crash, which lasted for approximately 360 minutes.- Buchanan, Bonnie. (2019), 'Artificial intelligence in finance'.2 April.- CMA (2021) 'Algorithms: How they can reduce competition and harm consumers'. 18 June.- HM Treasury (2019), 'Unlocking digital competition, Report of the Digital Competition Expert Panel'. 13 March.- CMA (2016), 'Online seller admits breaking competition law'. 21 July.- Lynskey, Orla (2019), 'Grappling with ""data power"": normative nudges from data protection and privacy'. Theoretical Inquiries in Law, 20 (1). 189 - 220.- CMA (2019), 'Online platforms and digital advertising market study'. 3 July.- Strauss, Stefan. 2021. ""Deep Automation Bias: How to Tackle a Wicked Problem of AI?"" Big Data and Cognitive Computing 5, no. 2: 18.- Vinuesa, R., Azizpour, H., Leite, I. et al. (2020), 'The role of artificial intelligence in achieving the Sustainable Development Goals'. Nature Communications 11, Article number 233.- Australian Retail Association(2020), 'How AI and ML can enhance sustainability for fresh retailers'. 13 January.- The Royal Society (2020), 'Digital technology and the planet Harnessing computing to achieve net zero'. December.- GPAI (2021), 'Climate change and AI: Recommendations for government action'. November.- The positive and negative impacts do not sum to 100% as AI could have both a positive and a negative impact on some of the targets depending on the scenario.- Vinuesa R., Azizpour H., Leite I. et al.(2020), 'The role of artificial intelligence in achieving the Sustainable Development Goals'. Nature Communications, 11, 233.- Emma S., Ananya G., Andrew M. (2019), 'Energy and Policy Considerations for Deep Learning in NLP'. 57th Annual Meeting of the Association for Computational Linguistics (ACL).- CDEI (2021), 'AI Barometer 2021'. 17 December.- CDEI (2021), 'AI Barometer 2021'. 17 December.- DCMS (2019), 'National Data Strategy'.8 July.- Graetzer, SN , Barker, J, Cox, TJ , Akeroyd, M, Culling, JF, Naylor, G, Porter, E and Viveros Munoz, R (2021), 'Clarity-2021 challenges : machine learning challenges for advancing hearing aid processing'. Interspeech 2021, 30th August - 3rd September.- CDEI (2019), 'Snapshot Paper - Deepfakes and Audiovisual Disinformation'. 12 September.- Michael L. Littman, Ifeoma Ajunwa, Guy Berger, Craig Boutilier, Morgan Currie, Finale Doshi-Velez, Gillian Hadfield, Michael C. Horowitz, Charles Isbell, Hiroaki Kitano, Karen Levy, Terah Lyons, Melanie Mitchell, Julie Shah, Steven Sloman, Shannon Vallor, and Toby Walsh (2021). ""Gathering Strength, Gathering Storms: The One Hundred Year Study on Artificial Intelligence(AI100) 2021 Study Panel Report."" Stanford University, Stanford, CA, September.- SmartCitiesWorld (2021), 'Manchester uses artificial intelligence to gain more insight into active travel'. 13 August.- PLANNING (2021), 'Birmingham Council to use artificial intelligence to help it find more housing land'. 30 July.- Verma, S., Dickerson, J., & Hines, K. (2020). 'Counterfactual explanations for machine learning: A review'. arXiv preprint. October.- VentureBeat (2021), 'Understanding dimensionality reduction in machine learning models'. 16 May.- Michael L. Littman, Ifeoma Ajunwa, Guy Berger, Craig Boutilier, Morgan Currie, Finale Doshi-Velez, Gillian Hadfield, Michael C. Horowitz, Charles Isbell, Hiroaki Kitano, Karen Levy, Terah Lyons, Melanie Mitchell, Julie Shah, Steven Sloman, Shannon Vallor, and Toby Walsh (2021). ""Gathering Strength, Gathering Storms: The One Hundred Year Study on Artificial Intelligence (AI100) 2021 Study Panel Report."" Stanford University, Stanford, CA, September.- Nature (2020), 'The long road to fairer algorithms'. 04 February.- HM Treasury (2019), 'Unlocking digital competition'. 13 March.- Such as plug-ins that block ads and trackers likePymk Inspector - Open Source AgendaandGhostery.- Ostmann, F., and Dorobantu C. (2021), 'AI in financial services'. The Alan Turing Institute.- Zeleznikow, J. (2017), 'Don't fear robo-justice. Algorithms could help more people access legal advice'. The Conversation. 23 October.- Microsoft (2020), 'Microsoft launches Anti-Corruption Technology and Solutions (ACTS)'. 9 December.- Adomavicius, G., Mobasher, B., Ricci, F., & Tuzhilin, A. (2011). 'Context-Aware Recommender Systems'. AI Magazine, 32(3), 67-80.- McKinsey Global Institute (2019), 'Artificial intelligence in the United Kingdom: Prospects and challenges'. 10 June.- CMA&ICO (2021), 'Competition and data protection in digital markets: a joint statement between the CMA and the ICO'. 19 May.- DRCF (2022) 'Auditing algorithms: the existing landscape, role of regulators and future outlook'. [INSERT HYPERLINK TO OTHER PAPER]- ICO and The Alan Turing Institute (2020), 'Explaining decisions made with AI'.No date.- GPT-3 is a language model that performs a wide variety of natural language tasks, including autocompleting sentences-",2023
govuk_032,govuk,Algorithm Transparency 2,"Updated 21 June 2021 (c) Crown copyright 2021 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visitnationalarchives.gov.uk/doc/open-government-licence/version/3or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:psi@nationalarchives.gov.uk. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned. This publication is available at https://www.gov.uk/government/publications/cdei-publishes-commissioned-research-on-algorithmic-transparency-in-the-public-sector/britainthinks-complete-transparency-complete-simplicity Participants started this research from a point of very limited awareness about algorithmic transparency.Awareness of two of the central elements of this topic - algorithms and broader transparency in the public sector - is very low. There is almost no awareness of the use of algorithms in the public sector (except for a few participants who remember the use of an algorithm to award A-level results in 2020), and no spontaneous understanding of why transparency would be important in this context. Once introduced to examples of potential public sector algorithm use participants became more engaged but still felt that, in their daily lives, they would be unlikely to make use of transparency information about the use of an algorithm in the public sector.Algorithmic transparency in the public sector is not a front-of-mind topic for participants. They expect to only be interested in transparency information if they were to personally have a problem or concern about the use of an algorithm, and struggle to foresee other circumstances when they would want to access this sort of information. Trust in the use of algorithms in the public sector varied across different scenarios in which they might be used. The setting had a stronger influence on trust than prior knowledge of or trust in technology and was influenced by: Perceptions of the potential risk created by deploying an algorithm in a use case - the likelihood of an unfavourable outcome occurring Perceptions of the potential severity of impact should an unfavourable outcome occur Despite their lack of personal interest in the information, participants felt that in principle all categories of information about algorithmic decision making should be made available to the public, both citizens and experts.For something to be meaningfully transparent, participants concluded, as much information as possible should be made accessible. Making all categories of information about an algorithm available somewhere is seen as the baseline for a transparency standard. Participants typically expect that this would be available on demand, most likely on a website. A central online repository is considered a sensible way of achieving this. They also expect that experts - such as journalists and researchers - would examine this information on their behalf and raise any potential concerns. Making information available via transparency standards has the potential to slightly increase public understanding and trust around the use of algorithmic decision making in the public sector.However, this is ultimately a very low engagement topic and simply having information about the algorithm available is unlikely to have a substantial impact on public knowledge and trust as few members of the public would know to seek it out. More active communications (such as signs, letters, or local announcements) which notify the public about the use and purpose of algorithms would be more likely to have an impact and would signpost people to transparency information. Taking more active steps to signpost people to information about the algorithm in use cases that might be perceived as higher risk and / or higher impact could do more to build understanding and trust, which is a key aim for the Centre for Data Ethics and Innovation (CDEI).Where the public are likely to feel there is more at stake, based on the perceived likelihood and severity of negative outcomes from the use of the algorithm, presenting transparency information before or at the point of interaction with the algorithm is felt to be more important. For use cases where participants are more accepting about the use of technology, and where they perceive a lower potential risk or impact, expectations around the availability of information about the algorithm are lower. Through the research processes, as participants explored the realities of transparency information some participants became more pragmatic about what information they would realistically engage with as individuals. Most of all, participants want to know what the role of the algorithm will be, why it is being used, and how to get further information or raise a query.These are the categories of information participants would expect to see in active communications and form a first tier of information. While making all categories of information about the algorithm available somewhere is important in principle, this is less of a priority for active communication, forming a second tier or layer of information, as shown in the model below. Reassurance about the 'quality' of public sector algorithms is also important.There is an assumption among participants that the public sector is unable to afford 'the best', and that this would extend to its use of algorithms. They expect that this would mean public sector algorithms are more likely to be inaccurate or ineffective. This is fuelled in part by a lack of understanding about what an algorithm is and the technology that sits behind it. Reassurance around the effectiveness of the algorithm within the information made available would be beneficial for improving trust. Ensuring that transparency information about algorithm use in the public sector is accessible and understandable is a priority for participants.This refers both to communicating information in a digestible, easy to understand manner, as well as to making it possible for different groups to find the information, for example those without an internet connection. This research asked a diverse group of citizens to consider how the public sector can be meaningfully transparent about algorithmic decision-making. Our core research questions were: Which algorithmic transparency measures would be most effective to increase: Publicunderstandingabout the use of algorithms in the public sector Publictrustin the use of algorithms in the public sector Because of the low levels of awareness about the topic among the public we used a deliberative process. This involved providing information about algorithm use in the public sector, listening to the experiences of participants, and discussing their expectations for transparency. We worked collaboratively with participants to develop a prototype information standard that reflected their needs and expectations regarding transparency. We recruited 36 members of the public, of whom 32 participants completed the research in full. We focused on having a diverse sample, rather than a representative sample, including people with different views on technology. To achieve this, participants were recruited to represent a mix of: Age, gender, ethnicity, and socioeconomic group Trust in institutions including an even spread of 'High' and 'Low' trust Digital literacy- those who are highly familiar with technology and those less so, including their awareness of algorithms 12 of the respondents were recruited because they had experience of one of the three use cases (examples of algorithms in use by the public sector) that we focused on in the research. We recruited 4 participants who were aware that they had used an artificial intelligence (AI) parking system (e.g. where an automatic number plate recognition tool - ANPR - had been used), 4 who had participated in a video job interview and 4 who had interacted personally with the police or court system, all within the last six months. Participants with previous experience of each scenario were mixed with those without for all three use cases so they could hear each other's views. We recruited participants from a wide range of geographical locations including: 6 participants from Scotland 3 participants from Northern Ireland 4 from Wales The remaining participants from England were recruited from a mixture of regions. Due to the online nature of the research, these participants were also spread across the discussion groups. All participants were paid an incentive on completion of each phase as a thank you for their time. In line with the COVID-19 restrictions, fieldwork was carried out remotely using secure, tested approaches with personal support for those with lower digital literacy to ensure they could participate fully. We used two formats to collect participant views: an online community and video discussion groups. We set up an Online Community called ""Tech in the Public Sector"", hosted on a web-based platform called 'Incling'. The platform is simple, accessible and most participants used it with ease. Personal support was given when necessary. The platform featured questions and gathered responses in a range of formats, including video, images and text. We moderated the platform, engaging with participants as they completed the tasks, probing them for further response where necessary and answering any questions and concerns participants had throughout the process. In addition to the online community, we conducted discussion groups (90 minutes) via Zoom in which groups of 6 participants discussed specific scenarios, areas of transparency and algorithms. The discussion sessions were moderated by BritainThinks researchers and attended by members of the CDEI team as observers. The research involved three phases, shown in the timeline below. Phase 1 Anintroductoryphase where participants were presented with one of the three algorithmic decision-making use cases to explore in their focus groups and on the online community, giving spontaneous, unprompted responses to transparency information. The focus was on their initial understanding of and trust in algorithms in the public sector and the level of information they expected to be made available to people when in use. Phase 2 This phase was moredeliberative. In the online community, participants responded to stimuli that outlined different categories of transparency information and ways of presenting this information, assessing their importance and necessity. They also provided their own personal examples of complex information being presented well, discussing particular features with each other. The focus group continued these conversations, discussing different transparency models, what information should be provided and how personal information should be stored. Phase 3 This was anin-depth designphase where participants worked collaboratively with the moderators in focus groups to develop prototype transparency information about each of the three scenarios. They specified what information they would like to see regarding algorithm use in each case, how they would like it to be displayed and established a tier system for the information available. The finished prototypes were then posted on the online community, with each participant reviewing and commenting on a prototype for each scenario. Our fieldwork began 9th April and ended 7th May when the final online community activities were taken down. During this time, Covid-19 government guidelines were in place meaning the focus groups were unable to take place in person and instead were conducted via Zoom. During the research, there were no major news stories regarding algorithmic or other data transparency issues raised by participants, however there were a number of news stories about transparency of government procurement decisions, which were referenced by some participants. Discussion groups were recorded, and detailed notes taken. Additionally, all responses to the exercises from the online community were exported from the platform. The findings of this report are based on this collated data, which was analysed by the research team. Where possible, quantitative data has been exported from the online community and inserted throughout the report often in the form of charts. It should be noted that whilst this helps to provide a general impression of levels of trust and expectations for transparency of participants, the low sample size means findings are not statistically significant and care should be taken in generalising. Quotes are used throughout this report, with comments made in the online community (and therefore in an individual setting) provided with the demographics of the participant. Demographics are not provided for quotes from discussion sessions which were in a group setting. Within the report there are also three case studies which explore how the attitudes of individual participants changed across the research. These were created by comparing a participant's individual responses in the online community, along with their focus group contributions, across the research period. Despite low levels of awareness and understanding of the subject matter at the start of the research project, when we asked participants in the final stage how far they felt they understood the content of the research on a scale of 0-10 (0 being no understanding, 10 fully understood)the average score was 7.9, suggesting that there was fairly high confidence about the purpose of the research by the end. When we asked participants to what extent they understood the purpose of this research from 0-10 (0 being no understanding,10 being fully understood) they answered positively, again with an average score of7.9.Participants frequently commented that they thought it was important research like this took place, and citizens were made aware of how public services were changing, even if they had been initially disinterested or completely unaware. I would say that, even though I would not be particularly interested in how a particular algorithm worked, the fullest information must be readily available for people to look at if they want to, especially if they think they have been unfairly dealt with. Female, 65+, Online community Most participants had low spontaneous understanding of the concept of algorithmic decision making, and low awareness of examples of it in practice. When given a description of what algorithmic decision-making is, commercial and low risk examples from everyday life are most front of mind, but there is little to no awareness of algorithm use in the public sector. When thinking about transparency in the public sector, participants see this as important and equate more transparency with higher levels of trust in the government. However, this is a low salience topic for participants and they struggle to spontaneously identify examples of public sector transparency, whether good or bad. Without examples of public sector algorithmic decision making, participants are unclear about how this relates to the need for transparency. However, with prompting of specific examples of public sector algorithmic decision making, there is wide acknowledgement that it is important for the public sector to make information accessible at the point of an individual encountering an algorithmic decision-making system. They prioritise information which flags that an algorithm is being used and why. Although ideally participants want all information to be accessible somewhere (complete transparency), they expect that they would only want or need to access this information if they encounter a problem in the scenario where the algorithm is being used. Before being introduced to the term 'algorithm', we asked participants about their awareness of existing 'computer assisted decision-making' systems. This is not a top-of-mind topic, and not something participants could spontaneously apply to their daily lives. Similarly, when initially introduced to the term algorithm', many participants had not heard of it before and were unsure exactly what it referred to and meant. Only a small number associate this term with decision making assisted by technology and based on data input. Participants were then provided with the following definition: 'algorithmic decision making, machine learning and artificial intelligence are all types of computer-assisted decision-making. This is where tasks are automated by computers, either to make suggestions based on past behaviour, to carry out simple and repetitive tasks or to analyse very big and complex data'. With this prompting, participants identify some examples in their own lives. However, it is worth noting that while some participants were able to identify commercial and low-risk examples for algorithmic decision-making, other participants gave examples where there is no algorithmic decision making (e.g. supermarket self-check-outs). This reflects participants' overall limited understanding of the term, and suggests it may be used as a proxy for modern technology in general. Examples given by participants include: Algorithmic recommendations in their buying/viewing/search histories (e.g. Netflix and Amazon) Text recognition and predictive text, responses, auto-completes Self-checkouts Credit card and other financial applications Driverless vehicles Robotics in manufacturing One of my favourite examples [that] makes my life happier currently while in lockdown is Netflix, they use AI & algorithms to decide from my past watching history what I will enjoy next. Female, 45-54, online community Outside their own day-to-day experiences, some participants also recall having heard or read about computer assisted decision making systems being used in the healthcare sector (e.g. vaccine development) and the financial sector (e.g. stock markets and credit scores). There is very little to no awareness of computer assisted decision making systems being used in the public sector. A very small number of participants spontaneously mention the A-level algorithm used by the Department for Education to give A-level results in 2020, and raise concerns about the use of an algorithm in this particular scenario. However, after discussing this example, participants remain open to algorithm use in other instances. It's that word algorithm which just makes me think back to last year and thousands of stressed-out students receiving inaccurate/unjustifiable grades based on algorithmic decision making as any algorithm can only be as good as the data that is inputted - millions of terabytes in even the simplest of human-human interactions. Male, 35-44, online community Please see Section 8, Appendix 1 for more information about participants' spontaneous attitudes to algorithmic decision-making, including attitudes towards the use of algorithms in general, attitudes towards algorithms in different settings and attitudes towards algorithms in the public sector. Transparency in the public sector Spontaneously, participants do not have strong top-of-mind associations with the concept of transparency in the public sector. There is some confusion about what both transparency and the public sector refer to; these are not terms or concepts many people regularly engage with in their daily lives. However, once explained, the notion of transparency in the public sector and in public sector decision making is seen as an important and fundamental component of trust in government. Participants feel transparency is necessary to provide evidence and reassurance that the government is working in the best interest of the public. This is especially true for circumstances and decisions which are either likely to have a significant direct impact on individuals or wide ranging/ high stakes impacts on society as a whole. I think that people being able to...openly see what taxes are spent on is essential to trust the government. It's currently lacking because there's been a scandal around PPE contracts, people don't have the full trust in what's being spent. Focus group participant I think it's important because you don't read everything but it's important to be there if you do need to find it, so if something happens you have the information there so you can see if you have the right to complain or not. Focus group participant The sense among participants that transparency in the public sector is important and to be encouraged sits in tension with the fact that some participants also associate public sector information with jargon and complex information that is hard to understand. Some participants feel that organisations in the public sector deliberately make it difficult to find or understand the decisions that they make. On the other hand, some suggest that there may be limits to the degree of transparency possible and feel that 'too much' transparency information may confuse the public and ultimately be unhelpful. I think a lot of it is just jumbled with jargon, it's not easy for people to understand. People have differing levels of experience and sometimes it's done on purpose. Focus group participant If you put all the info out there not only, are you going to confuse people, the general population aren't going to understand. I do think things are withheld. Overall, I don't think they are hiding too much. I do think there is a reason for it. Focus group participant Transparency about public sector use of algorithmic decision making Given the lack of awareness about algorithm use, participants do not have spontaneous expectations about what level of transparency is appropriate. In particular they find it difficult to identify the consequences of a lack of transparency, and so find it hard to form a view. However, when presented with the three use case examples of algorithmic decision making in the public sector (including parking, recruitment, and policing - see section 4 for more on the use cases), and having detailed discussions, participants formed strong views about the level, categories and format of information that should be made available (more detail is provided on this in the following sections). Ultimately, participants feel that being provided with transparency information will help to increase both public understanding and trust in public sector algorithmic decision making.At the same time, it is important to note that while participants feel it is important to be provided with this information, they only expect to need or want to access this information if they encounter a problem in the situation where the algorithm is being used. During the research, we also asked participants about the relative importance of individual citizens and experts having access to information, and preferences for the level of detail the information would have. Overall, in instances where participants are directly affected, participants prioritise making simple transparency information available for individual citizens over journalists and experts (see fig. 6). In circumstances where the outcome of an algorithmic decision making has a direct impact on them, participants feel it is important and 'their right' to have access to information about an algorithm. Further, given the perception that too much detail would be overwhelming and hard to navigate or understand, there is also a preference for this information to be simple rather than detailed technical information. However, among a small number of participants there is also an expectation and a desire that experts, journalists and civil society organisations like charities have access to wider, more detailed information about algorithms used by the public sector so that they are able to scrutinise and examine the impacts of new technology. There is a sense that these experts will go through the details on behalf of the public and would raise any concerns that might impact citizens. This became even more important for participants in later research sessions where we asked people to design information standards. During this process, when confronted with realities of transparency information specific to use cases, some participants became more pragmatic about what information they would realistically engage with as individuals. Given some participants were unlikely to engage with the information, these participants began to feel it was more important that experts had access to this information than individual citizens. If the info isn't being scrutinized by an independent party, then an untrained eye will just assume it's right... [it's important] the government should have someone there challenging them [with information]. Focus group participant I personally wouldn't [look at information on the algorithm]. I suppose it would depend on what the algorithm is and if it will make a difference on my everyday life, I would like to see it in that case. Focus group participant Unknown unknowns: initial desire for high levels of transparency As participants start to build their knowledge of algorithmic decision making and its uses, they move from low engagement and interest, to a desire for detailed information. This is a common reaction: as we become aware of a topic we realise how much more there is to learn. Once introduced to example use cases (policing, recruitment and parking in this case) participants are keen to gain more knowledge about the use of algorithms in the public sector and expect that other members of the public would feel the same. This sentiment comes through most strongly in the higher risk, higher impact use cases: recruitment and policing. In particular, participants spontaneously express a desire for: Evidence from pilot studies or previous uses to illustrate the effectiveness of algorithmic decision making, including specific locations, contexts and consequences. Details of when and where humans would be involved in the process if an algorithmic decision-making system is being used, especially whether they would have the option to contact somebody if they want to know more about the algorithm or to challenge a decision. In addition, participants welcome information on what personal data is used and how it is stored across all use cases, consistent with a generally high level of awareness of 'data protection'. Participants also felt strongly at this initial stage that anyone who was affected by use of an algorithmic system should be made aware of it, to be told explicitly that an algorithm is being used so they can understand what this means for them. Maybe there should be some statistics or examples of how it [the algorithm] is being used and a pilot study to show how it's working. If you put it out and it doesn't work, there is going to be backlash. Maybe they should include some views of the locals in the areas that it's being used in - they need to provide evidence that it is working. Focus group participant This initial desire for more information translates into an expectation that all transparency information for a particular use case should be made available somewhere. This view was held consistently throughout the research by many participants who see transparency as making as much information accessible as possible. The exception to this is where there is concern about possible risks associated with publicly sharing some types of information (e.g. the data used by the algorithm in the policing use case), and in these instances participants expect less transparency information to be available. You definitely need to know, it's your personal information. I always read stuff especially if it's to do with my personal information. You need to know why they are taking it and if they are going to keep it and how long. I think they should make all of the information available if somebody wants it. Focus group participant Known unknowns: increasing focus on the importance of simplicity As they move through the research, participants consider how they would encounter this information in their day-to-day life, and also become more comfortable with the idea of an algorithm being used in the public sector. As this happens, participants become more selective about the volume of information that they expect to see (although with the continuing expectation that ""all"" of the information would be available somewhere). This resulted in a common desire for transparency information - whether basic or more detailed - to be presented clearly and simply. In general.. [it needs to be] explained clearly and concisely - it's a lot of stuff that you might not even pay attention to, but there needs to be a brief going over everything and just bullet-pointing the most important things. Especially for people not able to process information as easily as others, it can all be overwhelming. A lot of people aren't really interested but you do have a right to know what's going on. There is a risk to giving too much information, but you have to find that balance to what we need to know and what we might be better off not knowing. Focus group participant Resolution: Two tiers of transparency information In phase three of the research, where participants worked together to design a prototype information format, this tension between transparency and simplicity was resolved by allocating information categories to different tiers. Participants expect the information in tier one to be immediately available at the point of, or in advance of, interacting with the algorithm, while they expect to have easy access to the information in tier two if they choose to seek it out. Tier two information is generally what is expected as a bare minimum across use cases; at the very least, it is felt that individuals should be able to access all of the transparency information if they would like or need to. NB: A few participants expect to see information about how personal data is used in tier 1 for the parking use case, while some feel information about data should be held back in case criminals take advantage of this information. I think they're [transparency categories] all important. If you want it, this will give you the info you need, a link will give you an option if you have questions about the process. Focus group participant The two-tiered approach balances participants' expectation that all transparency information is available to access on demand, whilst also ensuring that transparency information shared at the point of interacting with the algorithm is simple, clear, concise and unlikely to overwhelm individuals. Do people need to know all the detail? Just have a basic leaflet and then make the detail available online clearly and without any jargon! Focus group participant During the research we presented participants with three different use cases that involve public sector algorithmic decision making to understand overall trust, as well as expectations around public sector transparency. We found that the different use cases had a significant influence on the degree to which participants feel comfortable and trust an algorithm. This in turn impacts the level of transparency information they feel is appropriate in each instance. We found recruitment to be the most divisive use case, with concerns about a technologically- rather than human-driven application process translating to discomfort about the use of an algorithm. Conversely, participants' low emotional engagement with and relatively high levels of familiarity with the use of technological solutions in car parking means they are more accepting of an algorithm in this instance. There are two main factors that seem to influence how participants view the use cases and their level of comfort and trust in an algorithm being used: Risk- the perceived likelihood that the scenario will lead to an unfavourable outcome for them as an individual or society more broadly. This is typically driven by the degree to which participants trust the efficacy of an algorithm to make a decision in each scenario. For example, many were sceptical about the ability of an algorithm to make correct recruitment decisions. Impact- the severity of impact an unfavourable outcome would have directly on them as an individual or society more broadly. The degree of perceived potential impact and perceived potential risk influences how far participants trust an algorithm to make decisions in each use case, what transparency information they want to be provided, and how they want this to be delivered. For lower potential risk and lower potential impact use cases, passively available transparency information - in other words, information that individuals can seek out if they want to - is acceptable on its own. This could, for example, be information available on a website. It is also more acceptable for passive information to be held as part of a centralised system, rather than being targeted to those affected. For higher potential risk and higher potential impact use cases there is a desire not just for information to be passively available and accessible if individuals are interested to know more about the algorithm, but also for the active communication of basic information upfront to notify people that the algorithm is being used and to what end. As part of this, it is felt that information should be more targeted and personalised. For some use cases expectations around transparency information are very high (for example, for the policing use case, some expect to see door drop campaigns announcing the use of algorithms). This scenario is seen as having both high potential impact and high potential risk. There is less confidence in an algorithm being able to perform this task than a person, and therefore limited trust in an algorithm being used in this scenario. Initial views Participants have mixed views when shown the initial recruitment use case. Those with previous experience of an online job interview also report mixed experiences, and while they were able to complete the process and found it straightforward, some felt that the lack of face-to-face human interaction was unhelpful for rapport and easing nerves. Recruitment cannot be automated there are too many variables to consider when choosing the perfect candidate. Female, 45-54, Online community Some participants expect that this type of process would make it more efficient and easier for some people to apply for jobs, and the ability to record a response in their own time would make some feel more comfortable. This is especially the case for those who are more familiar and comfortable using technology. I am confident about doing videos and think it gives the employer a better idea of who you are as a person. It seems like an efficient way of doing things. Female, 35-44, Online community However, others feel that some people may not come across as well in a video response, and that their performance may suffer from the lack of human interaction. There is also some concern that people may be able to 'cheat' the system, for example by having someone else script the response for them, or having more time available to practise their response. I think that all job interviews, even preliminary ones should be conducted in person. You can't tell a lot about someone on video and you can't get a feel for their personality, they may end up excluding some people that could be really good workers based on the fact they don't video well! Female, 18-24, Online community Trust in algorithm use The recruitment use case is where participants have most doubts about the overall effectiveness of an algorithm being used. Specifically, there is scepticism that a computer can assess candidates as effectively as a human. Some participants are suspicious about why a video is needed, as opposed to a written response, and express concerns about the purpose of a visual and the potential for this to lead to biased decisions. This use case is felt to have a high potential impact as a negative outcome would not only have ramifications for the individual applying for the job, but could also have broader societal impacts, for example if cultural biases were built into the algorithm preventing certain groups from getting employed. The table below (fig. 11) summarises the perceived benefits and drawbacks of using an algorithm in the context of public sector recruitment. When asked on a scale of 1-10 (where 1=not at all, 10= totally trust) how much they trust an algorithmic decision-making system to be used in this context,participants give an average score of 3.9.Broadly participants feel that it is important for humans to be involved in this process to remove potential technological issues and errors and maintain human element of recruitment. Transparency information The perceived high stakes of this use case, combined with a lack of familiarity and trust in technology being used this way, mean that participants feel it is especially important for transparency information to include how exactly the algorithm works. As part of this, participants are particularly keen understand what criteria are being used to identify the most promising candidates so that they understand the process fully and can ensure their response and application is well-prepared. Understanding the criteria being used by the algorithm is also seen to help applicants understand why they have or haven't been successful, and a fundamental aspect of the feedback process. Given doubts and concerns about algorithms being used in a process that is seen to require human and emotional intelligence, transparency information about the role of human involvement in this process is also felt to increase trust in this algorithm being used. As part of this, participants would like clarity on where exactly algorithmic decision making 'begins and ends', as well as understanding where human decision-making fits into this. Further, participants also want to see information provided about where they can go if they have any questions about the process or the outcome. Due to the highly personal nature of the application and significant amount of personal information given, participants also feel it is important for transparency information to include reference to data security and privacy, so that applicants understand exactly how their information will be used and how long it will be stored for, before applying. Overall, desire for transparency information in this use case is higher than other use cases tested. With the algorithm playing a significant role in determining whether they get through to the next stage of an employment process, this use case is felt to have higher potential risk and higher potential impact, especially on an individual level. Given the emotional stakes of this use case, and an unfamiliarity with technology being used in this way, participants feel candidates should be informed about the fact an algorithm is being used and to what effect at the point of deciding whether or not to apply, and proactively throughout the process. The case study of Randeep below (Fig. 12) highlights some of the specific concerns some participants have about algorithmic decision making in the recruitment use case. Overall, this scenario is seen as being low risk and medium impact. Participants generally feel comfortable with an algorithm being used in this use case, as they recognise the potential value in terms of efficiency, and it is felt to have less direct personal impact on them than other use cases. However, there is some concern that publicly available information about police decision-making processes may enable potential criminals using this information to their advantage. This was a strong concern for some participants, even when it was explained that this would be an unlikely outcome of the scenario proposed. Initial views In this use case, participants are interested to find out more about how the way of allocating police had changed and the impact this could have on their local areas. The key positives related to this use case are understood as the potential for increased crime prevention and reduced crime levels, while others are pleased to learn more about an issue impacting their local area. More police officers will be better and the neighbourhood will feel safer. Female, 25-34, Online community Among participants there is also some concern around how a potential increase in police presence would be funded, and others wonder whether information about police allocation can be harmful to public safety, and lead potential criminals to use the information to their advantage. Could it be abused by people by sending the police to different areas deliberately? Male, 65+, Online community Trust in algorithm use Most participants believe that there are clear benefits to algorithmic decision making in this use case. The fact that this scenario feels more removed from participants' daily lives, also increases acceptance of an algorithm being used, as participants feel that it is less likely to have a direct negative impact on them. The table below (fig. 14) summarises the perceived benefits and drawbacks of using an algorithm in the context of policing. When asked on a scale of 1-10 (where 1=not at all, 10= totally trust) how much they trust an algorithmic decision making system to be used in this context,participants give an average trust score of 4.7.While participants feel positively about the potential for algorithmic decision making to increase the efficiency and the impact of police resourcing, there are some concerns about how effective this would be in practice, and potential 'unforeseen consequences' of under-resourced areas and people tricking the system. Transparency information Given the perception that good police resourcing should be informed by human experience, there is some desire for transparency information to provide reassurance on the human input and support going into this decision making. Participants also want to know that the algorithm is working appropriately and is being checked. However, in this use case participants feel strongly that any transparency information should not include the actual data which the algorithm uses, as it is felt that potential criminals could use this information to their advantage and to the detriment of public safety. In fact, some participants are concerned that any degree of transparency information may inadvertently lead to the system being 'tricked' and therefore also want reassurance that any and all transparency information has been deemed 'safe' to share more widely by experts. This was a particular concern for one participant in Northern Ireland, who felt that information about policing needed to be secure. Overall, similar to other use cases, participants do not expect to actively search for or access transparency information on this use case themselves unless they have personally had a negative experience directly related to police resourcing. However, given the potential for any errors in police resource allocation to have a significant negative impact on wider public safety, participants express a desire to be actively notified upfront that 'policing is changing' e.g. via public information campaigns. This also reflects the fact that among participants, there is a strong sense that algorithms being used in this way is new, and a significant development. The case study of Peter below highlights how becoming more informed about public sector algorithmic decision making, in this case when used in policing, can increase trust in algorithms being used. This scenario is seen by all participants as being both low risk and low impact. Participants generally have high levels of confidence in an algorithm to be able to perform this task and are less concerned about the impact of a negative outcome for them. Initial views When shown an outline of the initial parking use case, without explicit information regarding the use of algorithm (seeFigure 16), participants are broadly happy and comfortable with this scenario. There is familiarity with ANPR technology, and several have used this type of system before and had positive experiences. Participants associate a range of benefits with this scenario, including it being an easy, quick and efficient way of paying for parking, the ability to have evidence of online payment, and facilitating fairness by ensuring everyone pays for parking. I do actually prefer this method as years ago you would have to make sure you had change for the machine and often I would not have the right money...So having the camera at the entrance reading your number plate and you ringing a number to pay for your parking I feel is a good idea as you have proof that the payment has been taken from your debit card in case you do get a ticket. Female, 55-64, Online community Key issues associated with this scenario include having issues with phone and/or internet signal, not having a credit card, and ensuring the signage is clear enough for everyone to understand it. That's all well and good, assuming you have an internet enabled phone to pay online. Female, 55-64, Online community Trust in algorithm use On the whole, participants are comfortable with the use of algorithmic decision making in this context. Ultimately, participants do not see the use of an algorithm as likely to have a significant impact on them. An unfavourable outcome would mean getting a ticket, which in participants' view is 'how parking has always worked and always will.' The table below (Fig. 17) summarises the perceived benefits and drawbacks of using an algorithm in the context of parking. Overall, participants are positive about the use of an algorithmic decision making in the parking use case; when asked on a scale of 1-10 (where 1=not at all, 10= totally trust) how much they trust an algorithmic decision-making system to be used in this context,participants give an average score of 7.5. There is a general sense that in this parking scenario, the use of an algorithm is straightforward and efficient, and the most significant issue would be the reliability of the technology. Those who lacked trust were often sceptical about parking fees and ticketing generally, as much as about the algorithm. I do feel that this is a process in which computer assisted decision making is relatively straight forward to apply and drives up efficiency and enables cost-savings which can be used to better purposes. Male,35-44, Online community Transparency information Given the fact that this use case involves personal data, participants also feel that it is important for transparency information to include data security and privacy information. Participants generally want to know what data is being held, for how long and why. Participants often link this to awareness of General Data Protection Regulation, and a sense that they have a right to know how their data is handled. [I'd like to have] confirmation that your information is kept private and that your details are not shared with any third parties. Female, 55-64, Online community As with other use cases, participants also feel that information about who they can contact in case they encounter any problems or want to challenge a parking ticket is particularly important, and gives them reassurance that help is at hand in case there are any technological issues. However, overall, desire for transparency information in this use case is lower than other use cases tested. In general, participants feel strongly that it is their right to know an algorithm is being used and expect this to be explicitly stated at the car park entrance. Yet, beyond this, participants expect that people can seek out further information (including contact and data security and privacy information) if they want it, rather than expecting any active communications. As long as it's very clear that this is in operation, I would trust it. Male, 18-24, Online community The case study of Thomas below highlights some of the specific advantages participants recognise when algorithmic decision making is used in the parking use case. In Phase 1 of the research, where participants were initially introduced to the three use cases (recruitment, policing and parking), they were also shown basic transparency information about how and why an algorithmic decision-making system is deployed in each setting. In Phases 2 and 3 of the research, participants were presented with a total of nine categories of transparency information, which were based on discussions with CDEI, the Cabinet Office, and a review of the transparency standards used internationally. Across the research, adescriptionof the algorithm and itspurposeremained the most important categories of transparency information. The table below outlines participants' prioritisation of the nine information categories during the Phase 2 online community. This prioritisation remained largely consistent when asked generally and when discussed in relation to each of the use cases in turn. The main exception to this is contact, which came out as a higher priority during the Phase 3 focus group discussions and prototype development. The other exception is data which, when interpreted as personal data, was considered a higher priority to include. While one of the lower ranked categories in the online community, in the focus groups contact emerged as one of the most important categories. In a two-tier system, details about where to get more information or ask questions is elevated and prioritised as tier one information, alongside description. The main one for me is contact, I need to know who to contact if it goes wrong. I need to know who is responsible and their contact info so I can contact them by email or phone. Focus group participant Sharing information about risks is polarising among some participants and in some use cases due to a concern that it will cause additional and unnecessary anxieties among those already unsure about the use of an algorithm. In the focus groups this category was expected to appear in tier two but was prioritised below other types of information. I would like to know more than just the descriptors but not the risks because your trust is lowered. You don't want to know the risks because it's going to happen anyway. I'd rather not know risks and just go with it. The risks would put me off as I am already anxious about it. Focus group participant In the focus groups, it became apparent that data is understood by some to refer to the use of their personal data and data privacy - participants expect this to be called out and is of higher importance to them than information on other data sources. You just need to know where your information is going and who has that information. I think data is the most important for me [it is like] when you're on Facebook... your information is being sorted and you don't even know it. Focus group participant For some, there are concerns about sharing information on data when it comes to the policing use case particularly due to the fear of this information getting into the wrong hands. You shouldn't share too much data in case criminals get hold of it and start committing crimes in other areas or play the system. Focus group participant For the policing use case, some participants expect more information to be included in tier one. Commercial information is felt by some to be more important than other categories, and in some instances, participants feel this should be included along with tier one information. This is because the police are an institution that serves and protects the public and therefore making the public aware of other organisations that might be involved in delivering this service is felt to be important. As policing impacts entire communities, some participants would also like to see information on risks and impact included in tier one. However, this should be at the expense of the information remaining simple and clear. It's important to know if it's just the police that handle this information. I think it's important to understand the impact for policing, as it impacts the community. They need to explain this so everybody can understand. Focus group participant For each category of information, participants were presented with the following details: Heading A question that explains the category 3-4 examples of specific types of information that could be included in that category Breaking down the transparency information in this way is felt by participants to be clear and easy to digest. In particular, having the question that explains what each category of information covers is seen as a helpful aid to understanding. Description Consistently throughout the research, the description of how the algorithm works was seen as the most important piece of transparency information to be shared. Participants expect a description to be clear, simple and not overly technical, summarising the role of the algorithm rather than providing too detailed of an explanation. This category of information had high appeal when it was first presented in Phase 1, and this remained the case for the duration of the research process. Participants expect that all three of the bullet points would be included within each of the three use cases. The first and second bullet points 'when the algorithm is used' and 'how an algorithm works' are felt to be particularly important, as participants expect that many individuals would have limited knowledge about what an algorithm is, and providing this information would help to address this. The information outlining 'how the result of the algorithm is used/interpreted' is felt to be particularly relevant for the higher risk and higher impact use cases (policing and recruitment). Purpose Similarly to thedescriptionoutlining how an algorithm works, the information included within thepurposecategory is felt to provide further clarity about use of algorithms in the public sector. Participants often find it difficult to separate their feeling about whether an algorithm should be used in each use case, so information outlining 'why an algorithm is being used' is especially valuable. The first two bullet points 'reasons why an algorithm is being used' and 'why algorithmic decision making is better than human decision making' are the most interesting and important. While the extra information is welcomed, the third bullet point is felt to be less relevant across all use cases. Description and Purpose - I put these first as I think we should know exactly why this algorithm is being used. Having a basic understanding of why/how it's used would benefit the person it's being used on. I think it's someone's right to know why and how it's being used. Male, 25-34, Online community Contact Unlikedescriptionandpurpose, this category of transparency information was not presented to participants until Phase 2 of the research. However, in Phase 1, participants spontaneously mentioned that the relevant contact details needed to be included in the transparency information. The most important element of this information category is providing the public with clear details on how to get more information or get a query addressed. The last bullet point outlining 'details on the appeal process' is seen to be especially relevant to the parking and recruitment use cases. The first bullet point was felt to be less important for some. You need to know who to contact if you have any concerns about it [the algorithm] - this needs to be available. Focus group participant Data When discussingdataparticipants typically think first of their personal data - what is being used and how it is being stored - which is considered to be more important than getting information on other data sets used by the algorithm. This is particularly true for participants who are more aware of General Data Protection Regulation (GDPR). This is most relevant to the parking and recruitment use cases where individuals have more direct interactions with the algorithm. Particularly among those with a more advanced understanding of algorithms, knowing what data the algorithm is using is seen as very important, as this is seen as what will determine the effectiveness of the algorithm. However, these participants concede that not everyone will be interested in this information, and would expect it to fall within tier 2. For the policing use case, there is concern among some that too much information on data sources and data used to train the algorithm could lead to criminals 'playing the system'. I think there's a danger that once people know the algorithm they can then game it...if I know the algorithm about police ending up in my street and I'm a criminal I know that this is the best time to commit a crime in my area because police are being sent elsewhere. Focus group participant Human Oversight This category of information resonates with participants, as they want reassurance that a human is also involved in the process in each use case. However, they are more interested in information about how and when humans would be involved in the process as opposed to the risk of human error. Overall, while relevant, this category of information is not felt to be as important as the others. Would there be a human involved? When would they come in or would it be the algorithm throughout? This is important to know. Focus group participant Risks This is the most polarising category of transparency information. Many participants welcome this extra level of transparency, especially when there is a lot at stake. For others, this is considered as being 'too transparent' leading to worries that sharing this information would lead to unnecessary anxieties and concerns about the use of algorithms in the public sector, which could inadvertently cause more work for public sector bodies as they have to manage this. This is mainly a concern in the higher risk and higher impact use cases where there is lower immediate acceptance of an algorithm being used. Ultimately, most feel that this is important information to make available within tier two. I put risks as least important because any time you mention risks it will pre-emptively scare the reader because it's putting the risks out there for all to see and causing some doubt in the decision where maybe there was none before. Male, 25-34, Online community The case study of Anita below highlights the worries that some have about including information about risks as part of algorithmic decision-making transparency information. Impact Participants feel that information onimpactis necessary for transparency. However, they are less interested in 'impact reports for various demographic groups', and more motivated by how the algorithm is directly impacting them as individuals. I'd be interested to find out more to see how it would impact me, but I wouldn't be interested in the technical details - just how it will work and why they are doing it. Female, 45-54, Online community Commercial information & third parties, and Technicalities Figure 29: Stimulus used in Phase 2 focus groups. While participants feel details aboutcommercial information and third partiesand technicalities should be made available and accessible, they are less likely to be the main priority. Therefore, participants suggest that they should be available online if individuals are seeking extra information about the use of an algorithm in the public sector. A few participants feelcommercial information and third partiesshould be communicated upfront for the policing use case. Overall, participants want basic information about the use of the algorithmic decision-making system to be made immediately available when engaging with the information, or in advance, in tier 1 for all three use cases (refer to section 4) and would expect to be signposted to a website if they require additional information. However, many participants also feel this information should be provided in leaflets, the local newspaper, and on the radio to make it widely accessible to members of the public. In terms of information design, accessibility for a wide range of people is a key consideration and impacts the sort of language and layout participants would expect to see. Crucially participants want to avoid jargon, to easily be able to find the information they are looking for, and for options to be offered to those who are not online (e.g. automated phone lines). In general participants default to the two tiers of information in relation to the preferred channels they expect for each use case (parking, policing, recruitment). In tier 1 (across all use cases) participants expect active communication of basic information upfront to notify people that the algorithm is being used and to what end - this is either immediately at the point of interaction or in advance. For example, participants' suggestions included signs before entering the car park (parking), door drops, or radio announcements, to notify people an algorithm is being used (policing), and a notification in the job description (recruitment). In addition, for the use cases in which participants feel less comfortable with an algorithm being used (higher impact and higher risk) more personalised information is likely to be more effective than non-personalised information. For example, in the recruitment use-case, participants expect that they would receive a personalised letter informing them how algorithmic decision making was used to process their application. For more detailed information (tier 2), participants express the need to be directed to a website and being provided the option to call an automated line (which will provide the same information) for those who are offline and do not have access to a computer or the internet. Recruitment Overall, participants want to be notified that an algorithm is being used within the job application. Within the application, participants expect to be directed to the organisation's website if they require more detailed information about the use of an algorithm to identify the most promising candidates (as well as the written content, participants express that an option should be provided to listen to short videos to ensure the information is accessible). To make the process more transparent, for those that continue with their application, there is an expectation for organisations to send the applicant an email with the outcome outlining how their application was assessed by the algorithmic tool. PolicingParticipants feel that aleafletand radio would be the most suitable channels to receive basic information about algorithms being used to allocate police officers because it's both accessible and personalised. In addition, to make it more accessible to a range of different audiences, participants recommend this information should also be announced on the radio. Within the leaflet, participants expect to be directed to their local police website, via a link, to access more detailed information. Parking There was a widespread view that there should be a sign which clearly outlines that an algorithm is in usebeforepeople enter the car park. For additional information, participants propose that there should be a link and QR code on the sign, which directs individuals to their local council's website. In addition, to ensure everybody has access to this information, participants also suggest including an automated number for members of the public to contact if they are offline or do not have access to the internet. Participants highlighted a range of principles for the presentation of transparency information in any channel and format: Overall, participants expect the language that is included within the transparency information to be clear, concise and accessible (without the use of acronyms or jargon). Participants felt this was particularly important when they reflected on the fact that they originally had low understanding and awareness of algorithms at the beginning of the research process. To ensure that the content is user friendly, participants feel that both headlines and bullet points should be used to clearly group the content, both in paper form (leaflets/ local newspapers) and on websites. This is to ensure the information does not cause cognitive strain, especially because many members of the public are unfamiliar with what an algorithm is. In addition, drop down boxes and 'hover overs' that help to explain technical terms have high appeal to aid comprehension further. Although participants spontaneously mentioned using visuals and animations at the beginning of the research, this is not considered to be a priority at the end of the process and is therefore rarely mentioned in the design process in Phase 3. This is because simplicity is a key driver, and if the balance is correct between the headlines and the following content, visuals are not thought to be essential to aid understanding further. Some participants can see huge potential for having a recognised symbol indicating when an algorithm is in use. While this will increase transparency in the public sector, participants also feel that this will lead to more awareness and familiarity about the use of algorithms in the public sector more generally. In the final phase of the research, participants were presented with two existing examples of algorithmic transparency information that are used in Helsinki and New York. Participants were asked to provide their views on the information and presentation style within both models. The Helsinki model Overall, the Helsinki model has high appeal amongst all participants. They are positive about the information being in a centralised register and assume that people would be able to locate this information easily. In addition, they appreciate the simple headlines and summaries that are provided on the home page, and value the option of being able to click on 'read more' if they require more detailed information. While the visuals are not spontaneously mentioned by most, those that notice them acknowledge that they capture the essence of the content well and contribute to an overall aesthetically pleasing and clear design. When exploring the additional information available, 'contact information' and 'more detailed information on the system' really stand out to participants as being important to cover. Overall, this example is felt to be simple, user friendly and to have the right balance of information to increase transparency in Helsinki's use of artificial intelligence. Participants express a desire for the public sector in the UK to have a similar centralised register for the more detailed content and feel that they as individuals and others would be able to engage with the information. The Helsinki example brings to life how participants expect and want tier two information to be made available. If it's laid out like this, it's not too much, as you can pick out what you want to see as you can click through. It's bright, it's light and inviting - it definitely helps. Focus group participant Overall, this model is not particularly well received. On initial exposure to the New York Model, participants raise concerns about the length of the content and feel that it is not as user friendly as the first example. Participants like the clear headings, but without the filter for additional information (as in the Helsinki model), participants either feel this is the right level of information or too much. Most participants assume that this level of information is not targeted to the public, and is more likely to be suitable to academics or experts. In addition, a few participants raise concerns about the accessibility of the information because of its format and thought that some people would not be able to access it. There was some confusion about whether this would need to be downloaded from a website or whether it would open in a web browser. I don't think this is good, it's a document academics would go to as it's a more technical resource. Would the general public know all of the jargon being used? Focus group participant In summary, public understanding of and trust in the use of algorithms in the public sector is most likely to be improved through active communications that notify the public about where algorithmic decision-making is being deployed, with full information available on demand. In terms of understanding, at present awareness of the use of algorithms in the public sector is low, as is awareness of transparency in the public sector. The low engagement nature of these topics means that people are unlikely to seek out information about the use of algorithms. Signposting people to the fact that there is information available if they are interested has the potential to increase engagement, and therefore understanding about the role algorithms play in the public sector. Presenting all information in a simple, clear and easily digestible way is also key for aiding understanding. When it comes to trust in the use of an algorithm, the scenario in which it is being used is a highly influential factor. In scenarios where trust is lower (typically those which are perceived to carry a greater risk of a negative outcome, and where the potential impact of a negative outcome is felt to be higher) activities that build trust are more important. Therefore, active communications around the role of the algorithm, its purpose and how to access further information are most important in these instances. Importantly, active communications need to be supported by a baseline of having all available categories of information about the algorithm accessible to the public somewhere in an on-demand way. Although participants do not generally expect to make use of this information, they feel that in principle it should be available to them, and that without this the public sector could not claim to be truly transparent around its use of algorithms. They also expect that experts may access this information on their behalf, raising any concerns which may be relevant to citizens. In this report, we have summarised the need for different types of information in different scenarios or at different points in the journey of engaging with an algorithm through the two-tier framework. This model highlights the difference between the information about an algorithm that is expected to be passively available on-demand (tier two), and the types of information which, if actively communicated, would be most effective and increasing trust and understanding (tier one). Our recommendation to the public sector would be to flex this two-tier model depending on the specific scenario in which an algorithm is being used. For scenarios where the public are likely to perceive higher potential risk and impact, a two-tier approach with full transparency information available alongside signposting is likely to be most effective for building trust and understanding. For scenarios which are felt to be lower stakes, just having tier two information available is likely to be sufficient. Associations with algorithmic decision making When first asked about their associations with 'computer assisted decision making', participants have mixed views. While there is some positivity towards computer assisted decision making, participants also have a number of concerns. Where 'computer assisted decision-making' is viewed positively, it is associated with a sense of efficiency, optimism, and progress. For these participants computer decision making is seen as: An important form of technology that can save resources, time and costs An essential part of society and daily lives, and an inevitable and undeniable aspect of the future You can see it really well with technology right now. It has almost become difficult to live without your phone. You can do a lot of things and it saves a lot of time. Right now you can chat to any part of the world with anyone in a short amount of time as if they are next door. Technology has made the world smarter and more efficient. Focus group participant Others, while able to recognise the benefits of computer assisted decision making, are more apprehensive about it. This is mainly driven by: Sometimes it can be scary to people that don't know much about assisted decision making. It can be scary because you don't know if it will choose the right things, you just don't know. Focus group participant Negative, frustrating, and invasive experiences with technological systems and processes in general A sense that computer assisted decision making is not able to adequately account for 'human', emotional, and personal characteristics and circumstances This I feel makes things harder for humans now for example buying a house with a mortgage an algorithm makes the decision for you whether you can have it or not and it takes away the personal aspect and I feel it's too regimented! Female, 18-25, online community A fear that computer decision making may result in laziness or an inability for people to make decisions A concern that computer assisted decision may lead to unemployment I worry what it means for actual jobs and people, I don't understand how it's going to help people with jobs. You're kind of wiping out loads of jobs and taking the human out of things. Focus group participant Attitudes towards the use of algorithms in general The mix of views on algorithmic decision-making systems is reflected in the fact that the majority (60%) of participants believe that computer assisted decision making systems are as much of an opportunity as a threat (see Figure 36). The demographic profiles of those who see computer assisted decision making as an opportunity are varied. However, of the 5 participants who see computer assisted decision making systems as more of a threat, 4 are within the 55-64 age range. Although the numbers are small and should be treated with caution, this provides some indication that older participants are more likely to see computer assisted decision making as a threat. I think it's more of an opportunity, it takes out human error, computers can make a decision in half a second. I have been to Singapore where everything is run on a system and it's more efficient than the underground. Focus group participant I think it's open to being misused, the majority of people in general are not up to speed. Focus group participant Although there is some indication that older people are more likely to see computer assisted decision making as a threat, there is no clear correlation between attitudes to computer decision making and attitudes towards technology more widely. Algorithms in different settings Across the dialogue we found that the degree to which participants feel comfortable with and trust algorithmic decision making depends most strongly on the specific area and scenario in which it's being used. In general, participants are more comfortable with algorithms being used in instances where the decision making is perceived to be purely factual and data-driven, with no need for human or emotional insight. There is also a sense that algorithms work best for repetitive tasks and instances where speed is valued. Computer assisted decision making is beneficial in many situations, including travelling in an aircraft where autopilot is used, GPS used to drive cars, medical equipment used to save lives by constantly analysing data and computers making decisions about right medication and care. Male, 65+, online community If these systems enhance the user experience by saving time and recognising the user's probable needs and can then direct the user more specifically to the area they need to be, for example, in medical situations where there's an online questionnaire to determine what treatment would be the right one or not. Male, 35-44, online community In contrast, participants feel less comfortable with algorithms being used in more complex, nuanced and high-stakes situations, where emotional insight and a detailed understanding of personal situations is perceived to be important. Among participants, there is a strong sense that algorithmic decision making is not able to adequately account for the emotional aspects of the human experience. Interpersonal services which require empathy or emotional intelligence should not be replaced by computers. Male, 45-54, online community There are always going to be issues around the use of any type of artificial intelligence in weapons systems. When, if ever, should a drone be allowed to make an executive, autonomous decision on weapon deployment? Male, 35-44, online community Algorithms in the public sector While there is limited awareness of specific instances where algorithms are currently being used in the public sector, among some participants there is a general assumption that algorithms must be already being used by the public sector to streamline some processes. Given the perception that algorithmic decision making can help save resources, time and costs, participants are generally comfortable with the concept of algorithmic decision-making being used in the public sector. However, there is an assumption from some participants that algorithmic decision making may require expensive technology, and that the public sector may not be able to afford 'high-quality' systems. There is also a concern that algorithmic decision making in the public sector may also replace certain jobs, leading to unemployment. The issue I have with that is I know they cost a lot of money for those systems that are that intelligent, I would worry that the public sector won't be forking out the money for those machines - they'd be using the ones that don't have those features Focus group participant My fear in the public sector where I work, will they not need human beings anymore and we won't have jobs? Focus group participant Transparency in the public sector Despite the perceived importance of transparency in the public sector, there are very few specific positive or negative examples of public sector transparency that are front- of-mind for participants. On further probing, some participants identify the Covid-19 vaccination rollout as an example of where the Government has been successfully transparent. They feel that the public have been well-informed about the key decisions regarding the timings and order of the roll-out of vaccines, and the reasons behind these decisions. As well as being easy to understand, there is also a sense that the information has been timely and widely accessible. I think the COVID-19 vaccination plan is fairly understood by everyone. It was all very open and clear why the decisions were made the way they were. You can't get away from all the news briefings. Focus group participant In contrast, some participants express concern about the lack of transparency and clarity around individual decisions, including universal credit decisions. Not being given clear reasons and explanations for the final decision leads participants to feel that these decisions are inconsistent and unfair. A small number of participants feel that there is a certain degree of 'cronyism' in the current government, which they associate with a lack of transparency. I would like information on why they got it [financial support] and I didn't. All you get is a yes or a no. You don't get why. The government are not justifying their actions. Focus group participant",2023
govuk_033,govuk,Algorithm Transparency 3,"Updated 8 May 2025 (c) Crown copyright 2025 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visitnationalarchives.gov.uk/doc/open-government-licence/version/3or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:psi@nationalarchives.gov.uk. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned. This publication is available at https://www.gov.uk/government/publications/guidance-for-organisations-using-the-algorithmic-transparency-recording-standard/algorithmic-transparency-recording-standard-guidance-for-public-sector-bodies This guidance explains what the Algorithmic Transparency Recording Standard (ATRS) is, why it matters and how public sector organisations should use it. It includes section-by-section guidance for completing theATRStemplate. The Algorithmic Transparency Recording Standard (ATRS) enables public sector organisations to publish information about the algorithmic tools they are using and why they are using them. It consists of a template for organisations to fill in with key information about their algorithmic tools. This information is then published on the GOV.UK repository in the form of anATRSrecord. By using theATRS, public sector organisations can: TheATRSis a core part of the government'sBlueprint for Modern Digital Government, in particular the promise to 'Commit to transparency, drive accountability'. An algorithmic tool is a product, application, or device that supports or solves a specific problem using complex algorithms. We use 'algorithmic tool' as an intentionally broad term that covers different applications of artificial intelligence (AI), statistical modelling and complex algorithms. An algorithmic tool might often incorporate a number of different component models integrated as part of a broader digital tool. TheATRSis mandatory for certain organisations, and certain algorithmic tools within those organisations. It is mandatory for all government departments, and for ALBs which deliver public or frontline services, or directly interact with the general public. Within those organisations, theATRSis mandatory for algorithmic tools which have a significant influence on a decision-making process with public effect, or directly interact with the general public. This scope is designed to emphasise context, focusing on situations where a tool is influencing specific operational decisions about individuals, organisations or groups, not where a tool is an analytical model supporting broad government policymaking. Further detail, including examples of algorithmic tools in and out of mandatory scope, can be found in the scope and exemptions policy . If your organisation is within the mandatory scope of theATRSpolicy, it should have a single point of contact (SPOC) whose role is to coordinate with theATRSteam on identifying in-scope algorithmic tools, drafting and publishing records. You can email theATRSteam onalgorithmic-transparency@dsit.gov.ukif you are unsure who yourSPOCis. However, theATRSis recommended by the Data Standards Authority for use across the entire public sector and we have welcomedATRSrecords from local government, police forces and other broader public sector organisations. If you are from such an organisation, you can complete anATRStemplate and email it toalgorithmic-transparency@dsit.gov.ukdirectly. We recommend assigning a lead at your organisation to collate the relevant information from internal teams (and third-party providers, if applicable), to oversee the drafting and completion of the record, and to manage contact with theATRSteam. As outlined above, if your organisation falls within the mandatory scope of theATRS, aSPOCwill have been assigned. You should contact yourSPOCbefore beginning work on anATRSrecord. Email us onalgorithmic-transparency@dsit.gov.ukif you are unsure who yourSPOCis. If your supplier holds information that you need to complete a record, we encourage you to ask your commercial contact for the relevant details, explaining why you are asking for this information and why algorithmic transparency is important in the public sector. If your organisation and the tool is within mandatory scope of theATRSpolicy, you should highlight this. If the supplier is reluctant to share some information with you based on concerns around potentially revealing intellectual property, it can help to walk the supplier through the questions asked in the template, explain how they are designed to provide only a high-level picture of the tool. TheATRShas been designed to minimise possibly security or intellectual property risks that could arise from publications. Thescope and exemptions policy, modelled on theFOIAct, provides a detailed framework for exempting information from individualATRSrecords, or entireATRSrecords from publication. In general, publishing anATRSrecord and redacting certain fields with a brief explanation of why this has been done is preferable to not publishing anATRSrecord at all, particularly when partial information about the algorithmic tool is already in the public domain. Considerations for limiting the information in certain fields include: TheATRStemplate is available in two formats: an Excel version and a Google Sheets version for browser.Both can be downloaded here. Please do not alter or change the format of the template as this may affect our ability to process and publish yourATRSrecord. TheATRStemplate is divided into 2 tiers. Tier 2, whilst still accessible to the general public, is aimed at specialist audiences such as civil society, journalists, academic researchers and other public sector organisations wishing to learn from their peers. TheATRSaims to deliver meaningful transparency around public sector uses of algorithmic tools. This means not just acknowledging the existence of such tools, but providing an intelligible explanation of how and why they are being used. You should aim to complete theATRStemplate in full sentences, in clear and simple language. You may consider sharing the draft record with teams who are not connected to the algorithmic tool to check for understandability. For examples of existingATRSrecords which may help you complete the template, consult therepository. Fictional examples are also included in the guidance below. Tier 1 asks for basic, high-level information about the algorithmic tool aimed at a general audience without technical knowledge. All fields should be completed. The tool name will also appear in the title of yourATRSrecord, and will help people navigate theATRSrepository. It should be clear, concise and consistent throughout. Your description should be brief and clear, focusing on what the tool is and why it is being used (rather than technical detail of how it works, which comes later in the record). Remember that theATRSaims to show the public when and why algorithmic tools are being used in processes that affect them. Ideally the description should be no more than two or three sentences. Not all algorithmic tools will have a relevant website. If providing one, please ensure it is live and publicly accessible - otherwise, enter 'N/A'. The email address you provide should be that of the team responsible for the tool, not an individual, for business continuity and security purposes. When an individual leaves the organisation but the wider team remains, the email address will still be up to date. LeaseSureAIuses machine learning to analyse council housing rent accounts and create a prioritised caseload of rent arrears for housing officers. The tool is designed to work alongside existing housing management systems within the council to help improve arrears management. This section focuses on accountability for the development and deployment of the tool. All fields should be completed, with 'N/A' where necessary. TheSROshould be a role title, not a named individual, for business continuity and security purposes. It should be the role which is ultimately accountable for the tool in an operational context. This may be the policy or service owner, for example. Third parties include commercial suppliers and other public sector organisations who may, for example, have developed an in-house algorithmic tool which they are sharing with your organisation. A procured tool can involve multiple companies at different places in the supply chain. For instance, a public body could procure a tool from a company, which in turn procured the model and data from another company before integrating the model into a customisable tool. Ideally, you should describe those different supplier relationships as clearly and concisely as possible, detailing which organisation was or is responsible for which part of the final tool that you are deploying. Yes Sulentra Dynamics Ltd. 813004659779X Sulentra Dynamic has provided LeaseSureAIfor a six-month pilot. Proof-of-concept pilot (a formal procurement process will follow if the tool demonstrates measurable benefit after the trial period). Sulentra Dynamics Ltd. has been provided with controlled, read-only access to renting accounts data in the council's MundioTenancy platform, but it does not integrate with other systems. This has been done in compliance with data protection legislation and all Sulentra Dynamics Ltd. staff with access to the data have been subject to appropriate vetting checks. Access to the data is only granted for the limited period of time while the tool is developed. This section expands on the high-level description given in Tier 1, with more granular detail about the algorithmic tool, its scope and the justification for its use. In contrast to the basic description of the tool in Tier 1, which focuses on what the tool is and why it is being used, the Tier 2 detailed description here aims to explain how the algorithmic tool works. As such, you should describe the tool's purpose, its intended users, key aspects and functions at a more granular level. You should also include the tool's scope, as well as limitations or context where it does not apply. Whilst the amount of information provided here will vary between algorithmic tools, we typically expect a paragraph or two of text. LeaseSureAImonitors tenant payment patterns and predicts financial distress using models like Logistic Regression and Random Forest Classifier on historical rent data. It utilises the Logistic Regression (LR) model first to analyse changes in rent payment patterns (e.g. type and date of payment) and predicts the probability of falling into arrears. TheLRmodel produces a list of 'at risk' accounts, which is then analysed further by the Random Forest Classifier (RFC) model. Based on features such as payment trends and arrears duration (e.g. '30-Day Arrear', '60-Day Arrear', '90-Day Arrear', etc.), it classifies accounts into 'Low', 'Medium', and 'High' risk. The output of the tool is a weekly prioritised caseload that is integrated into the council's existing MundioTenancy platform for housing officers to review and action. You may choose to provide a list of individual benefits or a few sentences of description. Where possible, try to explain how and why the tool should deliver the benefit. For example, rather than just stating 'improved customer experience', explain how and why the tool should achieve this. Your algorithmic tool may have replaced a legacy tool or a manual process. In either case, you should provide a brief description of what it replaced. If your algorithmic tool is part of a brand-new process (for example, delivering a programme which did not previously exist), you should make this clear. Briefly explain why no alternatives were considered. For example, you may be using an algorithmic tool provided by a central government department for others to use. This section should help people understand how the algorithmic tool ultimately helps to deliver an operational process or service, and how humans are involved in this delivery. We typically expect a paragraph or two of text. It can be helpful to frame the answer around the output that the tool produces and how this is then used, for example to determine the outcome of an application process, or to deliver a public service. You should aim to make clear the degree of automation that the tool delivers within the broader process. LeaseSureAIdoes not automate decisions. Instead, it provides a recommended list of priority cases, which are categorised as 'Low', 'Medium' or 'High', based on their risk of falling into payment arrears. Each case on the list includes reasons, such as 'Escalating 60-Day Arrears', 'Payment Arrangement Broken', or 'Benefit Reduction Detected', that then help housing officers interpret the tool's outputs effectively. Housing officers review the list weekly and record any actions taken (e.g. sending notifications of payment arrears to tenants) directly in the council's tenancy management platform, MundioTenancy. You should consider both the outputs of the algorithmic tool itself and whether they can be challenged or appealed, and the outputs of the broader operational process and whether they can be challenged or appealed. This may involve providing a link to a public appeal or contact form. If no appeals or review process is necessary or relevant for your tool, include a short sentence explaining why you are not completing this section. You should also be aware of Article 22 UKGDPRwhich states that 'The data subject shall have the right not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her or similarly significantly affects him or her'. If your algorithmic tool falls within the scope of these provisions, you must complete this field. Further information can be found via theICO's guidance on 'Rights related to automated decision making, including profiling'. This section should detail the technical specifications of the algorithmic tool. As outlined above, the level of detail here should not infringe on a supplier's intellectual property rights, or generate cybersecurity risks. You should broadly describe how your tool is organised and provide context for the rest of the technical section. System representations such as AWS diagrams are ideal for conveying this type of information in a concise way - they capture the primary components of your technology stack and how they interact. You should think about the end-to-end process by which your tool processes inputs, the digital services and resources that it uses and the environments in which system processes occur. Any models that you consider later should be mentioned in this field. You can see a helpful example of the diagram of system architecture provided by the Department for Health and Social Care in their algorithmic transparency report for theQCovid tool here. For tools that consist of multiple machine learning models, this will be the primary input into or output from the system as a whole. For tools that consist of only one machine learning model, the system-level input and output, and the model input and output, may be the same. These fields should include the expected formats and data types. Both historical and near real-time structured, tabular tenancy-related and financial data such as rent transaction history, payment type and date history, payment due dates, broken promise amounts, rent status, etc. The tool's output is a prioritised caseload of accounts that are ranked by risk of non-payment - i.e. 'High', 'Medium' and 'Low'. The output list is delivered to, and integrated with, the council's interactive MundioTenancy platform in the form of an interactive caseload, with the option to export it in CSV or Excel file formats for reporting and audit purposes. This section should detail the model or models used within the algorithmic tool. Should your tool consist of more than one model, please duplicate this sheet and complete a separate sheet for each individual model. N.B. For off-the-shelf models that your organisation has not trained, validated, tested, or applied any refinement techniques to (e.g. Web UI-based LLMs), please leave both the Model Specification (2.4.2) and following Development Data Specification (2.4.3) sections blank and move straight to the Operational Data Specification (2.4.4) section instead. For tools that consist of more than one model, please make and complete copies of the Model Specification (2.4.2) section in the template for each model. As a minimum, the fields in this section should include the type of model. If using a pre-trained model, please also specify the name of the API provider, where applicable, or mention if it is 'self-hosted'. Using Logistic Regression from the scikit-learn library in Python, which has pre-defined parameters. Further details can be found on the scikit-learn website Whereas the system architecture refers to how the model is integrated into the broader technical architecture, while model architecture describes the internal structure of the model - i.e. how it works or how it transforms an input into an output. At a minimum, you should enter the type of model used (e.g. Logistic Regression, Decision tree, Random Forest Classifier, Convolutional Neural Network, Rule-Based System, etc.). If the model has been designed such that certain features or inputs are given more priority over others, and where this has significant bearing on the model's output, then indicate what those features are. For rule-based systems, describe how the rules are structured and indicate if any rules are weighted or prioritised over others. You may also provide a publicly accessible link to further resources. For security, do not include details of the network architecture to which the tool is connected. If it aids understanding of the model, you are also encouraged to provide further details or provide a link to publicly available resources that offer further information. Using Logistic Regression from scikit-learn library in Python, which has pre-defined parameters. NV-Administration is an optimisation-based automated planning model. The model consists of: Performance metrics will differ based on what type of method and tool you are developing or deploying. Useful metrics to consider may include accuracy metrics such as precision, recall or F1 scores, metrics related to privacy, and metrics related to computational efficiency. You should also describe any bias and fairness evaluation you have undertaken (i.e. model performance over subgroups within the dataset), and any measures taken to address issues you identify. For more information about setting performance metrics, you may find thisGOV.UK Service Manual guidancehelpful. Useful resources may include the government'sAIPlaybookand the former Centre for Data Ethics and Innovation'sReview into bias in algorithmic decision-making, especially chapters 1 and 2. For more information about bias in algorithmic decision-making, see theRTAU(formerly Centre for Data Ethics and Innovation) review into bias in algorithmic decision-making, especially Chapters 1 and 2. For more information about how to mitigate bias in algorithmic decision making, you may find it helpful to review theRTAU's repository of bias mitigation techniques which can be found here. This section aims to expand on '2.4.2.8 Datasets and their purposes' in the Model Specification section. It focuses on the data used to train, validate or test your model(s). Provided you have not trained, validated, tested or applied any refinement techniques to an off-the-shelf model (e.g. Web UI-based LLMs), you should leave the Development Data Specification section blank and move straight to the (Operational Data Specification) section. The aim of this field is to describe all of the datasets used for developing the tool as a whole. Where possible, please provide publicly accessible links to these datasets. (This differs from the datasets and their purposes field in the Model Specification, which simply asks for a list and specification of what each dataset was used for). The purpose of the 'data quantities' field is to sense-check proportionality of data in relation to the model task and complexity. Where a learning algorithm is applied to data, small datasets with few samples are more likely to yield underfitting models, while large datasets with numerous attributes may cause overfitting. In addition, too few samples may indicate insufficient representation of a target population, and too many attributes may indicate increased data security risks (such as de-identification). While we don't prescribe a specific definition of 'sensitive', we encourage you to consider: In certain cases, it might not be feasible to disclose all the sensitive attributes in the data. At a minimum, you should disclose the fact that you are processing sensitive data and add as much detail as appropriate. It is unlikely that theATRSrecord will lead to individuals being made identifiable as you are only being asked to provide a general description of the types of variables being used. If you are considering making the dataset you are using openly accessible and linking to it, you should comply with the relevant data protection legislation to prevent individuals from being made identifiable from the dataset. This should also be considered as part of a Data Protection Impact Assessment (DPIA). For further guidance on completingDPIAs, please refer to theICO'sguidance. You may also find it helpful to consult theICO'sAIand data protection risk toolkit. This section focuses on the data used or produced in your algorithmic tool's real-world operation, such as user inputs, retrieved documents, system-generated logs and other data generated during use. See above. This section should provide information on impact assessments conducted, identified risks, and mitigation efforts. No, there is no need to provide a summary if you are providing an openly accessible link to the full assessment. Categories of risk likely to be relevant include: This list is not exhaustive and there may be additional categories of risk that are helpful to include. The Government Finance Function'sOrange Bookprovides further guidance on conducting risk assessments for public sector projects. You may also find it helpful to consult theICO'sAIand data protection risk toolkit. Review and feedback Email your completedATRStemplate toalgorithmic-transparency@dsit.gov.uk(or send to yourSPOC, if your organisation has one). TheATRSteam will check for readability and provide feedback or suggested amendments if necessary. Before finalising your record, you should consider the possibility that publishing information on your algorithmic tool may invite attention and scrutiny from the public and media. This is particularly true for more high-profile use cases and where the use of an algorithmic tool has not been publicly disclosed before. You can help to mitigate these risks by ensuring you provide clear information and an appropriate level of detail in your record. You should also ensure that your organisation's communications team or press office is aware of the plan to publish, has reviewed the record and has prepared to respond to media requests if deemed necessary. If your organisation has aSPOC, you should ask them to coordinate this. You may wish to consider publishing supplementary information, for example a blog post explaining what the algorithmic tool is, or a link to theATRSrecord on the relevant service or policy pages on your website. TheATRSteam requires written confirmation that yourATRSrecord has gone through all appropriate internal signoff procedures before publishing it to the GOV.UK repository. At a minimum, this should include clearance by: In certain high-profile instances it may be appropriate to seek ministerial clearance. Should substantive details change in relation to yourATRStool, you should update theATRStemplate, go through internal clearance again, and send the updated template toalgorithmic-transparency@dsit.gov.ukasking for your record to be updated accordingly. Substantive changes might include a pilot tool moving to production, new datasets being used to train or refine the tool, or a change to the broader operational process of which the tool is part. Should you decommission an algorithmic tool for which you have published anATRSrecord, contact the team onalgorithmic-transparency@dsit.gov.uk.",2023
govuk_034,govuk,Algorithm Transparency 4,"A template for public sector organisations to share information about how and why they are using algorithmic tools. MS Excel Spreadsheet,270 KB This file may not be suitable for users of assistive technology. https://docs.google.com/spreadsheets/u/0/d/15YzxQiGXT2E4d4AiWEbqkumMVIh4UF6Dg5PjjeDUxEU/edit?gid=2100191493&pli=1&authuser=0 The ATRS template is available in Microsoft Excel and Google Sheets formats. It is divided into 2 tiers. Tier 1: Tier 2, whilst still accessible to the general public, is aimed at specialist audiences such as: Tier 2 is also split across eight further sheets. You should complete all sections of the template. For further assistance read theguidance to using the ATRS. Email the completed template to the ATRS team onalgorithmic-transparency@dsit.gov.ukfor publication to theATRS repository. Added the updated version of the template in two formats, Excel and Google Sheets. Added the updated version of the template in two formats, Excel and Google Sheets. Added an HTML version of the form content. First published.",2023
govuk_035,govuk,Algorithm Transparency 5,"Published 17 December 2024 (c) Crown copyright 2024 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visitnationalarchives.gov.uk/doc/open-government-licence/version/3or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:psi@nationalarchives.gov.uk. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned. This publication is available at https://www.gov.uk/government/publications/algorithmic-transparency-recording-standard-mandatory-scope-and-exemptions-policy/algorithmic-transparency-recording-standard-atrs-mandatory-scope-and-exemptions-policy This is the mandatory scope and exemptions policy for the Algorithmic Transparency Recording Standard (ATRS). It sets out which organisations and algorithmic tools are in scope of the mandatory requirement to publishATRSrecords, as announced in the previousgovernment's responseto the consultation on theAIWhite Paper ""A pro-innovation response toAIregulation"" in February 2024. It also sets out the required steps to ensure that sensitive information is handled appropriately. Since the beginning of 2022, the Algorithmic Transparency Recording Standard (ATRS) has been an established mechanism for the proactive publication of information on the use of algorithmic tools in the public sector. TheATRShas been piloted with various organisations, enhanced and iterated multiple times, and on 6 Feb 2024 it was mandated across all government departments, with a stated intent to extend the requirement to the broader public sector over time. To implement this mandatory rollout, we need to establish clear lines about which organisations and tools are in scope, and the type of information that, for various reasons, may be too sensitive for publication on GOV.UK. Transparency around how the public sector is using algorithmic tools is useful and appropriate in most circumstances and should be our default position. However, there is some need for caution to make sure that information that is sensitive or confidential is handled properly. This document sets out the scope for mandatory publication ofATRSrecords, implementing the cross-government policy set out in theAIWhite Paper consultation responseon 6 Feb 2024. It sets out: The organisations in scope of this policy, which currently comprise central government (with an intent to extend this more broadly across the public sector in future) The algorithmic tools that are in scope The steps to be taken to make sure that sensitive information is handled appropriately The mandatory requirement to completeATRSrecords currently applies to central government. For the purposes of this policy, this consists of the following: Ministerial departments, and Non-ministerial departments, and Arm's-length-bodies (ALBs), meaning executive agencies and non-departmental public bodies, whichprovide public or frontline services, or routinely interact with the general public. This scope is intended to capture the majority of central government uses of algorithmic tools, without placing a disproportionate burden on large numbers of very small or non-frontline arm's-length bodies that are unlikely to be responsible for any in-scope algorithmic tools. We will work with departments to finalise which arm's-length bodies fall in or out of scope, but as examples we would anticipate the following: Examples of organisations in mandatory scope: All ministerial departments e.g.MoJ,DfE,DSIT All non-ministerial departments e.g.HMRC, the National Archives, Competitions and Markets Authority All other arm's-length bodies (ALBs) that provide public or frontline services, or routinely interact with the general public e.g. HM Land Registry, HM Prisons and Probation Service Examples of organisations out of mandatory scope: Shared Business Services Limited National Infrastructure Commission Biometric and Forensics Ethics Group The initial rollout of the mandatory policy is proceeding in two phases: Phase 1:Most ministerial departments andHMRC, specifically: Cabinet Office Department for Business and Trade Department for Culture Media and Sport Department for Education Department for Energy Security and Net Zero Department for Environment Food and Rural Affairs Department for Science, Innovation and Technology Department for Transport Department for Work and Pensions Department for Health and Social Care Foreign, Commonwealth and Development Office HM Revenue and Customs HM Treasury Home Office Ministry of Defence Ministry of Housing, Communities and Local Government Ministry of Justice Phase 2:The remaining ministerial and non-ministerial departments, and arm's-length bodies that fall in the scope listed above. As a DSA-endorsed Standard, theATRSremains recommended across the broader public sector. Hence, the sections below will also be useful to other organisations in determining for which tools it would be good practice to publishATRSrecords. Organisations determined to be in mandatory scope above are required to publishATRSrecords for algorithmic tools they are currently using in relevant use cases. An algorithmic tool is a product, application, or device that supports or solves a specific problem using complex algorithms. We use 'algorithmic tool' as an intentionally broad term that covers different applications of artificial intelligence (AI), statistical modelling and complex algorithms. An algorithmic tool might often incorporate a number of different component models integrated as part of a broader digital tool. The mandatory requirement to publish anATRSrecord applies to algorithmic tools that either: have a significant influence on a decision-making process with public effect, or directly interact with the general public. 'Significant influence' includes where an algorithmic tool meaningfully assists, supplements, or fully automates a decision-making process. This could be a tool that plays a triaging or scoring function within a wider process. By 'public effect' we mean a decision-making process having an impact on members of the public, where the latter are understood as any individuals or groups of individuals, irrespective of their nationality or geographical location. Impact on members of the public also includes algorithmic tools directly processing data or information people have submitted as part of a wider process, e.g. an application, complaint or consultation submission. To decide whether a decision-making process has a public effect, you might want to consider whether usage of the tool assists, supplements or fully automates a process which: materially affects individuals, organisations or groups has a legal, economic, or similar impact on individuals, organisations or groups affects procedural or substantive rights impacts eligibility for, receipt of, or denial of a programme Note that this is intended to apply to situations where an algorithmic tool is influencing specific operational decisions about individuals, organisations or groups, not where a tool is an analytical model supporting broad government policy-making. Analytical models in scope of the guidance in theAqua Bookwill typically be outside ofATRSscope (though it is possible to envisage some specific circumstances where both would be applicable, see examples below). Examples of tools that could fall within the scope of these criteria are: a machine learning algorithm providing members of the public with a score to help a government department determine their eligibility for benefits (impact on decision-making with public effect) a chatbot on a government website interacting directly with the public which responds to individual queries and directs members of the public to appropriate content on the website (direct interaction with the public) Examples of tools that would likely not fall within the scope of the criteria include: A tool being used by a government department to transform image to text (e.g. used in digitisation of handwritten documents) as part of an archiving process (no significant decision or direct public interaction) An automated scheduling tool which sends out internal diary invites from a mailbox (doesn't have public effect) Further examples are listed below in Annex 1. To emphasise, the context of use of the algorithmic tool matters here. The same image to text algorithm above might be relevant if being used instead to digitise paper application forms for a government service (e.g. poor performance of the algorithm on some handwriting styles might well have an influence on success rates for individual applicants). Note that the algorithmic tool scope listed above is that of the policy for mandatoryATRSadoption in central government. If you are using an algorithmic tool that does not strictly meet these criteria but you would like to provide the general public with information about it, you can still fill out and publish an algorithmic transparency record. The mandatory requirement to publish anATRSrecord applies to tools that are in Beta/Pilot or Production phase. Teams are welcome to submit records for tools in earlier stages of the lifecycle, but it is not mandatory to do so. For tools that have previously been in use and had a record created for them and which are later being retired, the responsible team should submit an updated record changing the information in the phase field to 'Retired'. This update will be reflected on the published record on GOV.UK. TheATRShas been designed to minimise likely risks that could arise from publication of records (e.g. to security, privacy or intellectual property). Situations where no information can be safely published are expected to be unusual (e.g. in cases where even the existence of a tool cannot be made public for security reasons). More commonly, for some tools, there may be particular information requested in theATRSthat you may be concerned about releasing into the public domain, even if the majority of the information about the tool is publishable. This may relate to a risk of gaming the tool, risks to national security, infringing intellectual property or releasing commercially sensitive information. In most such instances, the appropriate response is to reduce the level of information supplied for relevant fields, for example giving a broad description of the type of data used by a tool instead of specific details of individual data sources, or a broad summary of how a tool works instead of precise information about the system architecture. In developing the rationale behind the exemptions for this policy, we align with those set out in the Freedom of Information Act 2000 (""FOIA""). AlthoughFOIAis a reactive means of providing information to the public while the publication ofATRSrecords is proactive, we settled on using theFOIAexemptions as a basis for our exemptions policy since the logic around which types of information are too sensitive to publish openly remains the same. Moreover, theFOIApolicy is firmly established in the public sector as business as usual, thus this will reduce the administrative burden for organisations to comply with theATRSpolicy mandate. As a general rule, thisATRSScope and Exemptions Policy does not require the publication of information that would be subject to an exemption under access to information legislation, i.e. theFOIA, Environmental Information Regulations and data protection legislation. To understand how this applies in practice, imagine that you created a full internal version of anATRSrecord and (hypothetically) received an FOI request to publish that record. How would you respond to such a request? If you would release the record in full, then the same applies to proactive publication of the record. If you would release some of the information in the record, but would need to redact some of it under FOI exemptions, then you should remove or de-sensitise the exempt content from theATRSrecord prior to publication. In (rare) circumstances where you were not able to confirm the existence of the tool, e.g. you would issue a neither-confirm-or-deny response to the hypothetical FOI request, then it would be inappropriate to publish theATRSrecord at all. Not allFOIAexemptions are relevant here. Specifically: TheATRSis designed to capture tool-level information rather than personal information. Concerns about publishing personal data should therefore not apply when considering whether to remove or de-sensitise partial or wholeATRSrecords (FOIAsection 40). There are limitations on cost of responses, vexatious queries and information already in the public domain, that are necessary for a reactive duty such as theFOIAto avoid disproportionate effort in responding to an unbounded number of incoming requests (FOIAsections 12, 14, 21, 22). They are not relevant to the publication ofATRSrecords which is inherently limited to one record per tool. Exemptions for reasons of commercial sensitivity (FOIAsection 23) need to be applied with care seesection on dealing with commercially sensitive information. As mentioned above, in most cases it will be sufficient to give higher-level information within particular fields. However, where entire fields are fully exempt from publication, or you wish to indicate explicitly why the information in a record is limited, we recommend recording this within a record in the following format, with the third column giving a general description of the reason. For example: Example 1 Example 2 If you have any concerns about publishing information that are not covered above, we would ask you to get in touch with theATRSteam to discussalgorithmic-transparency@dsit.gov.uk. Many algorithmic tools used in the public sector will involve an external supplier in some form, and hence publication of anATRSrecord will require some consideration of commercially sensitive information. There is a need for care in applying commercial exemptions, i.e. in applying Section 43 of the Freedom of Information Act. If this exemption is applied too broadly, it would undermine the overall intent of this policy to increase transparency on the government's use of algorithmic tools and limit its benefits. Commercial suppliers that wish to sell algorithmic solutions to public bodies that are then used in processes that impact members of the public should be comfortable with this level of transparency that is expected of the public sector. Public bodies that are procuring solutions from vendors should make this expectation clear in their invitation to tender or other route to market. The primary focus of theATRSis the use case that a tool is deployed into, and the steps taken to ensure that a tool is appropriate in that context. Though there is some information required about the technical aspects of the tool, there is flexibility on how much information is provided here, and the Standard is designed to be consistent with emerging industry good practice on model cards (and indeed information that might often be published by vendors in a white paper). As such, public authorities are encouraged to work with their supply chains at the start of the process to minimise the amount of information being withheld for commercial reasons.",2023
govuk_036,govuk,Algorithm Transparency 6,Organisations and algorithmic tools in scope of the mandatory requirement to publish ATRS records. HTML This is the scope and exemptions policy for the Algorithmic Transparency Recording Standard (ATRS). It sets out which organisations and algorithmic tools are in scope of the mandatory requirement to publishATRSrecords. It also sets out the required steps to ensure that sensitive information is handled appropriately.,2023
govuk_037,govuk,Algorithm Transparency 7,"This report, commissioned by the CDEI, outlines the findings from a deliberative public engagement exercise conducted by BritainThinks about algorithmic transparency in the public sector. PDF,1.32 MB,51 pages HTML In the CDEI'sreview into bias in algorithmic decision-making, it recommended that government should place a mandatory transparency obligation on all public sector organisations using algorithms when making significant decisions affecting individuals. To move this recommendation forward, the CDEI has been supporting theCentral Digital and Data Office (CDDO)as it develops a standard for algorithmic transparency. This report details findings from a deliberative public engagement exercise that the CDEI commissioned BritainThinks to carry out to explore what meaningful transparency about the use of algorithmic decision-making in the public sector could look like in practice. Participants felt that all categories of information about algorithmic decision-making should be made available to the public. Participants prioritised information about the role of the algorithm, why it is being used, and how to get further information or raise a query. When working to design a prototype information format, participants allocated information categories to different tiers, with information such as the purpose of the algorithm being in 'tier one', and more detailed information, such as technicalities of the algorithm, being in 'tier two'. Participants expected the information in 'tier one' to be immediately available at the point of, or in advance of, interacting with the algorithm, while they expect to have easy access to the information in 'tier two' if they choose to seek it out. Different use-cases impacted how proactively participants felt transparency information should be communicated. For lower potential risk and lower potential impact use-cases (such as the use of an algorithm in a car park), passively available transparency information was acceptable on its own. For higher potential risk and higher potential impact use-cases (such as the use of an algorithm in a recruitment process) there was a desire for active communication of basic information upfront to notify people that the algorithm is being used and to what end, as well as the passively available transparency information. It was felt that this information should be more targeted and personalised. In addition to this public engagement work,CDDO, as policy sponsor, have been running a series of workshops with internal stakeholders and external experts, to discover what information on the use of algorithmic decision-making in the public sector they would like to see published and in what format. Its findings will be consolidated with the outcomes of this public engagement project and will inform the development of a standard for algorithmic transparency. The prototype of this standard will then be tested and evaluated in an open and participatory manner. Added HTML version of the report First published.",2023
govuk_038,govuk,Algorithm Transparency 8,"Updated 2 December 2021 (c) Crown copyright 2021 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visitnationalarchives.gov.uk/doc/open-government-licence/version/3or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:psi@nationalarchives.gov.uk. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned. This publication is available at https://www.gov.uk/government/publications/algorithmic-transparency-data-standard/algorithmic-transparency-recording-standard-v21",2023
govuk_039,govuk,Algorithm Transparency 9,"The DRCF's Algorithmic Processing workstream held two workshops with 22 industry stakeholders to explore transparency during the procurement of algorithmic systems. This paper describes insights and key findings from our workshops. PDF,509 KB,15 pages In 2021, the DRCF established an Algorithmic Processing workstream to explore the impact of algorithms across our industries and regulatory remits. The workstream published two discussion papers on thebenefits and harms of algorithmic systemsand onauditing algorithms. In our 2022 to 2023 workplan, we committed to further research on transparency and accountability during the procurement process, after concerns were raised by stakeholders in previous research. In our workshops with industry stakeholders we explored how the procurement of algorithmic systems takes place, barriers that inhibit effective information sharing between parties, and potential solutions that could help to address these challenges. Transparency will continue to be a focus of the DRCF's work, as well as for other regulators, governments and regions who are developing approaches to AI regulation and governance. We hope that this paper will inform further research to support the development of good practice on algorithmic transparency.",2023
govuk_040,govuk,Automated Decision-Making 0,"A summary of changes to the UK's data protection and privacy legislation in the Data (Use and Access) Act. The Data (Use and Access) Act 2025 (""DUAA"", ""the Act"") received Royal Assent on 19 June 2025. This is a wide-ranging Act which includes provisions to enable the growth of digital verification services, new Smart Data schemes like Open Banking and a new National Underground Asset Register. It also includes some important changes to the UK's data protection and privacy legislation, which are the subject of this page. TheDUAAwill not replace the UK General Data Protection Regulation (""UKGDPR""), Data Protection Act 2018 or the Privacy and Electronic Communications (EC Directive) Regulations 2003, but it will make some changes to them to make the rules simpler for organisations, encourage innovation, help law enforcement agencies to tackle crime and allow responsible data-sharing while maintaining high data protection standards. This page covers the key changes. It is not a comprehensive list of all the data protection measures in the Act and is not regulatory guidance or legal advice. The Information Commissioner's Office (""ICO"") is responsible for publishing regulatory guidance on its website -www.ico.org.uk. TheDUAAcreates a more permissive framework under the UKGDPRfor organisations to make decisions based solely on automated processing that have legal or similarily significant effects on individuals. Organisations will be able to make such decisions in wider circumstances but must implement certain safeguards. These include: The Act broadly mirrors these provisions and safeguards across the law enforcement regime in the Data Protection Act 2018. However, for those processing under the law enforcement regime, the Act provides for an exemption to the safeguards for certain, limited, reasons, such as to safeguard national security, or to avoid the obstruction of an inquiry. This is as long as the decision is reconsidered, as soon as reasonably practicable after it is taken and the reconsideration involves meaningful human involvement. TheDUAAclarifies the time limits for organisations to respond to subject access requests (requests by individuals to access and receive a copy of their personal data). It includes a ""stop the clock"" rule, allowing organisations to pause the response time if they need more information from the requester. Once they get the information they need, the response time continues. Organisations need to make reasonable and proportionate searches when responding to requests. New rules require certain online services likely to be accessed by children to consider how to protect and support them when designing these services. TheDUAAclarifies that scientific research may include commercial research. It allows researchers to seek consent for broad areas of related research and clearly outlines the safeguards required for using personal data in research. The Act introduces a new lawful ground for processing personal data, giving businesses more confidence to use data for crime prevention, safeguarding, responding to emergencies, and other specified legitimate interests. The Act simplifies the rules and provides necessary clarification for transferring personal data internationally. TheDUAArequires organisations to handle complaints from individuals who are concerned that the way their information is used breaches the data protection legislation, for example by providing an electronic complaint form, and informing the individual about the outcome of a complaint. The Act allows the use of storage and access technologies without explicit consent in certain, low-risk situations. TheDUAAamends Parts 3 and 4 of the 2018 Act which regulate law enforcement processing and processing by the intelligence services. Some of these amendments mirror key changes being made to the UKGDPR, ensuring consistency across the data protection regimes. Others simplify the legislation, enabling those processing under the law enforcement regime to operate more efficiently and supporting closer working with the UK intelligence agencies to safeguard national security. The changes to data protection law will be commenced in stages, 2 - 12 months after Royal Assent. Exact dates for each measure will be set in commencement regulations. Information about the commencement regulations will be provided on GOV.UK when it is available. Parliamentary information and supporting documents about the Data (Use and Access) Act can be found at:https://bills.parliament.uk/bills/3825 Data (Use and Access) Act 2025 Links to commencement regulations will be provided on GOV.UK. TheDUAAwill amend (update) the below legislation. The updated legislation will be available within approximately 2 months of Royal Assent of theDUAA. Data Protection Act 2018 UK General Data Protection Regulation The Privacy and Electronic Communications (EC Directive) Regulations 2003 ICOplans for new and updated guidance.",2023
govuk_041,govuk,Automated Decision-Making 1,"Find guidance for the responsible use and development of data and data technologies developed by and for government and public sector bodies. Use this tool to find data ethics guidance from across government. You can use the filter and search functions to identify the most relevant pieces of information for your organisation's needs. Start now Emaildata.ethics@digital.cabinet-office.gov.ukif you have questions or suggestions about data ethics guidance, or are aware of further relevant guidance that should be included. We encourage anyone working in the government and public sector to refer to the documents below to develop a high-level understanding of key data ethics considerations. TheData Ethics Frameworkexplains how to use data appropriately and responsibly when planning, implementing and evaluating a new policy or service. TheModel for Responsible Innovationis a practical tool to help teams across the public sector and beyond to innovate responsibly with data andAI. When working with public sector data, you have a responsibility to establish whether the data you manage and use is fit for purpose. TheGovernment Data Quality FrameworkandData Sharing Governance Frameworkset out principles and practices to improve the quality and better use of data across government. TheGenerativeAIFramework forHMGexplains how to use generativeAIsafely and responsibly. TheAlgorithmic Transparency Recording Standard (ATRS)provides a standard for public sector organisations to publish information about how and why they are using algorithmic methods in decision-making processes that affect members of the public. TheEthics, Transparency and Accountability Framework for Automated Decision-Makingaims to help government departments with the safe, sustainable and ethical use of automated and algorithmic decision-making systems. AIassurance is vital to ensure the reliability and trustworthiness ofAIsystems. The Responsible Technology Adoption Unit's (RTA)introduction toAIAssuranceidentifies assurance techniques that can support the development of responsibleAI. You can find additional information about tools and processes that support the responsible use ofAIat theRTA'sResponsibleAIToolkitpage. You must use the criteria inThe Technology Code of Practiceto design, build and buy technology in government. When procuringAIsolutions from third parties, refer to theGuidelines forAIprocurement. This includes principles for buyingAItechnology and insights on tackling challenges that may arise during procurement.",2023
govuk_042,govuk,Automated Decision-Making 2,"This privacy notice explains what personal data is collected through the extended producer responsibility for packaging service and how that personal data is used. Extended Producer Responsibility (EPR) for packaging is a component part of wider Collection and Packaging Reforms (CPR) programme that will require certain producers of packaging to take greater responsibility for its disposal and recycling. This privacy notice explains what personal data is being collected through the services (including 'report packaging data') provided under the EPR for packaging and how that personal data is used. If you have any queries about the content of this privacy notice, email: pEPRdataandserviceownerteam@Defra.gov.uk The Department for Environment, Food and Rural Affairs (Defra) is controller for the personal data we collect: Department for Environment, Food and Rural AffairsSeacole Building2 Marsham StreetLondonSW1P 4DF If you need further information about how Defra uses your personal data and your associated rights, you can contact the Defra data protection manager at the above postal address or email: data.protection@Defra.gov.uk The data protection officer for Defra is responsible for checking that Defra complies with legislation. you can contact the Defra data protection manager at the above postal address or email: DefraGroupDataProtectionOfficer@Defra.gov.uk Defra is also acting as the 'data processor' for the personal data being collected through some services. As the data processor, we collect and share data on behalf of the following 4 environmental regulatory bodies: Depending on the services you use, Defra collects your: We collect this data for the following purposes. The environmental regulators will use your data to undertake their statutory monitoring role for EPR for packaging. This includes: The scheme administrator will use the data to: Defra will use your data: Depending on the services you use, Defra obtains your personal data from: The lawful basis for processing your personal data is that it is necessary to: EPR for packaging was introduced in support of the Government's25 Year Environment Plan. This is underpinned bysections 50 and 51 of Part 3andSchedule 4andSchedule 5of the Environment Act 2021. It is also supported by legislation in each of the four nations: The processing of your personal data is not based on consent. You cannot withdraw it. We will share the personal data collected under this privacy notice with the environmental regulators. We do this to deliver services as part of EPR for packaging. The environmental regulators are controllers in their own right. You can learn more about how your personal data is processed by readingthe environmental regulators' joint privacy notice. We will publish a public list of large producers (PLLP) in accordance with legislation in each nation and subsequent amendments. Defra respects your personal privacy when responding to access to information requests. We only share information when necessary to meet the statutory requirements of the Environmental Information Regulations 2004 and the Freedom of Information Act 2000. We will keep your personal data for 7 years in line with the requirements of the EPR for packaging, except data related to Invoicing activities which will be retained for 11 years. You will not be able to use EPR for packaging services. The personal data you provide is not used for: Defra will only transfer your personal data to another country that is deemed adequate for data protection purposes. Based on the lawful processing above, your individual rights are: Find out more about your individual rightsunder the UK General Data Protection Regulation (UK GDPR) and the Data Protection Act 2018 (DPA 2018). You have the right tomake a complaint to the Information Commissioner's Officeat any time. Defra's personal information charterexplains more about your rights over your personal data. Small update: the scheme administrator and Defra may use your data to contact you about your submission or new documents in your account. This updates the privacy notice to cover aspects of the extended producer responsibility for packaging programme beyond the report packaging data service. Added translation",2023
govuk_043,govuk,Automated Decision-Making 3,"Your technology should adapt to future demands and work with existing technologies, processes and infrastructure in your organisation. To meet point 9 of theTechnology Code of Practice(TCoP) your plans must show you've considered how your technology project or programme integrates into your organisation. If you're going through thespend control processyou must explain how you're trying to meet point 9. Good integration means making sure your new technology works with legacy solutions without limiting your ability to adapt to future demands or upgrade systems. Your programme will benefit from: Each organisation's technology and infrastructure will have unique services and issues. But there are some common elements to consider when fitting new technology into your current or legacy system, including: Read how theDepartment for Transport's Chief Architect has created a Digital Design Authorityto help integrate new technology with their current technology. To optimise systems integration consider: If you have chosen to use a systems integrator you should make sure they meet all of your requirements. A number of government organisations are using or investigating emerging technologies. If you're thinking about introducing emerging technology to your infrastructure, you should make sure it meets user needs. You'll need to investigate alternative mature technology solutions thoroughly to check if this is the case. Your emerging technology programme will also benefit from checking: You can find guidance onchoosing technology for services, including how to adopt new technology. 'Emerging technologies' is a broad term for a range of tools and techniques that are at different stages of development. Examples of emerging technology include: But although emerging technologies are sometimes categorised together, some are more mature than others. Severalgovernment organisations are already using artificial intelligence or machine learningin different ways. For example,GDS is using machine learningto process large amounts of data to aid human decision-making. AndOxford City Council is leading a group of local authorities in a joint discoveryon how chatbots and AI might help to solve service design problems. If you are considering using artificial intelligence, read the guidance onusing artificial intelligence in the public sector. Some government organisations are also fundingresearch into quantum computing. This technology is in the theoretical phase and the government is unlikely to use this technology in the short term. You can share your experiences and find out more about what other government organisations are doing in these areas oncross-government Slack. You will need to login with your government email to join the conversations. You can also share knowledge about AI technologies with colleagues in other parts of the Civil Service using thegovernment data science Slack workspace. You can contact your GDS advisor atdigital-spend-assurance@dsit.gov.ukto discuss whether an emerging technology solution is right for your organisation. Next: Technology Code of Practice point 10 -Make better use of data New email address for GDS assurance team due to recent DSIT machinery of government transfer updated references from CDDO to GDS Updates to CDDO email Addition of a temporary research survey to get user feedback on the Technology Code of Practice. Addition of a section on emerging technology First published.",2023
govuk_044,govuk,Automated Decision-Making 4,"AI is one of the fastest-moving areas of technological advancement in government defence, bringing the capability to counter threats and create opportunities. The Defence Science and Technology Laboratory (Dstl) provides theUKwith world-class capability in applying artificial intelligence (AI), data science and machine learning to defence and security challenges. From putting machine learning on-board Royal Navy ships, to using data science to support intelligence analysis,Dstlis at the heart of such innovation. One ofDstl's missions is tode-mystify the area ofAI. We help theUKMinistry of Defence (MOD) understand how it cansafely, responsibly and ethicallyuseAIto deter and de-escalate conflict, save lives and reduce harm. Our technical experts have their finger on the pulse of new developments inAIand provide critical guidance for theUK. We work in partnership with specialists from academia, industry and allied nations to understand and develop a broad spectrum of techniques for performing tasks and discovering insights from data using automatic processes, We are also an integral part of theDefenceAICentre (DAIC), working collaboratively to connectMODand the widerAIcommunity and enable teams acrossMODto adoptAIin their areas. The wide range of potential applications include streamlining back-office functions, and supporting: AIis critical to theUK's future as a science and technology superpower, and when it comes toAI,MOD's visionis to be ""the world's most effective, efficient, trusted and influential defence organisation for our size."" ...A radical upheaval in defence is underway andAI-related strategic competition is intensifying. Our response must be rapid, ambitious, and comprehensive.""(Defence Artificial Intelligence Strategy (2022) Our cutting edge work covers everything from very early research looking at how machines interact with humans, to applying data science to real-world challenges and operational requirements. Our experts collaborate with international partners to help theUKand our allies advance more quickly. The 3 nations ofAUKUS(Australia - United Kingdom - United States)recently testedAI-enabled uncrewed aerial vehiclesthat allow a human operator to locate, disable and destroy targets on the ground. AUKUS: Autonomy andAIcollaboration Royal Navy crews taking part in a 2021 NATO exercise inUKwaters were able to take advantage ofAIon-board ship for the first time, whenDstland our industry partners brought the latest technology into the command spaces of a Type 45 destroyer and a Type 23 frigate. Tested against a supersonic missile threat, ourAI-based applications were designed to help detect threats earlier and provide a rapid hazard assessment to recommend options the crew can take to counter the threat. This exciting real-world test was the culmination of almost10 years of research and collaboration with industry suppliers. Image taken from HMS Dragon, participating in Exercise Formidable Shield 2021. UK MOD (c) Crown copyright 2021 We work alongside the armed forces to develop and test new technologies in the field to get our research out of the lab and in to operational use as quickly as possible. Dstland theUSAir Force Research Laboratory carried out the firstdeployment of a jointly developed artificial intelligence toolbox in 2 military exercises. The goal was to address the challenge of how to makeAIagile, adaptable, trustworthy and accessible to the warfighter under differentUSandUKmilitary use cases. The trials showed how the toolbox would be deployed ontoUK-USuncrewed ground and aerial vehicles. Members of bothUKandUSarmed forces tested that theAIwas robust and that the intended users understood any limitations of theAI. DuringExercise Spring Storm, soldiers from the 20th Armoured Infantry Brigade used anAIengine developed byDstland the Army in collaboration with industry. This prototype was specifically designed for the way the Army is trained to operate. During the exercise,Dstlstaff analysed how theAIengine was used in practice, to understand the critical human factors including how we build trust in theAI. TheAIengine uses automation and smart analytics along with supervised learning algorithms to save time and effort and help military personnel operate much more effectively. By instantly exploiting information on the surrounding environment and terrain, theAImakes it much quicker to plan and analyse different courses of action. This is one of the first steps towards achieving machine-speed command and control. UK MOD (c) Crown copyright 2021 Dstlis also working with partners on howAIand autonomy pose opportunities and threats to traditional intelligence, surveillance and reconnaissance (ISR). Our research ranges fromAI-enabledISRtasking and collection, to investigating the advantage that could be brought by quantum information processing. This work isn't just done by ourAIspecialists but is a great example of how we work acrossDstl's areas of expertise, such assensingandrobotics and autonomy. We have developed a standard approach forAIand autonomy in networked multi-sensor systems in security and defence which has been evaluated during multinational NATO trials, adopted byMODand industry, and published as British Standards Institute (BSI) Flex 335. Sensing for Asset Protection with Integrated Electronic Networked Technology (SAPIENT) specifies standards and protocols to allowAIalgorithms to work together across a suite of sensors and share data with a mix of technologies. As well as improving efficiency,SAPIENTcan help deliver enhanced situational awareness to support control and command of operations, and givesMODand our allies access to advancedAIsolutions being developed by our innovative supplier base. During a recentAUKUStrial in Australia,Dstland theUKarmed forces collaborated with Australia and theUSin theTrusted Operation of Robotic Vehicles in Contested Environments (TORVICE) trial. A number of missions were conducted, such as route reconnaissance while subject to a range of effects, and these aimed to understand and improve the resilience ofAUKUSAIand autonomy systems when subjected to attack. Understanding and mitigating the impact of threats is a key step towards deploying effective and reliableAIand autonomy systems on future operations. TORVICEAITrial We also work in partnership with theNational Quantum Technology Programme (NQTP), which provides unique insights into quantum information processing, complementing our own research and building expertise to support defence. TheNQTP's work includesfunding researchinto quantum machine learning models, hybrid (traditional/quantum) generative modelling to improve satellite imaging, as well as quantum fingerprinting to protect such models from cyber attacks. Dstlis an outstanding centre for research and we have extensive collaboration underway with leadingAIorganisations. Whenever possible,Dstlworks with external suppliers in academia and industry to meet the needs of theUK's defence and security. We are working together withGoogle Cloudto accelerate the adoption ofAIin theUKdefence sector and withMicrosoftto deliver safe and responsible use ofAI. Dstlis a strategic partner in thedefence and security research programmerun byThe Alan Turing Institutewho conduct world leading research in a large number of areas related to our needs. As another example of our partnership with the Turing,Dstland theDefence and Security Accelerator (DASA)supported a studyidentifying hazardous chemical and biological contamination on surfaces. Working with industry and academic partners, we explored how machine learning and data science could be used to detect substances like anthrax and nerve agents, alongside the development of innovative sensor technologies. This could be safer and more efficient than current methods. We frequently collaborate with universities and other academic institutions, often accelerating the research they have been doing and applying it to solve problems forUKdefence and security - it's a fantastic opportunity for organisations to get to work on leading edge projects with real impact on the defence and security of the country. In 2022, we formed theDefence Data Research Centre(DDRC) - a dedicated centre of excellence focusing on problems related to the use of data forAIapplications with a defence context. We are working closely with staff at the universities of Southampton and Edinburgh, where2 new centres for doctoral traininghave been funded byMODto enable novel research in critical technology areas, such asAIand autonomy. We have close links with other research bodies, including: AIand data science underpin and enable all theUK's defence and security science and technology capabilities. They have the potential to change everything.Dstlplays a pivotal role bringing together our scientific and technical expertise acrossMOD. ""Future conflicts may be won or lost on the speed and efficacy of theAIsolutions employed."" (Defence in a competitive age) Our flagshipAIscience and technology programme delivers core defence-specific research, but also works across our capabilities to build up strong communities of practice to share and expand our expertise. DrivingAIcloser to capability We worked withDefence DigitalandMODto establish theDefenceAICentre(DAIC) to boost research and accelerate the adoption ofAItechnology (announced in theIntegrated Review). The DAIC works collaboratively across defence, united and supported by a core team made up of Defence Digital (as part of Strategic Command),Defence Equipment and Support(Future Capability Group) andDstl. The team champion, enable and innovateAIacrossUKdefence, working with government, industry, academia and our allies for the strategic advantage of our armed forces. This includes working withDASAto help connect with innovators both in theUKand abroad. In 2022, we opened abrand newAIand data science unit based at the National Innovation Centre for Data in Newcastle, opening up new local opportunities for individuals and institutions to work with us. As part of our work in government, we support learning and up-skilling of people working across the defence and security sector. We also want to make it easier for more people to work with us. Our approach to this is broad, from enabling government customers commissioning our services to attracting talent from outside the defence industry to work with us. We run regular events (such asAIFest) to bring together theAIcommunity across government, industry, academia and international partners. At these events, world-leading experts share their experiences including the challenges of military adoption ofAI. We also discuss how to build an effectiveAIcapability. Our series ofBiscuit Booksprovide simple introductions to some of the complex concepts related toAI, data science and machine learning. Topics covered includehuman-centred ways of working withAIin intelligence analysisandassurance ofAIand autonomous systems. We are responsible and ethical in our use ofAIand we are leading the way in helping others (across defence, wider government and internationally) with safe adoption. We helped formMOD's internal guidance on best practice for ethics in developing or using systems which useAI, to achieve theAmbitious, safe, responsiblepolicy statement. This statement sets out the5 ethical principleswe follow: Through our strong links with academia we are contributing toresearch into the ethical use ofAIin defence. Dstl's experts have also collaborated with the Institution of Engineering and Technology to help raise awareness of theeffective use ofAIin safety-related functions, highlighting the importance of underpinning regulation and good practice to embedAIsafety. Our staff, industry and academic partners work across a range of scientific and technology capabilities including: We are always looking for talented individuals to join us. You will work on real-world problems, with the chance to see your science and technology expertise put into practice, including hugely exciting opportunities for overseas travel and working with theUK's allies. Experience in defence is not necessary, and roles vary from apprenticeships to visiting fellowships (for example, we have a Visiting research scheme with the Alan Turing Institute). While we have a high intake of graduates, you can also be mid-career, looking to re-skill and re-train. We work across Defence to partner with academic institutions and build relationships with suppliers. We particularly welcome companies and research bodies who have not worked with defence before, and small and medium-sized enterprises (SMEs). There are several routes toworking with us: Talk to us about potential future partnerships and projects by emailingcentralenquiries@dstl.gov.uk Added video about 'Driving AI closer to capability'. New video of the 3 nations of AUKUS have trialled a futuristic integration of autonomy and AI. Updated with links to the latest case studies and information about Dstl's work on AI. First published.",2023
govuk_046,govuk,Automated Decision-Making 6,"Guidance to help you assess if artificial intelligence (AI) is the right technology for your challenge. This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. This guidance will help you assess if artificial intelligence (AI) is theright technology to help you meet user needs. As with all technology projects, you should make sure you can change your mind at a later stage and you can adapt the technology as your understanding of user needs changes. This guidance is relevant for anyone responsible for choosing technology in a public sector organisation. AI is just another tool to help deliver services. Designing any service starts withidentifying user needs. If you think AI may be an appropriate technology choice to help you meet user needs, you will need to consider your data and the specific technology you want to use. Yourdata scientistswill then use your data to build and train an AI model. When assessing if AI could help you meet users' needs, consider if: It's important to remember that AI is not an all-purpose solution. Unlike a human, AI cannot infer, and can only produce an output based on the data a team inputs to the model. When identifying whether AI is the right solution, it's important that you work with: For your AI model to work, it needs access to a large quantity of data. Work with specialists who have the knowledge of your data, such as data scientists, to assess your data state. You can assess whether your data is high enough quality for AI using a combination of: If your problem involves supporting an ongoing business decision process, you will need to plan to establish ongoing, up-to-date access to data. Remember tofollow data protection laws. There is no one 'AI technology'. Currently, widely-available AI technologies are mostly either supervised, unsupervised or reinforcement machine learning. The machine learning techniques that can provide you with the best insight depends on the problem you're trying to solve. There are certain types of problems for which machine learning is commonly used. For some of these you will be able to buy or adapt commercially available products. Because of its experimental and iterative nature, it can be difficult to specify the precise benefits which could come from an AI project. To explore this uncertainty and provide the right level of information around the potential benefits, you can: Once you have secured budget, you'll need to allow enough time and resources toconduct a substantial discoveryto show feasibility. Discovery for projects using AI can often takes longer for similar projects that do not use AI. If your organisation is a central government department, you may have toget approval from the Government Digital Service (GDS) to spend moneyon AI. At this point mostAI projects are classified as 'novel', which requires a high level of scrutiny. You should contact theGDS Standards Assuranceteam for help on the spend controls process. When assessing if AI could help you meet user needs, consider how you will procure the technology. You shoulddefine your purchasing strategyin the same way as you would for any other technology. Whether you build, buy or reuse (or combine these approaches) will depend on a number of considerations, including: It's also important toaddress ethical concernsabout the use of AI from the start of the procurement process. The Office for AI and the World Economic Forum are developing furtherguidance on AI procurement. Your team can build or adapt off-the-shelf AI models or open source algorithms in-house. When making this decision, you should work with data scientists to consider whether: You may be able to buy your AI technology as an off-the-shelf product. This is most suitable if you are looking for a common application of AI, for example optical character recognition. However, buying your AI technology may not always be suitable as the specifics of your data and needs could mean the supplier would have to build from scratch or significantly customise an existing model. Your AI solution will still need to be integrated into an end-to-end service for your users, even if you are able to buy significant components off the shelf. When using AI it's important to understand who is responsible if the system fails, as the problem may lie in a number of areas. For example, failures with the data chosen to train the model, design of the model, coding of the software, or deployment. You should establish a responsibility record which sets out who is responsible for different areas of the AI. It would be useful to consider whether: Depending on your organisation's maturity, it may be useful to set up a dedicated board, committee or forum to handle AI data and model governance. It can be useful to keep a central record of all AI technologies you use, listing:",2023
govuk_047,govuk,Automated Decision-Making 7,"A programme for the safe deployment of automated vehicles and implementing the Automated Vehicles Act 2024. The Automated Vehicles Act Implementation Programme was launched in 2024 to secure the safe deployment of automated vehicles (AVs) on roads in Great Britain. It is responsible for carrying out the full policy, legislative and operational programme to implement theAutomated Vehicles Act 2024in 2027. Safe trialling of the technology and developing use cases is an essential part of the path to implementation. We are working to support safe trialling of prototype automated vehicles on our roads and ensure the UK is the trialling destination of choice for industry. This work builds on theCode of Practice: automated vehicle trialling, delivering an automated passenger permitting route and enabling the deployment of automated passenger vehicles services. See theLaw Commission of England and Wales and the Scottish Law Commission's regulations reviewfor more information. The objectives of the Automated Vehicles Act Implementation Programme can be found below. Our objectives are to: produce the secondary legislation and guidance required to establish the act's regulatory framework develop the safe operations of the futureAVframework, ensuring that relevant government bodies are ready to fulfil the functions of the framework develop processes to approve and authoriseAVsand ongoing requirements to maintain the validity of this authorisation protect consumers, by ensuring that only vehicles that are authorised as automated under the framework can be marketed as such Our objectives are to: develop in-use regulation capabilities to ensure the safe and secure deployment and ongoing roadworthiness of automated vehicles develop an independent incident investigation capability to learn lessons from incidents involving automated vehicles to implement change and prevent similar future incidents develop an in-use regulatory scheme to safeguard the safe and secure operation of automated vehicles support safe trialling of prototypeAVson our roads and ensure the UK is the trialling destination of choice for industry - this will be achieved by building on theCode of Practice: automated vehicle trialling develop and/or adapt rules on the safe use ofAVs, such as through the Highway Code, driver, vehicle and service licencing and insurance Our objectives are to: ensure government has the skills, capabilities and access to assets to deliver safe and secure use ofAVs design and implement processes to ensure thatAVsare resilient to and can respond to cyber-attacks and that the data they hold is secure alongside developing our domestic regulations, we are playing a leading role in work to harmonise international rules on self-driving, which will enable our companies to export globally The programme team includes staff from across Centre for Connected and Autonomous Vehicles (CCAV), the Department for Transport (DfT) and its executive agencies, as well as other government departments and their agencies. The Automated Vehicles Act Implementation Programme is being delivered jointly by the: The programme works closely with experts from academia and industry to enable the safe development and deployment of connected andAVtechnologies. An automated vehicles expert advisory panel (EAP) provides independent, expert advice on the development of the act's implementation programme. Email:enquiries@ccav.gov.uk Centre for Connected and Autonomous VehiclesGreat Minster House33 Horseferry RoadLondonSW1P 4DRUnited Kingdom",2023
govuk_048,govuk,Automated Decision-Making 8,"Understand how to use artificial intelligence ethically and safely This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. The Office for Artificial Intelligence (OAI) and the Government Digital Service (GDS) have produced the following chapter's guidance in partnership with The Alan Turing Institute'spublic policy programme. This chapter is a summary ofThe Alan Turing Institute's detailed guidance, and readers should refer to the full guidance when implementing these recommendations. AI has the potential to make a substantial impact for individuals, communities, and society. To make sure the impact of your AI project is positive and does not unintentionally harm those affected by it, you and your team should make considerations of AI ethics and safety a high priority. This section introduces AI ethics and provides a high-level overview of the ethical building blocks needed for the responsible delivery of an AI project. The following guidance is designed to complement and supplement theData Ethics Framework. The Data Ethics Framework is a tool that should be used in any project. This guidance is for everyone involved in the design, production, and deployment of an AI project such as: Ethical considerations will arise at every stage of your AI project. You should use the expertise and active cooperation of all your team members to address them. AI ethics is a set of values, principles, and techniques that employ widely accepted standards to guide moral conduct in the development and use of AI systems. The field of AI ethics emerged from the need to address the individual and societal harms AI systems might cause. These harms rarely arise as a result of a deliberate choice - most AI developers do not want to build biased or discriminatory applications or applications which invade users' privacy. The main ways AI systems can cause involuntary harm are: The field of AI ethics mitigates these harms by providing project teams with the values, principles, and techniques needed to produce ethical, fair, and safe AI applications. The guidance summarised in this chapter and presented at length inThe Alan Turing Institute's further guidance on AI ethics and safetyis as comprehensive as possible. However, not all issues discussed will apply equally to each project using AI. An AI model which filters out spam emails, for example, will present fewer ethical challenges than one which identifies vulnerable children. You and your team should formulate governance procedures and protocols for each project using AI, following a careful evaluation of social and ethical impacts. You should establish ethical building blocks for the responsible delivery of your AI project. This involves building a culture of responsible innovation as well as a governance architecture to bring the values and principles of ethical, fair, and safe AI to life. To build and maintain a culture of responsibility you and your team should prioritise 4 goals as you design, develop, and deploy your AI project. In particular, you should make sure your AI project is: Prioritising these goals will help build a culture of responsible innovation. To make sure they are fully incorporated into your project you should establish a governance architecture consisting of a: You should understand the framework of ethical values which support, underwrite, and motivate the responsible design and use of AI. The Alan Turing Institute calls these 'the SUM Values': These values: You can read further guidance on SUM Values inThe Alan Turing Institute's comprehensive guidance on AI ethics and safety. While the SUM Values can help you consider the ethical permissibility of your AI project, they are not specifically catered to the particularities of designing, developing, and implementing an AI system. AI systems increasingly perform tasks previously done by humans. For example, AI systems can screen CVs as part of a recruitment process. However, unlike human recruiters, you cannot hold an AI system directly responsible or accountable for denying applicants a job. This lack of accountability of the AI system itself creates a need for a set of actionable principles tailored to the design and use of AI systems. The Alan Turing Institute calls these the 'FAST Track Principles': Carefully reviewing the FAST Track Principles helps you: If your AI system processes social or demographic data, you should design it to meet a minimum level of discriminatory non-harm. To do this you should: You should design your AI system to be fully answerable and auditable. To do this you should: The technical sustainability of these systems ultimately depends on their safety, including their accuracy, reliability, security, and robustness. You should make sure designers and users remain aware of: Designers and implementers of AI systems should be able to: To assess these criteria in depth,you should consult The Alan Turing Institute's guidance on AI ethics and safety. The final method to make sure you use AI ethically, fairly, and safely is building a process-based governance framework. The Alan Turing Institute calls it a 'PBG Framework'. Its primary purpose is to integrate the SUM Values and the FAST Track Principles across the implementation of AI within a service. Building a good PBG Framework for your AI project will provide your team with an overview of: You may find it useful toconsider further guidance on allocating responsibility and governance for AI projects.",2023
govuk_049,govuk,Automated Decision-Making 9,"This page provides details about DSIT's portfolio of AI assurance techniques and how to use it. The portfolio ofAIassurance techniques has been developed by the Responsible Technology Adoption Unit, a directorate within DSIT, initially in collaboration withtechUK. The portfolio is useful for anybody involved in designing, developing, deploying or procuringAI-enabled systems, and showcases examples ofAIassurance techniques being used in the real-world to support the development of trustworthyAI. Search portfolio Please note the inclusion of a case study in the portfolio does not represent a government endorsement of the technique or the organisation, rather we are aiming to demonstrate the range of possible options that currently exist. To learn more about different tools and metrics forAIassurance please refer toOECD's catalogue of tools and metrics for trustworthyAI, a one-stop-shop for tools and metrics designed to helpAIactors develop fair and ethicalAI. We will be developing the portfolio over time, and publishing future iterations with new case studies. If you would like to submit case studies to the portfolio or would like further information please get in touch atai-assurance@dsit.gov.uk. Building and maintaining trust is crucial to realising the benefits ofAI. Organisations designing, developing, and deployingAIneed to be able to check that these systems are trustworthy, and communicate this clearly to their customers, service users, or wider society. AIassurance is about building confidence inAIsystems by measuring, evaluating and communicating whether anAIsystem meets relevant criteria such as: Assurance can also play an important role in identifying and managing the potential risks associated withAI. To assureAIsystems effectively we need a range of assurance techniques for assessing different types ofAIsystems, across a wide variety of contexts, against a range of relevant criteria. To learn more aboutAIassurance, please refer tothe roadmap to anAIassurance ecosystem,AIassurance guide,industry temperature check, and co-developedRTA (formerlyCDEI) and The Alan Turing Institute introduction toAIassurancee-learning module. The Portfolio ofAIassurance techniques was developed by the Responsible Technology Adoption Unit (RTA), in collaboration with techUK, to showcase examples ofAIassurance techniques being used in the real-world. It includes a variety of case studies from across multiple sectors and a range of technical, procedural and educational approaches , illustrating how a combination of different techniques can be used to promote responsibleAI. We have mapped these techniques to the principles set out in the UK government's white paper onAIregulation, to illustrate the potential role of these techniques in supporting widerAIgovernance. To learn more about different tools and metrics forAIassurance, please refer toOECD's catalogue of tools and metrics for trustworthyAI. The portfolio is a helpful resource for anyone involved in designing, developing, deploying or procuringAI-enabled systems. It will help you understand the benefits ofAIassurance for your organisation, if you're someone who is: The portfolio allows you to explore a range of examples ofAIassurance techniques applied across a variety of sectors. You can search for case studies based on multiple features you might be interested in, including the type of technique and the sector you work within. Each case study is also mapped against the most relevant cross-sector regulatory principles published in the government white paper onAIregulation. There are a range of different assurance techniques that can be used to measure, evaluate, and communicate the trustworthiness ofAIsystems. Some of these are listed below: Impact assessment:Used to anticipate the effect of a system on environmental, equality, human rights, data protection, or other outcomes. Impact evaluation:Similar to impact assessments, but are conducted after a system has been implemented in a retrospective manner. Bias audit:Assessing the inputs and outputs of algorithmic systems to determine if there is unfair bias in the input data, the outcome of a decision or classification made by the system. Compliance audit:A review of a company's adherence to internal policies and procedures, or external regulations or legal requirements. Specialised types of compliance audit include system and process audits and regulatory inspection. Certification:A process where an independent body attests that a product, service, organisation or individual has been tested against, and met, objective standards of quality or performance. Conformity assessment:Provides assurance that a product, service or system being supplied meets the expectations specified or claimed, prior to it entering the market. Conformity assessment includes activities such as testing, inspection and certification. Performance testing:Used to assess the performance of a system with respect to predetermined quantitative requirements or benchmarks. Formal verification:Establishes whether a system satisfies some requirements using the formal methods of mathematics. Check with assurance techniques can be used across each stage of theAIlifecycle. TheNationalAIStrategysets out an ambitious plan for how the UK can lead the world as anAIresearch and innovation powerhouse. EffectiveAIregulation is key to realising this vision to unlock the economic and societal benefits ofAIwhile also addressing the complex challenges it presents. In its recentAIregulation white paperthe UK government describes its pro-innovation, proportionate, and adaptable approach toAIregulation that supports responsible innovation across sectors. The white paper outlines five cross-cutting principles forAIregulation: safety, security and robustness; appropriate transparency and explainability; fairness; accountability and governance; and contestability and redress. Due to the unique challenges and opportunities raised byAIin particular contexts the UK will leverage the expertise of existing regulators, who are expected to interpret and implement the principles in their domain and outline what compliance with the principles looks like across different use cases. In addition, the white paper sets out the integral role of tools for trustworthyAI, such as assurance techniques and technical standards, to support the implementation of these regulatory principles in practice, boost international interoperability, and enable the development and deployment of responsibleAI. The RTA has conducted extensive research to investigate current uptake and adoption of tools for trustworthyAI, the findings of which are published in itsindustry temperature check. This report highlights industry appetite for more resources and repositories showcasing what assurance techniques exist, and how these can be applied in practice across different sectors. The UK government is already supporting the development and use of tools for trustworthyAI, through publishing aroadmap to an effectiveAIassurance ecosystemin the UK, having established theUKAIStandards Hubto champion the use of international standards, and now through the publication of the portfolio ofAIassurance techniques. TheAIStandards Hub is a joint initiative led by The Alan Turing Institute in partnership with the British Standards Institution (BSI), the National Physical Laboratory (NPL), and supported by government. The hub's mission is to advance trustworthy and responsibleAIwith a focus on the role that standards can play as governance tools and innovation mechanisms. TheAIStandards Hub aims to help stakeholders navigate and actively participate in internationalAIstandardisation efforts and champion the use of international standards forAI. Dedicated to knowledge sharing, community and capacity building, and strategic research, the hub seeks to bring together industry, government, regulators, consumers, civil society and academia with a view to: To learn more, visit theAIStandards Hubwebsite. The catalogue of tools and metrics for trustworthyAIis a one-stop-shop for tools and metrics designed to helpAIactors develop and useAIsystems that respect human rights and are fair, transparent, explainable, robust, secure and safe. The catalogue gives access to the latest tools and metrics in a user-friendly way but also to use cases that illustrate how those tools and metrics have been used in different contexts. Through the catalogue,AIpractitioners from all over the world can share and compare tools and metrics and build upon each other's efforts to implement trustworthyAI. The OECD catalogue features relevant UK initiatives and works in close collaboration with theAIStandards Hub, showcasing relevant international standards for trustworthyAI. The OECD catalogue will also feature the case studies included in this portfolio. To learn more, visitThe OECD catalogue of tools and metrics for trustworthyAI. Data assurance is a set of processes that increase confidence that data will meet a specific need, and that organisations collecting, accessing, using and sharing data are doing so in trustworthy ways. Data assurance is vital for organisations to build trust, manage risks and maximise opportunities. But how can organisations assess, build and demonstrate trustworthiness with data? Through its data assurance work, theODIis working with partners and collaborators to explore this important and rapidly developing area in managing global data infrastructure.ODIbelieve the adoption of data assurance practices, products and services will reassure organisations and individuals who want to share or reuse data, and support better data governance practices, fostering trust and sustainable behaviour change. To learn more, visit theODIwebsite.",2023
govuk_050,govuk,Responsible Ai 0,"Toolkit for practitioners to support the responsible use of AI systems. This toolkit of guidance aims to support organisations and practitioners to safely and responsibly develop and deploy AI systems. Resources and guidance for organisations deploying data-driven tools and technologies including AI. Resources and guidance for practitioners interested in finding out how assurance techniques can support the development of responsible AI. The Algorithmic Transparency Recording Standard helps public sector organisations provide clear information about the algorithmic tools they use, and why they're using them. Research that engages the public, including tracker surveys that measure how public attitudes to data-driven technology and AI vary over time. Additional tools have been added in the AI assurance and Data-Driven Innovation categories. Added the research on parent and pupil attitudes towards the use of AI in education. Added the DfE and DSIT guidance on developing and using data analytics tools in children's social care. First published.",2023
govuk_051,govuk,Responsible Ai 1,"Understand how to use artificial intelligence ethically and safely This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. The Office for Artificial Intelligence (OAI) and the Government Digital Service (GDS) have produced the following chapter's guidance in partnership with The Alan Turing Institute'spublic policy programme. This chapter is a summary ofThe Alan Turing Institute's detailed guidance, and readers should refer to the full guidance when implementing these recommendations. AI has the potential to make a substantial impact for individuals, communities, and society. To make sure the impact of your AI project is positive and does not unintentionally harm those affected by it, you and your team should make considerations of AI ethics and safety a high priority. This section introduces AI ethics and provides a high-level overview of the ethical building blocks needed for the responsible delivery of an AI project. The following guidance is designed to complement and supplement theData Ethics Framework. The Data Ethics Framework is a tool that should be used in any project. This guidance is for everyone involved in the design, production, and deployment of an AI project such as: Ethical considerations will arise at every stage of your AI project. You should use the expertise and active cooperation of all your team members to address them. AI ethics is a set of values, principles, and techniques that employ widely accepted standards to guide moral conduct in the development and use of AI systems. The field of AI ethics emerged from the need to address the individual and societal harms AI systems might cause. These harms rarely arise as a result of a deliberate choice - most AI developers do not want to build biased or discriminatory applications or applications which invade users' privacy. The main ways AI systems can cause involuntary harm are: The field of AI ethics mitigates these harms by providing project teams with the values, principles, and techniques needed to produce ethical, fair, and safe AI applications. The guidance summarised in this chapter and presented at length inThe Alan Turing Institute's further guidance on AI ethics and safetyis as comprehensive as possible. However, not all issues discussed will apply equally to each project using AI. An AI model which filters out spam emails, for example, will present fewer ethical challenges than one which identifies vulnerable children. You and your team should formulate governance procedures and protocols for each project using AI, following a careful evaluation of social and ethical impacts. You should establish ethical building blocks for the responsible delivery of your AI project. This involves building a culture of responsible innovation as well as a governance architecture to bring the values and principles of ethical, fair, and safe AI to life. To build and maintain a culture of responsibility you and your team should prioritise 4 goals as you design, develop, and deploy your AI project. In particular, you should make sure your AI project is: Prioritising these goals will help build a culture of responsible innovation. To make sure they are fully incorporated into your project you should establish a governance architecture consisting of a: You should understand the framework of ethical values which support, underwrite, and motivate the responsible design and use of AI. The Alan Turing Institute calls these 'the SUM Values': These values: You can read further guidance on SUM Values inThe Alan Turing Institute's comprehensive guidance on AI ethics and safety. While the SUM Values can help you consider the ethical permissibility of your AI project, they are not specifically catered to the particularities of designing, developing, and implementing an AI system. AI systems increasingly perform tasks previously done by humans. For example, AI systems can screen CVs as part of a recruitment process. However, unlike human recruiters, you cannot hold an AI system directly responsible or accountable for denying applicants a job. This lack of accountability of the AI system itself creates a need for a set of actionable principles tailored to the design and use of AI systems. The Alan Turing Institute calls these the 'FAST Track Principles': Carefully reviewing the FAST Track Principles helps you: If your AI system processes social or demographic data, you should design it to meet a minimum level of discriminatory non-harm. To do this you should: You should design your AI system to be fully answerable and auditable. To do this you should: The technical sustainability of these systems ultimately depends on their safety, including their accuracy, reliability, security, and robustness. You should make sure designers and users remain aware of: Designers and implementers of AI systems should be able to: To assess these criteria in depth,you should consult The Alan Turing Institute's guidance on AI ethics and safety. The final method to make sure you use AI ethically, fairly, and safely is building a process-based governance framework. The Alan Turing Institute calls it a 'PBG Framework'. Its primary purpose is to integrate the SUM Values and the FAST Track Principles across the implementation of AI within a service. Building a good PBG Framework for your AI project will provide your team with an overview of: You may find it useful toconsider further guidance on allocating responsibility and governance for AI projects.",2023
govuk_052,govuk,Responsible Ai 2,"The Government Digital Service (GDS) Responsible AI Advisory Panel is seeking applicants for external panel members. TheGDSResponsibleAIAdvisory Panelwill advise and guide the Government Digital Service (GDS) on approaches to the development ofAIacross government. The duties of external panel members of the Government Digital Service (GDS) ResponsibleAIAdvisory Panel (the panel) involve, but are not limited to: External panel members should have: leading expertise in one or more of the following areas: Apply to be an external panel memberto theGDSResponsibleAIAdvisory Panel. The deadline is12pm on Monday 18 August 2025. The following links open in a new tab",2023
govuk_053,govuk,Responsible Ai 3,"This page provides details about DSIT's portfolio of AI assurance techniques and how to use it. The portfolio ofAIassurance techniques has been developed by the Responsible Technology Adoption Unit, a directorate within DSIT, initially in collaboration withtechUK. The portfolio is useful for anybody involved in designing, developing, deploying or procuringAI-enabled systems, and showcases examples ofAIassurance techniques being used in the real-world to support the development of trustworthyAI. Search portfolio Please note the inclusion of a case study in the portfolio does not represent a government endorsement of the technique or the organisation, rather we are aiming to demonstrate the range of possible options that currently exist. To learn more about different tools and metrics forAIassurance please refer toOECD's catalogue of tools and metrics for trustworthyAI, a one-stop-shop for tools and metrics designed to helpAIactors develop fair and ethicalAI. We will be developing the portfolio over time, and publishing future iterations with new case studies. If you would like to submit case studies to the portfolio or would like further information please get in touch atai-assurance@dsit.gov.uk. Building and maintaining trust is crucial to realising the benefits ofAI. Organisations designing, developing, and deployingAIneed to be able to check that these systems are trustworthy, and communicate this clearly to their customers, service users, or wider society. AIassurance is about building confidence inAIsystems by measuring, evaluating and communicating whether anAIsystem meets relevant criteria such as: Assurance can also play an important role in identifying and managing the potential risks associated withAI. To assureAIsystems effectively we need a range of assurance techniques for assessing different types ofAIsystems, across a wide variety of contexts, against a range of relevant criteria. To learn more aboutAIassurance, please refer tothe roadmap to anAIassurance ecosystem,AIassurance guide,industry temperature check, and co-developedRTA (formerlyCDEI) and The Alan Turing Institute introduction toAIassurancee-learning module. The Portfolio ofAIassurance techniques was developed by the Responsible Technology Adoption Unit (RTA), in collaboration with techUK, to showcase examples ofAIassurance techniques being used in the real-world. It includes a variety of case studies from across multiple sectors and a range of technical, procedural and educational approaches , illustrating how a combination of different techniques can be used to promote responsibleAI. We have mapped these techniques to the principles set out in the UK government's white paper onAIregulation, to illustrate the potential role of these techniques in supporting widerAIgovernance. To learn more about different tools and metrics forAIassurance, please refer toOECD's catalogue of tools and metrics for trustworthyAI. The portfolio is a helpful resource for anyone involved in designing, developing, deploying or procuringAI-enabled systems. It will help you understand the benefits ofAIassurance for your organisation, if you're someone who is: The portfolio allows you to explore a range of examples ofAIassurance techniques applied across a variety of sectors. You can search for case studies based on multiple features you might be interested in, including the type of technique and the sector you work within. Each case study is also mapped against the most relevant cross-sector regulatory principles published in the government white paper onAIregulation. There are a range of different assurance techniques that can be used to measure, evaluate, and communicate the trustworthiness ofAIsystems. Some of these are listed below: Impact assessment:Used to anticipate the effect of a system on environmental, equality, human rights, data protection, or other outcomes. Impact evaluation:Similar to impact assessments, but are conducted after a system has been implemented in a retrospective manner. Bias audit:Assessing the inputs and outputs of algorithmic systems to determine if there is unfair bias in the input data, the outcome of a decision or classification made by the system. Compliance audit:A review of a company's adherence to internal policies and procedures, or external regulations or legal requirements. Specialised types of compliance audit include system and process audits and regulatory inspection. Certification:A process where an independent body attests that a product, service, organisation or individual has been tested against, and met, objective standards of quality or performance. Conformity assessment:Provides assurance that a product, service or system being supplied meets the expectations specified or claimed, prior to it entering the market. Conformity assessment includes activities such as testing, inspection and certification. Performance testing:Used to assess the performance of a system with respect to predetermined quantitative requirements or benchmarks. Formal verification:Establishes whether a system satisfies some requirements using the formal methods of mathematics. Check with assurance techniques can be used across each stage of theAIlifecycle. TheNationalAIStrategysets out an ambitious plan for how the UK can lead the world as anAIresearch and innovation powerhouse. EffectiveAIregulation is key to realising this vision to unlock the economic and societal benefits ofAIwhile also addressing the complex challenges it presents. In its recentAIregulation white paperthe UK government describes its pro-innovation, proportionate, and adaptable approach toAIregulation that supports responsible innovation across sectors. The white paper outlines five cross-cutting principles forAIregulation: safety, security and robustness; appropriate transparency and explainability; fairness; accountability and governance; and contestability and redress. Due to the unique challenges and opportunities raised byAIin particular contexts the UK will leverage the expertise of existing regulators, who are expected to interpret and implement the principles in their domain and outline what compliance with the principles looks like across different use cases. In addition, the white paper sets out the integral role of tools for trustworthyAI, such as assurance techniques and technical standards, to support the implementation of these regulatory principles in practice, boost international interoperability, and enable the development and deployment of responsibleAI. The RTA has conducted extensive research to investigate current uptake and adoption of tools for trustworthyAI, the findings of which are published in itsindustry temperature check. This report highlights industry appetite for more resources and repositories showcasing what assurance techniques exist, and how these can be applied in practice across different sectors. The UK government is already supporting the development and use of tools for trustworthyAI, through publishing aroadmap to an effectiveAIassurance ecosystemin the UK, having established theUKAIStandards Hubto champion the use of international standards, and now through the publication of the portfolio ofAIassurance techniques. TheAIStandards Hub is a joint initiative led by The Alan Turing Institute in partnership with the British Standards Institution (BSI), the National Physical Laboratory (NPL), and supported by government. The hub's mission is to advance trustworthy and responsibleAIwith a focus on the role that standards can play as governance tools and innovation mechanisms. TheAIStandards Hub aims to help stakeholders navigate and actively participate in internationalAIstandardisation efforts and champion the use of international standards forAI. Dedicated to knowledge sharing, community and capacity building, and strategic research, the hub seeks to bring together industry, government, regulators, consumers, civil society and academia with a view to: To learn more, visit theAIStandards Hubwebsite. The catalogue of tools and metrics for trustworthyAIis a one-stop-shop for tools and metrics designed to helpAIactors develop and useAIsystems that respect human rights and are fair, transparent, explainable, robust, secure and safe. The catalogue gives access to the latest tools and metrics in a user-friendly way but also to use cases that illustrate how those tools and metrics have been used in different contexts. Through the catalogue,AIpractitioners from all over the world can share and compare tools and metrics and build upon each other's efforts to implement trustworthyAI. The OECD catalogue features relevant UK initiatives and works in close collaboration with theAIStandards Hub, showcasing relevant international standards for trustworthyAI. The OECD catalogue will also feature the case studies included in this portfolio. To learn more, visitThe OECD catalogue of tools and metrics for trustworthyAI. Data assurance is a set of processes that increase confidence that data will meet a specific need, and that organisations collecting, accessing, using and sharing data are doing so in trustworthy ways. Data assurance is vital for organisations to build trust, manage risks and maximise opportunities. But how can organisations assess, build and demonstrate trustworthiness with data? Through its data assurance work, theODIis working with partners and collaborators to explore this important and rapidly developing area in managing global data infrastructure.ODIbelieve the adoption of data assurance practices, products and services will reassure organisations and individuals who want to share or reuse data, and support better data governance practices, fostering trust and sustainable behaviour change. To learn more, visit theODIwebsite.",2023
govuk_054,govuk,Responsible Ai 4,"Find guidance for the responsible use and development of data and data technologies developed by and for government and public sector bodies. Use this tool to find data ethics guidance from across government. You can use the filter and search functions to identify the most relevant pieces of information for your organisation's needs. Start now Emaildata.ethics@digital.cabinet-office.gov.ukif you have questions or suggestions about data ethics guidance, or are aware of further relevant guidance that should be included. We encourage anyone working in the government and public sector to refer to the documents below to develop a high-level understanding of key data ethics considerations. TheData Ethics Frameworkexplains how to use data appropriately and responsibly when planning, implementing and evaluating a new policy or service. TheModel for Responsible Innovationis a practical tool to help teams across the public sector and beyond to innovate responsibly with data andAI. When working with public sector data, you have a responsibility to establish whether the data you manage and use is fit for purpose. TheGovernment Data Quality FrameworkandData Sharing Governance Frameworkset out principles and practices to improve the quality and better use of data across government. TheGenerativeAIFramework forHMGexplains how to use generativeAIsafely and responsibly. TheAlgorithmic Transparency Recording Standard (ATRS)provides a standard for public sector organisations to publish information about how and why they are using algorithmic methods in decision-making processes that affect members of the public. TheEthics, Transparency and Accountability Framework for Automated Decision-Makingaims to help government departments with the safe, sustainable and ethical use of automated and algorithmic decision-making systems. AIassurance is vital to ensure the reliability and trustworthiness ofAIsystems. The Responsible Technology Adoption Unit's (RTA)introduction toAIAssuranceidentifies assurance techniques that can support the development of responsibleAI. You can find additional information about tools and processes that support the responsible use ofAIat theRTA'sResponsibleAIToolkitpage. You must use the criteria inThe Technology Code of Practiceto design, build and buy technology in government. When procuringAIsolutions from third parties, refer to theGuidelines forAIprocurement. This includes principles for buyingAItechnology and insights on tackling challenges that may arise during procurement.",2023
govuk_055,govuk,Responsible Ai 5,"Understand how to manage a project which uses artificial intelligence. This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. Once you have planned and prepared for your AI implementation, you will need to make sure you effectively manage risk and governance. This guidance is for people responsible for: The Alan Turing Institute (ATI) has written guidance onhow to use AI ethically and safely. Governance in safety is important to make sure the model shows no signs of bias or discrimination. You can consider whether: Governance in purpose makes sure the model is achieving its purpose/business objectives. You can consider whether: Governance in accountability provides a clear accountability framework for the model. You can consider: Governance in testing and monitoring makes sure a robust testing framework is in place. You can consider: Governance in public narrative protects against reputational risks arising from the application of the model. You can consider whether: Governance in quality assurance makes sure the code has been reviewed and validated. You can consider whether:",2023
govuk_056,govuk,Responsible Ai 6,"Guidance to help you assess if artificial intelligence (AI) is the right technology for your challenge. This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. This guidance will help you assess if artificial intelligence (AI) is theright technology to help you meet user needs. As with all technology projects, you should make sure you can change your mind at a later stage and you can adapt the technology as your understanding of user needs changes. This guidance is relevant for anyone responsible for choosing technology in a public sector organisation. AI is just another tool to help deliver services. Designing any service starts withidentifying user needs. If you think AI may be an appropriate technology choice to help you meet user needs, you will need to consider your data and the specific technology you want to use. Yourdata scientistswill then use your data to build and train an AI model. When assessing if AI could help you meet users' needs, consider if: It's important to remember that AI is not an all-purpose solution. Unlike a human, AI cannot infer, and can only produce an output based on the data a team inputs to the model. When identifying whether AI is the right solution, it's important that you work with: For your AI model to work, it needs access to a large quantity of data. Work with specialists who have the knowledge of your data, such as data scientists, to assess your data state. You can assess whether your data is high enough quality for AI using a combination of: If your problem involves supporting an ongoing business decision process, you will need to plan to establish ongoing, up-to-date access to data. Remember tofollow data protection laws. There is no one 'AI technology'. Currently, widely-available AI technologies are mostly either supervised, unsupervised or reinforcement machine learning. The machine learning techniques that can provide you with the best insight depends on the problem you're trying to solve. There are certain types of problems for which machine learning is commonly used. For some of these you will be able to buy or adapt commercially available products. Because of its experimental and iterative nature, it can be difficult to specify the precise benefits which could come from an AI project. To explore this uncertainty and provide the right level of information around the potential benefits, you can: Once you have secured budget, you'll need to allow enough time and resources toconduct a substantial discoveryto show feasibility. Discovery for projects using AI can often takes longer for similar projects that do not use AI. If your organisation is a central government department, you may have toget approval from the Government Digital Service (GDS) to spend moneyon AI. At this point mostAI projects are classified as 'novel', which requires a high level of scrutiny. You should contact theGDS Standards Assuranceteam for help on the spend controls process. When assessing if AI could help you meet user needs, consider how you will procure the technology. You shoulddefine your purchasing strategyin the same way as you would for any other technology. Whether you build, buy or reuse (or combine these approaches) will depend on a number of considerations, including: It's also important toaddress ethical concernsabout the use of AI from the start of the procurement process. The Office for AI and the World Economic Forum are developing furtherguidance on AI procurement. Your team can build or adapt off-the-shelf AI models or open source algorithms in-house. When making this decision, you should work with data scientists to consider whether: You may be able to buy your AI technology as an off-the-shelf product. This is most suitable if you are looking for a common application of AI, for example optical character recognition. However, buying your AI technology may not always be suitable as the specifics of your data and needs could mean the supplier would have to build from scratch or significantly customise an existing model. Your AI solution will still need to be integrated into an end-to-end service for your users, even if you are able to buy significant components off the shelf. When using AI it's important to understand who is responsible if the system fails, as the problem may lie in a number of areas. For example, failures with the data chosen to train the model, design of the model, coding of the software, or deployment. You should establish a responsibility record which sets out who is responsible for different areas of the AI. It would be useful to consider whether: Depending on your organisation's maturity, it may be useful to set up a dedicated board, committee or forum to handle AI data and model governance. It can be useful to keep a central record of all AI technologies you use, listing:",2023
govuk_058,govuk,Responsible Ai 8,"The Sovereign AI Unit will ensure the government can harness AI's capabilities to unlock economic growth and enhance UK national security. Artificial Intelligence (AI) capabilities are developing at an extraordinary pace. In anAIenabled world, it matters who influences and builds the models, data andAIinfrastructure that are increasingly present in our lives. The Department for Science, Innovation and Technology (DSIT) has set up the SovereignAIUnit to build and harness the UK'sAIcapabilities to unlock economic growth and enhance UK national security. The Unit works closely with the Prime Minister's Adviser onAI. Announced in theAIOpportunities Action Planprepared by Matt Clifford, the SovereignAIUnit has an ambitious mandate to strengthen the UK's capabilities with up to PS500 million of funding. We will do this by: These partnerships are Memorandums of Understandings (MoU) which are non-legally binding, non-commercial agreements.",2023
govuk_059,govuk,Responsible Ai 9,"The government's new Compute Roadmap will harness AI to deliver on the UK's national priorities under the Plan for Change. New Compute Roadmap to boost AI breakthroughs. Artificial Intelligence will be used to deliver the UK's national priorities under the government'sPlan for Changeand position the country as anAImaker rather than anAItaker - accelerating economic growth and transforming public services, as a new strategy looks to bolster the country's compute capacity to power new breakthroughs inAI. Businesses and researchers use compute - essentially the computer chips that process huge amounts of data - to train and buildAImodels or process prompts and questions throughAIto discover everything from new drugs which treat and beat diseases to new tools to tackle climate change. Demand for cutting-edge compute power is already expected to surge by 5.7x between now and 2035, with the government taking vital steps to ensure the UK can stay ahead of the curve as the technology develops. Published today (Thursday 17 July), theCompute Roadmapwill deliver on the PS1 billion set aside in the Spending Review to increase the UK's compute infrastructure - allowing us to drive forwardAIdevelopment on our own terms to ensure the technology can deliver for the British people. This will mean reducing our reliance on foreign computing power to deliver the transformations which will improve public services and help to fix the foundations of the economy. The Roadmap also builds on the ambition of the 10-year infrastructure strategy and theModern Industrial Strategyto put the government's vision into action - increasing investment and growing the industries of the future. Compute is the raw processing power that drivesAI's development. Without enough power, we cannot deliver the breakthroughs to treat and beat diseases, make industries cleaner and greener, or find new ways to fight climate change. To help deliver on these shared national priorities, we will expand the UK'sAIResearch Resource (AIRR) twenty-fold over the next 5 years. The system, delivered in partnership with UK Research and Innovation (UKRI), Nvidia, HPE, Dell Technologies and Intel, brings together the country's most powerful supercomputers - Isambard-AIbased in Bristol and Dawn in Cambridge. The Technology Secretary flicked the switch on the Isambard supercomputer at its formal launch in Bristol today, meaning theAIResearch Resource (AIRR) is now fully up and running - transforming the UK's public compute capacity by being able to process in one second what it would take then entire global population 80 years to achieve. When the AIRR's planned expansion is complete in the coming years, it will be vastly more powerful than the world's current leading supercomputers. University College London researchers are already using Isambard to line up pioneeringAItools which could revolutionise NHS cancer screening. Using prostate cancer as its initial test case, they are harnessing the system to develop one of the first scalableAImodels dedicated to medical imaging - usingAIto analyseMRIscans and identify patients in need of treatment sooner. Secretary of State for Science, Innovation, and Technology Peter Kyle said: Britain has top of the class talent inAIand our plan will put a rocket under our brilliant researchers, scientists, and engineers - giving them the tools they need to make Britain the best place to do their work. This will mean we can harness the technology in Britain to transform our public services, drive growth, and unlock new opportunities for every community in the country. Chancellor of the Exchequer, Rachel Reeves, said: We are harnessing the power ofAIto transform our public services, drive innovation and fuel economic growth that puts money in people's pockets. As technology advances, our Plan for Change is ensuring we are ahead of the curve, expanding our sovereignAIcapabilities so we can make scientific breakthroughs, equip businesses with new tools for growth, and create new jobs across the country. The AIRR will see the UK's compute capacity increase to 420AIexaFLOP by 2030 - the equivalent of one billion people spending 13,316 years doing what the full AIRR will do in one second. That means all one billion people would have needed to start calculating more than 8,000 years before Stonehenge was built, without taking a break. Projects that matter most to the UK and align with national priorities will be prioritised access to the AIRR to help deliver the Plan for Change - as well as those which will have a real-world impact and deliver breakthroughs that change lives and grow the economy. Researchers at the University of Liverpool meanwhile have been using Isambard to develop their EIMCRYSTAL system. Their model harnessesAIto speed up the discovery of new chemical reactions for use in industry, sifting through 68 million chemical combinations to find new solutions which will decarbonise British industry to make it greener, cleaner, and more sustainable. Isambard is already supporting other areas of highly ambitiousAIresearch. The SovereignAIUnit has launched an early pilot supporting academic researchers inAIfor biosciences, foundationalAIresearch, and advanced materials. These will be some of the most compute-intensive training runs that academics have carried out on UK infrastructure. Working alongside theAIResearch Resource, a network of National Supercomputing Centres will also be set up across the country - with the first based in Edinburgh, the future home of the UK's most powerful research supercomputer. These will work as dedicated centres of expertise, connecting users not only with access to cutting-edge processing power, but catalysing greater collaboration between industry, academia, and researchers. They will help to build stronger links with existing talent in their regions - giving all areas of the country a supporting role in the UK's ability to be anAImaker. To further support the UK'sAIsovereignty ambitions, theSovereignAIUnithas been established in the Department for Science, Innovation, and Technology, backed with PS500 million of funding. Strengthening the UK's domesticAIcapabilities, including by developing the UK's compute ecosystem, will be a key focus for the unit. The strategy set out today and the work of the SovereignAIUnit will ensure the UK can roll out the next generation of champions in compute technology - sparking the creating of leaders in a range of fields to put British innovation and expertise on the map. Today's Compute Roadmap also puts Scotland and Wales in the frame to benefit from billions in private investment and thousands of new jobs as future homes toAIGrowth Zones. These dedicatedAIhotbeds offer accelerated planning permissions to speed up the roll-out of data centres, which will be powered by responsible and cutting-edge energy sources like small modular reactors (SMRs). AIGrowth Zones will not only deliver the infrastructure we need but also support the technology's evolution in a range of other areas. These will includeR&Dand Innovation Platforms, Adoption Testbeds and taking on a role as skills and talent hubs which will give people the tools they need to develop, use, and work with the technology. Further details of where these Growth Zones will be based in Wales and Scotland will be confirmed in due course. Capitalising on the ambition of today's announcements, the Technology Secretary is also launching a dedicatedAIfor Science strategy. This will set out the clear steps the government will take forward to cement the UK's position as a global leader inAI-enabled science breakthroughs, explore ways to boost adoption of the technology across the science sector and spark new commercial opportunities created byAIfor science. An expert group of senior academics, industry leaders and representatives of science institutions will advise on the strategy: Published in the Autumn, the strategy will help to accelerate the pace of scientific discovery throughAI, maximising its potential to drive innovation and growth. The roadmap set out today lays the groundwork for a golden age for BritishAI- supporting innovation, growth, and new opportunities in all sectors of the economy. It is a plan which delivers certainty to researchers, industry, and investors alike, cementing the UK's position as a world leader in artificial intelligence. Josh Payne, CEO, Nscale said: Nscale strongly welcomes the UK Government's compute roadmap. As the only full stack sovereignAIinfrastructure provider in the UK, we are delighted that the Government recognises the importance of sovereign capability in this area. We look forward to working with the Government and our partners to deliver this ambitious agenda. Professor Sir Peter Mathieson, Principal and Vice-Chancellor of the University of Edinburgh said: To be named the UK's first national supercomputing centre is a significant recognition of the University of Edinburgh's longstanding leadership in advanced computing. For more than thirty years, we have hosted the UK's national supercomputer and further developed our globally respected expertise in computer science and artificial intelligence. The new designation as the first national supercomputing centre will provide new opportunities for research and innovation across the UK, attracting further investment and talent. We look forward to working alongside the UK government and partners to bring this ambitious plan to life. Carolyn Dawson OBE, CEO of Founders Forum Group and Tech Nation: We know the UK'sAIecosystem is brimming with talent and ambition, but to lead globally, we must anchor this ambition in cutting-edge, sovereign compute infrastructure. Bold investment in compute power is exactly what's required to accelerate innovation and secure a leading role for Britain in the globalAIrace. By bringing together world-class supercomputers in partnership with industry leaders like Nvidia and Intel, and expanding access through National Supercomputing Centres andAIGrowth Zones in Scotland and Wales, this roadmap demonstrates the UK's ambition to shape the future ofAI. Julian David OBE, CEO of techUK, said: This ambitious roadmap, underpinned by actions with dates for delivery, shows that the UK Government is serious in its ambition to deliver innovative and real-world impact through transformativeAI, compute and cloud technologies. We are particularly encouraged to see alignment between compute recommendations andAIGrowth Zones - a vital move to better connect expertise, support UK innovators, and maximise the value of UK research and innovation. While there are still certain aspects to be explored, such as how these Growth Zones will develop the testbeds and platforms to help the most innovative emerging tech businesses grow and scale, techUK remains committed to working with government and our members to build on this ambition to power the next generation ofAI. Walter Goodwin, founder and CEO of Fractile, said: I wholeheartedly welcome the Compute Roadmap. The Roadmap is a joined up strategy that will both drive an immediate expansion ofAIcompute capacity in the UK, but further will ultimately see pull-through of breakthroughAIcompute platforms being built by UK semiconductor companies, like Fractile'sAIaccelerators, into widespread commercial deployment. This will close the loop on sustainable sovereign compute capacity and ensure the UK will be anAIcompute maker, not just a taker. Dr Antony Rowstron,CTOofARIA, said: I've built my career at the intersection of computing and science, and seen firsthand how the right technological leap can redefine what's possible.AIrepresents just such a leap - a once-in-a-lifetime opportunity to transform the speed of research and invention. I'm looking forward to bringing that experience, and my perspective fromARIA, to help put the UK at the forefront of this revolution. Chris Bishop,FRSFREngFRSEand Technical Fellow, Microsoft ResearchAIfor Science said: I personally believe that scientific discovery represents the most important and promising opportunity forAIin our generation. The consequences are far-reaching, from the discovery of life-saving drugs to the efficient design of sustainable materials. I am therefore delighted to participate, alongside other leading experts, in the new government strategic advisory panel onAIfor Science. Together, I know that we will ensure that the UK remains at the forefront ofAIdevelopment, in an area that is key to the future success of our society. Pushmeet Kohli, VP, Science and Strategic Initiatives, Google DeepMind said: Science can help us address some of humanity's greatest challenges, from climate change to disease. I'm excited to collaborate with the UK government and other industry leaders, experts and academics to help the nation leverageAIto accelerate scientific progress, and build upon the UK's strong history of scientific leadership. Professor Alison Noble, Vice-President of the Royal Society, said: The Royal Society welcomes the government's commitments to growing the UK's computing power andAIresearch resources. Today's launch of the government'sAIfor Science Strategy is an important step to advance the responsible use ofAIacross scientific disciplines. From drug discovery to robot-assisted laboratories,AIis already reshaping how science is done and enabling new discoveries that were previously out of reach. To fully realise its benefits, we must ensure that advances in speed and scale do not come at the expense of rigour, transparency, or trust. By embedding principles of openness, reproducibility, and collaboration, this strategy could help ensureAI-based science has a strong foundation. Charlotte Deane, Executive Chair of the Engineering and Physical Sciences Research Council and Professor of Structural Bioinformatics at the University of Oxford said: AIwill completely change the way research is done, from the way we ask questions to the questions we can ask. It has the power to transform so many areas across science and innovation, and we need to ensure that the UK is at the forefront of this change. It is an exciting time to be involved in driving the potential ofAIin science and for me an honour to be part of trying to make this change happen. DSIT media enquiries Emailpress@dsit.gov.uk Monday to Friday, 8:30am to 6pm 020 7215 3000 The following links open in a new tab",2023
