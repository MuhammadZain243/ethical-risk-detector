This page provides details about DSIT's portfolio of AI assurance techniques and how to use it.
The portfolio ofAIassurance techniques has been developed by the Responsible Technology Adoption Unit, a directorate within DSIT, initially in collaboration withtechUK. The portfolio is useful for anybody involved in designing, developing, deploying or procuringAI-enabled systems, and showcases examples ofAIassurance techniques being used in the real-world to support the development of trustworthyAI.
Search portfolio
Please note the inclusion of a case study in the portfolio does not represent a government endorsement of the technique or the organisation, rather we are aiming to demonstrate the range of possible options that currently exist.
To learn more about different tools and metrics forAIassurance please refer toOECD’s catalogue of tools and metrics for trustworthyAI, a one-stop-shop for tools and metrics designed to helpAIactors develop fair and ethicalAI.
We will be developing the portfolio over time, and publishing future iterations with new case studies. If you would like to submit case studies to the portfolio or would like further information please get in touch atai-assurance@dsit.gov.uk.
Building and maintaining trust is crucial to realising the benefits ofAI. Organisations designing, developing, and deployingAIneed to be able to check that these systems are trustworthy, and communicate this clearly to their customers, service users, or wider society.
AIassurance is about building confidence inAIsystems by measuring, evaluating and communicating whether anAIsystem meets relevant criteria such as:
Assurance can also play an important role in identifying and managing the potential risks associated withAI. To assureAIsystems effectively we need a range of assurance techniques for assessing different types ofAIsystems, across a wide variety of contexts, against a range of relevant criteria.
To learn more aboutAIassurance, please refer tothe roadmap to anAIassurance ecosystem,AIassurance guide,industry temperature check, and co-developedRTA (formerlyCDEI) and The Alan Turing Institute introduction toAIassurancee-learning module.
The Portfolio ofAIassurance techniques was developed by the Responsible Technology Adoption Unit (RTA), in collaboration with techUK, to showcase examples ofAIassurance techniques being used in the real-world.
It includes a variety of case studies from across multiple sectors and a range of technical, procedural and educational approaches , illustrating how a combination of different techniques can be used to promote responsibleAI. We have mapped these techniques to the principles set out in the UK government’s white paper onAIregulation, to illustrate the potential role of these techniques in supporting widerAIgovernance.
To learn more about different tools and metrics forAIassurance, please refer toOECD’s catalogue of tools and metrics for trustworthyAI.
The portfolio is a helpful resource for anyone involved in designing, developing, deploying or procuringAI-enabled systems.
It will help you understand the benefits ofAIassurance for your organisation, if you’re someone who is:
The portfolio allows you to explore a range of examples ofAIassurance techniques applied across a variety of sectors. You can search for case studies based on multiple features you might be interested in, including the type of technique and the sector you work within. Each case study is also mapped against the most relevant cross-sector regulatory principles published in the government white paper onAIregulation.
There are a range of different assurance techniques that can be used to measure, evaluate, and communicate the trustworthiness ofAIsystems. Some of these are listed below:
Impact assessment:Used to anticipate the effect of a system on environmental, equality, human rights, data protection, or other outcomes.
Impact evaluation:Similar to impact assessments, but are conducted after a system has been implemented in a retrospective manner.
Bias audit:Assessing the inputs and outputs of algorithmic systems to determine if there is unfair bias in the input data, the outcome of a decision or classification made by the system.
Compliance audit:A review of a company’s adherence to internal policies and procedures, or external regulations or legal requirements. Specialised types of compliance audit include system and process audits and regulatory inspection.
Certification:A process where an independent body attests that a product, service, organisation or individual has been tested against, and met, objective standards of quality or performance.
Conformity assessment:Provides assurance that a product, service or system being supplied meets the expectations specified or claimed, prior to it entering the market. Conformity assessment includes activities such as testing, inspection and certification.
Performance testing:Used to assess the performance of a system with respect to predetermined quantitative requirements or benchmarks.
Formal verification:Establishes whether a system satisfies some requirements using the formal methods of mathematics.
Check with assurance techniques can be used across each stage of theAIlifecycle.
TheNationalAIStrategysets out an ambitious plan for how the UK can lead the world as anAIresearch and innovation powerhouse. EffectiveAIregulation is key to realising this vision to unlock the economic and societal benefits ofAIwhile also addressing the complex challenges it presents.
In its recentAIregulation white paperthe UK government describes its pro-innovation, proportionate, and adaptable approach toAIregulation that supports responsible innovation across sectors.  The white paper outlines five cross-cutting principles forAIregulation: safety, security and robustness; appropriate transparency and explainability; fairness; accountability and governance; and contestability and redress. Due to the unique challenges and opportunities raised byAIin particular contexts the UK will leverage the expertise of existing regulators, who are expected to interpret and implement the principles in their domain and  outline what compliance with the principles looks like across different use cases. In addition, the white paper sets out the integral role of  tools for trustworthyAI, such as  assurance techniques and technical standards, to support the implementation of  these regulatory principles in practice, boost international interoperability, and enable the development and deployment of responsibleAI.
The RTA has conducted extensive research to investigate current uptake and adoption of tools for trustworthyAI, the findings of which are published in itsindustry temperature check. This report highlights industry appetite for more resources and repositories showcasing what assurance techniques exist, and how these can be applied in practice across different sectors.
The UK government is already supporting the development and use of tools for trustworthyAI, through publishing aroadmap to an effectiveAIassurance ecosystemin the UK, having established theUKAIStandards Hubto champion the use of international standards, and now through the publication of the portfolio ofAIassurance techniques.
TheAIStandards Hub is a joint initiative led by The Alan Turing Institute in partnership with the British Standards Institution (BSI), the National Physical Laboratory (NPL), and supported by government. The hub’s mission is to advance trustworthy and responsibleAIwith a focus on the role that standards can play as governance tools and innovation mechanisms. TheAIStandards Hub aims to help stakeholders navigate and actively participate in internationalAIstandardisation efforts and champion the use of international standards forAI. Dedicated to knowledge sharing, community and capacity building, and strategic research, the hub seeks to bring together industry, government, regulators, consumers, civil society and academia with a view to:
To learn more, visit theAIStandards Hubwebsite.
The catalogue of tools and metrics for trustworthyAIis a one-stop-shop for tools and metrics designed to helpAIactors develop and useAIsystems that respect human rights and are fair, transparent, explainable, robust, secure and safe. The catalogue gives access to the latest tools and metrics in a user-friendly way but also to use cases that illustrate how those tools and metrics have been used in different contexts. Through the catalogue,AIpractitioners from all over the world can share and compare tools and metrics and build upon each other’s efforts to implement trustworthyAI.
The OECD catalogue features relevant UK initiatives and works in close collaboration with theAIStandards Hub, showcasing relevant international standards for trustworthyAI. The OECD catalogue will also feature the case studies included in this portfolio.
To learn more, visitThe OECD catalogue of tools and metrics for trustworthyAI.
Data assurance is a set of processes that increase confidence that data will meet a specific need, and that organisations collecting, accessing, using and sharing data are doing so in trustworthy ways. Data assurance is vital for organisations to build trust, manage risks and maximise opportunities. But how can organisations assess, build and demonstrate trustworthiness with data? Through its data assurance work, theODIis working with partners and collaborators to explore this important and rapidly developing area in managing global data infrastructure.ODIbelieve the adoption of data assurance practices, products and services will reassure organisations and individuals who want to share or reuse data, and support better data governance practices, fostering trust and sustainable behaviour change.
To learn more, visit theODIwebsite.