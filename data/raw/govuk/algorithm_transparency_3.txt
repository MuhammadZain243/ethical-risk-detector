Updated 8 May 2025

© Crown copyright 2025
This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visitnationalarchives.gov.uk/doc/open-government-licence/version/3or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:psi@nationalarchives.gov.uk.
Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned.
This publication is available at https://www.gov.uk/government/publications/guidance-for-organisations-using-the-algorithmic-transparency-recording-standard/algorithmic-transparency-recording-standard-guidance-for-public-sector-bodies
This guidance explains what the Algorithmic Transparency Recording Standard (ATRS) is, why it matters and how public sector organisations should use it. It includes section-by-section guidance for completing theATRStemplate.
The Algorithmic Transparency Recording Standard (ATRS) enables public sector organisations to publish information about the algorithmic tools they are using and why they are using them.
It consists of a template for organisations to fill in with key information about their algorithmic tools. This information is then published on the GOV.UK repository in the form of anATRSrecord.
By using theATRS, public sector organisations can:
TheATRSis a core part of the government’sBlueprint for Modern Digital Government, in particular the promise to ‘Commit to transparency, drive accountability’.
An algorithmic tool is a product, application, or device that supports or solves a specific problem using complex algorithms.
We use ‘algorithmic tool’ as an intentionally broad term that covers different applications of artificial intelligence (AI), statistical modelling and complex algorithms. An algorithmic tool might often incorporate a number of different component models integrated as part of a broader digital tool.
TheATRSis mandatory for certain organisations, and certain algorithmic tools within those organisations.
It is mandatory for all government departments, and for ALBs which deliver public or frontline services, or directly interact with the general public.
Within those organisations, theATRSis mandatory for algorithmic tools which have a significant influence on a decision-making process with public effect, or directly interact with the general public.
This scope is designed to emphasise context, focusing on situations where a tool is influencing specific operational decisions about individuals, organisations or groups, not where a tool is an analytical model supporting broad government policymaking.   Further detail, including examples of algorithmic tools in and out of mandatory scope, can be found in the scope and exemptions policy .
If your organisation is within the mandatory scope of theATRSpolicy, it should have a single point of contact (SPOC) whose role is to coordinate with theATRSteam on identifying in-scope algorithmic tools, drafting and publishing records. You can email theATRSteam onalgorithmic-transparency@dsit.gov.ukif you are unsure who yourSPOCis.
However, theATRSis recommended by the Data Standards Authority for use across the entire public sector and we have welcomedATRSrecords from local government, police forces and other broader public sector organisations. If you are from such an organisation, you can complete anATRStemplate and email it toalgorithmic-transparency@dsit.gov.ukdirectly.
We recommend assigning a lead at your organisation to collate the relevant information from internal teams (and third-party providers, if applicable), to oversee the drafting and completion of the record, and to manage contact with theATRSteam.
As outlined above, if your organisation falls within the mandatory scope of theATRS, aSPOCwill have been assigned. You should contact yourSPOCbefore beginning work on anATRSrecord. Email us onalgorithmic-transparency@dsit.gov.ukif you are unsure who yourSPOCis.
If your supplier holds information that you need to complete a record, we encourage you to ask your commercial contact for the relevant details, explaining why you are asking for this information and why algorithmic transparency is important in the public sector. If your organisation and the tool is within mandatory scope of theATRSpolicy, you should highlight this. If the supplier is reluctant to share some information with you based on concerns around potentially revealing intellectual property, it can help to walk the supplier through the questions asked in the template, explain how they are designed to provide only a high-level picture of the tool.
TheATRShas been designed to minimise possibly security or intellectual property risks that could arise from publications.
Thescope and exemptions policy, modelled on theFOIAct, provides a detailed framework for exempting information from individualATRSrecords, or entireATRSrecords from publication. In general, publishing anATRSrecord and redacting certain fields with a brief explanation of why this has been done is preferable to not publishing anATRSrecord at all, particularly when partial information about the algorithmic tool is already in the public domain.
Considerations for limiting the information in certain fields include:
TheATRStemplate is available in two formats: an Excel version and a Google Sheets version for browser.Both can be downloaded here. Please do not alter or change the format of the template as this may affect our ability to process and publish yourATRSrecord.
TheATRStemplate is divided into 2 tiers.
Tier 2, whilst still accessible to the general public, is aimed at specialist audiences such as civil society, journalists, academic researchers and other public sector organisations wishing to learn from their peers.
TheATRSaims to deliver meaningful transparency around public sector uses of algorithmic tools. This means not just acknowledging the existence of such tools, but providing an intelligible explanation of how and why they are being used. You should aim to complete theATRStemplate in full sentences, in clear and simple language. You may consider sharing the draft record with teams who are not connected to the algorithmic tool to check for understandability.
For examples of existingATRSrecords which may help you complete the template, consult therepository. Fictional examples are also included in the guidance below.
Tier 1 asks for basic, high-level information about the algorithmic tool aimed at a general audience without technical knowledge. All fields should be completed.
The tool name will also appear in the title of yourATRSrecord, and will help people navigate theATRSrepository. It should be clear, concise and consistent throughout.
Your description should be brief and clear, focusing on what the tool is and why it is being used (rather than technical detail of how it works, which comes later in the record). Remember that theATRSaims to show the public when and why algorithmic tools are being used in processes that affect them. Ideally the description should be no more than two or three sentences.
Not all algorithmic tools will have a relevant website. If providing one, please ensure it is live and publicly accessible – otherwise, enter ‘N/A’. The email address you provide should be that of the team responsible for the tool, not an individual, for business continuity and security purposes. When an individual leaves the organisation but the wider team remains, the email address will still be up to date.
LeaseSureAIuses machine learning to analyse council housing rent accounts and create a prioritised caseload of rent arrears for housing officers. The tool is designed to work alongside existing housing management systems within the council to help improve arrears management.
This section focuses on accountability for the development and deployment of the tool. All fields should be completed, with ‘N/A’ where necessary.
TheSROshould be a role title, not a named individual, for business continuity and security purposes. It should be the role which is ultimately accountable for the tool in an operational context. This may be the policy or service owner, for example.
Third parties include commercial suppliers and other public sector organisations who may, for example, have developed an in-house algorithmic tool which they are sharing with your organisation.
A procured tool can involve multiple companies at different places in the supply chain. For instance, a public body could procure a tool from a company, which in turn procured the model and data from another company before integrating the model into a customisable tool.
Ideally, you should describe those different supplier relationships as clearly and concisely as possible, detailing which organisation was or is responsible for which part of the final tool that you are deploying.
Yes
Sulentra Dynamics Ltd.
813004659779X
Sulentra Dynamic has provided LeaseSureAIfor a six-month pilot.
Proof-of-concept pilot (a formal procurement process will follow if the tool demonstrates measurable benefit after the trial period).
Sulentra Dynamics Ltd. has been provided with controlled, read-only access to renting accounts data in the council’s MundioTenancy platform, but it does not integrate with other systems. This has been done in compliance with data protection legislation and all Sulentra Dynamics Ltd. staff with access to the data have been subject to appropriate vetting checks. Access to the data is only granted for the limited period of time while the tool is developed.
This section expands on the high-level description given in Tier 1, with more granular detail about the algorithmic tool, its scope and the justification for its use.
In contrast to the basic description of the tool in Tier 1, which focuses on what the tool is and why it is being used, the Tier 2 detailed description here aims to explain how the algorithmic tool works. As such, you should describe the tool’s purpose, its intended users, key aspects and functions at a more granular level. You should also include the tool’s scope, as well as limitations or context where it does not apply.
Whilst the amount of information provided here will vary between algorithmic tools, we typically expect a paragraph or two of text.
LeaseSureAImonitors tenant payment patterns and predicts financial distress using models like Logistic Regression and Random Forest Classifier on historical rent data.
It utilises the Logistic Regression (LR) model first to analyse changes in rent payment patterns (e.g. type and date of payment) and predicts the probability of falling into arrears. TheLRmodel produces a list of ‘at risk’ accounts, which is then analysed further by the Random Forest Classifier (RFC) model. Based on features such as payment trends and arrears duration (e.g. ‘30-Day Arrear’, ‘60-Day Arrear’, ’90-Day Arrear’, etc.), it classifies accounts into ‘Low’, ‘Medium’, and ‘High’ risk. The output of the tool is a weekly prioritised caseload that is integrated into the council’s existing MundioTenancy platform for housing officers to review and action.
You may choose to provide a list of individual benefits or a few sentences of description. Where possible, try to explain how and why the tool should deliver the benefit. For example, rather than just stating ‘improved customer experience’, explain how and why the tool should achieve this.
Your algorithmic tool may have replaced a legacy tool or a manual process. In either case, you should provide a brief description of what it replaced. If your algorithmic tool is part of a brand-new process (for example, delivering a programme which did not previously exist), you should make this clear.
Briefly explain why no alternatives were considered. For example, you may be using an algorithmic tool provided by a central government department for others to use.
This section should help people understand how the algorithmic tool ultimately helps to deliver an operational process or service, and how humans are involved in this delivery.
We typically expect a paragraph or two of text. It can be helpful to frame the answer around the output that the tool produces and how this is then used, for example to determine the outcome of an application process, or to deliver a public service. You should aim to make clear the degree of automation that the tool delivers within the broader process.
LeaseSureAIdoes not automate decisions. Instead, it provides a recommended list of priority cases, which are categorised as ‘Low’, ‘Medium’ or ‘High’, based on their risk of falling into payment arrears. Each case on the list includes reasons, such as ‘Escalating 60-Day Arrears’, ‘Payment Arrangement Broken’, or ‘Benefit Reduction Detected’, that then help housing officers interpret the tool’s outputs effectively. Housing officers review the list weekly and record any actions taken (e.g. sending notifications of payment arrears to tenants) directly in the council’s tenancy management platform, MundioTenancy.
You should consider both the outputs of the algorithmic tool itself and whether they can be challenged or appealed, and the outputs of the broader operational process and whether they can be challenged or appealed. This may involve providing a link to a public appeal or contact form.
If no appeals or review process is necessary or relevant for your tool, include a short sentence explaining why you are not completing this section.
You should also be aware of Article 22 UKGDPRwhich states that ‘The data subject shall have the right not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her or similarly significantly affects him or her’. If your algorithmic tool falls within the scope of these provisions, you must complete this field. Further information can be found via theICO’s guidance on ‘Rights related to automated decision making, including profiling’.
This section should detail the technical specifications of the algorithmic tool. As outlined above, the level of detail here should not infringe on a supplier’s intellectual property rights, or generate cybersecurity risks.
You should broadly describe how your tool is organised and provide context for the rest of the technical section. System representations such as AWS diagrams are ideal for conveying this type of information in a concise way – they capture the primary components of your technology stack and how they interact. You should think about the end-to-end process by which your tool processes inputs, the digital services and resources that it uses and the environments in which system processes occur. Any models that you consider later should be mentioned in this field.
You can see a helpful example of the diagram of system architecture provided by the Department for Health and Social Care in their algorithmic transparency report for theQCovid tool here.
For tools that consist of multiple machine learning models, this will be the primary input into or output from the system as a whole. For tools that consist of only one machine learning model, the system-level input and output, and the model input and output, may be the same. These fields should include the expected formats and data types.
Both historical and near real-time structured, tabular tenancy-related and financial data such as rent transaction history, payment type and date history, payment due dates, broken promise amounts, rent status, etc.
The tool’s output is a prioritised caseload of accounts that are ranked by risk of non-payment – i.e. ‘High’, ‘Medium’ and ‘Low’. The output list is delivered to, and integrated with, the council’s interactive MundioTenancy platform in the form of an interactive caseload, with the option to export it in CSV or Excel file formats for reporting and audit purposes.
This section should detail the model or models used within the algorithmic tool. Should your tool consist of more than one model, please duplicate this sheet and complete a separate sheet for each individual model.
N.B. For off-the-shelf models that your organisation has not trained, validated, tested, or applied any refinement techniques to (e.g. Web UI-based LLMs), please leave both the Model Specification (2.4.2) and following Development Data Specification (2.4.3) sections blank and move straight to the Operational Data Specification (2.4.4) section instead.
For tools that consist of more than one model, please make and complete copies of the Model Specification (2.4.2) section in the template for each model.
As a minimum, the fields in this section should include the type of model. If using a pre-trained model, please also specify the name of the API provider, where applicable, or mention if it is ‘self-hosted’.
Using Logistic Regression from the scikit-learn library in Python, which has pre-defined parameters.
Further details can be found on the scikit-learn website
Whereas the system architecture refers to how the model is integrated into the broader technical architecture, while model architecture describes the internal structure of the model – i.e. how it works or how it transforms an input into an output. At a minimum, you should enter the type of model used (e.g. Logistic Regression, Decision tree, Random Forest Classifier, Convolutional Neural Network, Rule-Based System, etc.). If the model has been designed such that certain features or inputs are given more priority over others, and where this has significant bearing on the model’s output, then indicate what those features are. For rule-based systems, describe how the rules are structured and indicate if any rules are weighted or prioritised over others. You may also provide a publicly accessible link to further resources. For security, do not include details of the network architecture to which the tool is connected.   If it aids understanding of the model, you are also encouraged to provide further details or provide a link to publicly available resources that offer further information.
Using Logistic Regression from scikit-learn library in Python, which has pre-defined parameters.
NV-Administration is an optimisation-based automated planning model. The model consists of:
Performance metrics will differ based on what type of method and tool you are developing or deploying. Useful metrics to consider may include accuracy metrics such as precision, recall or F1 scores, metrics related to privacy, and metrics related to computational efficiency.
You should also describe any bias and fairness evaluation you have undertaken (i.e. model performance over subgroups within the dataset), and any measures taken to address issues you identify.
For more information about setting performance metrics, you may find thisGOV.UK Service Manual guidancehelpful.
Useful resources may include the government’sAIPlaybookand the former Centre for Data Ethics and Innovation’sReview into bias in algorithmic decision-making, especially chapters 1 and 2.
For more information about bias in algorithmic decision-making, see theRTAU(formerly Centre for Data Ethics and Innovation) review into bias in algorithmic decision-making, especially Chapters 1 and 2. For more information about how to mitigate bias in algorithmic decision making, you may find it helpful to review theRTAU’s repository of bias mitigation techniques which can be found here.
This section aims to expand on ‘2.4.2.8 Datasets and their purposes’ in the Model Specification section. It focuses on the data used to train, validate or test your model(s).
Provided you have not trained, validated, tested or applied any refinement techniques to an off-the-shelf model (e.g. Web UI-based LLMs), you should leave the Development Data Specification section blank and move straight to the (Operational Data Specification) section.
The aim of this field is to describe all of the datasets used for developing the tool as a whole. Where possible, please provide publicly accessible links to these datasets. (This differs from the datasets and their purposes field in the Model Specification, which simply asks for a list and specification of what each dataset was used for).
The purpose of the ‘data quantities’ field is to sense-check proportionality of data in relation to the model task and complexity. Where a learning algorithm is applied to data, small datasets with few samples are more likely to yield underfitting models, while large datasets with numerous attributes may cause overfitting. In addition, too few samples may indicate insufficient representation of a target population, and too many attributes may indicate increased data security risks (such as de-identification).
While we don’t prescribe a specific definition of ‘sensitive’, we encourage you to consider:
In certain cases, it might not be feasible to disclose all the sensitive attributes in the data. At a minimum, you should disclose the fact that you are processing sensitive data and add as much detail as appropriate.
It is unlikely that theATRSrecord will lead to individuals being made identifiable as you are only being asked to provide a general description of the types of variables being used. If you are considering making the dataset you are using openly accessible and linking to it, you should comply with the relevant data protection legislation to prevent individuals from being made identifiable from the dataset.
This should also be considered as part of a Data Protection Impact Assessment (DPIA). For further guidance on completingDPIAs, please refer to theICO’sguidance.
You may also find it helpful to consult theICO’sAIand data protection risk toolkit.
This section focuses on the data used or produced in your algorithmic tool’s real-world operation, such as user inputs, retrieved documents, system-generated logs and other data generated during use.
See above.
This section should provide information on impact assessments conducted, identified risks, and mitigation efforts.
No, there is no need to provide a summary if you are providing an openly accessible link to the full assessment.
Categories of risk likely to be relevant include:
This list is not exhaustive and there may be additional categories of risk that are helpful to include.
The Government Finance Function’sOrange Bookprovides further guidance on conducting risk assessments for public sector projects. You may also find it helpful to consult theICO’sAIand data protection risk toolkit.
Review and feedback
Email your completedATRStemplate toalgorithmic-transparency@dsit.gov.uk(or send to yourSPOC, if your organisation has one). TheATRSteam will check for readability and provide feedback or suggested amendments if necessary.
Before finalising your record, you should consider the possibility that publishing information on your algorithmic tool may invite attention and scrutiny from the public and media. This is particularly true for more high-profile use cases and where the use of an algorithmic tool has not been publicly disclosed before.
You can help to mitigate these risks by ensuring you provide clear information and an appropriate level of detail in your record. You should also ensure that your organisation’s communications team or press office is aware of the plan to publish, has reviewed the record and has prepared to respond to media requests if deemed necessary. If your organisation has aSPOC, you should ask them to coordinate this.
You may wish to consider publishing supplementary information, for example a blog post explaining what the algorithmic tool is, or a link to theATRSrecord on the relevant service or policy pages on your website.
TheATRSteam requires written confirmation that yourATRSrecord has gone through all appropriate internal signoff procedures before publishing it to the GOV.UK repository. At a minimum, this should include clearance by:
In certain high-profile instances it may be appropriate to seek ministerial clearance.
Should substantive details change in relation to yourATRStool, you should update theATRStemplate, go through internal clearance again, and send the updated template toalgorithmic-transparency@dsit.gov.ukasking for your record to be updated accordingly. Substantive changes might include a pilot tool moving to production, new datasets being used to train or refine the tool, or a change to the broader operational process of which the tool is part.
Should you decommission an algorithmic tool for which you have published anATRSrecord, contact the team onalgorithmic-transparency@dsit.gov.uk.