Published 14 June 2023

© Crown copyright 2023
This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visitnationalarchives.gov.uk/doc/open-government-licence/version/3or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:psi@nationalarchives.gov.uk.
Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned.
This publication is available at https://www.gov.uk/government/publications/enabling-responsible-access-to-demographic-data-to-make-ai-systems-fairer/report-enabling-responsible-access-to-demographic-data-to-make-ai-systems-fairer
The use of artificial intelligence (AI), and broader data-driven systems, is becoming increasingly commonplace across a variety of public and commercial services.[footnote 1]With this, therisks associated with biasin these systems have become a growing concern. Organisations deploying such technologies have both legal and ethical obligations to consider these risks. TheWhite PaperonAIRegulation, published in March 2023, reinforced the importance of addressing these risks by including fairness as one of five proposed key regulatory principles to guide and inform the responsible development and use ofAI.
Many approaches to detecting and mitigating bias require access to demographic data. This includes characteristics that are protected under the Equality Act 2010, such as age, sex, and race, as well as other socioeconomic attributes.[footnote 2]
However, many organisations building or deployingAIsystems struggle to access the demographic data they need. Organisations face a number of practical, ethical, and regulatory challenges when seeking to collect demographic data for bias monitoring themselves, and must ensure that collecting or using such data does not create new risks for the individuals that the data refers to.
There is growing interest in the potential of novel approaches to overcome some of these challenges. These include techniques to generate synthetic training data that is more representative of the demographics of the overall population, as well as a variety of governance or technical interventions to enable more responsible data access.
Access to demographic data to address bias is important for those working across theAIlifecycle, including organisations developing, deploying and regulatingAI. This report primarily explores approaches with the potential to assist service providers, i.e. those who are deploying data-driven systems (includingAI) to offer a service, to responsibly access data on the demographics of their users to assess for potential bias. This has led us to focus on two contrasting sets of promising data access solutions: data intermediaries and proxies. Of course, these approaches may have relevance to other parties. However, we have not considered in detail techniques such as synthetic generation of training data, which are specifically relevant to developers.
Data intermediary is a broad term that covers a range of different activities and governance models for organisations that facilitate greater access to or sharing of data.[footnote 3]TheNational Data Strategyidentified data intermediaries as a promising area to enable greater use and sharing of data, andCDEIhas previously published areportexploring the opportunities they present.
There is potential for various forms of data intermediary to help service providers collect, manage and/or use demographic data. Intermediaries could help organisations navigate regulatory complexity, better protect user autonomy and privacy, and improve user experience and data governance standards. However, the overall market for data intermediaries remains nascent, and to our knowledge there are currently no intermediaries offering this type of service in the UK. This gap may reflect the difficulties of being a first mover in this complex area, where demand is unclear and the risks around handling such data require careful management.
If gathering demographic data is difficult, another option is to attempt to infer it from other proxy data already held. For example, an individual’s forename gives some information about their gender, with the accuracy of the inference highly dependent on context, and the name in question. There are already some examples of service providers using proxies to detect bias in theirAIsystems.[footnote 4]
Proxies have the potential to offer an approach to understanding bias where direct collection of demographic data is not feasible. In some circumstances, proxies can enable service providers to infer data that is the source of potential bias under investigation, which is particularly useful for bias detection.[footnote 5]Methods that draw inferences at higher levels of aggregation could enable bias analysis without requiring service providers to process individually-identifiable demographic data.
However, significant care is needed. Using proxies does not avoid the need for compliance with data protection law. Inferred demographic data (and in some cases proxy data itself) will likely fall under personal or special categories of data under the UKGDPR. Use of proxies without due care can give rise to damaging inaccuracies and pose risks to service users’ privacy and autonomy, and there are some cases in which the use of proxies is likely to be entirely inappropriate. Inferring demographic data for bias detection using proxies should therefore only be considered in certain circumstances, such as when bias can be more accurately identified using a proxy than information about an actual demographic characteristic, where inferences are drawn at a level of aggregation that means no individual is identifiable, or where no realistic better alternative exists. In addition, proxies should only be used with robust safeguards and risk mitigations in place.
In the short term, direct collection of demographic data is likely to remain the best option for many service providers seeking to understand bias. It is worth emphasising that, in most circumstances, organisations are able to legally collect most types of demographic data for bias detection provided they take relevant steps to comply with data protection law. Where this is not feasible, use of proxies may be an appropriate alternative, but significant care is needed.
However, there is an opportunity for an ecosystem to emerge that offers better options for the responsible collection and use of demographic data to improve the fairness ofAIsystems. In a period where algorithmic bias has been a major focus in academia and industry, approaches to data access have received relatively little attention, despite often being highlighted as a major constraint. This report aims to highlight some of the opportunities for responsible innovation in this area.
This kind of ecosystem would be characterised by increased development and deployment of a variety of data access solutions that best meet the needs of service providers and service users, such as data intermediaries. This is one area thatCDEIis keen to explore further through the Fairness Innovation Challenge announced in parallel to this report.
However, this is only a partial answer to the genuine challenges in this area. Ongoing efforts by others to develop a robust data assurance ecosystem, ensure regulatory clarity, support research and development, and amplify the voices of marginalised groups are also crucial to enable a better landscape for the responsible use of demographic data.
Over the last year,CDEIhas been exploring the challenges around access to demographic datafor detecting and mitigating bias inAIsystems, and the potential of novel solutions to address these challenges. Organisations who useAIsystems should be seeking to ensure that the outcomes of these systems are fair. However, many techniques for detecting and mitigating bias inAIsystems rely on access to data about the demographic traits of service users, and many service providers struggle to access the data they need. Despite this, with a few notable exceptions, the topic has received relatively little attention.[footnote 6]
In this report:
This report has been informed by the work thatCDEIhas conducted over the last year, including:
CDEIis grateful to those who contributed to these workshops, or otherwise contributed to this work.
This report has been published alongside the announcement ofCDEI’s Fairness Innovation Challenge. The challenge will provide an opportunity to test new ideas for addressingAIfairness challenges in collaboration with government and regulators. We hope that it will generate innovative approaches to addressing some of the data access challenges described here.
Disclaimer: The information in this report is not intended to constitute legal advice. If you do require legal advice on any of the topics covered by this report, you should seek out independent legal advice.
The use of data-driven systems, includingAI, is becoming increasingly commonplace across a variety of public and commercial services.[footnote 7]In the public sector,AIis being used for tasks ranging fromfraud detection to answering customer queries. Companies in the financial services, technology, and retail sectors also make use ofAIto understand customers’ preferences and predict consumer behaviour.
When service providers make use ofAIsystems in their services or decision-making processes, they can have direct and significant impacts on the lives of those who use these services. As this becomes increasingly commonplace, the risks associated with bias in these systems are becoming a growing concern. Bias inAIsystems can lead to unfair and potentially discriminatory outcomes for individuals. In 2020,CDEIpublished itsReview into Bias in Algorithmic Decision-Making, which explored this topic in detail.
In March 2023, the government published theWhite PaperonAIregulation, which included fairness as one of five key proposed principles that might guide and inform the responsible development and use ofAI. The fairness principle states thatAIsystems should not undermine the legal rights of individuals or organisations, discriminate unfairly against individuals or create unfair market outcomes. The fairness principle considers issues of fairness in a wider sense than exclusively in terms of algorithmic bias, but addressing bias would be a key consideration in implementing it.
In some circumstances, bias inAIsystems can lead to discriminatory outcomes. TheEquality Act 2010is the key UK legislation related to discrimination. It protects individuals from discrimination, victimisation and harassment and promotes a fair and more equal society. Age, race, disability, sex, gender reassignment, marriage and civil partnership, pregnancy and maternity, religion or belief and sexual orientation are all protected characteristics under the Equality Act 2010. WhereAIsystems produce unfair outcomes for individuals on the basis of these protected characteristics and are used in a context in scope of the act (e.g. the provision of a service), this might result in discrimination. Even when protected characteristics are not present in the training data,AIsystems still have the potential to discriminate indirectly by identifying patterns or combinations of features in the data, which enable them to infer these protected characteristics from other types of data. As noted inCDEI’sReview into Bias in Algorithmic Decision-Making, ‘fairness through unawareness’ is often not an effective approach.
Service providers must address bias inAIsystems to ensure they are not acting unlawfully. Public sector service providers must also have due regard to advance equality of opportunity and eliminate discrimination under thePublic Sector Equality Duty(PSED). The Equality and Human Rights Commission (EHRC) has publishedguidancefor public bodies about how thePSEDapplies when they are usingAI, whichoutlinesthe need to monitor the impact ofAI-related policies and services.
When processing personal data in relation toAI, service providers also have obligations relating tofairness under data protection law. The ICO has producedguidanceon how to operationalise the fairness principle in the context of developing and usingAI, as well as moretargeted guidancefor developers.
Many approaches to detecting and mitigating bias inAIsystems require access to demographic data about service users. Demographic data refers to information about socioeconomic attributes. This includes characteristics that are protected under theEquality Act 2010, as well as other socioeconomic attributes such as socioeconomic status, geographic location, or other traits that might put people at risk of abuse, discrimination or disadvantage.[footnote 8]
In some cases, service user demographic data might be compared to the datasets used to train a model in order to test whether the training data is representative of the population the model is being deployed on. In other cases, service user demographic data could be used to assess performance or make standardisations to identify where a model is treating individuals from different demographic groups differently. Access to good quality demographic data about a service’s users is therefore often a prerequisite to detection, mitigation, and monitoring of bias, and an important first step in the fairness lifecycle.
However,research byCDEIandothershas found that service providers currently face a range of legal, ethical and practical challenges in accessing the demographic data they need to effectively detect and mitigate bias in theirAIsystems. Routine collection of demographic data to improve the fairness ofAIis not common practice in either the public or private sectors, except in recruitment.[footnote 9]Without the ability to access demographic data about their users, service providers are severely limited in their ability to detect, mitigate, and monitor for bias, and thereby improve the fairness of theirAIsystems.
Service providers are faced with a number of barriers when seeking to collect demographic data themselves.
Concerns around public trust
CDEI’sreview into bias in algorithmic decision-makingfound that some service providers think that the public do not want their data collected for the purpose of bias monitoring, and may be concerned why they are being asked for it.
Evidence from public attitudes research thatCDEIhas conducted suggests that the public’s willingness to share their data for bias monitoring varies depending on the organisation collecting it. Our2022 Tracker Surveyfound that 65% of the total respondents would be comfortable providing the government with demographic data about themselves in order to check if services are fair to different groups.Further researchwe conducted found that 77% of the public say they are not concerned with sharing their demographic data when applying for a job.
The Tracker Surveyalso found that individuals were most reluctant to share their data with big technology and social media companies. Some companies havehighlighted this as a key challenge, suggesting that commercial organisations may need to provide additional safeguards to demonstrate their trustworthiness.
Navigating regulatory compliance
Most demographic data is also personal, and often special category, data under UK data protection legislation.[footnote 10]This data must be collected, processed and stored in a lawful, fair and transparent manner for specific, explicit and legitimate purposes only.
Service providers must have a lawful basis and meet a separate condition for processing in order to process special category data underthe UKGDPR. In some circumstances, data controllers may be required to meet additional terms and safeguards set out inSchedule 1 of the Data Protection Act 2018. Schedule 1 includes a public interest condition around equality of opportunity or treatment, which is satisfied where processing certain kinds of special category data is “necessary for the purposes of identifying or keeping under review the existence or absence of equality of opportunity or treatment between groups of people specified in relation to that category with a view to enabling such equality to be promoted or maintained”. This might provide a lawful basis for organisations to process special category data for bias detection and mitigation without requiring direct consent from individual data subjects.[footnote 11]
Despite this, navigating the existing legal framework to process demographic data for bias detection and mitigation can be complex for service providers. Uncertainty around how equality and data protection law interact in this context can lead to misperceptions about what is or is not permitted under data protection law. TheCDEI’sReview into Bias in Algorithmic Decision-Makingfound that some service providers were concerned that collecting demographic data is not permitted at all under data protection law, or that it is difficult to justify collecting this data, and then storing and using it in an appropriate way.
The ICO recently publishedguidanceto support service providers in navigating data protection law to address bias and discrimination inAIsystems.
Data quality
When used for bias detection and mitigation, inaccurate or misrepresentative data can be ineffective in identifying bias or even exacerbate existing biases, particularly when marginalised groups are poorly represented in the data. However,collecting good quality demographic data can be challenging in practice.
Data collected directly from service users is likely to contain at least a degree of inaccuracy due to some users accidentally or intentionally misreporting their demographic traits. In addition, some users may choose to opt out of providing their data, leading to selection bias that results in a dataset that is not representative of service users. This selection bias may particularly impact individuals from groups who have experienced discrimination and marginalisation, who might be less comfortable sharing their data due to concerns about data privacy and misuse.
Data collection expertise
Collecting demographic data from service users requires establishing data collection procedures, and there is alack of clarity around how service providers should go about doing this. Setting up effective procedures that enable the collection of good quality data may require in-house expertise, which some service providers deployingAIsystems, particularly smaller organisations, may lack.
Collecting and using demographic data for bias detection can also pose risks to the individual service users.
Privacy
Due to the sensitive and personal nature of demographic data, the collection and use of this data exposes individuals to risks of privacy violations. This is particularly problematic given that detecting and mitigating bias requires data on vulnerable and marginalised groups, who may be less comfortable sharing information on their demographic attributes given their disproportionate experiences of discrimination. This has beendescribed by someas a trade-off between ‘group invisibility’ and privacy.
Misrepresentation
When collecting demographic data, service providers have to decide which categories of data to collect and how this data will be disaggregated, and this can be challenging. Demographic categories are not static and tend to evolve over time with societal and cultural change. For example, the Race Disparity Unitrecently announcedthat the government would no longer use the demographic category ‘BAME’ (black, Asian, and minority ethnic), as it obscures meaningful differences in outcomes across ethnic groups. Ensuring that demographic categories remain up-to-date requires that service providers regularly update the data they collect to reflect such changes.
In addition, when demographic categories are imposed on individuals, theyrisk misrepresentingthose who do not identify with them, further disempowering groups who are often already vulnerable and marginalised. There are also‘unobserved’ demographic characteristics, such as sexual orientation and gender identity, which can be fluid and are challenging to measure.
Data theft or misuse
The collection of demographic data by service providers increases the risk that this data is either stolen or intentionally misused. Cyberattacks by malicious actors could expose individuals to risks of information theft, which could be used for financial gain. Demographic data could also be intentionally misused by ill-intentioned actors for malicious purposes, such as identity theft, discrimination, or reputational damage. Concerns about data misuse may be particularly acute for individuals from demographic groups that have been historically marginalised or discriminated against.
Due to the challenges that organisations face when collecting demographic data themselves, there is growing interest in novel approaches that could address these challenges and enable more widespread and responsible access to demographic data for bias detection and mitigation.
The term ‘access’ is broad, and could involve:
We have focused particularly on examples where a service provider is offering a service to users, and wants to understand how the outcomes of that service affect different groups. Though our interest in this area is driven by cases where a service or decision-making process is driven by data orAI, similar approaches to gathering data to monitor for potential bias are also relevant in other non data-driven contexts (e.g. monitoring the fairness of interview processes in recruitment).
This has led us to a focus on two groups of potential approaches which seem applicable: data intermediaries and proxies.
Data intermediaryis a broad term that covers a range of different activities and governance models for organisations that facilitate greater access to or sharing of data.[footnote 12]This can encompass a wide range of different stewardship activities and governance models. Data intermediaries can reduce risks and practical barriers for organisations looking to access data while promoting data subjects’ rights and interests.
Proxiesare inferences that are associated with and could be used in place of an actual demographic trait. For example, anindividual’s postcode could be used as a proxy for their ethnicityor socio-economic status. Though the presence of proxies in algorithmic decision-making systems can be a source of bias, proxy methods could also be applied to infer the demographic characteristics of a service provider’s users to enable bias monitoring.
These two solutions offer contrasting approaches to the challenges surrounding data access, with differing opportunities and limitations. There has been significant interest in the concept of data intermediaries for some time, and there are a growing number of pilots and real-world examples of their use.[footnote 13]Despite this, data intermediaries have still not been widely adopted, nor used to enable access to demographic data for bias detection and mitigation in the UK.
By contrast, proxies offer a relatively simple and implementable alternative to collecting demographic data, but careful consideration of legal and ethical issues is needed if they are to be used. By focusing on these two contrasting approaches, we will explore the range of possibilities in this space, capturing the scope of potential benefits and challenges that novel solutions have to offer.
Some service providers, notably technology companies such as Meta andAirbnb, have started to experiment with these solutions in order to access demographic data to make theirAIsystems fairer. This experimentation with data intermediaries and proxies as a means of accessing demographic data demonstrates that they are perceived to be promising solutions to address the challenges surrounding data access. However, this also demonstrates an urgent need to better understand their potential and limitations.
Data intermediary is a broad term that covers a range of different activities and governance models for organisations that facilitate greater access to or sharing of data.[footnote 14]Data intermediaries can perform a range of different administrative functions, including providing legal and quality assurances, managing transfer and usage rights, negotiating sharing arrangements between parties looking to share, access or pool data, and empowering individuals to have greater control over their data. Intermediaries can also provide the technical infrastructure and expertise to support interoperability and data portability, or provide independent analytical services, potentially using privacy-enhancing technologies (PETs). This range of administrative and technical functions is explored in detail inCDEI’s 2021report exploring the role of data intermediaries.
In simple terms, for the purposes of this report, a (demographic) data intermediary can be understood as an entity that facilitates the sharing of demographic data between those who wish to make their own demographic data available and those who are seeking to access and use demographic data they do not have.
Some data intermediaries that collect and share sensitive data for research already operate at scale in the UK. One example isGenomics England, a data custodian that collects sensitive data on the human genome, stores it in a trusted research environment, and grants researchers access to anonymised data for specific research projects. Another prominent example is the Office for National Statistics’Secure Research Service, which provides accredited researchers with secure access to de-identified, unpublished data to work on research projects for the public good. As opposed to facilitating data sharing between service users and providers, these intermediaries provide researchers with access to population-level demographic datasets.
Outside the UK, there are also limited examples of intermediaries being used to steward demographic data specifically for bias auditing. The USNational Institute of Standards and Technology (NIST)provides a trustworthy infrastructure for sharing demographic information (including photographs and personally identifying metadata on subjects’ age, sex, race, and country of birth) for the purposes of testing the performance of facial recognition algorithms. By providing researchers with access to sensitive personal and demographic data that enables them to quality-assure algorithms for fairness, NIST has significantly expanded the evidence base on algorithmic bias and helped developers improve the performance of their facial recognition algorithms.
Here we are focused on a different but related set of use cases. Can intermediaries help service providers to access demographic data about the individuals that interact with their service so that they can understand potential biases and differential impacts?
Data intermediaries could play a variety of roles. Here, we primarily consider how they could support the collection, storage, and sharing of demographic data with service providers, though a third-party organisation could also take on an auditing role in certain circumstances.
Intermediary models have emerged in other domains where large numbers of service providers have a need to provide common functions, and doing so in a consistent way is beneficial to consumer trust, user experience and/or regulatory compliance. Examples include:
Some of the challenges that these intermediaries address are similar in nature to those above, so it is natural to ask the question of whether a similar model could emerge to address the challenges of demographic data access.
Intermediaries could offer a number of potential benefits over direct collection of data by service providers, including:
Various different types of organisations could act as demographic data intermediaries. For example:
The type of organisation acting as an intermediary might have some implications for the type of demographic data intermediary service that is offered; given the sensitivity of the data concerned and the variety of different needs, an ecosystem where multiple options are available to service providers and users seems more desirable than a single intermediary service holding large amounts of data and attempting to meet all needs.
There are a variety of different models for the role that intermediaries could play in supporting access to demographic data. To describe these potential roles, we have used the following terms:
There are then two different roles specifically related to demographic data which often do not exist today:
In the different data intermediary models described below, these roles might or might not be played by the same organisation.
Potential model 1
One model could be for a data intermediary to collect and steward demographic data on behalf of a service provider, sharing this with them so they can audit their model for bias.
This could potentially operate as follows:
Diagram depicting indicative relationships and data flows for a data intermediary collecting and managing data on behalf of a service provider.
There are examples of somewhat analogous models being used to enable safe research access to population-level datasets including some demographic data, for example theONS Secure Research ServiceorOpenSAFELY. We are not aware of any similar examples targeted at users of individual services, but it is feasible that a suitably trusted organisation could provide a similar kind of service collecting user or customer demographic data, and sharing this with service providers.
Potential model 2
Beyond collection and management of demographic data, there is a growingecosystem ofAIassurance service providersseeking to provide independent bias audits using such data. A potential variant of the intermediary model described above is for such a bias audit provider to also act as an intermediary collecting the data.
In this model, the intermediary acts as custodian of users’ demographic data, collecting and storing it in a secure environment (as in model 1), but then also auditing service providers’ models without ever giving them access to the data itself.
This provides an additional layer of separation between the service provider and the demographic data about their customers as compared to the previous model, i.e. they receive only results of an audit, not any demographic data itself. For this model to be feasible, a service provider will typically need to provide additional internal service-related data to the bias audit provider, and therefore is likely to require appropriate legal and technical protection for personal data and intellectual property contained within it.
Diagram depicting indicative relationships and data flows for a data.
We have identified a couple of real world examples of similar approaches to this in the context of fairness:
Potential model 3
As an alternative variant of model 1, there are a variety of intermediary models that seek to give users stronger personal control over how their data is used; variously referred to as personal data stores (PDSs), personal information management systems (PIMS) or data wallets.
Each of these offers a form of decentralised store of an individual’s personal data controlled by that individual. There are several such platforms already in existence, including commercial companies such asWorld Data Exchange, community interest companies likeMyDex, or cooperatives likeMiData, or open specifications such asSOLID.
In this context, such platforms could allow individual data subjects to manage and maintain their own demographic data and share it with service providers for bias audit and mitigation on their own terms.
Diagram depicting a service user collecting and managing their own demographic data in a personal data store, and sharing it with a service provider.
Common features
These contrasting models demonstrate the variety and breadth of data intermediaries that could support access to demographic data for bias detection and mitigation. They are not exhaustive or mutually exclusive, and their features could be changed or adapted. It is unlikely that one solution will suit every sector and group of stakeholders, and an ecosystem offering a combination of different demographic data intermediary types could be the most efficient and effective way to support the responsible use of demographic data for bias monitoring.
There are additional legal mechanisms and technical interventions that could be integrated into any of these models to provide additional protections for service users who share their data. Novel data governance mechanisms could provide service users with more autonomy over how their demographic data is used. These includedata trusts(mechanisms for individuals to pool their data rights into a trust in which trustees make decisions about data use on their behalf) anddata cooperatives, in which individuals can voluntarily pool their data and repurpose it in their interests. While such mechanisms have been proposed by academics for some time, there have recently been a number of schemes to pilot them in real-world settings. These include the Data Trusts Initiative’spilot projects, the ODI’sdata trusts pilots, and the Liverpool City Region’sCivic Data Cooperative. Pilots like these indicate a shift towards the development and use of novel data governance mechanisms in practice.
Data intermediaries could also integrate the use of technical interventions like PETs to provide stronger privacy and security protections. Large technology companies likeAirbnband Meta have experimented with the use of third parties to access demographic data using privacy-preserving techniques, including secure multi-party computation and p-sensitive k-anonymity, to better protect the privacy of their users.
Despite offering a range of potential benefits, such an ecosystem of data intermediaries has not yet emerged. To the best of our knowledge, there are currently no intermediaries providing services specifically designed to support service providers to access demographic data from their users to improve the fairness of theirAIsystems in the UK.
Our work suggests that the potential of data intermediaries to enable access to demographic data is constrained by a range of barriers and risks.
The absence of organisations offering this type of service suggests that there is not sufficient incentive for such data intermediaries to exist. Incentives might be commercial (i.e. confidence that offering such a service would be a viable commercial proposition), but might also be broader, for example an opportunity for a third sector organisation to support fairness.
What drives this absence? Demand among service providers and users for third-party organisations sharing demographic data is unclear. Given the relative immaturity of the market for data intermediaries, there may be a lack of awareness about their potential to enable responsible access to demographic data. In addition, the incentives driving data sharing to monitorAIsystems for bias are primarily legal and ethical as opposed to commercial, meaning demand for demographic data intermediation services relies on service providers’ motivation to assess their systems for bias, and service users’ willingness to provide their data for this purpose.
More broadly, the market for many kinds of data intermediary is still relatively nascent. In the EU, the 2022Data Governance Actintroduced new regulations for the ‘providers of data intermediation services’, requiring them to demonstrate their compliance with conditions placed on their economic activities. The UK government acknowledged in theNational Data Strategy Mission 1 Policy Frameworkthat there is currently no established market framework for the operation of data intermediaries in the UK and has committed to support the development of a thriving data intermediary ecosystem that enables responsible data sharing. The lack of commercial incentives for data intermediaries sharing demographic data, combined with the challenges of operating in this complex area, has created little impetus for first movers.
In addition, in order to use their services to share sensitive data, service providers and users must have confidence that data intermediaries are trustworthy. Ourpublic attitudes researchindicates that one of the most common concerns members of the public have around data intermediaries is that third parties are not sufficiently trustworthy.
Non-regulatory approaches, such as data assurance, could help to build confidence in data intermediaries and demonstrate their trustworthiness. TheODIdefines data assurance as “the process, or set of processes, that increase confidence that data will meet a specific need, and that organisations collecting, accessing, using and sharing data are doing so in trustworthy ways”.Research by Frontier Economicssuggests the data assurance sector in the UK is nascent but growing, with approximately 900 firms currently offering a range of different data assurance products and services in the UK.
Standards could provide one way to encourage consistent data governance and management across demographic data intermediaries. This could include adoption of mature and commonplace standards such asISO/IEC 27001, as well as other relevant data standards.[footnote 15]In addition, a number of relevant certification and accreditation schemes already exist, such asCoreTrustSeal,FairDataand theMyData Global Operator Award. These could help data intermediaries sharing demographic data to demonstrate their adherence to data protection and ethical standards.
Despite the burgeoning ecosystem for data assurance in the UK, work to understand how assurance products and services could demonstrate the trustworthiness and support the uptake of data intermediaries is in its early stages. No one standard-setting body or certification scheme can cover all the areas required to effectively assure data intermediaries and, given their diversity, a one-size-fits all approach is unlikely to be appropriate. For this reason, a greater understanding of how existing assurance products and services can demonstrate the trustworthiness of data intermediaries, how well these can meet the needs of different stakeholders, and where there may be remaining gaps in the data assurance ecosystem is required. This could support third parties sharing demographic data to demonstrate their trustworthiness, and encourage uptake among service providers and service users.
Of course, intermediaries gathering sensitive data of this nature must contend with many of the same challenges that a service provider would have managing the same data. Where data intermediaries are collecting and storing sensitive demographic information about service users, they still need to take steps to minimise the risk that personal data is stolen or intentionally misused; ourpublic attitudes researchfound that data theft and misuse was a common concern among respondents in relation to data intermediaries. In addition, any third party collecting demographic data must ensure the data they collect is good quality. Much like service providers seeking to collect this data themselves, third parties must contend with similar challenges around data accuracy and representativeness.
Data intermediaries hold real promise as a means to enable responsible access to demographic data for bias detection and mitigation. They could promote the collection and use of demographic data in ways that supports the regulatory compliance of service providers, protects service user privacy and autonomy, and elicits public trust, while providing better user experience and higher standards than service providers collecting this data themselves.
Despite this potential, a market for such services is yet to emerge. We have discussed some of the barriers to this above, but for a service provider that could benefit from this approach, the absence of third parties offering such services in the UK prevents this being a straightforward option at present.
Longer term, there remains  clear potential for intermediaries to play a useful role. There is a need for piloting potential solutions in this area to support the development of the market for data intermediaries, and demonstrate the opportunities to service providers and users that might use them. This is one area thatCDEIhopes to explore further in the Fairness Innovation Challenge announced in parallel to this report.
Many organisations hold a range of data about individuals that they provide services to. However, for the reasons discussed above, they often do not hold some or all of the demographic data that they need to audit their own systems and processes for bias.
In contexts where collecting this demographic data directly is hard, an alternative is to infer it from data that you already hold, using proxies for the demographic traits that you are interested in.
Proxies can be a source of discrimination inAIsystems where algorithms are able to deduce protected characteristics from relevant data points. For example, most insurance pricing models include postcode as a factor for a variety of valid reasons. However, the mix of ethnic groups varies significantly between different postcode areas, and there is a risk that insurance pricing models indirectly treat individuals from certain ethnicities differently via the proxy of their postcode. This is one reason why monitoring for potential bias is increasingly important asAIsystems become more complex and sophisticated.
Conversely, there is potential for proxies to be used to detect and address bias inAIsystems. Using data they already hold as proxies, service providers could infer the demographic traits of their service users, and use this data to detect bias in theirAIsystems.
Examples of this include:
The inferences you might make about demographic traits from such proxy data will inevitably not be fully accurate, and whether this accuracy is enough to be practically useful for bias monitoring will be dependent on both the proxy data that is available, and the use case.
Proxies raise a number of challenging ethical and legal concerns. These are discussed in more detail below.
There are a wide range of proxy methods and tools in existence, varying from relatively simple methods to more complex machine learning approaches. These can be used to infer different demographic attributes, although ethnicity and gender have been the most common target variables to date. Many proxy methods and tools involve inferring the demographic traits of identifiable individuals (see Example 2 below). However, some approaches avoid this by using personas (see Example 1 below) or by drawing group inferences in such a way that ensures individuals are not identifiable.[footnote 16]
Many of the most popular proxy methods and tools have been developed in the US, although some have been trained on large datasets spanning multiple geographies. These methods and tools vary in their accessibility to service providers, with some available open source and other commercial tools requiring payment for access.
Some of these methods and tools were developed specifically to assess bias and discrimination, such as RAND’sBayesian Improved Surname Geocoding(BISG). In recent years, a few prominent technology companies including Meta andAirbnbhave begun to pilot more advanced, privacy-preserving proxy methods with the explicit aim of generating demographic data to improve the fairness of theirAIsystems.
The examples below provide three contrasting approaches to using proxies, demonstrating the breadth of possibilities for their use to enable bias monitoring.
Example 1: Citizens Advice using name and postcode to infer ethnicity
In 2022, Citizens Advice conductedexploratory researchto better understand whether people from ethnic minority backgrounds experience worse outcomes in the car insurance market than white consumers. To measure this, they conducted mystery shopping using 649 personas that varied by name and postcode, comparing the prices paid by shoppers with names that are common among people from different ethnic backgrounds and postcodes with different proportions of ethnic minority communities in the population.
They found no significant difference in prices charged to people with different names in the same postcode area. However, average quotes were higher in areas where black or South Asian people make up a large proportion of the population, and this could not be explained by common risk factors such as crime rates, road accidents or levels of deprivation in the area.
By using personas, Citizens Advice was able to assess a service for bias without requiring access to the personal data of service users. Although their methodology allowed them to test the outcomes of pricing mechanisms, Citizens Advice acknowledge that it cannot explain exactly why the outcomes they identified occurred.
Example 2: Airbnb’s Project Lighthouse using first name and photos of faces to infer perceived race
Airbnb’s Anti-Discrimination product team havedeveloped a privacy-by-design approachto infer the perceived race of their service users using their first name and an image of their face. By measuring inequities on the basis of perceived race, they aimed to account for the fact that discrimination often occurs because of people’s perceptions of one another’s race as opposed to their actual race.
The team sent a k-anonymized version of this service user data to a research partner organisation, who was under a confidentiality agreement and had their systems reviewed by Airbnb security. The research partner assigned perceived race to service users and this data was returned to Airbnb, who perturbed the data to achieve a level of p-sensitivity (i.e. ensuring that each equivalence class in the dataset had at least p distinct values for a sensitive attribute) before storing it. Finally, this p-sensitised k-anonymised dataset was used to measure the acceptance rate gap between different perceived racial groups.
By including a research partner and making careful use of privacy techniques, Airbnb’s approach enables them to analyse whether hosts exhibit bias on the basis of perceived race while protecting the privacy of service users.
Example 3: NamSor using first name and surname to infer gender and ethnicity
NamSoris a commercial product which uses machine learning to infer ethnicity and gender from first names and surnames. Namsor SAS, the company who owns the product, suggests it can be used to measure gender or ethnic biases inAI-driven processes, and theyoffer a range of toolsto suit different customers, including API documentation, CSV and Excel files analysis, and developer tools.
NamSor has processed over 7.5 billion names and is continually maintained with new training data.The company claimsit is the most accurate ethnicity and gender inference service in the world. Oneindependent, comparative studysupports this claim, suggesting the tool achieves an F1 score of 97.9%.
There are a range of reasons why a service provider or technology developer might be motivated to use proxies rather than collect data directly.
Despite some potential benefits, the use of proxies presents a number of legal and ethical risks, and practical challenges.
Legal risk
Most demographic data inferred through the use of proxies is likely to be classified as personal or special category data under the UKGDPR, and must be processed in accordance with data protection legislation.
The ICO’sguidance around the legal status of inferred datastates that whether an inference counts as personal data or not depends on whether it relates to an identified or identifiable individual. In addition, it may also be possible to infer or guess details about someone which fall withinspecial categories of data.[footnote 17]Whether or not this counts as special category data will depend on the specific circumstances of how the inference is drawn.
Given that the use of proxies to generate demographic data for bias detection involves the intentional inference of relevant information about an individual, proxy methods will likely involve the processing of special category data, regardless of whether these inferences are correct or not.
Where proxies are used to infer demographic traits at a higher level of aggregation, such that inferences are drawn only about so-called ‘affinity groups’ and not specific individuals, theICO states thatthese inferences may also count as personal data depending on how easy it is to identify an individual through group membership. When using proxy methods to draw group inferences, service providers should still comply with the data protection principles, including fairness.
The use of proxies may pose additional legal risks for service providers where they are unaware of their legal obligations with respect to inferences or find them difficult to interpret and apply in practice.
Accuracy
Proxies can generate inaccurate inferences which can obscure or even exacerbate bias inAIsystems when used for bias detection and mitigation. Ourpublic attitudes researchsuggests the accuracy of proxy methods is a key concern for members of the public. There are a number of distinct issues related to the accuracy of proxies.
Privacy
The use of individual-level proxies may interfere with service users’ privacy as they reveal personal information about them. Privacy was a key concern about proxies among participants inour public attitudes study.
The inference of some demographic traits may not interfere with privacy much, if at all. However, information relating to more sensitive demographic categories, which form part of the individual’s private life, could seriously impede on the privacy of service users. This is supported by evidence from thepublic attitudes study, which found that members of the public are more comfortable with organisations inferring their age than they are other demographic traits, such as disability status or sexuality. The sensitivity of demographic traits may also be compounded by other contextual factors, like the individuals’ attributes (e.g. if they are a child or otherwise vulnerable) or their circumstances (e.g. if they live in a homophobic environment).
Transparency and user autonomy
The use of proxies to infer demographic data is inherently less visible to service users than collecting demographic data directly from them. The low visibility of proxy use raises concerns around transparency and service users’ autonomy. When processing personal or special category data for bias monitoring, service providers haveobligations related to transparencyunder the UKGDPR. The ICO has providedguidanceon the right to be informed, which is a key transparency requirement under the UKGDPR.
Public trust
Proxies are a controversial topic, and the public appear to be less comfortable with their use than with providing their data to a third party.Our public attitudes studyindicated that only 36% of respondents were fairly comfortable with the use of proxies, and 23% were uncomfortable. Levels of public comfort varied depending on the type of proxy, the target demographic trait, and the type of organisation using the proxies. Members of the public were particularly concerned about their privacy, the accuracy of the inferences, and the risks of data misuse.
Accessibility
The use of proxy methods relies on access to relevant proxy data. The type of proxy required will vary depending on the target variable but could include service user postcodes, names, social media data, or facial photographs. Some of this data may already be held by service providers but some may not. The accessibility of proxy data will place limitations on the applicability of different proxy methods.
Data quality
When used for bias detection, poor quality data can be ineffective in detecting biases or even introduce new ones, particularly when marginalised groups are poorly represented in the data. The ability to draw inferences that are useful for bias detection purposes therefore relies on access to good quality proxy data.
Where service providers do hold data that could be used to infer demographic traits of interest, this data may be incomplete or inaccurate. Where poor quality proxy data is used to infer demographic information about service users, it will produce poor quality inferences. This raises related concerns aroundcompliance with the accuracy principleunder data protection law, which applies to input as well as output data.
Proxies offer an alternative approach to accessing demographic data for bias detection and mitigation. Proxies can be a practical approach to bias detection for service providers who already hold relevant data, and can prevent the need for service users to provide their demographic data numerous times to different organisations. In some circumstances, proxies may be the best way for service providers to effectively analyse theirAIsystems for bias, particularly where the proxy is more helpful in identifying bias than the demographic trait itself. Methods that rely on personas or group inferences at a level of aggregation such that individuals are not identifiable may pose few privacy risks to individual service users.
Despite this, the use of proxies poses a number of legal and ethical risks, as well as practical challenges. There are some cases in which the use of proxies is likely to be entirely inappropriate and should be avoided. Other methods, although not illegal, will likely involve the processing of special category data, which may entail legal risk for service providers. In addition, proxies can give rise to damaging inaccuracies and pose challenges to the privacy and autonomy of service users, and members of the public appear to be less comfortable with their use than other data access solutions.
Proxies are therefore likely to be a viable solution to enable access to demographic data for bias detectiononly in certain circumstances, such as when bias can be more accurately identified using a proxy than information about an actual demographic characteristic, or where inferences are drawn at a level of aggregation that means no individual is identifiable. In addition, proxies shouldonly be used with robust safeguards and risk mitigations in place.
Here, we set out the key ethical issues for service providers to consider when seeking to use proxies for bias detection and mitigation. Alongside these ethical considerations, service providers using proxies should consider their legal obligations by referring to the ICO’s Guidance onAIand Data Protection, includingAnnex A ‘Fairness in theAILifecycle’.
Step 1: Establish a strong use case for the use of proxies as opposed to other alternatives
This is central to ensuring the ethics of using proxy methods, and helps service providers to exclude the use of proxies where a reasonable, less intrusive alternative exists.
There are certain demographic traits for which the use of proxies is not advisable. In particular, where service providers wish to test the system for bias relating to demographic traits that are unobservable, such as sexual orientation, they should seek an alternative approach.
However, there are a limited number of scenarios in which the use of proxies to address bias may be justifiable. These include:
The strength of these justifications should be weighed up in light of the risk that theAIsystem in question is biased, and the severity of the real-world impact of this bias. To make this assessment, knowledge of the context in which theAIsystem is being deployed is critical, and service providers should engage with civil society organisations and affected groups in determining whether using proxies is appropriate in any given use case.
Service providers should also refer to the ICO’sGuidance on Data Protection andAIat this stage to establish that their proposed use of proxies is lawful.
Step 2: Select an appropriate method and assess associated risks
If a strong case for the use of proxies as opposed to other alternatives has been established, service providers need to select an appropriate proxy method and assess the risks and trade-offs associated with its use. There are a number of commercial tools and open source methods available to service providers. A non-exhaustive list of some methods that are applicable in the UK context can be found in thetechnical report by Frazer Nash.
When selecting a method, service providers should consider:
Testing the performance of proxy methods by conducting an independent review using a representative dataset to determine which may be most appropriate to use.
Looking at historic data and current social and cultural trends to make predictions about likely model drift, and consider its implications for the need for model retraining or continuous learning.
Alongside these considerations, service providers need to assess the feasibility of using the method or tool, including factors such as the availability and cost of the method or tool, the availability and quality of proxy data, and available resources and expertise within the organisation.
Service providers should also consider conducting a risk assessment to assess the risks and trade-offs associated with the use of this method in the specific context they intend to use it in. They should also carefully consider the limitations of the method or approach they have chosen, and whether there are further actions they can take to overcome these limitations.
Step 3: Design and develop robust safeguards and risk mitigations
If an appropriate method is chosen and the risks and limitations of this method have been identified, service providers should consider the development of risk mitigations and safeguards, including:
Measures to ensure model accuracy, such as regular monitoring of model performance and retraining or revalidation of the model at appropriate intervals.
No set of safeguards will entirely eliminate the risks associated with the use of sensitive data and there will always be a degree of residual risk. Service providers should consider and document what that residual risk might look like, and whether it is proportionate compared to the established benefits of using the proxy method. This assessment would again benefit from engagement with civil society and affected groups.
If residual risks are deemed acceptable given those benefits, the last step is to implement safeguards and proceed with the use of proxies. Otherwise, service providers may need to consider whether further safeguards might be required, or whether the use of proxies is justifiable at all. Residual risk should also be reviewed on an ongoing basis to ensure new risks associated with changes in context are captured and mitigated.
The current landscape of options for accessing demographic data is not ideal, and has significant limitations. Organisations are required to navigate significant legal, ethical, and practical challenges to either collect demographic data or infer it via the use of proxies.Evidence suggeststhat members of the public are likely to feel more comfortable sharing their data when governance mechanisms offer them greater privacy and control over their demographic data, particularly in sectors where levels of public trust in data sharing are lower.
In this section, we reflect on what needs to happen to improve this ecosystem, and make it easier for organisations to responsibly use demographic data to address bias.
This requires the development and scaling up of ambitious data access solutions that best mitigate ethical risks, are most practical for service providers and users, and are trusted by members of the public. Data intermediaries are one promising area for further development, as well as complimentary governance mechanisms like data trusts and technical interventions such as privacy-enhancing technologies.
As government, we have a key role to play in spurring responsible innovation in this area, and a variety of work is underway to support this.
Firstly, although demographic data can already be legally processed by service providers for bias detection and mitigation, some organisations may find that the existing data protection framework is complex and difficult to navigate in this area.
In September 2021, the government launched aconsultationon reforms to the UK’s data protection laws, including seeking views on provisions relating to processing personal data for bias detection and mitigation.Respondents agreedthere should be additional legal clarity on how sensitive data can be lawfully processed for bias detection and correction, and some felt that introducing a new processing condition under Schedule 1 of the Data Protection Act 2018 would be beneficial. As outlined in thegovernment response, the government is introducing a statutory instrument to enable the processing of sensitive personal data for the purpose of monitoring and correcting bias inAIsystems, with appropriate safeguards. This measure fits in the wider approach the government is developing around this issue, as proposed in the White Paper onAIregulation, currently out for consultation.
Regulators have already published relevantdata protectionandequalitiesguidance related toAI, as well as some data access solutions, includingproxiesandprivacy-enhancing technologies. However, greater clarity around service providers’ equality obligations with respect to detecting and mitigating bias in theirAIsystems would be welcome, and could further incentivise service providers to take action to improve the fairness of their systems. Continued regulatory collaboration between the ICO,EHRCand relevant sectoral regulators will also be critical moving forward to ensure the responsible collection and use of demographic data to improve the fairness ofAIsystems, particularly where novel solutions to generate and share this data are being tested.
The government also has an important role to play in incentivising innovation and supporting the development and scaling up of promising solutions to enable responsible access to demographic data.As announced alongside this report, theCDEIplans to run a Fairness Innovation Challenge to support the development of novel solutions to address bias and discrimination across theAIlifecycle. The challenge aims to provide greater clarity about which data access solutions andAIassurance tools and techniques can be applied to address and improve fairness inAIsystems, and encourage the development of holistic approaches to bias detection and mitigation, that move beyond purely technical notions of fairness.
In theNational Data Strategy, the government also committed to support the development of a thriving data intermediary ecosystem by considering the role of competition, horizontal governance structures, and strategic investment in intermediary markets. The ongoing work in this area could serve to support the emergence of intermediaries that are able to play a useful role in this area.
One specific area of focus for this work relevant here is support for the development of a data assurance ecosystem to ensure that new data access solutions, particularly data intermediaries, are trustworthy. There is a burgeoning ecosystem for data assurance in the UK but work to understand how such services could demonstrate the trustworthiness and support the uptake of data intermediaries is in its early stages. The ODI has publishedresearchexploring the data assurance landscapein support of the government’s National Data Strategy. Further research could explore the extent to which the existing data assurance market can engender confidence in new data access solutions and meet the needs of different stakeholders, and identify potential gaps in the ecosystem.
As discussed above, service providers should already be taking action to identify and address bias inAIsystems that they deploy.
Those seeking to collect demographic data themselves should refer to guidance from the ICO around processing ofpersonal data, includingspecial category data, to ensure their collection and use of demographic data is legally compliant. In addition, the ONS has issuedguidancearound some demographic categories that service providers could use when seeking to collect data for the purposes of measuring equality. Service providers should give consideration to the ways in which they can mitigate the risks associated with demographic data collection, for example, by using more participatory and inclusive approaches to data collection.
In some cases, proxies may be a more suitable alternative to collecting demographic data themselves. Service providers should refer to the key ethical considerations in the previous section of this report, as well as the ICO’sGuidance onAIand Data Protectionand other sector-specific guidance, to determine whether such approaches are appropriate and, if so, how they could be used responsibly.
Given the growing imperative on many service providers to access demographic data, they should demand solutions from the market that better meet their needs, and the needs of their users, by embedding ethical best practice, supporting them to navigate regulation, and providing more practical services. Novel data governance approaches, such as data intermediaries, alongside complementary governance and technical interventions, could help to meet service providers’ needs, and demand for these solutions could stimulate innovation in these areas.
There is also an important role for the research community in providing continued research into and piloting of solutions to enable responsible demographic data access. More comparative studies of proxy methods using the same test datasets and performance criteria, particularly filtered or weighted accuracy scores, could help service providers to make better informed decisions as to whether such methods are sufficiently accurate for different demographics and acceptable for use to make assessments about fairness.Data quality is also a persistent challenge whether service providers collect demographic data themselves or access it using a generative method or through a third party. Further research into and piloting of novel approaches to improve data quality, such as participatory approaches to data collection, would be beneficial.
Finally, civil society groups have a key role to play in informing and mobilising members of the public and ensuring that solutions and services to enable responsible access to demographic data protect their rights and interests. Demographic data is of vital importance in detecting and correcting bias inAIsystems, yet the collection and use of this data poses risks to individuals, particularly those from marginalised groups. Civil society groups can help to raise awareness among individuals, including members of marginalised communities, about the importance of access to demographic data in tackling bias, whilst simultaneously calling for the development of solutions and services that give people greater autonomy, protect their privacy, and are worthy of their trust. Crucially, civil society groups can also help to amplify the voices of marginalised communities in debates around the design and development of new solutions, ensuring they are consulted and their views accounted for.
For example, see Bank of England,‘Machine Learning in UK Financial Services’, Local Government Association (LGA), ‘Using predictive analytics in local public services’, and NHS England, ‘Artificial Intelligence’.↩
See theEHRC’s ‘Five components of data collection and analysis’ (pg. 54) in‘Measurement Framework for Equality and Human Rights’.↩
Definition fromCDEI’s report,‘Unlocking the value of data: Exploring the role of data intermediaries’.↩
Meta, ‘How Meta is working to assess fairness in relation to race in the U.S. across its products and systems’, Airbnb, ‘Measuring discrepancies in Airbnb guest acceptance rates using anonymized demographic data’.↩
See Airbnb, ‘Measuring discrepancies in Airbnb guest acceptance rates using anonymized demographic data’, where Airbnb assessed bias on their platform on the basis of ‘perceived race’.↩
Notable exceptions include thePartnership onAI’s Workstreamon Demographic Data, as well as some academic scholarship, including Michael Veale and Reuben Binns,‘Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data’(2017).↩
For example, see Bank of England,‘Machine Learning in UK Financial Services’, Local Government Association (LGA), ‘Using predictive analytics in local public services’, and NHS England, ‘Artificial Intelligence’.↩
See theEHRC’s ‘Five components of data collection and analysis’ (pg. 54) in‘Measurement Framework for Equality and Human Rights’.↩
SeeCDEI, ‘Review in bias in algorithmic decision-making’ and Open Data Institute (ODI),‘Monitoring Equality in Digital Public Services’.↩
Some protected characteristics, including race, ethnicity, disability, and sexual orientation, are also special category data under the UK General Data Protection Regulation (GDPR) and the Data Protection Act 1998.↩
The equality of opportunity condition (Schedule 1, 8.1(b) of the Data Protection Act 1998) does not cover all special category data (e.g. trade union membership is not included).↩
Definition fromCDEI’s report,‘Unlocking the value of data: Exploring the role of data intermediaries’.↩
Pilots include those by theOpen Data Institute (ODI),Data Trusts Initiative, and theLiverpool City Region Civic Data Cooperative.↩
Definition fromCDEI’s report,‘Unlocking the value of data: Exploring the role of data intermediaries’.↩
Including, for example, theISO/IEC CD 5259-1series, which is currently under development.↩
The ICO providesguidanceabout when group inferences are personal data.↩
Special category data includes personal data revealing or concerning data about a data subject’s racial or ethnic origin, political opinions, religious and philosophical beliefs, trade union membership, genetic data, biometric data for the purpose of uniquely identifying a natural person, data concerning health, sex life and sexual orientation.↩