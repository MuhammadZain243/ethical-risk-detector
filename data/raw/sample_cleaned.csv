id,source,title,text,year
govuk_028,govuk,Responsible Ai 3,"Guidance for procuring and deploying AI responsibly in the HR and recruitment sector. PDF,791 KB,49 pages This file may not be suitable for users of assistive technology. HTML This 'Responsible AI in Recruitment' guidance focuses on assurance good practice for the procurement and deployment of AI systems for HR and recruitment. It specifically focuses on technologies used in the hiring process, such as sourcing, screening, interview and selection. Adopting AI-enabled tools in HR and recruitment processes offers automation and simplification of existing processes. However, these technologies also pose novel risks, including perpetuating existing biases, digital exclusion, and discriminatory job advertising and targeting. Tools for trustworthy AI, including AI assurance mechanisms and global technical standards, can play a vital role in managing these risks and building trust. The guidance is written for a non-technical audience and is appropriate for organisations with or without a comprehensive AI strategy. For more information on AI assurance and how it can be applied to your own organisation, you can contact the AI assurance team:ai-assurance@dsit.gov.uk.",2023
govuk_017,govuk,Algorithm Transparency 2,"Updated 21 June 2021 (c) Crown copyright 2021 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visitnationalarchives.gov.uk/doc/open-government-licence/version/3or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:psi@nationalarchives.gov.uk. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned. This publication is available at https://www.gov.uk/government/publications/cdei-publishes-commissioned-research-on-algorithmic-transparency-in-the-public-sector/britainthinks-complete-transparency-complete-simplicity Participants started this research from a point of very limited awareness about algorithmic transparency.Awareness of two of the central elements of this topic - algorithms and broader transparency in the public sector - is very low. There is almost no awareness of the use of algorithms in the public sector (except for a few participants who remember the use of an algorithm to award A-level results in 2020), and no spontaneous understanding of why transparency would be important in this context. Once introduced to examples of potential public sector algorithm use participants became more engaged but still felt that, in their daily lives, they would be unlikely to make use of transparency information about the use of an algorithm in the public sector.Algorithmic transparency in the public sector is not a front-of-mind topic for participants. They expect to only be interested in transparency information if they were to personally have a problem or concern about the use of an algorithm, and struggle to foresee other circumstances when they would want to access this sort of information. Trust in the use of algorithms in the public sector varied across different scenarios in which they might be used. The setting had a stronger influence on trust than prior knowledge of or trust in technology and was influenced by: Perceptions of the potential risk created by deploying an algorithm in a use case - the likelihood of an unfavourable outcome occurring Perceptions of the potential severity of impact should an unfavourable outcome occur Despite their lack of personal interest in the information, participants felt that in principle all categories of information about algorithmic decision making should be made available to the public, both citizens and experts.For something to be meaningfully transparent, participants concluded, as much information as possible should be made accessible. Making all categories of information about an algorithm available somewhere is seen as the baseline for a transparency standard. Participants typically expect that this would be available on demand, most likely on a website. A central online repository is considered a sensible way of achieving this. They also expect that experts - such as journalists and researchers - would examine this information on their behalf and raise any potential concerns. Making information available via transparency standards has the potential to slightly increase public understanding and trust around the use of algorithmic decision making in the public sector.However, this is ultimately a very low engagement topic and simply having information about the algorithm available is unlikely to have a substantial impact on public knowledge and trust as few members of the public would know to seek it out. More active communications (such as signs, letters, or local announcements) which notify the public about the use and purpose of algorithms would be more likely to have an impact and would signpost people to transparency information. Taking more active steps to signpost people to information about the algorithm in use cases that might be perceived as higher risk and / or higher impact could do more to build understanding and trust, which is a key aim for the Centre for Data Ethics and Innovation (CDEI).Where the public are likely to feel there is more at stake, based on the perceived likelihood and severity of negative outcomes from the use of the algorithm, presenting transparency information before or at the point of interaction with the algorithm is felt to be more important. For use cases where participants are more accepting about the use of technology, and where they perceive a lower potential risk or impact, expectations around the availability of information about the algorithm are lower. Through the research processes, as participants explored the realities of transparency information some participants became more pragmatic about what information they would realistically engage with as individuals. Most of all, participants want to know what the role of the algorithm will be, why it is being used, and how to get further information or raise a query.These are the categories of information participants would expect to see in active communications and form a first tier of information. While making all categories of information about the algorithm available somewhere is important in principle, this is less of a priority for active communication, forming a second tier or layer of information, as shown in the model below. Reassurance about the 'quality' of public sector algorithms is also important.There is an assumption among participants that the public sector is unable to afford 'the best', and that this would extend to its use of algorithms. They expect that this would mean public sector algorithms are more likely to be inaccurate or ineffective. This is fuelled in part by a lack of understanding about what an algorithm is and the technology that sits behind it. Reassurance around the effectiveness of the algorithm within the information made available would be beneficial for improving trust. Ensuring that transparency information about algorithm use in the public sector is accessible and understandable is a priority for participants.This refers both to communicating information in a digestible, easy to understand manner, as well as to making it possible for different groups to find the information, for example those without an internet connection. This research asked a diverse group of citizens to consider how the public sector can be meaningfully transparent about algorithmic decision-making. Our core research questions were: Which algorithmic transparency measures would be most effective to increase: Publicunderstandingabout the use of algorithms in the public sector Publictrustin the use of algorithms in the public sector Because of the low levels of awareness about the topic among the public we used a deliberative process. This involved providing information about algorithm use in the public sector, listening to the experiences of participants, and discussing their expectations for transparency. We worked collaboratively with participants to develop a prototype information standard that reflected their needs and expectations regarding transparency. We recruited 36 members of the public, of whom 32 participants completed the research in full. We focused on having a diverse sample, rather than a representative sample, including people with different views on technology. To achieve this, participants were recruited to represent a mix of: Age, gender, ethnicity, and socioeconomic group Trust in institutions including an even spread of 'High' and 'Low' trust Digital literacy- those who are highly familiar with technology and those less so, including their awareness of algorithms 12 of the respondents were recruited because they had experience of one of the three use cases (examples of algorithms in use by the public sector) that we focused on in the research. We recruited 4 participants who were aware that they had used an artificial intelligence (AI) parking system (e.g. where an automatic number plate recognition tool - ANPR - had been used), 4 who had participated in a video job interview and 4 who had interacted personally with the police or court system, all within the last six months. Participants with previous experience of each scenario were mixed with those without for all three use cases so they could hear each other's views. We recruited participants from a wide range of geographical locations including: 6 participants from Scotland 3 participants from Northern Ireland 4 from Wales The remaining participants from England were recruited from a mixture of regions. Due to the online nature of the research, these participants were also spread across the discussion groups. All participants were paid an incentive on completion of each phase as a thank you for their time. In line with the COVID-19 restrictions, fieldwork was carried out remotely using secure, tested approaches with personal support for those with lower digital literacy to ensure they could participate fully. We used two formats to collect participant views: an online community and video discussion groups. We set up an Online Community called ""Tech in the Public Sector"", hosted on a web-based platform called 'Incling'. The platform is simple, accessible and most participants used it with ease. Personal support was given when necessary. The platform featured questions and gathered responses in a range of formats, including video, images and text. We moderated the platform, engaging with participants as they completed the tasks, probing them for further response where necessary and answering any questions and concerns participants had throughout the process. In addition to the online community, we conducted discussion groups (90 minutes) via Zoom in which groups of 6 participants discussed specific scenarios, areas of transparency and algorithms. The discussion sessions were moderated by BritainThinks researchers and attended by members of the CDEI team as observers. The research involved three phases, shown in the timeline below. Phase 1 Anintroductoryphase where participants were presented with one of the three algorithmic decision-making use cases to explore in their focus groups and on the online community, giving spontaneous, unprompted responses to transparency information. The focus was on their initial understanding of and trust in algorithms in the public sector and the level of information they expected to be made available to people when in use. Phase 2 This phase was moredeliberative. In the online community, participants responded to stimuli that outlined different categories of transparency information and ways of presenting this information, assessing their importance and necessity. They also provided their own personal examples of complex information being presented well, discussing particular features with each other. The focus group continued these conversations, discussing different transparency models, what information should be provided and how personal information should be stored. Phase 3 This was anin-depth designphase where participants worked collaboratively with the moderators in focus groups to develop prototype transparency information about each of the three scenarios. They specified what information they would like to see regarding algorithm use in each case, how they would like it to be displayed and established a tier system for the information available. The finished prototypes were then posted on the online community, with each participant reviewing and commenting on a prototype for each scenario. Our fieldwork began 9th April and ended 7th May when the final online community activities were taken down. During this time, Covid-19 government guidelines were in place meaning the focus groups were unable to take place in person and instead were conducted via Zoom. During the research, there were no major news stories regarding algorithmic or other data transparency issues raised by participants, however there were a number of news stories about transparency of government procurement decisions, which were referenced by some participants. Discussion groups were recorded, and detailed notes taken. Additionally, all responses to the exercises from the online community were exported from the platform. The findings of this report are based on this collated data, which was analysed by the research team. Where possible, quantitative data has been exported from the online community and inserted throughout the report often in the form of charts. It should be noted that whilst this helps to provide a general impression of levels of trust and expectations for transparency of participants, the low sample size means findings are not statistically significant and care should be taken in generalising. Quotes are used throughout this report, with comments made in the online community (and therefore in an individual setting) provided with the demographics of the participant. Demographics are not provided for quotes from discussion sessions which were in a group setting. Within the report there are also three case studies which explore how the attitudes of individual participants changed across the research. These were created by comparing a participant's individual responses in the online community, along with their focus group contributions, across the research period. Despite low levels of awareness and understanding of the subject matter at the start of the research project, when we asked participants in the final stage how far they felt they understood the content of the research on a scale of 0-10 (0 being no understanding, 10 fully understood)the average score was 7.9, suggesting that there was fairly high confidence about the purpose of the research by the end. When we asked participants to what extent they understood the purpose of this research from 0-10 (0 being no understanding,10 being fully understood) they answered positively, again with an average score of7.9.Participants frequently commented that they thought it was important research like this took place, and citizens were made aware of how public services were changing, even if they had been initially disinterested or completely unaware. I would say that, even though I would not be particularly interested in how a particular algorithm worked, the fullest information must be readily available for people to look at if they want to, especially if they think they have been unfairly dealt with. Female, 65+, Online community Most participants had low spontaneous understanding of the concept of algorithmic decision making, and low awareness of examples of it in practice. When given a description of what algorithmic decision-making is, commercial and low risk examples from everyday life are most front of mind, but there is little to no awareness of algorithm use in the public sector. When thinking about transparency in the public sector, participants see this as important and equate more transparency with higher levels of trust in the government. However, this is a low salience topic for participants and they struggle to spontaneously identify examples of public sector transparency, whether good or bad. Without examples of public sector algorithmic decision making, participants are unclear about how this relates to the need for transparency. However, with prompting of specific examples of public sector algorithmic decision making, there is wide acknowledgement that it is important for the public sector to make information accessible at the point of an individual encountering an algorithmic decision-making system. They prioritise information which flags that an algorithm is being used and why. Although ideally participants want all information to be accessible somewhere (complete transparency), they expect that they would only want or need to access this information if they encounter a problem in the scenario where the algorithm is being used. Before being introduced to the term 'algorithm', we asked participants about their awareness of existing 'computer assisted decision-making' systems. This is not a top-of-mind topic, and not something participants could spontaneously apply to their daily lives. Similarly, when initially introduced to the term algorithm', many participants had not heard of it before and were unsure exactly what it referred to and meant. Only a small number associate this term with decision making assisted by technology and based on data input. Participants were then provided with the following definition: 'algorithmic decision making, machine learning and artificial intelligence are all types of computer-assisted decision-making. This is where tasks are automated by computers, either to make suggestions based on past behaviour, to carry out simple and repetitive tasks or to analyse very big and complex data'. With this prompting, participants identify some examples in their own lives. However, it is worth noting that while some participants were able to identify commercial and low-risk examples for algorithmic decision-making, other participants gave examples where there is no algorithmic decision making (e.g. supermarket self-check-outs). This reflects participants' overall limited understanding of the term, and suggests it may be used as a proxy for modern technology in general. Examples given by participants include: Algorithmic recommendations in their buying/viewing/search histories (e.g. Netflix and Amazon) Text recognition and predictive text, responses, auto-completes Self-checkouts Credit card and other financial applications Driverless vehicles Robotics in manufacturing One of my favourite examples [that] makes my life happier currently while in lockdown is Netflix, they use AI & algorithms to decide from my past watching history what I will enjoy next. Female, 45-54, online community Outside their own day-to-day experiences, some participants also recall having heard or read about computer assisted decision making systems being used in the healthcare sector (e.g. vaccine development) and the financial sector (e.g. stock markets and credit scores). There is very little to no awareness of computer assisted decision making systems being used in the public sector. A very small number of participants spontaneously mention the A-level algorithm used by the Department for Education to give A-level results in 2020, and raise concerns about the use of an algorithm in this particular scenario. However, after discussing this example, participants remain open to algorithm use in other instances. It's that word algorithm which just makes me think back to last year and thousands of stressed-out students receiving inaccurate/unjustifiable grades based on algorithmic decision making as any algorithm can only be as good as the data that is inputted - millions of terabytes in even the simplest of human-human interactions. Male, 35-44, online community Please see Section 8, Appendix 1 for more information about participants' spontaneous attitudes to algorithmic decision-making, including attitudes towards the use of algorithms in general, attitudes towards algorithms in different settings and attitudes towards algorithms in the public sector. Transparency in the public sector Spontaneously, participants do not have strong top-of-mind associations with the concept of transparency in the public sector. There is some confusion about what both transparency and the public sector refer to; these are not terms or concepts many people regularly engage with in their daily lives. However, once explained, the notion of transparency in the public sector and in public sector decision making is seen as an important and fundamental component of trust in government. Participants feel transparency is necessary to provide evidence and reassurance that the government is working in the best interest of the public. This is especially true for circumstances and decisions which are either likely to have a significant direct impact on individuals or wide ranging/ high stakes impacts on society as a whole. I think that people being able to...openly see what taxes are spent on is essential to trust the government. It's currently lacking because there's been a scandal around PPE contracts, people don't have the full trust in what's being spent. Focus group participant I think it's important because you don't read everything but it's important to be there if you do need to find it, so if something happens you have the information there so you can see if you have the right to complain or not. Focus group participant The sense among participants that transparency in the public sector is important and to be encouraged sits in tension with the fact that some participants also associate public sector information with jargon and complex information that is hard to understand. Some participants feel that organisations in the public sector deliberately make it difficult to find or understand the decisions that they make. On the other hand, some suggest that there may be limits to the degree of transparency possible and feel that 'too much' transparency information may confuse the public and ultimately be unhelpful. I think a lot of it is just jumbled with jargon, it's not easy for people to understand. People have differing levels of experience and sometimes it's done on purpose. Focus group participant If you put all the info out there not only, are you going to confuse people, the general population aren't going to understand. I do think things are withheld. Overall, I don't think they are hiding too much. I do think there is a reason for it. Focus group participant Transparency about public sector use of algorithmic decision making Given the lack of awareness about algorithm use, participants do not have spontaneous expectations about what level of transparency is appropriate. In particular they find it difficult to identify the consequences of a lack of transparency, and so find it hard to form a view. However, when presented with the three use case examples of algorithmic decision making in the public sector (including parking, recruitment, and policing - see section 4 for more on the use cases), and having detailed discussions, participants formed strong views about the level, categories and format of information that should be made available (more detail is provided on this in the following sections). Ultimately, participants feel that being provided with transparency information will help to increase both public understanding and trust in public sector algorithmic decision making.At the same time, it is important to note that while participants feel it is important to be provided with this information, they only expect to need or want to access this information if they encounter a problem in the situation where the algorithm is being used. During the research, we also asked participants about the relative importance of individual citizens and experts having access to information, and preferences for the level of detail the information would have. Overall, in instances where participants are directly affected, participants prioritise making simple transparency information available for individual citizens over journalists and experts (see fig. 6). In circumstances where the outcome of an algorithmic decision making has a direct impact on them, participants feel it is important and 'their right' to have access to information about an algorithm. Further, given the perception that too much detail would be overwhelming and hard to navigate or understand, there is also a preference for this information to be simple rather than detailed technical information. However, among a small number of participants there is also an expectation and a desire that experts, journalists and civil society organisations like charities have access to wider, more detailed information about algorithms used by the public sector so that they are able to scrutinise and examine the impacts of new technology. There is a sense that these experts will go through the details on behalf of the public and would raise any concerns that might impact citizens. This became even more important for participants in later research sessions where we asked people to design information standards. During this process, when confronted with realities of transparency information specific to use cases, some participants became more pragmatic about what information they would realistically engage with as individuals. Given some participants were unlikely to engage with the information, these participants began to feel it was more important that experts had access to this information than individual citizens. If the info isn't being scrutinized by an independent party, then an untrained eye will just assume it's right... [it's important] the government should have someone there challenging them [with information]. Focus group participant I personally wouldn't [look at information on the algorithm]. I suppose it would depend on what the algorithm is and if it will make a difference on my everyday life, I would like to see it in that case. Focus group participant Unknown unknowns: initial desire for high levels of transparency As participants start to build their knowledge of algorithmic decision making and its uses, they move from low engagement and interest, to a desire for detailed information. This is a common reaction: as we become aware of a topic we realise how much more there is to learn. Once introduced to example use cases (policing, recruitment and parking in this case) participants are keen to gain more knowledge about the use of algorithms in the public sector and expect that other members of the public would feel the same. This sentiment comes through most strongly in the higher risk, higher impact use cases: recruitment and policing. In particular, participants spontaneously express a desire for: Evidence from pilot studies or previous uses to illustrate the effectiveness of algorithmic decision making, including specific locations, contexts and consequences. Details of when and where humans would be involved in the process if an algorithmic decision-making system is being used, especially whether they would have the option to contact somebody if they want to know more about the algorithm or to challenge a decision. In addition, participants welcome information on what personal data is used and how it is stored across all use cases, consistent with a generally high level of awareness of 'data protection'. Participants also felt strongly at this initial stage that anyone who was affected by use of an algorithmic system should be made aware of it, to be told explicitly that an algorithm is being used so they can understand what this means for them. Maybe there should be some statistics or examples of how it [the algorithm] is being used and a pilot study to show how it's working. If you put it out and it doesn't work, there is going to be backlash. Maybe they should include some views of the locals in the areas that it's being used in - they need to provide evidence that it is working. Focus group participant This initial desire for more information translates into an expectation that all transparency information for a particular use case should be made available somewhere. This view was held consistently throughout the research by many participants who see transparency as making as much information accessible as possible. The exception to this is where there is concern about possible risks associated with publicly sharing some types of information (e.g. the data used by the algorithm in the policing use case), and in these instances participants expect less transparency information to be available. You definitely need to know, it's your personal information. I always read stuff especially if it's to do with my personal information. You need to know why they are taking it and if they are going to keep it and how long. I think they should make all of the information available if somebody wants it. Focus group participant Known unknowns: increasing focus on the importance of simplicity As they move through the research, participants consider how they would encounter this information in their day-to-day life, and also become more comfortable with the idea of an algorithm being used in the public sector. As this happens, participants become more selective about the volume of information that they expect to see (although with the continuing expectation that ""all"" of the information would be available somewhere). This resulted in a common desire for transparency information - whether basic or more detailed - to be presented clearly and simply. In general.. [it needs to be] explained clearly and concisely - it's a lot of stuff that you might not even pay attention to, but there needs to be a brief going over everything and just bullet-pointing the most important things. Especially for people not able to process information as easily as others, it can all be overwhelming. A lot of people aren't really interested but you do have a right to know what's going on. There is a risk to giving too much information, but you have to find that balance to what we need to know and what we might be better off not knowing. Focus group participant Resolution: Two tiers of transparency information In phase three of the research, where participants worked together to design a prototype information format, this tension between transparency and simplicity was resolved by allocating information categories to different tiers. Participants expect the information in tier one to be immediately available at the point of, or in advance of, interacting with the algorithm, while they expect to have easy access to the information in tier two if they choose to seek it out. Tier two information is generally what is expected as a bare minimum across use cases; at the very least, it is felt that individuals should be able to access all of the transparency information if they would like or need to. NB: A few participants expect to see information about how personal data is used in tier 1 for the parking use case, while some feel information about data should be held back in case criminals take advantage of this information. I think they're [transparency categories] all important. If you want it, this will give you the info you need, a link will give you an option if you have questions about the process. Focus group participant The two-tiered approach balances participants' expectation that all transparency information is available to access on demand, whilst also ensuring that transparency information shared at the point of interacting with the algorithm is simple, clear, concise and unlikely to overwhelm individuals. Do people need to know all the detail? Just have a basic leaflet and then make the detail available online clearly and without any jargon! Focus group participant During the research we presented participants with three different use cases that involve public sector algorithmic decision making to understand overall trust, as well as expectations around public sector transparency. We found that the different use cases had a significant influence on the degree to which participants feel comfortable and trust an algorithm. This in turn impacts the level of transparency information they feel is appropriate in each instance. We found recruitment to be the most divisive use case, with concerns about a technologically- rather than human-driven application process translating to discomfort about the use of an algorithm. Conversely, participants' low emotional engagement with and relatively high levels of familiarity with the use of technological solutions in car parking means they are more accepting of an algorithm in this instance. There are two main factors that seem to influence how participants view the use cases and their level of comfort and trust in an algorithm being used: Risk- the perceived likelihood that the scenario will lead to an unfavourable outcome for them as an individual or society more broadly. This is typically driven by the degree to which participants trust the efficacy of an algorithm to make a decision in each scenario. For example, many were sceptical about the ability of an algorithm to make correct recruitment decisions. Impact- the severity of impact an unfavourable outcome would have directly on them as an individual or society more broadly. The degree of perceived potential impact and perceived potential risk influences how far participants trust an algorithm to make decisions in each use case, what transparency information they want to be provided, and how they want this to be delivered. For lower potential risk and lower potential impact use cases, passively available transparency information - in other words, information that individuals can seek out if they want to - is acceptable on its own. This could, for example, be information available on a website. It is also more acceptable for passive information to be held as part of a centralised system, rather than being targeted to those affected. For higher potential risk and higher potential impact use cases there is a desire not just for information to be passively available and accessible if individuals are interested to know more about the algorithm, but also for the active communication of basic information upfront to notify people that the algorithm is being used and to what end. As part of this, it is felt that information should be more targeted and personalised. For some use cases expectations around transparency information are very high (for example, for the policing use case, some expect to see door drop campaigns announcing the use of algorithms). This scenario is seen as having both high potential impact and high potential risk. There is less confidence in an algorithm being able to perform this task than a person, and therefore limited trust in an algorithm being used in this scenario. Initial views Participants have mixed views when shown the initial recruitment use case. Those with previous experience of an online job interview also report mixed experiences, and while they were able to complete the process and found it straightforward, some felt that the lack of face-to-face human interaction was unhelpful for rapport and easing nerves. Recruitment cannot be automated there are too many variables to consider when choosing the perfect candidate. Female, 45-54, Online community Some participants expect that this type of process would make it more efficient and easier for some people to apply for jobs, and the ability to record a response in their own time would make some feel more comfortable. This is especially the case for those who are more familiar and comfortable using technology. I am confident about doing videos and think it gives the employer a better idea of who you are as a person. It seems like an efficient way of doing things. Female, 35-44, Online community However, others feel that some people may not come across as well in a video response, and that their performance may suffer from the lack of human interaction. There is also some concern that people may be able to 'cheat' the system, for example by having someone else script the response for them, or having more time available to practise their response. I think that all job interviews, even preliminary ones should be conducted in person. You can't tell a lot about someone on video and you can't get a feel for their personality, they may end up excluding some people that could be really good workers based on the fact they don't video well! Female, 18-24, Online community Trust in algorithm use The recruitment use case is where participants have most doubts about the overall effectiveness of an algorithm being used. Specifically, there is scepticism that a computer can assess candidates as effectively as a human. Some participants are suspicious about why a video is needed, as opposed to a written response, and express concerns about the purpose of a visual and the potential for this to lead to biased decisions. This use case is felt to have a high potential impact as a negative outcome would not only have ramifications for the individual applying for the job, but could also have broader societal impacts, for example if cultural biases were built into the algorithm preventing certain groups from getting employed. The table below (fig. 11) summarises the perceived benefits and drawbacks of using an algorithm in the context of public sector recruitment. When asked on a scale of 1-10 (where 1=not at all, 10= totally trust) how much they trust an algorithmic decision-making system to be used in this context,participants give an average score of 3.9.Broadly participants feel that it is important for humans to be involved in this process to remove potential technological issues and errors and maintain human element of recruitment. Transparency information The perceived high stakes of this use case, combined with a lack of familiarity and trust in technology being used this way, mean that participants feel it is especially important for transparency information to include how exactly the algorithm works. As part of this, participants are particularly keen understand what criteria are being used to identify the most promising candidates so that they understand the process fully and can ensure their response and application is well-prepared. Understanding the criteria being used by the algorithm is also seen to help applicants understand why they have or haven't been successful, and a fundamental aspect of the feedback process. Given doubts and concerns about algorithms being used in a process that is seen to require human and emotional intelligence, transparency information about the role of human involvement in this process is also felt to increase trust in this algorithm being used. As part of this, participants would like clarity on where exactly algorithmic decision making 'begins and ends', as well as understanding where human decision-making fits into this. Further, participants also want to see information provided about where they can go if they have any questions about the process or the outcome. Due to the highly personal nature of the application and significant amount of personal information given, participants also feel it is important for transparency information to include reference to data security and privacy, so that applicants understand exactly how their information will be used and how long it will be stored for, before applying. Overall, desire for transparency information in this use case is higher than other use cases tested. With the algorithm playing a significant role in determining whether they get through to the next stage of an employment process, this use case is felt to have higher potential risk and higher potential impact, especially on an individual level. Given the emotional stakes of this use case, and an unfamiliarity with technology being used in this way, participants feel candidates should be informed about the fact an algorithm is being used and to what effect at the point of deciding whether or not to apply, and proactively throughout the process. The case study of Randeep below (Fig. 12) highlights some of the specific concerns some participants have about algorithmic decision making in the recruitment use case. Overall, this scenario is seen as being low risk and medium impact. Participants generally feel comfortable with an algorithm being used in this use case, as they recognise the potential value in terms of efficiency, and it is felt to have less direct personal impact on them than other use cases. However, there is some concern that publicly available information about police decision-making processes may enable potential criminals using this information to their advantage. This was a strong concern for some participants, even when it was explained that this would be an unlikely outcome of the scenario proposed. Initial views In this use case, participants are interested to find out more about how the way of allocating police had changed and the impact this could have on their local areas. The key positives related to this use case are understood as the potential for increased crime prevention and reduced crime levels, while others are pleased to learn more about an issue impacting their local area. More police officers will be better and the neighbourhood will feel safer. Female, 25-34, Online community Among participants there is also some concern around how a potential increase in police presence would be funded, and others wonder whether information about police allocation can be harmful to public safety, and lead potential criminals to use the information to their advantage. Could it be abused by people by sending the police to different areas deliberately? Male, 65+, Online community Trust in algorithm use Most participants believe that there are clear benefits to algorithmic decision making in this use case. The fact that this scenario feels more removed from participants' daily lives, also increases acceptance of an algorithm being used, as participants feel that it is less likely to have a direct negative impact on them. The table below (fig. 14) summarises the perceived benefits and drawbacks of using an algorithm in the context of policing. When asked on a scale of 1-10 (where 1=not at all, 10= totally trust) how much they trust an algorithmic decision making system to be used in this context,participants give an average trust score of 4.7.While participants feel positively about the potential for algorithmic decision making to increase the efficiency and the impact of police resourcing, there are some concerns about how effective this would be in practice, and potential 'unforeseen consequences' of under-resourced areas and people tricking the system. Transparency information Given the perception that good police resourcing should be informed by human experience, there is some desire for transparency information to provide reassurance on the human input and support going into this decision making. Participants also want to know that the algorithm is working appropriately and is being checked. However, in this use case participants feel strongly that any transparency information should not include the actual data which the algorithm uses, as it is felt that potential criminals could use this information to their advantage and to the detriment of public safety. In fact, some participants are concerned that any degree of transparency information may inadvertently lead to the system being 'tricked' and therefore also want reassurance that any and all transparency information has been deemed 'safe' to share more widely by experts. This was a particular concern for one participant in Northern Ireland, who felt that information about policing needed to be secure. Overall, similar to other use cases, participants do not expect to actively search for or access transparency information on this use case themselves unless they have personally had a negative experience directly related to police resourcing. However, given the potential for any errors in police resource allocation to have a significant negative impact on wider public safety, participants express a desire to be actively notified upfront that 'policing is changing' e.g. via public information campaigns. This also reflects the fact that among participants, there is a strong sense that algorithms being used in this way is new, and a significant development. The case study of Peter below highlights how becoming more informed about public sector algorithmic decision making, in this case when used in policing, can increase trust in algorithms being used. This scenario is seen by all participants as being both low risk and low impact. Participants generally have high levels of confidence in an algorithm to be able to perform this task and are less concerned about the impact of a negative outcome for them. Initial views When shown an outline of the initial parking use case, without explicit information regarding the use of algorithm (seeFigure 16), participants are broadly happy and comfortable with this scenario. There is familiarity with ANPR technology, and several have used this type of system before and had positive experiences. Participants associate a range of benefits with this scenario, including it being an easy, quick and efficient way of paying for parking, the ability to have evidence of online payment, and facilitating fairness by ensuring everyone pays for parking. I do actually prefer this method as years ago you would have to make sure you had change for the machine and often I would not have the right money...So having the camera at the entrance reading your number plate and you ringing a number to pay for your parking I feel is a good idea as you have proof that the payment has been taken from your debit card in case you do get a ticket. Female, 55-64, Online community Key issues associated with this scenario include having issues with phone and/or internet signal, not having a credit card, and ensuring the signage is clear enough for everyone to understand it. That's all well and good, assuming you have an internet enabled phone to pay online. Female, 55-64, Online community Trust in algorithm use On the whole, participants are comfortable with the use of algorithmic decision making in this context. Ultimately, participants do not see the use of an algorithm as likely to have a significant impact on them. An unfavourable outcome would mean getting a ticket, which in participants' view is 'how parking has always worked and always will.' The table below (Fig. 17) summarises the perceived benefits and drawbacks of using an algorithm in the context of parking. Overall, participants are positive about the use of an algorithmic decision making in the parking use case; when asked on a scale of 1-10 (where 1=not at all, 10= totally trust) how much they trust an algorithmic decision-making system to be used in this context,participants give an average score of 7.5. There is a general sense that in this parking scenario, the use of an algorithm is straightforward and efficient, and the most significant issue would be the reliability of the technology. Those who lacked trust were often sceptical about parking fees and ticketing generally, as much as about the algorithm. I do feel that this is a process in which computer assisted decision making is relatively straight forward to apply and drives up efficiency and enables cost-savings which can be used to better purposes. Male,35-44, Online community Transparency information Given the fact that this use case involves personal data, participants also feel that it is important for transparency information to include data security and privacy information. Participants generally want to know what data is being held, for how long and why. Participants often link this to awareness of General Data Protection Regulation, and a sense that they have a right to know how their data is handled. [I'd like to have] confirmation that your information is kept private and that your details are not shared with any third parties. Female, 55-64, Online community As with other use cases, participants also feel that information about who they can contact in case they encounter any problems or want to challenge a parking ticket is particularly important, and gives them reassurance that help is at hand in case there are any technological issues. However, overall, desire for transparency information in this use case is lower than other use cases tested. In general, participants feel strongly that it is their right to know an algorithm is being used and expect this to be explicitly stated at the car park entrance. Yet, beyond this, participants expect that people can seek out further information (including contact and data security and privacy information) if they want it, rather than expecting any active communications. As long as it's very clear that this is in operation, I would trust it. Male, 18-24, Online community The case study of Thomas below highlights some of the specific advantages participants recognise when algorithmic decision making is used in the parking use case. In Phase 1 of the research, where participants were initially introduced to the three use cases (recruitment, policing and parking), they were also shown basic transparency information about how and why an algorithmic decision-making system is deployed in each setting. In Phases 2 and 3 of the research, participants were presented with a total of nine categories of transparency information, which were based on discussions with CDEI, the Cabinet Office, and a review of the transparency standards used internationally. Across the research, adescriptionof the algorithm and itspurposeremained the most important categories of transparency information. The table below outlines participants' prioritisation of the nine information categories during the Phase 2 online community. This prioritisation remained largely consistent when asked generally and when discussed in relation to each of the use cases in turn. The main exception to this is contact, which came out as a higher priority during the Phase 3 focus group discussions and prototype development. The other exception is data which, when interpreted as personal data, was considered a higher priority to include. While one of the lower ranked categories in the online community, in the focus groups contact emerged as one of the most important categories. In a two-tier system, details about where to get more information or ask questions is elevated and prioritised as tier one information, alongside description. The main one for me is contact, I need to know who to contact if it goes wrong. I need to know who is responsible and their contact info so I can contact them by email or phone. Focus group participant Sharing information about risks is polarising among some participants and in some use cases due to a concern that it will cause additional and unnecessary anxieties among those already unsure about the use of an algorithm. In the focus groups this category was expected to appear in tier two but was prioritised below other types of information. I would like to know more than just the descriptors but not the risks because your trust is lowered. You don't want to know the risks because it's going to happen anyway. I'd rather not know risks and just go with it. The risks would put me off as I am already anxious about it. Focus group participant In the focus groups, it became apparent that data is understood by some to refer to the use of their personal data and data privacy - participants expect this to be called out and is of higher importance to them than information on other data sources. You just need to know where your information is going and who has that information. I think data is the most important for me [it is like] when you're on Facebook... your information is being sorted and you don't even know it. Focus group participant For some, there are concerns about sharing information on data when it comes to the policing use case particularly due to the fear of this information getting into the wrong hands. You shouldn't share too much data in case criminals get hold of it and start committing crimes in other areas or play the system. Focus group participant For the policing use case, some participants expect more information to be included in tier one. Commercial information is felt by some to be more important than other categories, and in some instances, participants feel this should be included along with tier one information. This is because the police are an institution that serves and protects the public and therefore making the public aware of other organisations that might be involved in delivering this service is felt to be important. As policing impacts entire communities, some participants would also like to see information on risks and impact included in tier one. However, this should be at the expense of the information remaining simple and clear. It's important to know if it's just the police that handle this information. I think it's important to understand the impact for policing, as it impacts the community. They need to explain this so everybody can understand. Focus group participant For each category of information, participants were presented with the following details: Heading A question that explains the category 3-4 examples of specific types of information that could be included in that category Breaking down the transparency information in this way is felt by participants to be clear and easy to digest. In particular, having the question that explains what each category of information covers is seen as a helpful aid to understanding. Description Consistently throughout the research, the description of how the algorithm works was seen as the most important piece of transparency information to be shared. Participants expect a description to be clear, simple and not overly technical, summarising the role of the algorithm rather than providing too detailed of an explanation. This category of information had high appeal when it was first presented in Phase 1, and this remained the case for the duration of the research process. Participants expect that all three of the bullet points would be included within each of the three use cases. The first and second bullet points 'when the algorithm is used' and 'how an algorithm works' are felt to be particularly important, as participants expect that many individuals would have limited knowledge about what an algorithm is, and providing this information would help to address this. The information outlining 'how the result of the algorithm is used/interpreted' is felt to be particularly relevant for the higher risk and higher impact use cases (policing and recruitment). Purpose Similarly to thedescriptionoutlining how an algorithm works, the information included within thepurposecategory is felt to provide further clarity about use of algorithms in the public sector. Participants often find it difficult to separate their feeling about whether an algorithm should be used in each use case, so information outlining 'why an algorithm is being used' is especially valuable. The first two bullet points 'reasons why an algorithm is being used' and 'why algorithmic decision making is better than human decision making' are the most interesting and important. While the extra information is welcomed, the third bullet point is felt to be less relevant across all use cases. Description and Purpose - I put these first as I think we should know exactly why this algorithm is being used. Having a basic understanding of why/how it's used would benefit the person it's being used on. I think it's someone's right to know why and how it's being used. Male, 25-34, Online community Contact Unlikedescriptionandpurpose, this category of transparency information was not presented to participants until Phase 2 of the research. However, in Phase 1, participants spontaneously mentioned that the relevant contact details needed to be included in the transparency information. The most important element of this information category is providing the public with clear details on how to get more information or get a query addressed. The last bullet point outlining 'details on the appeal process' is seen to be especially relevant to the parking and recruitment use cases. The first bullet point was felt to be less important for some. You need to know who to contact if you have any concerns about it [the algorithm] - this needs to be available. Focus group participant Data When discussingdataparticipants typically think first of their personal data - what is being used and how it is being stored - which is considered to be more important than getting information on other data sets used by the algorithm. This is particularly true for participants who are more aware of General Data Protection Regulation (GDPR). This is most relevant to the parking and recruitment use cases where individuals have more direct interactions with the algorithm. Particularly among those with a more advanced understanding of algorithms, knowing what data the algorithm is using is seen as very important, as this is seen as what will determine the effectiveness of the algorithm. However, these participants concede that not everyone will be interested in this information, and would expect it to fall within tier 2. For the policing use case, there is concern among some that too much information on data sources and data used to train the algorithm could lead to criminals 'playing the system'. I think there's a danger that once people know the algorithm they can then game it...if I know the algorithm about police ending up in my street and I'm a criminal I know that this is the best time to commit a crime in my area because police are being sent elsewhere. Focus group participant Human Oversight This category of information resonates with participants, as they want reassurance that a human is also involved in the process in each use case. However, they are more interested in information about how and when humans would be involved in the process as opposed to the risk of human error. Overall, while relevant, this category of information is not felt to be as important as the others. Would there be a human involved? When would they come in or would it be the algorithm throughout? This is important to know. Focus group participant Risks This is the most polarising category of transparency information. Many participants welcome this extra level of transparency, especially when there is a lot at stake. For others, this is considered as being 'too transparent' leading to worries that sharing this information would lead to unnecessary anxieties and concerns about the use of algorithms in the public sector, which could inadvertently cause more work for public sector bodies as they have to manage this. This is mainly a concern in the higher risk and higher impact use cases where there is lower immediate acceptance of an algorithm being used. Ultimately, most feel that this is important information to make available within tier two. I put risks as least important because any time you mention risks it will pre-emptively scare the reader because it's putting the risks out there for all to see and causing some doubt in the decision where maybe there was none before. Male, 25-34, Online community The case study of Anita below highlights the worries that some have about including information about risks as part of algorithmic decision-making transparency information. Impact Participants feel that information onimpactis necessary for transparency. However, they are less interested in 'impact reports for various demographic groups', and more motivated by how the algorithm is directly impacting them as individuals. I'd be interested to find out more to see how it would impact me, but I wouldn't be interested in the technical details - just how it will work and why they are doing it. Female, 45-54, Online community Commercial information & third parties, and Technicalities Figure 29: Stimulus used in Phase 2 focus groups. While participants feel details aboutcommercial information and third partiesand technicalities should be made available and accessible, they are less likely to be the main priority. Therefore, participants suggest that they should be available online if individuals are seeking extra information about the use of an algorithm in the public sector. A few participants feelcommercial information and third partiesshould be communicated upfront for the policing use case. Overall, participants want basic information about the use of the algorithmic decision-making system to be made immediately available when engaging with the information, or in advance, in tier 1 for all three use cases (refer to section 4) and would expect to be signposted to a website if they require additional information. However, many participants also feel this information should be provided in leaflets, the local newspaper, and on the radio to make it widely accessible to members of the public. In terms of information design, accessibility for a wide range of people is a key consideration and impacts the sort of language and layout participants would expect to see. Crucially participants want to avoid jargon, to easily be able to find the information they are looking for, and for options to be offered to those who are not online (e.g. automated phone lines). In general participants default to the two tiers of information in relation to the preferred channels they expect for each use case (parking, policing, recruitment). In tier 1 (across all use cases) participants expect active communication of basic information upfront to notify people that the algorithm is being used and to what end - this is either immediately at the point of interaction or in advance. For example, participants' suggestions included signs before entering the car park (parking), door drops, or radio announcements, to notify people an algorithm is being used (policing), and a notification in the job description (recruitment). In addition, for the use cases in which participants feel less comfortable with an algorithm being used (higher impact and higher risk) more personalised information is likely to be more effective than non-personalised information. For example, in the recruitment use-case, participants expect that they would receive a personalised letter informing them how algorithmic decision making was used to process their application. For more detailed information (tier 2), participants express the need to be directed to a website and being provided the option to call an automated line (which will provide the same information) for those who are offline and do not have access to a computer or the internet. Recruitment Overall, participants want to be notified that an algorithm is being used within the job application. Within the application, participants expect to be directed to the organisation's website if they require more detailed information about the use of an algorithm to identify the most promising candidates (as well as the written content, participants express that an option should be provided to listen to short videos to ensure the information is accessible). To make the process more transparent, for those that continue with their application, there is an expectation for organisations to send the applicant an email with the outcome outlining how their application was assessed by the algorithmic tool. PolicingParticipants feel that aleafletand radio would be the most suitable channels to receive basic information about algorithms being used to allocate police officers because it's both accessible and personalised. In addition, to make it more accessible to a range of different audiences, participants recommend this information should also be announced on the radio. Within the leaflet, participants expect to be directed to their local police website, via a link, to access more detailed information. Parking There was a widespread view that there should be a sign which clearly outlines that an algorithm is in usebeforepeople enter the car park. For additional information, participants propose that there should be a link and QR code on the sign, which directs individuals to their local council's website. In addition, to ensure everybody has access to this information, participants also suggest including an automated number for members of the public to contact if they are offline or do not have access to the internet. Participants highlighted a range of principles for the presentation of transparency information in any channel and format: Overall, participants expect the language that is included within the transparency information to be clear, concise and accessible (without the use of acronyms or jargon). Participants felt this was particularly important when they reflected on the fact that they originally had low understanding and awareness of algorithms at the beginning of the research process. To ensure that the content is user friendly, participants feel that both headlines and bullet points should be used to clearly group the content, both in paper form (leaflets/ local newspapers) and on websites. This is to ensure the information does not cause cognitive strain, especially because many members of the public are unfamiliar with what an algorithm is. In addition, drop down boxes and 'hover overs' that help to explain technical terms have high appeal to aid comprehension further. Although participants spontaneously mentioned using visuals and animations at the beginning of the research, this is not considered to be a priority at the end of the process and is therefore rarely mentioned in the design process in Phase 3. This is because simplicity is a key driver, and if the balance is correct between the headlines and the following content, visuals are not thought to be essential to aid understanding further. Some participants can see huge potential for having a recognised symbol indicating when an algorithm is in use. While this will increase transparency in the public sector, participants also feel that this will lead to more awareness and familiarity about the use of algorithms in the public sector more generally. In the final phase of the research, participants were presented with two existing examples of algorithmic transparency information that are used in Helsinki and New York. Participants were asked to provide their views on the information and presentation style within both models. The Helsinki model Overall, the Helsinki model has high appeal amongst all participants. They are positive about the information being in a centralised register and assume that people would be able to locate this information easily. In addition, they appreciate the simple headlines and summaries that are provided on the home page, and value the option of being able to click on 'read more' if they require more detailed information. While the visuals are not spontaneously mentioned by most, those that notice them acknowledge that they capture the essence of the content well and contribute to an overall aesthetically pleasing and clear design. When exploring the additional information available, 'contact information' and 'more detailed information on the system' really stand out to participants as being important to cover. Overall, this example is felt to be simple, user friendly and to have the right balance of information to increase transparency in Helsinki's use of artificial intelligence. Participants express a desire for the public sector in the UK to have a similar centralised register for the more detailed content and feel that they as individuals and others would be able to engage with the information. The Helsinki example brings to life how participants expect and want tier two information to be made available. If it's laid out like this, it's not too much, as you can pick out what you want to see as you can click through. It's bright, it's light and inviting - it definitely helps. Focus group participant Overall, this model is not particularly well received. On initial exposure to the New York Model, participants raise concerns about the length of the content and feel that it is not as user friendly as the first example. Participants like the clear headings, but without the filter for additional information (as in the Helsinki model), participants either feel this is the right level of information or too much. Most participants assume that this level of information is not targeted to the public, and is more likely to be suitable to academics or experts. In addition, a few participants raise concerns about the accessibility of the information because of its format and thought that some people would not be able to access it. There was some confusion about whether this would need to be downloaded from a website or whether it would open in a web browser. I don't think this is good, it's a document academics would go to as it's a more technical resource. Would the general public know all of the jargon being used? Focus group participant In summary, public understanding of and trust in the use of algorithms in the public sector is most likely to be improved through active communications that notify the public about where algorithmic decision-making is being deployed, with full information available on demand. In terms of understanding, at present awareness of the use of algorithms in the public sector is low, as is awareness of transparency in the public sector. The low engagement nature of these topics means that people are unlikely to seek out information about the use of algorithms. Signposting people to the fact that there is information available if they are interested has the potential to increase engagement, and therefore understanding about the role algorithms play in the public sector. Presenting all information in a simple, clear and easily digestible way is also key for aiding understanding. When it comes to trust in the use of an algorithm, the scenario in which it is being used is a highly influential factor. In scenarios where trust is lower (typically those which are perceived to carry a greater risk of a negative outcome, and where the potential impact of a negative outcome is felt to be higher) activities that build trust are more important. Therefore, active communications around the role of the algorithm, its purpose and how to access further information are most important in these instances. Importantly, active communications need to be supported by a baseline of having all available categories of information about the algorithm accessible to the public somewhere in an on-demand way. Although participants do not generally expect to make use of this information, they feel that in principle it should be available to them, and that without this the public sector could not claim to be truly transparent around its use of algorithms. They also expect that experts may access this information on their behalf, raising any concerns which may be relevant to citizens. In this report, we have summarised the need for different types of information in different scenarios or at different points in the journey of engaging with an algorithm through the two-tier framework. This model highlights the difference between the information about an algorithm that is expected to be passively available on-demand (tier two), and the types of information which, if actively communicated, would be most effective and increasing trust and understanding (tier one). Our recommendation to the public sector would be to flex this two-tier model depending on the specific scenario in which an algorithm is being used. For scenarios where the public are likely to perceive higher potential risk and impact, a two-tier approach with full transparency information available alongside signposting is likely to be most effective for building trust and understanding. For scenarios which are felt to be lower stakes, just having tier two information available is likely to be sufficient. Associations with algorithmic decision making When first asked about their associations with 'computer assisted decision making', participants have mixed views. While there is some positivity towards computer assisted decision making, participants also have a number of concerns. Where 'computer assisted decision-making' is viewed positively, it is associated with a sense of efficiency, optimism, and progress. For these participants computer decision making is seen as: An important form of technology that can save resources, time and costs An essential part of society and daily lives, and an inevitable and undeniable aspect of the future You can see it really well with technology right now. It has almost become difficult to live without your phone. You can do a lot of things and it saves a lot of time. Right now you can chat to any part of the world with anyone in a short amount of time as if they are next door. Technology has made the world smarter and more efficient. Focus group participant Others, while able to recognise the benefits of computer assisted decision making, are more apprehensive about it. This is mainly driven by: Sometimes it can be scary to people that don't know much about assisted decision making. It can be scary because you don't know if it will choose the right things, you just don't know. Focus group participant Negative, frustrating, and invasive experiences with technological systems and processes in general A sense that computer assisted decision making is not able to adequately account for 'human', emotional, and personal characteristics and circumstances This I feel makes things harder for humans now for example buying a house with a mortgage an algorithm makes the decision for you whether you can have it or not and it takes away the personal aspect and I feel it's too regimented! Female, 18-25, online community A fear that computer decision making may result in laziness or an inability for people to make decisions A concern that computer assisted decision may lead to unemployment I worry what it means for actual jobs and people, I don't understand how it's going to help people with jobs. You're kind of wiping out loads of jobs and taking the human out of things. Focus group participant Attitudes towards the use of algorithms in general The mix of views on algorithmic decision-making systems is reflected in the fact that the majority (60%) of participants believe that computer assisted decision making systems are as much of an opportunity as a threat (see Figure 36). The demographic profiles of those who see computer assisted decision making as an opportunity are varied. However, of the 5 participants who see computer assisted decision making systems as more of a threat, 4 are within the 55-64 age range. Although the numbers are small and should be treated with caution, this provides some indication that older participants are more likely to see computer assisted decision making as a threat. I think it's more of an opportunity, it takes out human error, computers can make a decision in half a second. I have been to Singapore where everything is run on a system and it's more efficient than the underground. Focus group participant I think it's open to being misused, the majority of people in general are not up to speed. Focus group participant Although there is some indication that older people are more likely to see computer assisted decision making as a threat, there is no clear correlation between attitudes to computer decision making and attitudes towards technology more widely. Algorithms in different settings Across the dialogue we found that the degree to which participants feel comfortable with and trust algorithmic decision making depends most strongly on the specific area and scenario in which it's being used. In general, participants are more comfortable with algorithms being used in instances where the decision making is perceived to be purely factual and data-driven, with no need for human or emotional insight. There is also a sense that algorithms work best for repetitive tasks and instances where speed is valued. Computer assisted decision making is beneficial in many situations, including travelling in an aircraft where autopilot is used, GPS used to drive cars, medical equipment used to save lives by constantly analysing data and computers making decisions about right medication and care. Male, 65+, online community If these systems enhance the user experience by saving time and recognising the user's probable needs and can then direct the user more specifically to the area they need to be, for example, in medical situations where there's an online questionnaire to determine what treatment would be the right one or not. Male, 35-44, online community In contrast, participants feel less comfortable with algorithms being used in more complex, nuanced and high-stakes situations, where emotional insight and a detailed understanding of personal situations is perceived to be important. Among participants, there is a strong sense that algorithmic decision making is not able to adequately account for the emotional aspects of the human experience. Interpersonal services which require empathy or emotional intelligence should not be replaced by computers. Male, 45-54, online community There are always going to be issues around the use of any type of artificial intelligence in weapons systems. When, if ever, should a drone be allowed to make an executive, autonomous decision on weapon deployment? Male, 35-44, online community Algorithms in the public sector While there is limited awareness of specific instances where algorithms are currently being used in the public sector, among some participants there is a general assumption that algorithms must be already being used by the public sector to streamline some processes. Given the perception that algorithmic decision making can help save resources, time and costs, participants are generally comfortable with the concept of algorithmic decision-making being used in the public sector. However, there is an assumption from some participants that algorithmic decision making may require expensive technology, and that the public sector may not be able to afford 'high-quality' systems. There is also a concern that algorithmic decision making in the public sector may also replace certain jobs, leading to unemployment. The issue I have with that is I know they cost a lot of money for those systems that are that intelligent, I would worry that the public sector won't be forking out the money for those machines - they'd be using the ones that don't have those features Focus group participant My fear in the public sector where I work, will they not need human beings anymore and we won't have jobs? Focus group participant Transparency in the public sector Despite the perceived importance of transparency in the public sector, there are very few specific positive or negative examples of public sector transparency that are front- of-mind for participants. On further probing, some participants identify the Covid-19 vaccination rollout as an example of where the Government has been successfully transparent. They feel that the public have been well-informed about the key decisions regarding the timings and order of the roll-out of vaccines, and the reasons behind these decisions. As well as being easy to understand, there is also a sense that the information has been timely and widely accessible. I think the COVID-19 vaccination plan is fairly understood by everyone. It was all very open and clear why the decisions were made the way they were. You can't get away from all the news briefings. Focus group participant In contrast, some participants express concern about the lack of transparency and clarity around individual decisions, including universal credit decisions. Not being given clear reasons and explanations for the final decision leads participants to feel that these decisions are inconsistent and unfair. A small number of participants feel that there is a certain degree of 'cronyism' in the current government, which they associate with a lack of transparency. I would like information on why they got it [financial support] and I didn't. All you get is a yes or a no. You don't get why. The government are not justifying their actions. Focus group participant",2023
govuk_013,govuk,Ai Procurement 3,"PPN 02/24 provides optional questions to help identify the use of Artificial Intelligence in procurements, and in the delivery of Government Services. PDF,217 KB,7 pages This file may not be suitable for users of assistive technology. HTML PPN 02/24 provides optional questions to help identify the use of Artificial Intelligence in procurements, and in the delivery of Government Services.",2023
govuk_023,govuk,Automated Decision-Making 3,"A summary of changes to the UK's data protection and privacy legislation in the Data (Use and Access) Act. The Data (Use and Access) Act 2025 (""DUAA"", ""the Act"") received Royal Assent on 19 June 2025. This is a wide-ranging Act which includes provisions to enable the growth of digital verification services, new Smart Data schemes like Open Banking and a new National Underground Asset Register. It also includes some important changes to the UK's data protection and privacy legislation, which are the subject of this page. TheDUAAwill not replace the UK General Data Protection Regulation (""UKGDPR""), Data Protection Act 2018 or the Privacy and Electronic Communications (EC Directive) Regulations 2003, but it will make some changes to them to make the rules simpler for organisations, encourage innovation, help law enforcement agencies to tackle crime and allow responsible data-sharing while maintaining high data protection standards. This page covers the key changes. It is not a comprehensive list of all the data protection measures in the Act and is not regulatory guidance or legal advice. The Information Commissioner's Office (""ICO"") is responsible for publishing regulatory guidance on its website -www.ico.org.uk. TheDUAAcreates a more permissive framework under the UKGDPRfor organisations to make decisions based solely on automated processing that have legal or similarily significant effects on individuals. Organisations will be able to make such decisions in wider circumstances but must implement certain safeguards. These include: The Act broadly mirrors these provisions and safeguards across the law enforcement regime in the Data Protection Act 2018. However, for those processing under the law enforcement regime, the Act provides for an exemption to the safeguards for certain, limited, reasons, such as to safeguard national security, or to avoid the obstruction of an inquiry. This is as long as the decision is reconsidered, as soon as reasonably practicable after it is taken and the reconsideration involves meaningful human involvement. TheDUAAclarifies the time limits for organisations to respond to subject access requests (requests by individuals to access and receive a copy of their personal data). It includes a ""stop the clock"" rule, allowing organisations to pause the response time if they need more information from the requester. Once they get the information they need, the response time continues. Organisations need to make reasonable and proportionate searches when responding to requests. New rules require certain online services likely to be accessed by children to consider how to protect and support them when designing these services. TheDUAAclarifies that scientific research may include commercial research. It allows researchers to seek consent for broad areas of related research and clearly outlines the safeguards required for using personal data in research. The Act introduces a new lawful ground for processing personal data, giving businesses more confidence to use data for crime prevention, safeguarding, responding to emergencies, and other specified legitimate interests. The Act simplifies the rules and provides necessary clarification for transferring personal data internationally. TheDUAArequires organisations to handle complaints from individuals who are concerned that the way their information is used breaches the data protection legislation, for example by providing an electronic complaint form, and informing the individual about the outcome of a complaint. The Act allows the use of storage and access technologies without explicit consent in certain, low-risk situations. TheDUAAamends Parts 3 and 4 of the 2018 Act which regulate law enforcement processing and processing by the intelligence services. Some of these amendments mirror key changes being made to the UKGDPR, ensuring consistency across the data protection regimes. Others simplify the legislation, enabling those processing under the law enforcement regime to operate more efficiently and supporting closer working with the UK intelligence agencies to safeguard national security. The changes to data protection law will be commenced in stages, 2 - 12 months after Royal Assent. Exact dates for each measure will be set in commencement regulations. Information about the commencement regulations will be provided on GOV.UK when it is available. Parliamentary information and supporting documents about the Data (Use and Access) Act can be found at:https://bills.parliament.uk/bills/3825 Data (Use and Access) Act 2025 Links to commencement regulations will be provided on GOV.UK. TheDUAAwill amend (update) the below legislation. The updated legislation will be available within approximately 2 months of Royal Assent of theDUAA. Data Protection Act 2018 UK General Data Protection Regulation The Privacy and Electronic Communications (EC Directive) Regulations 2003 ICOplans for new and updated guidance.",2023
govuk_009,govuk,Ai Fairness 4,"Published 17 May 2024 (c) Crown copyright 2024 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visitnationalarchives.gov.uk/doc/open-government-licence/version/3or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:psi@nationalarchives.gov.uk. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned. This publication is available at https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai/international-scientific-report-on-the-safety-of-advanced-ai-interim-report Disclaimer The report does not represent the views of the Chair, any particular individual in the writing or advisory groups, nor any of the governments that have supported its development. This report is a synthesis of the existing research on the capabilities and risks of advancedAI. The Chair of the report has ultimate responsibility for it, and has overseen its development from beginning to end. Research series number:DSIT2024/009 Prof. Yoshua Bengio, Universite de Montreal / Mila - QuebecAIInstitute Prof. Bronwyn Fox, The Commonwealth Scientific and Industrial Research Organisation (CSIRO) (Australia) Andre Carlos Ponce de Leon Ferreira de Carvalho, Institute of Mathematics and Computer Sciences, University of Sao Paulo (Brazil) Dr. Mona Nemer, Chief Science Advisor of Canada (Canada) Raquel Pezoa Rivera, Federico Santa Maria Technical University (Chile) Dr. Yi Zeng, Institute of Automation, Chinese Academy of Sciences (China) Juha Heikkila, DG Connect (European Union) Guillaume Avrin, General Directorate of Enterprises (France) Prof. Antonio Kruger, German Research Center for Artificial Intelligence (Germany) Prof. Balaraman Ravindran, Indian Institute of Technology, Madras (India) Prof. Hammam Riza, KORIKA (Indonesia) Dr. Ciaran Seoighe, Science Foundation Ireland (Ireland) Dr. Ziv Katzir, Israel Innovation Authority (Israel) Dr. Andrea Monti, University of Chieti-Pescara (Italy) Dr. Hiroaki Kitano, Sony Group (Japan) [Interim]Mary Kerema, Ministry of Information Communications Technology and Digital Economy (Kenya) Dr. Jose Ramon Lopez Portillo, Q Element (Mexico) Prof. Haroon Sheikh, Netherlands' Scientific Council for Government Policy (Netherlands) Dr. Gill Jolly, Ministry of Business, Innovation and Employment (New Zealand) Dr. Olubunmi Ajala, Innovation and Digital Economy (Nigeria) Dominic Ligot, CirroLytix (Philippines) Prof. Kyoung Mu Lee, Department of Electrical and Computer Engineering, Seoul National University (Republic of Korea) Ahmet Halit Hatip, Turkish Ministry of Industry and Technology (Republic of Turkey) Crystal Rugege, National Center forAIand Innovation Policy (Rwanda) Dr. Fahad Albalawi, Saudi Authority for Data and Artificial Intelligence (Kingdom of Saudi Arabia) Denise Wong, Data Innovation and Protection Group, Infocomm Media Development Authority (IMDA) (Singapore) Dr. Nuria Oliver, ELLIS Alicante (Spain) Dr. Christian Busch, Federal Department of Economic Affairs, Education and Research (Switzerland) Oleksii Molchanovskyi, Expert Committee on the Development of Artificial intelligence in Ukraine (Ukraine) Marwan Alserkal, Ministry of Cabinet Affairs, Prime Minister's Office (United Arab Emirates) Saif M. Khan, U.S. Department of Commerce (United States) Dame Angela McLean, Government Chief Scientific Adviser (United Kingdom) Amandeep Gill,UNTech Envoy (United Nations) Soren Mindermann, Mila - QuebecAIInstitute Daniel Privitera(lead writer), KIRA Center Tamay Besiroglu, EpochAI Rishi Bommasani, Stanford University Stephen Casper, Massachusetts Institute of Technology Yejin Choi, University of Washington/A12 Danielle Goldfarb, Mila - QuebecAIInstitute Hoda Heidari, Carnegie Mellon University Leila Khalatbari, Hong Kong University of Science and Technology Shayne Longpre, Massachusetts Institute of Technology Vasilios Mavroudis, Alan Turing Institute Mantas Mazeika, University of Illinois at Urbana-Champaign Kwan Yee Ng, ConcordiaAI Chinasa T. Okolo, Ph.D, The Brookings Institution Deborah Raji, Mozilla Theodora Skeadas, Humane Intelligence Florian Tramer, ETH Zurich Bayo Adekanmbi, Data Science Nigeria Paul Christiano, contributed as a Senior Adviser prior to taking up his role at the USAISafety Institute David Dalrymple, Advanced Research + Invention Agency (ARIA) Thomas G. Dietterich, Oregon State University Edward Felten, Princeton University Pascale Fung, Hong Kong University of Science and Technology, contributed as a Senior Adviser prior to taking up her role at Meta Pierre-Olivier Gourinchas, International Monetary Fund (IMF) Nick Jennings CB FREng FRS, University of Loughborough Andreas Krause, ETH Zurich Percy Liang, Stanford University Teresa Ludermir, Federal University of Pernambuco Vidushi Marda, REAL ML Helen Margetts OBE FBA, University of Oxford/Alan Turing Institute John A. McDermid OBE FREng, University of York Arvind Narayanan, Princeton University Alondra Nelson, Institute for Advanced Study Alice Oh, KAIST School of Computing Gopal Ramchurn, RAI UK/UKRI TAS Hub/University of Southampton Stuart Russell, University of California, Berkeley Marietje Schaake, Stanford University Dawn Song, University of California, Berkeley Alvaro Soto, Pontificia Universidad Catolica de Chile Lee Tiedrich, Duke University Gael Varoquaux, The National Institute for Research in Digital Science and Technology (Inria) Andrew Yao, Institute for Interdisciplinary Information Sciences, Tsinghua University Ya-Qin Zhang, Tsinghua University UK Government Secretariathosted by theAISafety Institute Benjamin Prud'homme, Mila - QuebecAIInstitute The Secretariat appreciate the helpful support, comments, and feedback from the following UK-based organisations: Ada Lovelace Institute, The Alan Turing Institute, The Centre for Long-Term Resilience, Centre for the Governance ofAI, and UKAISafety Institute. Also a special thanks to Dan Hendrycks, Dylan Hadfield-Menell, and Pamela Samuelson. I am honoured to be chairing the delivery of the inaugural International Scientific Report on AdvancedAISafety. I am proud to publish this interim report which is the culmination of huge efforts by many experts over the 6 months since the work was commissioned at the Bletchley ParkAISafety Summit in November 2023. We know that advancedAIis developing very rapidly, and that there is considerable uncertainty over how these advancedAIsystems might affect how we live and work in the future.AIhas tremendous potential to change our lives for the better, but it also poses risks of harm. That is why having this thorough analysis of the available scientific literature and expert opinion is essential. The more we know, the better equipped we are to shape our collective destiny. Our mission is clear: to drive a shared, science-based, up-to-date understanding of the safety of advancedAI, and to continue to develop that understanding over time. The report rightly highlights that there are areas of consensus among experts and also disagreements over the capabilities and risks of advancedAI, especially those expected to be developed in the future. In order to meet our mission effectively, we have aimed to address disagreement amongst the expert community with intellectual honesty. By dissecting these differences, we pave the way for informed policy-making and stimulate the research needed to help clear the fog and mitigate risks. I am grateful to our international Expert Advisory Panel for their invaluable comments, initially shaping the report's scope and later providing feedback on the full draft. Their diverse perspectives and careful review have broadened and strengthened this interim report. Equally deserving of recognition are my dedicated team of writers and senior advisers. Their commitment over the past few months has created an interim product that has surpassed my expectations. My thanks also go to the UK Government for starting this process and offering outstanding operational support. It was also important for me that the UK Government agreed that the scientists writing this report should have complete independence. This interim report is only the beginning of a journey. There are no doubt perspectives and evidence that this report has failed to capture in this first attempt. In a scientific process such as this, feedback is precious. We will incorporate additional evidence and scientific viewpoints as we work toward the final version. Professor Yoshua Bengio,Universite de Montreal / Mila - QuebecAIInstitute & Chair I am delighted to present this interim update on the first International Scientific Report on the Safety of AdvancedAI, a key outcome of the groundbreakingAISafety Summit held at Bletchley Park in November 2023. This landmark report represents an unprecedented global effort to build a shared, science-based understanding of the opportunities and risks posed by rapid advancements inAI, and is a testament to the 'Bletchley Effect' - the power of convening brilliant minds to tackle one of humanity's greatest challenges. We believe that realising the immense potential ofAIto benefit humanity will require proactive efforts to ensure these powerful technologies are developed and deployed safely and responsibly. No one country can tackle this challenge alone. That is why I was so passionate about bringing together a diverse group of world-leading experts to contribute their knowledge and perspectives. I want to especially thank Professor Yoshua Bengio for his leadership as Chair in skilfully shepherding this complex international effort. Crucially the report also shines a light on the significant gaps in our current knowledge and the key uncertainties and debates that urgently require further research and discussion. It is my sincere hope that this report, and the cooperative process behind it, can serve as a catalyst for the research and policy efforts needed to close critical knowledge gaps and a valuable input for the challenging policy choices that lie ahead. We still have much to learn, but this report marks an important start. The UK looks forward to continuing to work with international partners to promote a responsible, human-centric approach toAIdevelopment - one that harnesses these powerful tools to improve lives and livelihoods while vigilantly safeguarding against downside risks and harms. Together, we can work to build a future in which all of humanity can benefit from the wonders ofAI. The Rt Hon Michelle Donelan MP,Secretary of State, Department for Science, Innovation, and Technology The rapid advancement ofAIstands poised to reshape our world in ways both profound and unforeseen. From revolutionising healthcare and transportation to automating complex tasks and unlocking scientific breakthroughs,AI's potential for positive impact is undeniable. However, alongside these notable possibilities lie significant challenges that necessitate a forward-looking approach. Concerns range from unintended biases embedded in algorithms to the possibility of autonomous systems exceeding human control. These potential risks highlight the urgent need for a global conversation to ensure the safe, and responsible advancement ofAI. In this context, the InternationalAISafety Report will provide vital groundwork for global collaboration. The report represents a convergence of knowledge from experts across 30 countries, the European Union, and the United Nations, providing a comprehensive analysis ofAIsafety. By focusing on the early scientific understanding of capabilities and risks from general purposeAIand evaluating technical methods for assessing and mitigating them, the report will spark ongoing dialogue and collaboration among multi-stakeholders. I hope that based on this report, experts from 30 countries, theEU, and theUNcontinue to engage in balanced discussions, achievingAIrisk mitigation that is acceptable and tailored to the specific context of both developed and developing countries, thereby creating a future where innovation and responsibleAIcoexist harmoniously. Lee Jong-Ho,Minister of MSIT, Republic of Korea This is the interim publication of the first 'International Scientific Report on the Safety of AdvancedAI'. A diverse group of 75 artificial intelligence (AI) experts contributed to this report, including an international Expert Advisory Panel nominated by 30 countries, the European Union (EU), and the United Nations (UN). Led by the Chair of this report, the independent experts writing this report collectively had full discretion over its content. At a time of unprecedented progress inAIdevelopment, this first publication restricts its focus to a type ofAIthat has advanced particularly rapidly in recent years: General-purposeAI, orAIthat can perform a wide variety of tasks. Amid rapid advancements, research on general-purposeAIis currently in a time of scientific discovery and is not yet settled science. People around the world will only be able to enjoy general-purposeAI's many potential benefits safely if its risks are appropriately managed. This report focuses on identifying these risks and evaluating technical methods for assessing and mitigating them. It does not aim to comprehensively assess all possible societal impacts of general-purposeAI, including its many potential benefits. For the first time in history, this interim report brought together experts nominated by 30 countries, theEU, and theUN, and other world-leading experts, to provide a shared scientific, evidence-based foundation for discussions and decisions about general-purposeAIsafety. We continue to disagree on several questions, minor and major, around general-purposeAIcapabilities, risks, and risk mitigations. But we consider this project essential for improving our collective understanding of this technology and its potential risks, and for moving closer towards consensus and effective risk mitigation to ensure people can experience the potential benefits of general-purposeAIsafely. The stakes are high. We look forward to continuing this effort. If properly governed, general-purposeAIcan be applied to advance the public interest, potentially leading to enhanced wellbeing, more prosperity, and new scientific discoveries. However, malfunctioning or maliciously used general-purposeAIcan also cause harm, for instance through biased decisions in high-stakes settings or through scams, fake media, or privacy violations. As general-purposeAIcapabilities continue to advance, risks such as large-scale labour market impacts,AI-enabled hacking or biological attacks, and society losing control over general-purposeAIcould emerge, although the likelihood of these scenarios is debated among researchers. Different views on these risks often stem from differing expectations about the steps society will take to limit them, the effectiveness of those steps, and how rapidly general-purposeAIcapabilities will be advanced. There is considerable uncertainty about the rate of future progress in general-purposeAIcapabilities. Some experts think a slowdown of progress is by far most likely, while other experts think that extremely rapid progress is possible or likely. There are various technical methods to assess and reduce risks from general-purposeAIthat developers can employ and regulators can require, but they all have limitations. For example, current techniques for explaining why general-purposeAImodels produce any given output are severely limited. The future of general-purposeAItechnology is uncertain, with a wide range of trajectories appearing possible even in the near future, including both very positive and very negative outcomes. But nothing about the future ofAIis inevitable. It will be the decisions of societies and governments that will determine the future ofAI. This interim report aims to facilitate constructive discussion about these decisions. The capabilities of systems usingAIhave been advancing rapidly. This has highlighted the many opportunities thatAIcreates for business, research, government, and private life. It has also led to an increased awareness of current harms and potential future risks associated with advancedAI. The purpose of the International Scientific Report on the Safety of AdvancedAIis to take a step towards a shared international understanding ofAIrisks and how they can be mitigated. This first interim publication of the report restricts its focus to a type ofAIwhose capabilities have advanced particularly rapidly: general-purposeAI, orAIthat can perform a wide variety of tasks. Amid rapid advancements, research on general-purposeAIis currently in a time of scientific discovery and is not yet settled science. The report provides a snapshot of the current scientific understanding of general-purposeAIand its risks. This includes identifying areas of scientific consensus and areas where there are different views or open research questions. People around the world will only be able to enjoy the potential benefits of general-purposeAIsafely if its risks are appropriately managed. This report focuses on identifying risks from general-purposeAIand evaluating technical methods for assessing and mitigating them, including the beneficial use of general-purposeAIto mitigate risks. It does not aim to comprehensively assess all possible societal impacts of general-purposeAI, including what benefits it may offer. According to many metrics, general-purposeAIcapabilities are progressing rapidly. Five years ago, the leading general-purposeAIlanguage models could rarely produce a coherent paragraph of text. Today, some general-purposeAImodels can engage in multi-turn conversations on a wide range of topics, write short computer programs, or generate videos from a description. However, the capabilities of general-purposeAIare difficult to estimate reliably and define precisely. The pace of general-purposeAIadvancement depends on both the rate of technological advancements and the regulatory environment. This report focuses on the technological aspects and does not provide a discussion of how regulatory efforts might affect the speed of development and deployment of general-purposeAI. AIdevelopers have rapidly advanced general-purposeAIcapabilities in recent years mostly by continuously increasing resources used for training new models (a trend called 'scaling') and refining existing algorithms. For example, state-of-the-artAImodels have seen annual increases of approximately 4x in computational resources ('compute') used for training, 2.5x in training dataset size, and 1.5-3x in algorithmic efficiency (performance relative to compute). Whether 'scaling' has resulted in progress on fundamental challenges such as causal reasoning is debated among researchers. The pace of future progress in general-purposeAIcapabilities has substantial implications for managing emerging risks, but experts disagree on what to expect even in the near future. Experts variously support the possibility of general-purposeAIcapabilities advancing slowly, rapidly, or extremely rapidly. This disagreement involves a key question: will continued 'scaling' of resources and refining existing techniques be sufficient to yield rapid progress and solve issues such as reliability and factual accuracy, or are new research breakthroughs required to substantially advance general-purposeAIabilities? Several leading companies that develop general-purposeAIare betting on 'scaling' to continue leading to performance improvements. If recent trends continue, by the end of 2026 some general-purposeAImodels will be trained using 40x to 100x more compute than the most compute-intensive models published in 2023, combined with training methods that use this compute 3x to 20x more efficiently. However, there are potential bottlenecks to further increasing both data and compute, including the availability of data,AIchips, capital expenditure, and local energy capacity. Companies developing general-purposeAIare working to navigate these potential bottlenecks. Approaches to managing risks from general-purposeAIoften rest on the assumption thatAIdevelopers and policymakers can assess the capabilities and potential impacts of general-purposeAImodels and systems. But while technical methods can help with assessment, all existing methods have limitations and cannot provide strong assurances against most harms related to general-purposeAI. Overall, the scientific understanding of the inner workings, capabilities, and societal impacts of general-purposeAIis very limited, and there is broad expert agreement that it should be a priority to improve our understanding of general-purposeAI. Some of the key challenges include: This report classifies general-purposeAIrisks into 3 categories: malicious use risks, risks from malfunctions, and systemic risks. It also discusses several cross-cutting factors that contribute to many risks. Like all powerful technologies, general-purposeAIsystems can be used maliciously to cause harm. Possible types of malicious use range from relatively well-evidenced ones, such as scams enabled by general-purposeAI, to ones that some experts believe might occur in the coming years, such as malicious use of scientific capabilities of general-purposeAI. Even when users have no intention to cause harm, serious risks can arise due to the malfunctioning of general-purposeAI. Such malfunctions can have several possible causes and consequences: The widespread development and adoption of general-purposeAItechnology poses several systemic risks, ranging from potential labour market impacts to privacy risks and environmental effects: Underpinning the risks associated with general-purposeAIare several cross-cutting risk factors - characteristics of general-purposeAIthat increase the probability or severity of not one but several risks: While this report does not discuss policy interventions for mitigating risks from general-purposeAI, it does discuss technical risk mitigation methods on which researchers are making progress. Despite this progress, current methods have not reliably prevented even overtly harmful general-purposeAIoutputs in real-world contexts. Several technical approaches are used to assess and mitigate risks: The future of general-purposeAIis uncertain, with a wide range of trajectories appearing possible even in the near future, including both very positive and very negative outcomes. But nothing about the future of general-purposeAIis inevitable. How general-purposeAIgets developed and by whom, which problems it gets designed to solve, whether societies will be able to reap general-purposeAI's full economic potential, who benefits from it, the types of risks we expose ourselves to, and how much we invest into research to mitigate risks -- these and many other questions depend on the choices that societies and governments make today and in the future to shape the development of general-purposeAI. To help facilitate constructive discussion about these decisions, this report provides an overview of the current state of scientific research and discussion on managing the risks of general-purposeAI. The stakes are high. We look forward to continuing this effort. We are in the midst of a technological revolution that will fundamentally alter the way we live, work, and relate to one another. Artificial Intelligence (AI) promises to transform many aspects of our society and economy. There is broad scientific consensus that the capabilities ofAIsystems have progressed rapidly on many tasks in the last 5 years. Large Language Models (LLMs) are a particularly salient example. In 2019, GPT-2, then the most advancedLLM, could not reliably produce a coherent paragraph of text and could not always count to 10. At the time of writing, the most powerfulLLMslike Claude 3, GPT-4, and Gemini Ultra can engage consistently in multi-turn conversations, write short computer programs, translate between multiple languages, score highly on university entrance exams, and summarise long documents. This step-change in capabilities, and the potential for continued progress, could help advance the public interest in many ways. Among the most promising prospects areAI's potential for education, medical applications, research advances in a wide range of fields, and increased innovation leading to increased prosperity. This rapid progress has also increased awareness of the current harms and potential future risks associated with the most capable types ofAI. To begin forging a shared international understanding of the risks of advancedAI, government representatives and leaders from academia, business, and civil society convened in Bletchley Park in the United Kingdom in November 2023 for the first internationalAISafety Summit. At the Summit, the nations present, as well as theEUand theUN, agreed to support the development of an International Scientific Report on AdvancedAISafety. This report aims to contribute to an internationally shared scientific understanding of advancedAIsafety. This is the first interim publication of that report: the final version of the first report will be published ahead of the FranceAISummit. An international group of 75AIexperts across a breadth of views and, where relevant, a diversity of backgrounds contributed to this interim report. The evidence considered for the report includes relevant scientific, technical, and socio-economic evidence. Since the field ofAIis developing at pace, not all sources used for this report are peer-reviewed. However, the report is committed to citing only high-quality sources. Criteria for a source being of high quality include: Because a scientific consensus on the risks from advancedAIis still being forged, in many cases the report does not put forward confident views. Rather, it offers a snapshot of the current state of scientific understanding and consensus, or lack thereof. Where there are gaps in the literature, the report identifies them, in the hope that this will be a spur to further research. Further, this report does not comment on what policy options are appropriate responses to the risks it discusses. Ultimately, policymakers must choose how to balance the opportunities and risks that advancedAIposes. Policymakers must also judge the appropriate level of prudence and caution to display in response to risks that remain ambiguous. Artificial Intelligence (AI) refers to advanced machine-based systems developed with broadly applicable methodologies to achieve given goals or answer given questions.AIis a broad and quickly evolving field of study, and there are many different kinds ofAI. This interim report does not address all potential risks from all types of advancedAI. This first iteration of the report focuses on general-purposeAI, orAIthat can perform a wide range of tasks. General-purposeAIsystems, now known to many through applications like ChatGPT, have generated unprecedented interest inAIboth among the public and policymakers in the last 18 months. Its capabilities have been improving particularly rapidly. General-purposeAIis different from so-called 'narrowAI', a kind ofAIthat is specialised to perform one specific task or a few very similar tasks. To better understand how we define general-purposeAIfor this report, making a distinction between 'AImodels' and 'AIsystems' is useful.AImodels can be thought of as the raw, mathematical essence that is often the 'engine' ofAIapplications. AnAIsystem is an ensemble of several components, including one or moreAImodels, that is designed to be particularly useful to humans in some way. For example, the ChatGPT app is anAIsystem. Its core engine, GPT-4, is anAImodel. This report covers risks fromAImodels andAIsystems if they are 'general-purpose'AImodels or systems. We consider anAImodel to be general-purpose if it can perform, or can be adapted to perform, a wide variety of tasks. We consider anAIsystem to be general-purpose if it is based on a general-purpose model, but also if it is based on a specialised model that was derived from a general-purpose model. Within the domain of general-purposeAI, this report focuses on general-purposeAIthat is at least as capable as today's most advanced general-purposeAIsuch as GPT-4 Turbo, Claude 3 and Gemini Ultra. In our definition, a model or system does not need to have multiple modalities, like speech, text, and image, to be considered general-purpose. Instead,AIthat can perform a wide variety of tasks within specific domains, like structural biology, also counts as general-purpose in our definition. Importantly, general-purposeAIis not to be confused with 'Artificial General Intelligence' (AGI), a term sometimes used to refer to a potential futureAIsystem that equals or surpasses human performance on all or almost all cognitive tasks. General-purposeAIis a much weaker concept. This report does not address risks from 'narrowAI', which is trained to perform a very limited task and captures a correspondingly very limited body of knowledge. The limited timeframe for writing this interim report has led to this focus on advanced general-purposeAI, where progress has been most rapid, and the associated risks are less studied and understood. NarrowAI, however, can also be highly relevant from a risk and safety perspective, and evidence relating to the risks of these systems is used across the report. NarrowAImodels and systems are used in a vast range of products and services in fields like medicine, advertising, or banking, and can pose significant risks in many of them. These risks can lead to harms like biased hiring decisions, car crashes, or harmful medical treatment recommendations. NarrowAIalso gets used in various military applications. One application, though a very small subset of the application ofAIto militaries,[reference 1]involves, for instance, Lethal Autonomous Weapon Systems (LAWS). Such topics are covered in other fora and are outside the scope of this interim report. A large and diverse group of leading international experts contributed to this report, including representatives nominated by 30 nations from allUNRegional Groups, and theEUand theUN. While our individual views sometimes differ, we share the conviction that constructive scientific and public discourse onAIis necessary for people around the world to reap the benefits of this technology safely. We hope that this interim report can contribute to that discourse and be a foundation for future reports that will gradually improve our shared understanding of the capabilities and risks of advancedAI. The report is organised into 6 main sections. After this introduction,2. Capabilitiesprovides information on the current capabilities of general-purposeAI, underlying principles, and potential future trends.3. Methodologyto assess and understand general-purposeAIsystems explains how researchers try to understand what general-purposeAIcan do and what risks it might pose.4. Risksdiscusses specific risks and cross-cutting risk factors.5. Technical approaches to mitigate riskspresents technical approaches to mitigating risk from general-purposeAIand evaluates their strengths and limitations.6. Conclusionsummarises and concludes. General-purposeAImodels rely on deep learning[reference 15], or the training of artificial neural networks, which areAImodels composed of multiple layers of interconnected nodes, loosely inspired by the structure of biological neural networks brains. Most state-of-the-art general-purposeAImodels are based on the 'Transformer' neural network architecture[reference 16], which has proven particularly efficient at converting increasingly large amounts of training data and computational power into better model performance. General-purposeAImodels are, broadly speaking, developed and deployed following the same series of distinct stages: pre-training, fine-tuning, system integration, deployment, and post-deployment updates. Each requires different methods and resources. Both pre-training and fine-tuning are ways of 'training' a general-purposeAImodel. During training, a general-purposeAImodel is given some data, which it processes to predict some other data. For example, the model might be given the first 500 words of a Wikipedia article and then predict the 501st word. Initially, it predicts randomly, but as it sees more data it is automatically adapted to learn from its mistakes, and its predictions improve. Each prediction requires some amount of computational resources ('compute'), and so training requires both data and compute. The model architecture, designed by the developers, dictates the broad types of calculations that occur when the model makes a prediction, and the exact numbers used in those calculations are adjusted during training. The goal of pre-training is to build general background knowledge into a general-purposeAImodel. During pre-training, general-purposeAImodels typically learn from patterns in large amounts of data (usually taken from the Internet). Collecting and preparing pre-training data are large-scale operations, and in most cases, pre-training is the most computationally intensive stage of development. The pre-training of general-purposeAImodels today takes weeks or months and uses thousands of Graphics Processing Units (GPUs) - specialised computer chips, designed to rapidly process complex parallelised calculations. For example, the Falcon-180B model used 4,096GPUsfor multiple months, and PaLM (540B) used 6,144 chips for 50 days[reference 13]. Today, this process uses roughly 10 billion times more compute compared to state-of-the-art model training in 2010[reference 17]. Some developers conduct pre-training with their own compute, while others use resources provided by specialised cloud compute providers. After pre-training, most general-purposeAImodels undergo one or more additional fine-tuning stages, to refine their ability to accomplish the intended tasks. Fine-tuning can include various techniques including learning from desirable examples[reference 18], pairs of desirable and undesirable examples[reference 19], or rewards and penalties[references 20, 21*]. Fine-tuning usually requires significant human involvement, and tends to be the most labour-intensive part of training, with millions of instances of human feedback needed to fine-tune modern models[reference 22*]. Often, this feedback is provided by thousands of contracted knowledge workers. After a model is trained, it can be used to build a general-purposeAIsystem by integrating it with other system components aimed at enhancing both capabilities and safety. In practice, general-purposeAImodels are typically integrated with user interfaces, input pre-processors, output postprocessors and content filters. Once they are trained, models can be deployed for use. Deployment can be 'internal', where a system is only used by the developers, or 'external', allowing the public or other non-developer entities to use it. External deployments can be 'closed-source' or 'open-source'. Closed-source means that the public can only use the system through a limited interface. Open-source means that the entire system, including all of the model parameters, are made available. Some state-of-the-art general-purposeAIsystems, such as GPT-4[reference 2*], are closed source, while others like Llama-3[reference 6*]are open source. From a risk mitigation perspective, there are advantages and disadvantages of open-source models which are the subject of ongoing discussions in the scientific community. This interim report does not provide a detailed discussion of the advantages and disadvantages of open-source models. Many general-purposeAIsystems are continually updated after deployment. This lets developers update capabilities and try to address flaws and vulnerabilities as they are discovered. These changes often amount to a type of 'cat-and-mouse' game where developers continually update high-profile systems in response to newly discovered vulnerabilities[reference 22*]. This section focuses on the capabilities of general-purposeAImodels and systems categorised by modality (such as video and language) and by skill (such as reasoning and knowledge). Capabilities can also be categorised by performance on specific benchmarks (see3. Methodology to assess and understand general-purposeAIsystems). While this section covers capabilities generally,4.4.1. Cross-cutting technical risk factorsfocuses on 'high-risk' capabilities. Although general-purposeAIsystems are often described in terms of their capabilities, there is no widely-accepted definition of the term 'capability' in the field ofAI. Part of the difficulty of defining a capability is that it is not directly observed -AIresearchers can only observe anAIsystem's behaviour: the set of outputs or actions that a system actually produces and the context in which it does so (for example, the prompt that leads to the observed behaviour)[reference 23].AIresearchers can merely summarise the observed system behaviour in many contexts, and thus arrive at an impression of what the system is capable of - the capability. It is difficult to define and measure the full capabilities of a new general-purposeAImodel, even after the model is built; researchers and users have often discovered new ways to elicit capabilities after a model is deployed, for example through prompting a model to 'think step-by-step'[references 24, 25]. Another complication in defining a general-purposeAIsystem's capabilities is that they are shaped by the affordances in its environment - the tools and resources it can access. For instance, when a general-purposeAIsystem is connected to the internet and equipped with a web browser, it gains new affordances for retrieving information and interacting with the real world, effectively expanding its capabilities[reference 26]. General-purposeAImodels can be categorised by the modalities they process (e.g. text, images, video) as input and generate as output. General-purposeAImodels exist for 10+ modalities[reference 27]such as time series[reference 28*]and music[reference 29*], but text-processing models are the source of much of the present attention on general-purposeAImodels. Advanced general-purposeAImodels are increasingly able to process and generate text, images, video, audio, robotic actions, and proteins and large molecules: To assess general-purposeAIcapabilities fully, it can be helpful to categorise them by well-known skills such as displaying knowledge, reasoning, and creativity. Compared to categorising by modality, skills are harder to precisely define, but provide a more intuitive lens into general-purposeAIcapabilities. Viewed through this lens of skills, today's most capable general-purposeAIsystems show partial proficiency but are not perfectly reliable. Experts often disagree on whether current general-purposeAIsystems can be said to have a specific skill or not. One way to look at this is through capability-limitation pairs. Increased investment in computing resources, enhancements in hardware efficiency, the existence of readily accessible datasets online, and incremental innovations in algorithms have contributed to advances in general-purposeAIover the last decade. This section examines recent trends in computing power, data, and algorithms. Computing resources used for trainingAImodels have been increasing fast. Computing resources, often referred to as 'compute', represent the number of operations performed. This has grown exponentially since the early 2010s, with the average amount used to train machine learning models doubling approximately every 6 months[reference 17]. In 2010, notable machine learning models[references 62, 63, 64]used an average of approximately 1e15 floating-point operations (FLOP)[reference 65], but by 2023 Inflection-2, the largest model with a publicly reported compute budget, used 1e25 FLOP[reference 66*]- a 10 billion-fold increase. This progress is driven by industry labs' willingness to use more data centre capacity for large-scale general-purposeAItraining. There is insufficient data to determine if this trend is changing over a shorter period such as the 2020s. Figure 1. Training compute of notable machine learning models over time[references 17, 65]. Computation is measured in total floating-point operations (FLOP) estimated fromAIliterature. Estimates are expected to be accurate within a factor of 2, or a factor of 5 for recent undisclosed models like GPT-4. Reproduced with the kind permission of EpochAIfrom 'Parameter, Compute and Data Trends in Machine Learning'. Published online at epochai.org. Retrieved from: 'https://epochai.org/data/epochdb/visualization'. See an accessible version of figure 1 Over the last 15 years, the amount of compute per dollar has increased between around 50- to 200-fold[references 67, 68]. However, the total amount of compute used for training general-purposeAImodels far outpaced the reduction in computing costs: for example, Google's Word2vec model was trained using around 3e16 FLOP in 2013, around a billion-fold smaller than current frontier models[reference 65]. WhileGPUperformance improvements have helped, these have been partially limited by data centreGPUshortages and high prices for top-tierGPUsused inAIapplications. Supply chain shortages of high-end processors, packaging, high-bandwidth memory, and other components are delaying the technology sector's ability to meet the enormous demand for artificial intelligence hardware likeAIservers[reference 69]. The expansion in general-purposeAIcompute usage is mainly the result of industry labs being increasingly willing to allocate data centre resources and engineering staff to large-scale general-purposeAItraining runs. The discovery of neural 'scaling laws', which describe predictable relationships between the amount of compute, the size of the model and data, and performance, has contributed to a compute-centric view ofAIdevelopment that is prominent at some leadingAIlabs. The development of flagship general-purposeAImodels such as Google Gemini Ultra and OpenAI's GPT-4 was guided by work on scaling laws[references 2, 3]. As a result, there is a greater need for hardware infrastructure expertise, and there are tighter collaborations betweenAIlabs and technology giants such as Microsoft and Google. Computational resources for deployment have also seen significant growth. Companies are rapidly expanding infrastructure to meet these growing demands. The computational resources required for inference (a key part of serving general-purposeAIsystems to users) have experienced significant growth[reference 76]because the number of users for deployed general-purposeAIsystems has increased rapidly. In April 2023, OpenAI'sAIsystems were reportedly estimated to incur $700k/day in inference costs[reference 77]. Some estimates indicate that the total computation spent on general-purposeAIinference already exceeds that devoted to training new models for example,AIinference represented 60% of Google'sAIinfrastructure emissions as of 2022[reference 78]. Growing compute resources both for training and inference have also rapidly expandedAI's energy usage (see4.3.4 Risks to the environment). General-purposeAIdevelopers have been able to significantly increase training dataset sizes thanks to the availability of content from the internet including open repositories of web data. These larger datasets contribute to higher performance on a wide range of metrics. Dataset sizes for training general-purposeAIhave increased from around 2 billion tokens (a token is a word, a character, or sometimes part of a word) for the original Transformer model in 2017 to over 3 trillion tokens in 2023[references 79, 80], growing approximately 10x every 3 years[reference 65]. However, general-purposeAIdevelopers only have a limited amount of text data available on the internet to draw on[references 81, 82]. While this could be overcome, for example by training on the same data many times, usingAI-generated data, or training on other non-text data sources like YouTube videos, some believe that by 2030 shortages of accessible online high-quality text data could slow the rate at which models can be productively scaled (see2.4.2 Will resources be scaled rapidly?). Data quality plays a critical role in training high-performing language models. Selecting high-quality data and optimising the overall composition of the dataset can significantly improve model performance, but this process is labour-intensive[references 83, 84, 85]. Moreover, measuring and analysing data to identify and mitigate problematic artefacts, such as biases and lack of diversity, is essential for producing high-quality models[reference 86*]. Training general-purposeAImodels on diverse modalities like images, audio, and video alongside text, has recently gained traction. General-purposeAImodels such as GPT-4, Claude 3, and Gemini Ultra combine different modalities to perform tasks requiring joint processing of textual, visual, and auditory information, such as analysing documents with text and graphics or creating multimedia presentations[references 2, 3, 4*]. 'Human preference' data captures the types of outputs users prefer and has become crucial for developing general-purposeAIsystems. This data cannot be mined from publicly available sources, but must be produced specifically for training; as such it is more expensive than the text data used for pre-training. This data helps fine-tune language models to conform with user and developer needs, adapt to diverse preferences, and ground the models in human judgments of quality and helpfulness[references 20, 21, 87].AIlabs and large companies may have an advantage in producing and accessing large quantities of proprietary human preference data. The techniques and training methods underpinning the most capable general-purposeAImodels have consistently and reliably improved over time[references 88*, 89]. The efficiency ofAItechniques and training methods has been increasing 10x approximately every 2 to 5 years in key domains such as image classification, game-playing, and language modelling. For example, the amount of compute required to train a model to perform image classification to achieve a set level of performance decreased by 44x between 2012 and 2019, meaning that efficiency doubled every 16 months. Game-playingAIsystems require half as many training examples every 5 to 20 months[reference 90]. In language modelling, the compute required to reach a fixed performance level has halved approximately every 8 months on average since 2012[reference 89]. These advances have enabled general-purposeAIresearchers and labs to develop more capable models over time within a limited hardware budget. There have also been incremental advances in algorithms that are not best understood as increasing compute efficiency. For example, new techniques have significantly increased the size of context windows, allowing general-purposeAIsystems to process larger quantities of information[references 31, 91, 92*], and post-training algorithms allow general-purposeAIsystems to use tools and take actions in the world without human assistance (see2.4.3. Will algorithmic progress lead to rapid progress?). Despite significant advancements inAIalgorithms, general-purposeAIhas seen relatively few major conceptual breakthroughs in recent years. The 'Transformer architecture' remains perhaps the most significant innovation, and is used by most advanced general-purposeAIsystems[reference 16]. While many alternative architectures have been proposed, none have yet substantially and consistently outperformed the Transformer. Recent 'selective state space models'[reference 93]might prove to be more efficient than the Transformer, once properly tested. These models reinforce the recent trend of allowing language models to analyse longer contexts, such as books and large software projects. If more fundamental conceptual breakthroughs are needed to advance general-purposeAIcapabilities, this could be a key barrier to further development even if incremental improvements and scaling continue to drive rapid progress in some areas (see2.4.1. If resources continue to be scaled rapidly, would this lead to rapid advancements?). The pace of recent general-purposeAIprogress has been rapid, often surpassing the expectations ofAIexperts on some metrics. Over the last decade,AIhas achieved or exceeded average human-level performance on some benchmarks in domains such as computer vision, speech recognition, image recognition, and natural language understanding (Figure 2). The latest advances inLLMsbuild upon this longer-running trend. Figure 2. Performance ofAImodels on various benchmarks from 1998 to 2024, including computer vision (MNIST, ImageNet), speech recognition (Switchboard), natural language understanding (SQuAD1.1,MMLU,GLUE), general language model evaluation (MMLU, Big-Bench, andGPQA), and mathematical reasoning (MATH). Many models surpass human-level performance (black solid line) by 2024, demonstrating significant advancements inAIcapabilities across different domains over the past 2 decades. Data are from[reference 94]for MNSIT, Switchboard, ImageNet,SQuAD1.1, 2 andGLUE. Data forMMLU, Big Bench,GPQAare from the relevant papers[reference 95, 96, 97]. LLMcapabilities have advanced significantly in multiple domains between 2020 and 2024, shown by broad benchmarks such as Massive Multitask Language Understanding (MMLU)[reference 95], Big-Bench[reference 96], and Graduate-Level Google-Proof Q&A (GPQA)[reference 97]. In 2020, general-purposeAImodels performed substantially worse than average human test subjects on many of these benchmarks; in 2024, advanced general-purposeAImodels have approached human-level performance. For example, consider the MATH benchmark[reference 98], which tests mathematical problem-solving skills. Initially general-purposeAIsystems performed weakly on this benchmark, but 2 years after its release, GPT-4 seemed to achieve 42.5% accuracy[reference 99*]and subsequent work pushed state-of-the-art performance using GPT-4 to 84.3%[reference 100], which is close to the score obtained by expert human testers. Despite rapid progress on benchmark metrics, these benchmarks are highly limited compared to real-world tasks, and experts debate whether these metrics effectively evaluate true generalisation and meaningful understanding[reference 101]. State-of-the-art general-purposeAImodels often exhibit unexpected weaknesses on some benchmarks, indicating that they partly or fully rely on memorising patterns rather than employing robust reasoning or abstract thinking[references 102, 103]. In some cases, models were accidentally trained on the benchmark solutions, leading to high benchmark performance despite the absence of the actual capability[references 104, 105]. Models also struggle to adapt to cultures that are less represented in the training data[reference 106]. This underscores the significant disparity between benchmark results and the capacity to reliably apply knowledge to practical, real-world scenarios. While it may be tempting to compare the cognitive capabilities of humans to the capabilities of general-purposeAIsystems, they have distinct strengths and weaknesses, making these comparisons less meaningful in many cases. While general-purposeAIexcels in some domains, it is arguably lacking the deep conceptual understanding and abstract reasoning capabilities of humans[reference 102]. Current general-purposeAIsystems often demonstrate uneven performance, excelling in some narrow domains while struggling in others[reference 102]. Current general-purposeAIsystems are prone to some failures that humans are not[reference 107, 108]. General-purposeAIreasoning can be 'brittle' (unable to cope with novel scenarios) and overly influenced by superficial similarities[reference 102].LLMscan fail at reasoning in contexts where humans typically excel. For example, a model trained on data including the statement: ""Olaf Scholz was the ninth Chancellor of Germany"" will not automatically be able to answer the question ""Who was the ninth Chancellor of Germany?""[reference 107]. In addition,LLMscan be exploited by nonsensical input to deviate from their usual safeguards, while humans would recognise these prompts (see5.2. Training more trustworthy models). The aggregate performance of language models has improved reliably and predictably with the scale of computing power and data. Researchers have discovered empirical 'scaling laws' that quantify the relationship between these inputs and the capabilities of the model on broad performance measures like next-word-prediction[references 109, 110]. Empirical studies across diverse domains have evidenced performance improvements in machine learning systems with increased computational resources, including in vision[references 111*, 112], language modelling[references 109, 110], and games[reference 113*]. Figure 3. Cross-entropy loss scales predictably with compute across a broad range of empirically-studied training runs.FLOPSrefers to the number of operations performed during training. Figure from[reference 110*]. Different colours represent models with different numbers of parameters. Each line shows how loss falls as training FLOP increases for a model. Permissions were sought from the author. The best-known scaling law predicts that, as language models are grown in size and are trained on more data, they improve by a predictable amount[references 109, 110]. Specifically, these models become more accurate at predicting the next 'token' in a sequence, which can be a word, a character, or a number. When this performance improves, the model is effectively getting better at the tasks implicit in the dataset. For example, general-purposeAImodel performance has been observed to consistently improve on broad benchmarks that test many capabilities, such asMMLU[reference 95], as the models are scaled up. Figure 4. Performance on broad benchmarks such as Big-Bench has been strongly associated with parameter scaling, and compute scaling more generally. This figure was taken from[reference 96]. These scaling laws are derived from empirical observations, not from inviolable principles, although theoretical models have been proposed to explain them[references 115, 116, 117, 118, 119]. As a result, there is no mathematical guarantee that they will continue to hold for scales beyond the range of the empirical data used to establish them. Aggregate performance across many tasks within comprehensive benchmarks can be partially predicted based on the model scale (see Figure 4) However, it is unclear whether we can currently reliably predict far in advance if and when specific capabilities will appear. There are many documented examples of capabilities that appear when models reach a certain scale, sometimes suddenly, without being explicitly programmed into the model[references 120, 121, 122*, 123]. For example, large language models at a certain scale have gained the ability to perform the addition of large numbers with high accuracy, when prompted to perform the calculation step-by-step. Some researchers sometimes define these as 'emergent' capabilities[references 120, 121, 122*, 123], indicating that they are present in larger models but not in smaller models and so their emergence may be hard to predict in advance. This suggests that new capabilities, including beneficial and potentially harmful ones, may emerge unexpectedly. It is debated whether these capabilities appear gradually or suddenly, and there is disagreement about how far in advance they can be predicted. Recent research has found that some such capabilities appear more gradually and predictably, if more linear and continuous metrics are used to measure progress[reference 124]. This has led some researchers to question that capabilities are 'emergent', since some definitions of emergence inAIrequire that abilities appear suddenly at a certain scale[reference 124]. This suggests that capabilities that currently seem to appear suddenly may turn out to be predictable further in advance if different progress metrics are used. It is an open question whether, and how far in advance, it will become possible to predict the new capabilities of new models. Recent research has identified examples of 'inverse scaling', where language model performance worsens as model size and training compute increase[reference 125]. For instance, when asked to complete a common phrase with a novel ending, larger models are more likely to fail and simply reproduce the memorised phrase instead[reference 125]. While model scaling sometimes leads to decreased performance, research on this phenomenon also finds random-seeming fluctuations in performance. Some apparent inverse scaling trends may not persist when extrapolating to much larger models, with performance eventually improving again[reference 126]. The full implications of inverse scaling remain unclear, and further research is likely needed to better understand this phenomenon. This section examines the feasibility and effectiveness of further scaling up compute and training data, and the potential for rapid advancements through algorithm development. Overall, both approaches are likely to add up and lead to further advancements, but no agreed methodology exists to predict the pace of advancements. For a discussion of global disparities inAIcapabilities, see4.3.2 GlobalAIdivide. A key driver of disagreements about future progress is how to interpret past progress. In recent years, scaling computational resources and data has led to consistent performance gains in general-purposeAIsystems as evidenced by higher scores on a range of benchmarks (seeAs general-purposeAImodels are scaled up, their capabilities improve overall, but to date, this growth has been hard to predict for specific capabilities). SomeAIresearchers judge that this past progress involved significant and meaningful advancements in the understanding and reasoning capabilities of general-purposeAIsystems and that, with substantially more compute and perhaps moderate conceptual innovations, continued progress of this kind could lead to the development of general-purposeAIsystems that perform at a broadly human level or beyond, for most cognitive tasks[reference 127]. Other researchers are sceptical. They argue that current general-purposeAIsystems, which are based on deep learning (the currently dominant approach to machine learning relying on deep artificial neural networks[reference 15]), fundamentally lack crucial components of intelligence. In particular, current deep learning systems are thought by some to lack causal reasoning abilities[reference 128], abstraction from limited data, common sense reasoning[references 102, 129, 130], and flexible predictive world models[references 55, 128, 130]. These researchers argue that these shortcomings cannot be simply resolved by scaling alone[references 102, 129, 130]. This point is supported by the significant limitations of current systems, which are discussed in2.2 What current general-purposeAIsystems are capable of. Addressing these limitations, they argue, may require significant conceptual breakthroughs and innovations beyond the current deep learning paradigm. This would suggest that achieving human-level performance in general-purposeAIsystems requires significant conceptual breakthroughs, and that the current type of progress, driven by incremental improvements, is insufficient to reach this goal. Fundamental conceptual breakthroughs resulting in significant leaps in general-purposeAIcapabilities are rare and unpredictable. Even if novel techniques were invented, existing general-purposeAIsystems' infrastructure and developer convention could provide barriers to applying them at scale. So,ifa major conceptual breakthrough is required, it could take many years to achieve. These opposing views are not necessarily incompatible: although current state-of-the-art deep learning systems broadly have weaker reasoning abilities than most humans, we are seeing progress from one generation of a general-purposeAImodel to the next, and manyAIresearchers are exploring ways to adapt general-purposeAImodels to unlock or improve 'system 2' reasoning (analytic, rule-based, and controlled reasoning) and 'autonomous agent' abilities. Progress on these capabilities, if it occurs, could have important implications forAIrisk management in the coming years. See4.4.1 Cross-cutting technical risk factorsfor a discussion of the potential for progress in and risk from autonomous agent capabilities. There is ongoing debate among experts about whether, and for how long, it will be possible to continue rapidly increasing the resources going intoAIdevelopment. Technology giants such as Google and Amazon are making substantial capital investments in data centres andGPUacquisitions to support further scaling up general-purposeAImodels. If this yields substantial improvements in the near future, the resulting capabilities may inspire market confidence and justify additional rounds of spending. Large technology companies have the cash reserves needed to scale the latest training runs by multiples of 100 to 1,000[reference 131*]. However, further scaling past this point may be more difficult to finance. In addition, access to capital investment is not the only potential bottleneck. Lack of data, energy, andGPUsare all potential barriers to further rapid scaling of resources that are discussed below. In addition, there are wide global disparities in the quality of digital infrastructure, posing additional barriers to many countries and contributing to a widening global divide inAIcapabilities (see4.3.2 GlobalAIdivide). While data availability may limit general-purposeAIscaling over the medium term, solutions like synthetic data and transfer learning could help to address this bottleneck. Current state-of-the-art general-purposeAImodels use text datasets with trillions of words. The rapid scaling up of training dataset size may soon be limited by the quantity of accessible online high-quality text data[references 81, 82]and by legal disputes about data access. Data availability bottlenecks for training large general-purposeAImodels are a recent challenge, and methods to overcome them are still in the early stages of exploration. A range of approaches for overcoming such challenges are available, such as: The increasing energy demands for training general purposeAIsystems could start to strain energy infrastructure. Globally, computation used forAIis projected to require at least 70 TWh of electricity in 2026[reference 139], roughly the amount consumed by smaller European countries such as Austria or Finland. This could divert energy from other purposes and have environmental costs[reference 140]. In the US, for example, where many leadingAIcompanies are currently located, electric grids and transmission infrastructures may struggle to accommodate the surge inAI-related electricity demand. Upgrading the electrical grid to transport more power from generation plants to data centres involves a lengthy process of planning, approval, and construction. This slow build-out process poses particular challenges for general-purposeAItraining facilities, which will likely require manyGPUsto be closely co-located geographically, resulting in an immense local power draw. KeyAIcompanies are responding by proactively seeking to secure their power supply. For example, a compute provider recently purchased a data centre with a 960MW energy supply[reference 141*]. This is enough to power a training run with around 100x more computational resources than was used to train GPT4[reference 2*], but more energy would be required to continue scaling at the current pace for more than a few years. Over the past few years, the production of data-centreGPUshas been a bottleneck in scaling up compute for general-purposeAIsystems, partly due to the limited capacity of semiconductor manufacturing plants and the constraints and priorities in the global semiconductor supply chain[references 142, 143].AIchip manufacturing depends on a complex, hard-to-scale supply chain, featuring sophisticated lithography, advanced packaging, specialised photoresists, and unique chemicals. Constructing new semiconductor fabrication plants ('fabs') is very expensive and typically takes 3 to 5 years[references 144, 145]making the industry slow to respond to market demand. These factors make projecting future supply complex, and a range of scenarios for chip availability have been proposed. While state-of-the-artGPUproduction is picking up substantially, supply chains forAIchips may not be able to adapt to demand. This may slow down the ambitions of frontierAIcompanies for further rapid growth, thoughAIcompanies might continue to scale in the near-term if they can acquire large fractions of the total number ofGPUsproduced. ImprovedGPUperformance has also contributed to the recent scaling of compute[references 67, 68]. It is possible that over this decade, progress in individualGPUperformance may slow due to physical limits on transistor size[references 146, 147]and energy efficiency[reference 148]. This would have only a limited impact on scaling because the primary driver of compute scaling has not been improvedGPUperformance, but the increase in the number ofGPUsused.GPUprice performance and energy efficiency for relevant computation have been improving roughly 30% annually, with an additional factor of 10x - 100x from the addition of tensor cores and the move to lower precision formats[references 67, 68]. However, the total compute used in training has increased by approximately 4x per year since 2010[reference 17], outpacing the rate of hardware efficiency improvements. This suggests that increased spending, rather than gains in hardware efficiency, has been the primary driver of the growth in compute budgets forAItraining. There is continued rapid growth in the efficiency of training algorithms. Techniques and algorithms underpinning general-purposeAImodels have consistently and robustly improved over time (see2.3.1 Recent trends in compute, data, and algorithms). This includes incremental changes to key algorithms such as the architecture of transformers, improvements to howAIalgorithms are implemented on hardware, a better understanding of how to scale models, better methods to process representations in a neural network, and other advances. These advances enable researchers and labs to produce ever-more capable general-purposeAImodels without increasing their hardware budget, for example in vision[reference 88*], reinforcement learning[reference 90]and language modelling[reference 89]. The rate of improving algorithmic efficiency in language modelling shows no sign of slowing down[reference 89], though there may be diminishing returns to progress in the near future. Post-training algorithms can be used to significantly improve general-purposeAImodel capabilities at low cost, including via fine-tuning, tool use, and structured reasoning techniques. Recent years have seen significant advancements in techniques and approaches for enhancing the performance of general-purposeAImodels after pre-training. Many post-training algorithms improved model performance on a given benchmark by more than using 5x the training compute, and in some cases more than 20x[reference 25]. Post-training algorithms can be applied to a given model for various use-cases with the aim of better serving end-users specific needs, at a much lower cost than developing the original model. This low cost means that a wide range of actors, including low-resource actors, could advance frontier general-purposeAIcapabilities by developing better post-training algorithms. Governance processes need to take into account post-training algorithms. Post-training algorithms include fine-tuning models for better performance, equipping them with the ability to leverage external tools, crafting prompts to guide their outputs, structuring their reasoning processes for more coherent and logical responses, and selecting from multiple responses the most relevant and accurate candidate outputs. There is a rapidly growing body of work on post-training enhancements, to improve frontierLLMsperformance broadly[reference 149], and in specific domains such as code generation[reference 150]and mathematics[reference 151]. Such innovations have the potential to further improve the performance of general-purposeAIsystems over the next few years. However, if there is a slowdown in the scaling of resources going to training, due to the bottlenecks discussed above, this might in turn slow progress in finding better post-training algorithms. General-purposeAIsystems might be deployed to automate and accelerateAIR&D. NarrowAIsystems have already been used to develop and improve algorithms[references 152, 153]. RecentLLMsare used in areas related toAIR&D, particularly in programming[reference 26], generating and optimising prompts[references 154, 155, 156, 157], replacing human fine-tuning data[reference 158*], and selecting high-quality training data[reference 159*]. As the capabilities of general-purposeAIsystems advance, it becomes harder to predict the effect on algorithmic progress and engineering inAI. Modern general-purposeAIsystems can contain multiple large-scale models and hundreds of billions of parameters, often deployed as general-purpose products. Consequently, it is difficult to anticipate how such general-purposeAIproducts may function in the many possible deployment scenarios, and even harder to appropriately characterise the downstream consequences of their deployment. Scientists are often surprised at the unexpected capabilities and impacts of general-purposeAIsystems. There are 2 broad reasons to assess general-purposeAImodels and systems: Various stakeholders (i.e.AIdevelopers, users, impacted population members, etc.) have expectations of how a general-purposeAIsystem should behave in terms of model capabilities and preventing negative downstream social impacts. Researchers have developed a variety of methods to compare model outcomes with these expectations[reference 160]. This model performance analysis is integral in understanding how a model performs and which limitations, benefits, or risks may arise in deployment. In many research papers, the evaluation of model capabilities is qualitative, relying on anecdotal demonstrations of model performance[reference 161]and human judgments. For instance, early evaluation of image-generating models often relied on simply demonstrating a small number of examples[references 162, 163]. When GPT-4 was newly released[reference 99*], examples of model outputs on a curated set of tasks, beyond conventional benchmarks, were used to illustrate model performance. Now, several popular benchmarks rely on asking human raters to rate the responses of different models against one another[references 164, 165]. Meanwhile, risks are sometimes gauged through 'uplift studies', where the aim is to test how much more competent a human is at accomplishing a potentially harmful task when they have access to a general-purposeAIsystem versus when they do not[reference 166*]. Most machine learning evaluations are completed on standardised benchmark measurements[reference 167]. For example, a relatively small set of image processing benchmarks involving classification[references 168, 169, 170, 171], segmentation[reference 36*, 172], and question answering[references 173, 174]have been integral toAIvision research. Similarly, research on language modelling has been shaped by several publicly available benchmarks that are meant to measure general capabilities[references 95, 96, 165, 175, 176, 177*, 178, 179]and trustworthiness[references 180, 181, 182]. More recently, benchmarks are being designed to measure the growing general-purposeAIcapabilities at combining information from multiple modalities and using software tools like web browsers[reference 183](see also4.4.1 Cross-cutting technical risk factors). Performance on benchmarks can be imperfect measures of downstream task performance. Benchmarks are inherently proxy measures for desired performance, and good scores on a benchmark do not always translate to desired performance in practice[reference 184]because of various validity challenges. Internal validity challenges relate to the reliability of the benchmark measurement, i.e. how reliable the reported metric is over repeated executions in comparison to a strong baseline. For instance, internal validity issues arise if the benchmark does not contain enough examples to make statistically valid claims about model performance[reference 185]or contains false labels[references 186, 187]. External validity refers to how well benchmark performance translates to real-world settings. Benchmarks themselves can be poorly constructed, inadequate, or incomplete representations of real-world tasks. For instance, limitations on how state-of-the-art models are prompted and combined with other systems can lead to their capabilities being underestimated[references 24, 188, 189]. Currently, benchmarks are often not explicit about their scope of applicability. Benchmarks that lay claim to 'general' performance often disguise biases in cultural representation and annotator differences, contested notions of ground truth, and more[references 101, 167, 190, 191, 192]. Also, as modern general-purposeAImodels have been trained on large amounts of internet data, it can be difficult to disentangle novel capabilities from memorised ones[references 193, 194]. Interpreting human performance on benchmarks can be difficult. A critical aspect of intuitively understanding model performance is a comparison with human performance[reference 195*]. Human performance measures are often unreliable - for instance, the baseline for 'human performance' on the ImageNet benchmark consists of the annotations of a single graduate student[reference 196]. Human annotators of 'ground truth' are notoriously fickle[reference 197], often disagreeing in meaningful ways[reference 198]due to differences in cultural context, values, or expertise. Furthermore, there is a meaningful difference between human performance and human competence at a task - the latter often involves, for example, judgments of robustness and not just accuracy[reference 199]. Evaluations which intentionally diversify their annotator populations[reference 200], allow for a multiplicity of ground truth labels[reference 198], or properly consider the context of human performance claims[reference 201]tend to be more trustworthy assessments of human-AIcomparisons. Before deploying systems in real-world conditions, evaluators use 'adversarial attacks and red-teaming' to identify worst-case behaviours, malicious use opportunities, and the system's potential to fail unexpectedly. In cybersecurity, an adversarial attack refers to a deliberate attempt to make a system fail. For example, attacks against language models can take the form of automatically generated attacks[references 202, 203, 204*, 205, 206]or manually generated attacks[references 204*, 207]. These can include 'jailbreaking' attacks which subvert the models' safety restrictions[references 208, 209, 210, 211, 212]. A 'red team' refers to a set of people who aim to find vulnerabilities in a system by attacking it. In contrast to benchmarks which are a fixed set of test cases, a key advantage of red-teaming is that it adapts the evaluation to the specific system being tested. Through interaction with a system, red-teamers can design custom tests for models. Researchers approach red teaming through various strategies and tools. Ojewale et al.[reference 213]map out an ecosystem of tools leveraged in theAIaccountability process including resources for 'harm discovery', such as 'bug bounty' platforms, incident databases[reference 214], and more. These tools support the identification of potential vectors of harm and enable broader participation in harm discovery. However, evaluators can sometimes fail to represent public interests. Red-teaming for state-of-the-art general-purposeAIsystems is predominantly done by the organisations that develop them[references 2, 22, 204*]. Academia, auditing networks, and dedicated evaluation organisations can also play a key role[references 215, 216, 217]. As is the case withAIdevelopers themselves, red team evaluators are not always representative of public interests or demographics and can display bias, or omit important considerations in their identification or assessment ofAI-related harms[reference 215]. Best practices for red-teaming are not established yet[reference 216]. Various faults in general-purposeAImodels have remained undetected during red teaming. Red-teaming and adversarial attacks are useful for developing a better understanding of a model's worst-case performance and assessing performance at capabilities benchmarks don't adequately cover. However, they come with limitations. Previous work on benchmarking red-teaming and adversarial attack techniques has found that bugs often evade detection[reference 218]. A real-world example is jailbreaks for current state-of-the-art general-purposeAIchat systems[references 208, 209, 210, 211, 212], which seem to have evaded initial detection by developers who designed them[references 2, 22, 204*]. Overall, red-teaming is just one of several assessment tools necessary for meaningful understanding of general-purposeAIcapabilities[reference 219]. Red-teaming can also fail to catch downstream harms, which only arise when theAIsystem is more widely deployed in society. This is discussed further in4.3. Systemic risks. Design choices throughout the general-purposeAIdevelopment process affect how the resulting system works. Auditing provides a mechanism to scrutinise and ensure accountability for those choices. Different groups of general-purposeAIauditors achieve differing degrees of success when it comes to the quality of the evidence gathered and the accountability outcomes achieved[references 215, 217]. One survey of a range of approaches toAIauditing[reference 217]suggests independently assessing deployed general-purposeAIsystems across different dimensions (see Figure 1) to hold stakeholders accountable for the choices they make regarding the development of the general-purposeAIsystem, and its use. Analysis of training data can reveal problematic content. The analysis of training data, otherwise known as 'data audits'[reference 217], is one concrete approach to the analysis of key model design choices, and can reveal problematic content. In the machine learning development process, data is collected, curated, and annotated[reference 190]. Examining how these data engineering decisions could lead to indirect harm, and influence its outcomes is useful in understanding the model and its ultimate downstream impacts. For example, the analysis of text and image data from the internet used to train modern systems has identified copyrighted content[references 220, 221, 222], hateful speech and memes[references 223, 224, 225], malign stereotypes[references 224, 226, 227], sexually-explicit content[references 223, 226], and depictions of sexual violence[reference 226]including child abuse material[reference 228]. Furthermore, investigations into training datasets often reveal issues in terms of the demographic, geographic, and linguistic under-representation of certain populations in mainstream data sources[references 229, 230]. Such data audits have provided the evidence necessary for legal challenges on copyright. For example, the New York Times lawsuit against OpenAI[reference 231]heavily cites the data audit from Dodge et al.[reference 232]and has led to the attempted redaction of some datasets deemed to contain inappropriate material[references 233, 234]. However, modern general-purposeAImodels are often trained on extremely large amounts of internet datasets, and these datasets are often not made public, so data provenance remains a systemic challenge[references 235, 236]. As a result, it is challenging to systematically search for potentially harmful examples in training data at scale. The analysis ofAImodelling and product choices can reveal trade-offs and indicate downstream risks. Aside from training data, other methodological choices can contribute to specific problems. Audits that scrutinise how a model is developed are typically referred to as 'process audits'. For example, human feedback-based methods are state-of-the-art for training general-purposeAImodels, but can contribute to problems such as sycophancy[references 237, 238]and producing non-diverse outputs[references 239, 240, 241]. Similarly, engineering decisions such as 'model pruning' can result in inequitable impacts for certain test demographics[reference 242]. In the same vein, generative image model architecture choices have been shown to impact the performance of these systems in representing different races[reference 243]. It is challenging for researchers to analyse developers' methodology because the full development details of proprietary and even of open-source general-purposeAImodels are rarely documented and disclosed[reference 244]. Meanwhile, 'ecosystem audits' help to assess human-AIinteractions. An increasing number of laboratory studies investigate how human users interact with general-purposeAIsystems. These often take the form of controlled studies[reference 245]where participants interact with models and the impact of modelling and user interaction settings on the decisions and behaviour of participants is measured directly. Such studies have revealed the tendency of users to trust certain presentations of model outputs over others[references 246, 247]. However, since participants are recruited, it can be hard to design the study and recruit a wide enough range of participants to meaningfully reflect the full range of user impacts in practice. Several researchers have begun conducting natural and controlled experiments on the impact ofAIuse in real deployment settings. For instance, there have been studies on the impact ofAIrisk assessments on judges' bail decisions in Kentucky, USA[reference 248], the use of automated hiring tools on hiring manager discretion[reference 249], and the use of generativeAIon middle manager performance[reference 250]. Qualitative interviews of stakeholders have proven effective at illustrating more systematic impacts such as some of the social implications ofAIimplementation[reference 251], which can endure after the removal of theAIsystem[reference 252]. AnalysingAIsystems in the real world, post-deployment, allows researchers to study them as components of larger societal systems. Post-market surveillance is quite common in other industries with audit ecosystems[reference 253]. Users often discover capabilities and failure modes that developers do not, and monitoring the real-world usage of a system can further scientific understanding. For example, jailbreaks against modern large language chat models were first studied following findings from ordinary users[reference 254]. The study of deepfakes in the real world has also helped to shape scientific research on studying and mitigating harms[references 255, 256]. In contrast to studying general-purposeAImodel outputs, another common approach to assessing models is to study the inner mechanism through which models produce the outputs. This can help researchers contextualise assessments of model performance and deepens their understanding of model functionality. Studying how general-purposeAImodels and systems operate internally is a popular topic of research with thousands of academic papers produced. Areas of research that aim to enhance transparency include documentation, third-party access mechanisms, black box analysis, explaining model actions, and interpreting the inner workings of models. Documentation templates record decisions made and facilitate transparency at an operational level. Currently, one of the most practical ways to increase transparency about a general-purposeAImodel is through documenting and communicating the engineering decisions that define the model. Several documentation solutions have been proposed to communicate such decisions to a broader range of internal and external stakeholders. Some of these efforts, such as the development of Model Cards[reference 257]have been successful. One recent study revealed ""a widespread uptake of model cards within theAIcommunity""[reference 258]. There are documentation templates available for communication on dataset practices[references 259, 260, 261], broader system features[references 262, 263], and broader procedural decision-making[reference 264]. Model explanation and interpretability techniques can improve researchers' understanding of how general-purposeAIsystems operate internally. There are several tools that allow for the external scrutiny of general-purposeAIsystems, enabling access for external actors to query general-purposeAIsystems directly, or otherwise gain visibility into model details[reference 213]. One prominent technical approach to this involves studying how a model's output can be explained as a consequence of a given input[references 265, 266, 267, 268]. These explanations can play a unique role in supporting accountability, by helping to determine liability, in cases where humans may be wrongfully harmed or discriminated against by automatedAIsystems[references 269, 270, 271]. Another approach used to study the computations in neural networks has involved interpreting the role of parameters[reference 272], neurons[references 273, 274, 275], subnetworks[references 276, 277], or layer representations[references 278, 279, 280, 281]inside ofAIsystems. Interpretations of models have sometimes aided researchers in finding vulnerabilities. Examples have involved red-teaming[reference 207], identifying internal representations of spurious features[reference 282], brittle feature representations[references 283, 284, 285, 286], and limitations of factual recall in transformers[reference 287]. It is challenging to understand how general-purposeAIsystems operate internally and to use this understanding effectively. A persistent problem is that, in the absence of an objective standard to compare to, it is difficult to ensure that an interpretation of how a general-purposeAIsystem operates is correct. A number of 'interpretability illusions', in which an interpretability technique suggests a misleading interpretation for how a model works, have been documented[references 288*, 289]. In addition, some research has also critically examined how algorithmic transparency tools can be maliciously used to construct false dichotomies, obscure, and mislead[reference 290]. To rigorously evaluate interpretability techniques, the interpretations that they generate need to be demonstrably useful for some downstream task[references 291, 292, 293, 294*, 295], yetAIinterpretability tools are not consistently competitive with simpler techniques for many tasks yet[reference 296]. In particular, different techniques to explain model actions often disagree with one another, and fail at sanity checks or downstream uses[references 218, 297, 298, 299]. Nonetheless, interpretability has sometimes improved practical diagnostics and understanding, especially with recent progress in the field[reference 300]. Further progress may enable better applications. However, high-level interpretations of general-purposeAIsystems cannot be used to make formal guarantees about their behaviour with current models and methods. The potential for current explainability and interpretability techniques to practically aid in rigorous model assessments is debated. It is extremely difficult to conduct thorough evaluations and generate strong assurances of general-purposeAIcapabilities and risks. Modern general-purposeAIsystems are the results of complex, decentralised projects involving data collection, training runs, system integration, and deployment applications. Meanwhile, there are a very large number of real-world uses for general-purposeAIsystems. This complexity makes it difficult for any single actor to understand the entire process. The quality of evaluations depends on levels of access and transparency. Different techniques for evaluatingAIsystems require different types of access. For example, evaluating a model's performance on test data typically only requires the ability to query the target model and analyse its outputs. This is typically referred to as 'black-box' access. The ability to query a black-box system is useful but many types of evaluation techniques depend on greater levels of access. Historically,AIresearchers have benefitted from open-source methods, models, and data. Today, however, companies are increasingly keeping state-of-the-art general-purposeAIsystems private[reference 244]. Lacking 'white-box' access (access to model parameters) makes it challenging for researchers to perform adversarial attacks, model interpretations, and fine-tuning[references 300, 301]. Meanwhile, lacking 'outside-the-box' access to information about how a system was designed, including data, documentation, techniques, implementation details, and organisational details makes it difficult to perform evaluations of the development process[references 226, 227, 257, 300, 302, 303]. Meanwhile, the third-party audit ecosystem is nascent but growing. VariousAIaudit tools, some of which are open-source, allow external users to query and access details of the model[reference 213]. Several studies have advocated for legal 'safe harbours'[reference 304]or government-mediated access regimes[reference 253]to enable independent red-teaming and audit efforts. Methods for structured access have been proposed that do not require making the code and weights public[reference 305]but make it possible for independent researchers and auditors to perform their analysis with full access to the model in a secured environment designed to avoid leaks. Thoroughly assessing downstream societal impacts requires nuanced analysis, interdisciplinarity, and inclusion. Although understanding the overall societal impact is the ultimate goal of manyAIevaluations, many fall short of this goal. Firstly, there are always differences between the settings in which researchers studyAIsystems and the ever-changing real-world settings in which they will be deployed. Secondly, evaluatingAIimpacts on society is a complex socio-technical problem[references 306, 307]. For example, while it is known that large language models exhibit significant cross-lingual differences in safety[references 308, 309], capabilities[references 310, 311*], and tendencies[references 312, 313*], it is challenging for researchers to thoroughly evaluate language models across languages. Furthermore, an over-reliance on simplified technical proxies when dealing with ethical concepts like 'fairness' and 'equity', can be misleading or exclude under-represented stakeholders[references 314, 315]. Evaluations of the broader impacts of general-purposeAIare also highly multifaceted, requiring interdisciplinarity and representation of multiple stakeholders who may hold very different perspectives[references 316, 317, 318*]. Modelling the effects ofAIdeployment in society, in advance of deployment, is inherently complex and difficult to anchor in quantitative analysis. While there has been progress on societal impact evaluation for general-purposeAI[references 319, 320, 321, 322, 323], implementation remains challenging due to the need to balance the interests of multiple stakeholders[reference 324]and resourcing challenges[reference 325]that often make this work difficult to do in practice. Increasing participation and representation of perspectives in theAIdevelopment and evaluation process is an ongoing technical and institutional challenge. Specifying relevant goals of evaluation is highly influenced by who is at the table and how the discussion is organised, meaning it is easy to miss or mis-define areas of concern. Broadening the range of those participating in the audit process also broadens the range of experiences incorporated into the process of discovering and characterising current or anticipated harms[reference 215]. Expanding participation has been a focus of the machine learning community in recent years, highlighting the need to incorporate and engage a broader range of perspectives and stakeholders into the process ofAImodel design, development, evaluation, and governance[references 326, 327, 328]. Multiple strategies have been proposed from facilitating a broader notion of impacts during the implementation of impact assessments[reference 329]to enable a more inclusive range of human feedback[reference 330]. However, seeking out and incorporating more voices is a nuanced endeavour, requiring sensitivity and respect for participating parties to minimise the potential for exploitation[reference 331]. Crucially, the challenge of increasing participation also involves the necessary negotiation of hard choices between incompatible values or priorities[references 332, 333, 334]. While transparency is critical for assessingAIsystems, it is difficult to achieve in practice. Transparency efforts are not always meaningful interventions toward accountability[reference 290]. Bhatt et al.[reference 335]surveyed dozens of corporate implementations of explainability techniques and found that many explainability efforts are meaningfully used during the model development process but rarely live up to the policy aspirations of providing end-user transparency or justification. Furthermore, recent surveys of proprietary, open-source explainability and interpretability tooling reveal that such tools are rarely adequately validated to support claims, and are vulnerable themselves to manipulation and robustness challenges[references 213, 336, 337]. Similarly, the logistics of operationalising documentation practice in corporate settings can be fraught with internal politics[reference 338]. The development and deployment of general-purposeAIgives rise to several risks, which are discussed in this section. This report distinguishes between 'risks' and 'cross-cutting risk factors'. For the purposes of this report 'risk' means the combination of the probability of an occurrence of harm and the severity of that harm[reference 339]. 'Cross-cutting risk factor' refers to a condition that contributes to not one but several risks. As general-purposeAIcovers a broad set of knowledge areas, it can be repurposed for malicious ends, potentially causing widespread harm. This section discusses some of the major risks of malicious use, but there are others and new risks may continue to emerge. While the risks discussed in this section range widely in terms of how well-evidenced they are, and in some cases, there is evidence suggesting that they may currently not be serious risks at all, we include them to provide a comprehensive overview of the malicious use risks associated with general-purposeAIsystems. General-purposeAIcan amplify the risk of frauds and scams, increasing both their volume and their sophistication. Their volume can be increased because general-purposeAIfacilitates the generation of scam content at greater speeds and scale than previously possible. Their sophistication can be increased because general-purposeAIfacilitates the creation of more convincing and personalised scam content at scale[references 340, 341]. General-purposeAIlanguage models can be used to design and deploy 'phishing' attacks in which attackers deceive people into sharing passwords or other sensitive information[reference 342]. This can include spear-phishing, a type of phishing campaign that is personalised to the target, and business email compromise, a type of cybercrime where the malicious user tries to trick someone into sending money or sharing confidential information. Research has found that between January to February 2023, there was a 135% increase in 'novel social engineering attacks' in a sample of email accounts[reference 343*], which is thought to correspond to the widespread adoption of ChatGPT. General-purposeAIlanguage models have the potential to dramatically scale the reach of fraudsters by automating conversations with potential victims to find promising targets[reference 340]. General-purposeAIcould also contribute to targeted identity theft, and generating fake identities which could be used for illegal purposes. For example, a few years ago, research had discussed potential risks fromAI'voice cloning' - where anAIsystem analyses the tone, pitch, and other characteristics of a human voice to create synthetic copies - and how such technology might be used by fraudsters to pretend to be their victim's friends or a trusted authority[reference 344]. General-purposeAIsystems can also help criminals to evade security software by correcting language errors and improving the fluency of messages that might otherwise be caught by spam filters. Recent data indicates thatAI-enabled fraud, particularly using deepfakes, is growing globally[references 345, 346]. Detecting the use of general-purposeAIsystems to generate content used to commit fraud can be difficult, and institutions may be reluctant to disclose the challenges they are facing withAI-powered fraud[reference 347]. General-purposeAI-generated fake content can also be employed to harm individuals by featuring them in that content against their consent, thereby violating their right to privacy and damaging their reputation or dignity. This can happen in the form of fake content featuring any compromising or reputationally damaging activity, but has received particular attention in cases of deepfake pornography, where general-purposeAIis used to create pornographic audiovisual content of individuals without their consent. This may include the creation of child sexual abuse material (CSAM) and other forms of intimate images used to abuse, for example, former domestic partners or for blackmail[references 348, 349, 350]. AI, particularly general-purposeAI, can be maliciously used for disinformation[reference 351], which for the purpose of this report refers to false information that was generated or spread with the deliberate intent to mislead or deceive. General-purposeAI-generated text can be indistinguishable from genuine human-generated material[references 352, 353], and may already be disseminated at scale on social media[reference 354]. In addition, general-purposeAIsystems can be used to not only generate text but also fully synthetic or misleadingly altered images, audio, and video content[reference 355]. Humans often find such content indistinguishable from genuine examples, and generating such content is both relatively simple and extremely cheap[reference 356]. An example of this is images of human faces that were altered, or completely generated, using general-purposeAIor narrower types ofAIsystems[reference 357]. Such 'Deepfake' images and videos are thought to have been deployed in several national elections over recent months to defame political opponents, with potentially significant impact, but there is currently not much scientific evidence about the impact of such campaigns. General-purposeAItools might be used to persuade and manipulate people, which could have serious implications for political processes. General-purposeAIsystems can be used to generate highly persuasive content at scale. This could, for example, be used in a commercial setting for advertising, or during an election campaign to influence public opinion[reference 358]. Recent work has measured the persuasiveness of general-purposeAI-generated political messages and found that it can somewhat sway the opinion of the people reading these messages[references 359, 360]. In addition, general-purposeAIcan be used to tailor persuasive content to specific individuals or demographics (known as 'microtargeting'), for example, by using information scraped from social media. However, there is currently only limited evidence that micro-targeted messages are more persuasive than generic general-purposeAI-produced content[references 361, 362], which is in line with emerging scepticism about the effectiveness of microtargeting in general[reference 363]. An underexplored frontier is the use of conversational general-purposeAIsystems to persuade users over multiple turns of dialogue. In one study, human-human or human-general-purposeAIpairs debated over a cycle of opinion-and-rebuttal, and general-purposeAIsystems were found to be as persuasive as humans[reference 364]. Another study in which general-purposeAIsystems attempted to dissuade humans of their belief in conspiracy theories over 3 conversational turns found reductions in reported belief in those theories of 15-20% that endured for up to 2 months[reference 365]. These results raise the possibility that in conversational settings, general-purposeAIsystems could be used to very powerful persuasive effect. As general-purposeAIsystems grow in capability, it might become easier to maliciously use them for deceptive or manipulative means, possibly even with higher effectiveness than skilled humans, to encourage users to take actions that are against their own best interests[references 366, 367*]. In doing so, they may utilise new manipulation tactics against which humans are not prepared because our defences against manipulation have been developed through the influencing attempts of other humans[reference 368]. The overall impact of disinformation campaigns in general as well as the impact of widespread dissemination of general-purposeAI-generated media are still not well understood. Despite indications of potentially serious risks to public discourse, and the integrity of the information ecosystem posed by general-purposeAI, there are caveats. First, there is a lack of evidence about the effectiveness of large-scale disinformation campaigns in general (whether using general-purposeAIor not). Second, some experts have argued that the main bottleneck for actors trying to have a large-scale impact with fake content is not generating that content, but distributing it at scale[reference 369]. Similarly, some research suggests that 'cheapfakes' (less sophisticated methods of manipulating audiovisual content that are not dependent on general-purposeAIuse), might be as harmful as more sophisticated deepfakes[reference 370]. If true, this would support the hypothesis that the quality of fake content is currently less decisive for the success of a disinformation campaign than challenges around distributing that content to many users. Social media platforms like Meta or X employ various techniques such as human content moderation and labelling for reducing the reach of content likely to be disinformation, including general-purposeAI-generated disinformation. On the other hand, research has shown for years that social media algorithms often prioritise engagement and virality over the accuracy or authenticity of content, which could aid the rapid spread ofAI-generated disinformation[references 371, 372]. In general, as general-purposeAIcapabilities grow and are increasingly used for generating and spreading messages at scale, be they accurate, intentionally false, or unintentionally false, people might come to trust any information less, which could pose serious problems for public deliberation. Malicious actors could exploit such a generalised loss of trust by denying the truth of real, unfavourable evidence, claiming it isAI-generated, a phenomenon coined as the 'liars' dividend'[reference 373]. Potential measures to identify general-purposeAI-generated content, such as watermarking, can be helpful but are easy to circumvent for moderately sophisticated actors. Researchers have employed various methods attempting to identify potentialAIauthorship[references 374, 375]. Content analysis techniques explore statistical properties of text, such as unusual character frequencies or inconsistent sentence length distributions, which may deviate from patterns typically observed in human writing[references 376, 377, 378]. Linguistic analysis techniques examine stylistic elements like sentiment or named entity recognition to uncover inconsistencies or unnatural language patterns indicative ofAIgeneration[references 379, 380]. Readability scores can also be used to examine where general-purposeAI-generated text might score unusually high or low on metrics like the Flesch Reading Ease Score compared to human-written content[reference 381].AIresearchers have also proposed other disinformation detection approaches, such as watermarking, in which an invisible signature identifies digital content as generated or altered byAI[reference 382]. However, current techniques are relatively easy to circumvent for moderately skilled actors[reference 374], and perfect defences against the removal of watermarks are probably impossible[reference 383]. Still, these safeguards may deter relatively unsophisticated threat actors[reference 384].5. Technical approaches to mitigate risksprovides a more in-depth discussion of watermarking techniques. General-purposeAIsystems can exacerbate existing cybersecurity risks in several ways. Firstly, they may lower the barrier to entry of more sophisticated cyber attacks, so the number of people capable of such attacks might increase. Secondly, general-purposeAIsystems could be used to scale offensive cyber operations, through increasing levels of automation and efficiency. Moreover, general-purposeAIcan inadvertently leave systems vulnerable to traditional cybersecurity attacks. For example, general-purposeAIsystems are used pervasively as coding assistants and can inadvertently introduce software vulnerabilities[references 385, 386*, 387]. General-purposeAIsystems reduce the cost, technical know-how, and expertise needed to conduct cyber-attacks. Offensive cyber operations include designing and spreading malicious software as well as discovering and exploiting vulnerabilities in critical systems. They can lead to significant security breaches, for example in critical national infrastructure (CNI), and pose a threat to public safety and security. Given the labour-intensive nature of these operations, advanced general-purposeAIthat automates certain aspects of the process, reducing the number of experts needed and lowering the required level of expertise, could be useful for attackers. So far, current general-purposeAIsystems have been shown to be capable of autonomously carrying out basic cybersecurity challenges and narrow cyber tasks such as hacking a highly insecure website[references 388, 389*, 390, 391]. However, despite consistent research efforts, existing models appear to not be able to carry out multi-step cybersecurity tasks that require longer horizon planning[references 367*, 391, 392]. Given thatLLMscan already process and manipulate some cybersecurity concepts[reference 393], planning could unlock more sophisticated cyber capabilities such as independently navigating complex digital environments, identifying and exploiting vulnerabilities at scale, and executing long-horizon strategies, without the need for direct human guidance. Figure 5. AnAI-enabled pipeline for automating simple attacks against websites (from[reference 391]). A novice hacker supplies the general-purposeAImodel with security information sources and textbooks, and prompts it to interact with their target website. Through trial and error, the general-purposeAImodel identifies an attack that works on this website and returns the details to the hacker. General-purposeAIsystems could improve defensive cyber capabilities if applied across relevant areas. Despite improved offensive capabilities, adversaries will not inevitably succeed, as advancements in general-purposeAIsystems will also enhance defensive capabilities[references 394, 395]. For instance, general-purposeAIsystems can significantly cut down the time humans spend identifying and fixing vulnerabilities[references 396*, 397]. However, similar to their attacking capabilities, the cyber-defence benefits of existing models also have limitations[references 367, 398, 399, 400, 401]. Given the labour and time required to implement security patches across the digital infrastructure, attackers still have the opportunity to achieve successful breaches in systems that have not yet been fixed. As general-purposeAIsystems improve and become more widely used, the dynamics between attackers and defenders can be influenced by organisational factors such as resource availability and expertise levels. Major cyber defence competitions, such as the[reference 402*], alongside high-quality datasets and benchmarks[reference 403], can drive the development of more sophisticated and resilient cybersecurity measures. There are 2 avenues by which general-purposeAIsystems could, speculatively, facilitate malicious use in the life sciences: firstly by providing increased access to information and expertise relevant to malicious use, and secondly by increasing the ceiling of capabilities, which may enable the development of more harmful versions of existing threats or, eventually, lead to novel threats[references 404, 405]. Increased access to information- Advanced general-purposeAImodels can provide scientific knowledge, step-by-step experimental protocols, and guidance for troubleshooting experiments, all of which could potentially be exploited for malicious purposes[references 405, 406, 407, 408]. However, information relevant to biological threat creation is already broadly accessible, given its 'dual-use' nature[references 409, 410]. The capabilities of current general-purposeAIsystems to 'uplift' a malicious actor's ability to maliciously use biology, by increasing their ability to access information relative to existing resources such as the internet, is unclear. Few empirical studies have assessed if current general-purposeAImodels provide an uplift and current evidence is mixed[references 166*, 411]. Methodologies to quantify uplift are also nascent and face limitations. Increased access to hands-on expertise- Access to information alone is insufficient to enable biological malicious use. Malicious actors must also be able to successfully synthesise, weaponise, and deliver the biological agent. Historically, those processes have required specialised expertise and hands-on skills for practical laboratory tasks[references 412, 413]. Experts theorise that bioengineering combined with general-purposeAIadvances may have lowered these barriers[reference 414]. However, existing studies have not evaluated whether general-purposeAIsystems uplift malicious actors with practical laboratory research tasks. Existing advanced general-purposeAImodels have some ability to design and troubleshoot laboratory experiments[reference 407]. These capabilities can be enhanced by equippingLLMswith the ability to access tools such as web search and specialised computational tools[reference 415]. When connected to laboratory robots or cloud labs,LLMshave been experimentally used to directly instruct these platforms to carry out experiments[reference 189]. However further empirical work is required to determine whether these existing general-purposeAIsystems' capabilities 'uplift' actors with practical laboratory research tasks, relative to the internet's capabilities. Increasing the ceiling of capabilities- While this report focuses on general-purposeAIsystems (as defined in1. Introduction), dual-use science risks are also impacted by the existence of narrowAItools and the ability for general-purposeAIsystems to interact with those narrowAItools. NarrowAIbiological tools can already redesign existing proteins to enhance existing functionality[reference 416]and confer new biological functions[reference 417], as well as generate novel proteins[references 418, 419]. Analogous design capabilities have been demonstrated in other scientific domains, including chemistry[reference 420]. The capabilities of narrow tools themselves have dual-use implications: for instance, their use to predict viral mutations likely to demonstrate immune evasion ([reference 421], or generate plausible novel toxic molecules[reference 422]. NarrowAItools are also often widely available, which makes implementing meaningful safeguards challenging[references 43, 419, 423*]. In addition, general-purposeAIsystems are able to direct laboratory robots using language instructions, and make use of specialised chemical computational tools[references 189, 415, 424]. The malicious use potential of future general-purposeAIsystems could be influenced by a range of projected advances. These include: advances in model capabilities, the integration of general-purposeAIsystems with narrowAItools, and the integration of general-purposeAIwith automated laboratory equipment. Though there is some evidence of these projected advances being realised, it remains highly uncertain whether these capabilities will uplift users' abilities relative to existing resources. Advances in general-purposeAImodel capabilities- Future general-purposeAIsystems will, plausibly, feature even greater domain-specific knowledge, reasoning abilities, and the capacity to formulate complex plans. Experts disagree on how much general-purposeAIsystems could enable troubleshooting practical laboratory tasks, compared to using internet search. There is agreement that more effective, real-time troubleshooting for practical laboratory tasks would be enabled by 'multimodal' general-purposeAIsystems that can incorporate information from images and videos[references 405, 407, 425]. However, this possibility has not been tested yet. Additionally, some researchers argue that general-purposeAIsystems trained on biological data may eventually outperform narrower tools designed for specific biological applications[reference 426], but general-purposeAIsystems are yet to demonstrate such capabilities, and the future performance and scalability of general-purposeAIsystems in the biological domain remain unclear. Integration with narrow tools- Though general-purposeAIsystems can make use of narrowAItools, this type of integration has so far been limited[references 189, 415, 427*]. It is uncertain how increasing the integration of general-purposeAIsystems with narrowAItools will affect the risk of malicious use. Today, narrowAItools require specialist expertise to use effectively for scientific research[reference 428]. Though the ability to direct these tools using natural language will make specialised tasks more accessible, leveraging the outputs will likely still require technical expertise until substantial advances are made with general-purposeAI. Existing tools also have limited outputs and require extensive laboratory validation, and the extent to which these limitations will be overcome is unclear. Autonomous science capabilities- While advanced general-purposeAIsystems are already enabling some autonomous science capabilities, it is not clear what the near-term implications are for potential biological malicious use. While aspects of chemistry workflows such as chemical synthesis are already automatable[references 189, 415], experts are uncertain if these advances will transfer well to biology workflows, due to challenges automating work involving living systems[reference 407]. Furthermore, the high costs of automated laboratories are likely to make large-scale automation largely inaccessible to all but the most advanced malicious actors, although commercial cloud labs could partially offset this. In summary, the degree to which current state-of-the-art general-purposeAIsystems enhance the capabilities of malicious actors to use the life sciences over existing resources, such as the internet, remains unclear. Though some empirical work has assessed this uplift with respect to information access and biological threats[references 166*, 429], additional studies evaluating a broader range of tasks and scientific domains are needed to provide greater insight into this question. It has not yet been investigated whether general-purposeAIsystems could also strengthen defences against dual-use science hazards. Risks may arise where general-purposeAImodels and systems fail to comply with general tenets of product safety and product functionality. As with many products, risks from general-purposeAI-based products occur because of misunderstandings of functionality and inadequate guidance for appropriate and safe use. In that respect, general-purposeAI-based products may be no different[reference 430]. Product functionality issues, and the risks they pose may be clustered by potential failure modes (see Table 1). 'Impossible' tasks arise from instances of an attempt to accomplish goals with a general-purposeAIsystem that goes beyond the general-purposeAIsystem's capability. It can be hard to say definitively what constitutes an impossible task in a modern setting. Historically, large language models have not been able to consider events or developments that occurred after the end of their training. However, enablingAIproducts to retrieve information from databases has improved their ability to consider what happened after their training - although models still perform worse on tests that require novel information[reference 431]. Another potentially impossible task may be tasks requiring data that is inherently inaccessible - such as information that does not exist in the format of computable media, or data not available for training due to legal or security reasons. Impossible tasks pose risks because often, salient types of failure- including many of the engineering failures, post-deployment failures and communication failures (see Table 1) - might be the by-product of mismeasurements, misapprehensions or miscommunication around what a model can do, and the misinformed deployments that result. For instance, the GPT-4 model achieved results of ""passing a simulated bar exam with a score around the top 10% of test takers"" and being in the 88th percentile of LSAT test takers[reference 2*]. Confidence in this result even led some lawyers to adopt the technology for their professional use[reference 432]. Under different circumstances, such as changes to test-taking settings or when comparing to first-time bar examinees who passed the exam, the model achieved substantially lower percentile results[reference 433]. Those who were attempting to make use of the model in actual legal practice encountered these inadequacies, facing severe professional consequences for the errors produced by these models (i.e. inaccurate legal citations, inappropriate format and phrasing, etc.)[reference 434]. Similar misapprehensions regarding model performance are thought to apply in the medical context[reference 435], where real world use and re-evaluations reveal complexity to the claims of these models containing reliable clinical knowledge[reference 436]or passing medical tests such as the MCAT[reference 2*]or USMLE[reference 437]. More generally, some deployed large language models struggle under some linguistic circumstances: They might, for instance, have trouble navigating negations and consequently fail to distinguish between advising for and against a course of action - though some research suggests these issues are addressed by general capability gains[references 438, 439]. Some shortcomings are only revealed after deployment. Although many thorough evaluations have examined large language model use for code generation[reference 440*], including in relevant real-world tasks[reference 441], instances of real-world deployment of large language models for coding suggest that the use of these models could lead to the potential introduction of critical overlooked bugs[reference 442], as well as confusing or misleading edits[reference 443]that could be especially impactful when guiding engineering programmers, particularly in applications that automate parts of the workflow[reference 444]. Table 1: Taxonomy ofAIfunctionality issues, reproduced with permission[reference 430]. Functionality misapprehensions arise from different underlying issues. Firstly, as noted in3. Methodology to assess and understand general-purposeAIsystems, there are technical difficulties in designing and representative evaluations of performance for general-purposeAIsystems, making definite statements about functionality difficult. Secondly, functionality issues might only manifest or manifest differently in a real-world realistic context, rendering even informative model evaluation insufficient for robust statements about general-purposeAIsystem and product functionality. Thirdly, failures can result not just from inadequate evaluation, but a lack of appropriate communication to product users around the product's limitations and the potential consequences. Misleading advertising, as it occurs in many markets, could become a substantial source of risks from functionality in general-purposeAI[reference 445]. In general, for many machine learning based products, it can be unclear exactly which context of deployment is well represented in the data and suitable for the model. However, more general purposeAItools specifically are more difficult to vet for deployment readiness than lower-capability or narrowerAIsystems: With General PurposeAI, it can be difficult to clearly define and restrict potential use cases that may not be suitable or may be premature, although substantial progress on restricting use cases is feasible. Harmful bias and underrepresentation inAIsystems have been challenges since well before the increased attention to general-purposeAI. They remain an issue with general-purposeAI, and will likely be a major challenge with general-purposeAIsystems for the foreseeable future. Decisions by anAImight be biased if their decision-making is skewed based on protected characteristics, such as gender, race, etc. They might hence be discriminatory when this bias informs decisions to the disadvantage of members of these protected groups; thereby creating harm to fairness. This section discusses present and future risks resulting from bias and underrepresentation risks inAI. Because of the rich history of research in this space, this section explores research both on narrowAIand general-purposeAI. AIsystems can demonstrate bias as a result of skewed training data, choices made during model development, or the premature deployment of flawed systems. Despite extensive research, reliable methods to fully mitigate any discrimination remain elusive. There are particular concerns over the tendency of advanced general-purposeAIsystems to replicate and amplify bias present within their training data[reference 446]. This poses a significant risk of discrimination in high-impact applications such as job recruitment, financial lending, and healthcare[reference 447]. In these areas biased decisions resulting from general-purposeAIsystems outputs can have profoundly negative consequences for individuals, potentially limiting employment prospects[references 448, 449], hindering upward financial mobility, and restricting access to essential healthcare services[references 450, 451]. There are several well-documented cases ofAIsystems displaying discriminatory behaviour based on race, gender, age, and disability status, causing substantial harm. Given increasingly widespread adoption ofAIsystems across various sectors, such behaviour can perpetuate various types of bias, including race, gender, age, and disability. This can cause serious harm if these systems are entrusted with increasingly high-stakes decisions which can have severe consequences for individuals. Racial bias inAIsystems has been shown to be present in commercially available facial recognition algorithms[reference 452]and has caused ineffectiveness in predicting recidivism outcomes for defendants of colour, underestimation of the needs of patients from marginalised racial and ethnic backgrounds[references 453, 454], and the perpetuation of inappropriate race-based medicine in responses from text-generation models[references 435, 450]. Gender biases in the outputs ofAIsystems are another key concern. Research has uncovered sexist, misogynistic, and gender-stereotyping content being produced from general-purposeAI[references 455, 456]and male-dominated results from gender-neutral internet searches using narrowAIalgorithms[reference 457]. Age bias is also a key issue: someAIsystems have exhibited bias against older job seekers[reference 458], and age bias appears in some outputs from sentiment analysis models[reference 459]. One reason for this could be biases in training data. For example,LLM-powered HR screening tools may be trained on resumes skewed towards younger workers that may inadvertently discount the experiences and skill sets of older applicants. Similarly, healthcare allocation algorithms developed by health insurance companies may disadvantage older individuals based on age-related health risks, even if these individuals are healthy. Lending algorithms may not appropriately handle older adults' financial circumstances, particularly regarding social security income, which could impact approval outcomes[reference 460]. Research has also shown thatAIsystems and tools can discriminate against users with disabilities, for example by disproportionately denying coverage claims from disabled individuals with complex medical needs[reference 461], reproducing societal stereotypes about disabilities[reference 462], and inaccurately classifying sentiments about people with disabilities[reference 463]. Despite growing research on sign language recognition[reference 464],AIsystems have limited automated transcription abilities for sign language speakers[reference 143], and limited diversity in sign language datasets may also exacerbate disability bias from advanced general-purposeAIsystems, as the majority of sign language datasets represent American Sign Language. Recent work that, for instance, has developed datasets for 6 African sign languages[reference 465]is a step, albeit a modest one, toward achieving more equitable inclusion of sign language dialects. AIsystems exhibit a tendency towards intersectional biases. Intersectionality describes how individuals who share more than one marginalised characteristic may experience compounded bias or discrimination (for example, a low-income woman of colour). Intersectional bias exacerbates existing social inequalities and can limit individuals' access to vital resources and opportunities. While there is an emerging area of research focused on developing methods for detecting intersectional bias withinAImodels[references 466, 467, 468], there has been much less progress on mitigating potential impacts[reference 469]. Bias in general-purposeAIsystem outputs may result from a lack of representation in training datasets, leading to biased outputs. Different groups and different cultures are unequally represented at various stages of theAIlifecycle - from the input data to the outputs of language and image generation models. Many of the issues associated with bias inAIdemonstrate the harms of limited representation in training datasets, which are overwhelmingly likely to be in English[reference 236]. This has led to major disparities in the safety and reliability of modern general-purposeAIsystems in different societies[references 309, 310, 313*]. Datasets used to trainAImodels have also been shown to under-represent various demographic markers such as age, race, gender, and disability status[references 470, 471].AIlanguage models predominantly rely on digitised books and online text in their training, which fails to reflect oral traditions and non-digitised cultures. The inherent historical biases embedded in these sources, coupled with potential inequalities in the data collection process, can perpetuate systemic injustices[reference 472*]and leadAIsystems to reflect dominant cultures, languages, and worldviews, at the detriment of marginalised groups such as indigenous communities[references 196, 230, 239, 473, 474]. Issues of bias and representation remain an unsolved problem. Despite considerable attention in the literature and economic incentives for companies to avoid reputational damage from biasedAIproducts, mitigating bias remains an unsolved challenge. While developers may attempt to explicitly address bias during model fine-tuning,AImodels can still pick up on implicit associations between features[reference 475]or perpetuate significant biases and stereotypes even when prompts do not contain explicit demographic identifiers[reference 476]. While methods such as Reinforcement Learning from Human Feedback (RLHF) aim to align model decision-making with human preferences, these methods could inadvertently introduce biases based on the diversity and representativeness of the humans providing feedback[reference 303].RLHFhas been shown to lead to more politically biased models[references 239, 477]and incorporate user beliefs over factual information[reference 238]. In addition, rater feedback is often inconsistent[references 478, 479]. Understanding issues of representation within datasets, models, and evaluation methods such asRLHFis crucial, as skewed representation can lead to biased outputs inAImodels. More research is needed to address these issues. AIcompanies and researchers are increasingly interested in developing general-purposeAI'agents' (sometimes also referred to as 'autonomous general-purposeAIsystems'). General-purposeAIagents are systems that can autonomously interact with the world, plan ahead, and pursue goals. Although general-purposeAIagents are beginning to be developed, they still demonstrate only very limited capabilities[references 26, 178, 480*]. Various researchers andAIlabs ultimately hope to create general-purposeAIagents that can operate and accomplish long-term tasks with little or no human oversight or intervention. Autonomous general-purposeAIsystems, if fully realised, could be useful in many sectors. However, some researchers worry about risks from their malicious use, or from accidents and unintended consequences of their deployment[references 481*, 482]. Some researchers have also expressed concern about society's ability to exercise reliable oversight and control over autonomous general-purposeAIsystems. For decades, concerns about a potential loss of control have been raised by computer scientists looking ahead toward these kinds ofAIsystems, includingAIpioneers such as Alan Turing[reference 483], I. J. Good[reference 484], and Norbert Wiener[reference 485]. These concerns have gained more prominence recently[reference 486], partly because a subset of researchers now believe that sufficiently advanced general-purposeAIagents could be developed sooner than previously thought[references 127, 487, 488]. The plausibility of such risks remains highly contentious. This section seeks to clarify the nature of the proposed risk, outline the main arguments and evidence that currently inform researchers' views about its likelihood, and summarise current expert opinion. AnAIsystem is considered 'controllable' when its behaviours can be meaningfully determined or constrained by humans. While a lack of control is not intrinsically harmful, it significantly increases the risks of various harms. Current general-purposeAIsystems are generally considered to be controllable, but, if autonomous general-purposeAIsystems are fully developed, then risk of the loss of control may grow considerably. In4.4. Cross-cutting risk factors, this report discusses technical and societal risk factors for dangerousAIsystems. In this section, we expand on how these factors may affect loss of control risk. Overall, the risk of losing control over currently known general-purposeAIsystems appears negligible. The degree to which future, potentially much more capable, autonomous general-purposeAIagents can be controlled remains unclear. It is not yet known whether it will be easier or harder in future to ensure that highly capableAIsystems pursue the objectives their developers intend. One reason for this is thatAIsystems can 'game' their objectives by achieving them in unintended and potentially harmful ways. For example, widely deployed general-purposeAIlanguage models adjust their stated view to better match their user's view, regardless of truth, and consequently gain more positive feedback[references 237, 238]. There are observations that objective-gaming can become more prevalent as system capability increases, because more capable systems can find more ways to achieve a given objective[references 489, 490]. Furthermore, more capable systems have more ways to coherently 'generalise' to behave differently in situations that differ from their training setting including potentially harmful ways - although this is only a hypothesised concern to date[references 489, 490]. However, some recent more capable general-purposeAIsystems have been more controllable due to better tools for training and oversight[references 19, 20, 491], and are less likely to generalise incoherently beyond their training data[reference 492]. It is therefore unclear if general-purposeAIsystems will become more or less controllable as more advanced systems are created. For a discussion of how controllable present general-purposeAIsystems are, see5.2 Training more trustworthy modelsand4.4.1 Cross-cutting technical risk factors. Some mathematical findings suggest that future general-purposeAIagents may use strategies that hinder human control, but as yet it is unclear how well these findings will apply to real-world general-purposeAIsystems. Some mathematical models of idealised goal-directedAIagents have found that, with sufficiently advanced planning capabilities, many suchAIagents would hinder human attempts to interfere with their goal pursuit[references 493, 494, 495*, 496]. Similar mathematical findings suggest that many suchAIagents could have a tendency to 'seek power' by accumulating resources, interfering with oversight processes, and avoiding being deactivated, because these actions help them achieve their given goals[references 493, 494, 495, 497, 498, 499]. However, drawing real-world implications from this mathematical research is not straightforward. For instance, most research assumes that anAIsystem is trained using a randomly chosen goal[references 493, 494, 495, 497, 498, 499], but in practice,AIdevelopers can substantially influence which goals are potentially encoded in general-purposeAImodels (see below and5.2 Training more trustworthy models). Further, philosophical research suggests that not all of the above behaviours are implied by pursuing a random goal[reference 500]. Currently, one mathematical finding and early-stage empirical findings support, although weakly as of this writing, that such behaviours may be found under more realistic circumstances[references 181, 237, 490, 497*]. There have also been case studies where, without being prompted to do so, general-purposeAIsystems and otherAIsystems learned to systematically induce false beliefs in others because this was useful for achieving a goal[references 366, 501]. If observed more widely, such behaviour is also relevant to near-term applications of general-purposeAIchatbots and agents. If people entrust general-purposeAIsystems with increasingly critical responsibilities, then this could increase the risk of loss of control. A range of social and economic forces would influence the interaction between human and autonomous agents in such scenarios. For example, economic pressures may favour general-purposeAI-enabled automation in the absence of intervention, despite potentially negative consequences[reference 502], and human over-reliance on general-purposeAIagents would make it harder to exercise oversight[reference 481*]. Using general-purposeAIagents to automate decision-making in government, military, or judicial applications might elevate concerns overAI's influence on important societal decisions[references 503, 504, 505, 506]. As a more extreme case, some actors have stated an interest in purposefully developing uncontrolledAIagents[reference 507]. Certain specific capabilities could disproportionately increase the risk of loss of control. These capabilities - which are currently limited - include identifying and exploiting software vulnerabilities, persuasion, automatingAIresearch and development, and capabilities needed to autonomously replicate and adapt[references 367, 508, 509]. The relevant sections in this report discuss how capable current general-purposeAIsystems are in some of these areas (4.1.3 Cyber offence,4.1.2 Disinformation and manipulation of public opinion,4.1.4 Dual use science risks). Particularly relevant are agent capabilities, which increase the ability for general-purposeAIsystems to operate autonomously, such as planning and using memory. These are discussed in4.4.1 Cross-cutting technical risk factors. An irreversible loss of control of some general-purposeAIsystems is not necessarily catastrophic. As an analogy, computer viruses have long been able to proliferate near-irreversibly and in large numbers[reference 510]without causing the internet to collapse. Some researchers have explored hypothetical scenarios where highly advanced future general-purposeAIagents acting autonomously might cause catastrophic harm to humans, especially if humans present obstacles to achieving the agents' given goals[references 127, 511]. The mechanisms of harm are often imagined to stem from capabilities such as the ones listed in the previous paragraph and in4.1.4 Dual use science risks. However, these scenarios remain hypothetical as they are not exhibited by current general-purposeAIsystems. AIexperts debate the likelihood of losing control over future general-purposeAIsystems. Key questions include whether sufficiently capable general-purposeAIagents will be developed in the medium-term, and whether technical safety and governance solutions can be developed in time to keep them adequately controlled. As there is limited research that assesses the risk of loss of control, expert opinion can provide alternative guidance, but it cannot replace research. A subset of researchers suggest that risks of loss of control deserves consideration. However, the overall likelihood of extreme control failures remains highly contentious. Some researchers have argued that there has been little progress in developing the types ofAIsystems that others fear could pose risks of loss of control[references 129, 512]. Nevertheless, broad hypothetical scenarios for how society might lose control overAIsystems have been proposed[references 507, 511]and these scenarios have been emphasised by some leading researchers[reference 127]. For example, several hundredAIresearchers have signed a statement declaring that ""Mitigating the risk of extinction fromAIshould be a global priority""[reference 486], though without explicitly referring to loss of control. The actual risk remains highly contentious as there is only limited research assessing it, and the opinions of scientists cannot replace this research. Economists expect general-purposeAIto impact the workforce by automating tasks, augmenting worker productivity and earnings, changing the skills needed for various occupations, and displacing workers from certain occupations[references 513, 514, 515]. Economists hold a wide range of views about the magnitude and timing of these effects, with some expecting widespread economic transformation in the next 10 years, while others do not think a step-change inAI-related automation and productivity growth is imminent[reference 516]. The uncertainty about future general-purposeAIprogress contributes to this uncertainty about the labour market effects of general-purposeAI. Previous waves of automation by computing have primarily affected 'routine' tasks which could be easily codified and programmed into computers[reference 517]. General-purposeAI, in contrast, has the potential to perform a wide array of tasks that are typically carried out by humans, including complex problem-solving and decision-making. An extensive body of literature has studied the exposure of jobs to automation andAIgenerally, without a particular focus on general-purposeAI[reference 518]. In comparison to this research, the exploration of likely labour market impacts of general-purposeAIis at a very early stage. In advanced economies, it is estimated that, due to the prevalence of cognitive-task-oriented jobs, 60% of current jobs could be affected by the introduction of general-purposeAIsystems such as today'sLLMs[reference 519]. This means that general-purposeAIhas the potential to either automate substantial portions of the work or to complement and significantly alter how the work is done. In emerging economies, the share of jobs thought to be potentially affected by general-purposeAIsystems is lower, but still substantial, at 40%[reference 519]. Recent empirical studies have begun to demonstrate the impact of current-generation general-purposeAIsystems on various industries, notably in knowledge work: A key question is whether there will be significant job losses as general-purposeAIsystems become more advanced and more widespread. Some economists consider it likely that job losses will be offset by increased demand for labour in existing occupations and the creation of new kinds of jobs[references 516, 517, 525]. This would be in line with the impact of previous waves of automation: For instance, more than 60% of employment in 2018 was in jobs with titles that did not exist in 1940[reference 526]. Other economists emphasise that the future effects of general-purposeAIsystems on job markets is very difficult to predict[references 519, 527]general-purposeAIcould lead to a substantial reduction of the value of human labour compared with capital[reference 528]. Even if overall demand for labour in the economy is not reduced, the process of displacement can create unemployment if the labour market fails to match workers with new employment opportunities quickly enough. This can happen because of various labour market frictions[reference 529], such as: These factors influence how quickly and whether workers displaced by automation can move to new roles. Some workers may therefore be temporarily unemployed even though there may be job vacancies in the economy as a whole. General-purposeAIcould lead to concentrated job losses, while productivity gains are likely to be more spread out throughout the economy, potentially leading to a difficult transition for some workers unless support is made available. Some economists consider it plausible or even likely that the rate of job displacement due to general-purposeAIsystems enabling automation could outpace the creation of new job opportunities, especially if the development of general-purposeAIsystems is focused on substituting human labour rather than augmenting it[references 502, 530]. However, there are few precise quantitative predictions, and the existing research is consistent with a wide range of possible outcomes. AIresearchers disagree on the pace of future general-purposeAIadvancements, but there is some support for the possibility of extremely rapid advancements, including the possibility of general-purposeAIsystems matching or surpassing the abilities of human experts in almost any cognitive task. (see2.4.3. Will algorithmic progress lead to rapid advancements?). This latter scenario has only been considered in little economic research. That research assumes thatAIsystems can perform virtually all knowledge tasks more cost effectively than humans and indicates that such a scenario could lead to a collapse in wages across many sectors, severe unemployment and a dramatic reduction in labour force participation[references 531, 532]. However, even in such a scenario some demand for human labour may persist due to consumer preferences and ethical or control-related reasons[references 531, 532]. Among economists, extreme scenarios involving a dramatic decline in labour demand due to mass general-purposeAIautomation are currently considered relatively fringe. It is very difficult to forecast how and when general-purposeAIsystems might affect various labour markets. Firstly, there is considerable uncertainty about how quickly general-purposeAItechnology will advance and what its future capabilities may be. Secondly, even for a given level of technological sophistication, the extent to which general-purposeAIsystems enable automation and how this kind of automation could affect labour markets is unclear. One reason for this is that in many sectors there can be considerable lags in deploying and adopting general-purposeAIsystems at scale. For instance, if employees lack the necessary skills to effectively employ general-purposeAIassistance, this can slow down adoption. Overall, many economists expect modest macroeconomic impacts of general-purposeAIsystems over the next decade[reference 533], while some economists expect substantial labour market and macroeconomic effects in the next 5 to 10 years[reference 534]. These forecasts often indicate high uncertainty due to the unknown pace of future advancements in general-purposeAIcapabilities (see2.4 Capability progress in coming years). The impact on wages of general-purposeAIsystems leading to automation is uncertain and the evidence is mixed. General-purposeAIautomation could increase wages in some sectors by: If general-purposeAIsystems complement human labour, then as the capabilities of general-purposeAIsystems advance, wage growth could accelerate alongside economic growth, potentially much faster than historically[reference 539]. Recent large-scale surveys of the use of current general-purposeAIsystems support this view. For example, a recent survey of a total of 5,334 workers and 2,053 firms in the manufacturing and financial sectors across 7 OECD countries found that around 80% of workers who useAIsaid thatAIhad improved their performance at work[reference 540]. However, if in the future automation reduces labour demand faster than new jobs are created, then it is possible that wages and the share of income going to human workers may substantially decline[reference 541]. It is also possible that general-purposeAIsystems might have different economic effects at different points in time: the use of general-purposeAIsystems might initially boost wages, but as more and more tasks are automated, increasing competition for the remaining jobs could push wages down[reference 532]. There is currently no clear consensus among experts regarding the net effects of general-purposeAIautomation on average wages: the effects are likely to differ over time and across occupations, depending on many factors including social acceptance of the technology and organisational decision-making, as well as government policies. General-purposeAIcould increase income inequality both within and between countries. Historically, the automation of routine jobs has likely increased wage inequality within countries by displacing workers from the types of jobs in which they held a comparative advantage[reference 517]. Similarly, general-purposeAIcould systematically compete with some tasks that human knowledge workers currently hold a comparative advantage in, potentially depressing wages if they cannot easily find work elsewhere[references 542, 543]. At the same time, general-purposeAIcould improve the productivity of high-income occupations, and so high-wage earners could see disproportionately larger increases in labour income, thereby amplifying labour income inequality. One simulation suggests that widespread adoption ofAIcould increase wage inequality between high and low-income occupations by 10% within a decade of adoption in advanced economies[reference 519]. Automation could also exacerbate inequality by reducing labour's share of income, which would boost the relative incomes of wealthier capital owners[reference 528]. This would be part of an ongoing trend: globally, the share of income from labour has fallen by roughly 6 percentage points between 1980 to 2022, with similar patterns seen in the United States, Asia and Europe[reference 544]. Further automation could result in a continuation of this trend. Typically, 10% of earners earn the majority of capital income[references 545, 546]. Hence, the greater productivity of capital would therefore act as a boon for high-earners. This dynamic could be particularly pronounced if general-purposeAIaids the creation of 'superstar' firms with strong market power, as they would capture an outsized share of economic profits[reference 519]. Finally, general-purposeAItechnology could exacerbate global inequality if it is primarily adopted by advanced economies (also see4.3.2. GlobalAIdivide). These countries have a higher share of cognitive task-oriented jobs exposed to the potential impacts of general-purposeAI, stronger digital infrastructure, skilled workforces, and more developed innovation ecosystems. This positions them to captureAIproductivity gains more rapidly than emerging markets and developing economies, potentially leading to divergent income growth trajectories and a widening gap between high- and low-income countries[references 519, 547]. There is a well-documented concentration ofAIresearch and development, including research on potential societal impacts ofAI, in Western countries and China[references 316, 548, 549]. This global 'AIDivide' could become even larger for general-purposeAIspecifically because of the high costs associated with general-purposeAIdevelopment. Some countries face substantial barriers to benefiting from general-purposeAIdevelopment and deployment, including lower digital skills literacy, limited access to computing resources, infrastructure challenges, and economic dependence on entities in higher-income countries[references 519, 550]. Because general-purposeAIsystem development is so dominated by a few companies, particularly those based in the US, there are concerns that prominent general-purposeAIsystems which are used worldwide primarily reflect the values, cultures and goals of large Western corporations. In addition, the recent trend towards aiming to develop ever-larger, more powerful general-purposeAImodels could also exacerbate global supply chain inequalities[reference 551], place demands on energy usage, and lead to harmful climate effects which also worsen global inequalities[references 552, 553]. The global general-purposeAIdivide could also be harmful if biased or inequitable general-purposeAIsystems are deployed globally. Disparities in the concentration of skilled talent and the steep financial costs of developing and sustaining general-purposeAIsystems could align theAIdivide with existing global socioeconomic disparities. The United States has the largest percentage of eliteAIresearchers, contains a majority of the institutions who conduct top-tier research, and is the top destination forAItalent globally[reference 554]. However, countries leading inAIdevelopment also experience issues with the distribution of skilledAItalent, which is rapidly shifting towards industry. For example, 70 percent of graduates of North American universities withAIPhDs end up getting a job in private industry compared with 21% of graduates 2 decades ago[reference 555]. In April 2023, OpenAI'sAIsystems were reportedly estimated to incur $700k/day in inference costs[reference 77], a cost that is widely inaccessible for the vast majority of academic institutions and companies and even more so for those based in the Global South[references 556, 557]. Low-resource regions also experience challenges with access to data given the high costs of collection, labelling, and storage. The lower availability of skilled talent to leverage these datasets for model development purposes could further contribute to theAIdivide. Infrastructure concerns are a major factor that prohibit equitable access to the resources needed to train and implement general-purposeAIdue to issues such as inadequate access to broadband internet[references 558, 559], power blackouts and insufficient access to electricity[references 560, 561]. Academic institutions in China and the United States lead in general-purposeAIresearch production, but industry is increasingly influential. China currently publishes the most research onAI, as measured by the total volume of articles in journals, conferences, and online repositories. Geographically, the development of significant machine learning models is concentrated in nations such as the US, Canada, the UK, and China, with at least one coming from Africa. US industry currently dominates the development of advanced general-purposeAIsystems. American institutions produced the majority (54%) of large language and multimodal models in 2022[reference 562]. Industry now surpasses academia in producing significant machine learning models (32 vs 3 in 2022) and industry co-authorship on papers presented at the top 10 leadingAIconferences rose from 22% in 2000 to 38% in 2020[reference 563]. The rising 'compute divide' is contributing to disparities in the distribution of computing resources, and unequal participation in general-purposeAIdevelopment. The term 'compute divide' describes the different extent to which large industrialAIlabs and typical academicAIlabs have access to computing resources[reference 556]. In recent years, this divide has widened[references 556, 557]. Estimates show that US technology companies are the major buyers of NVIDIA H100GPUs, one of the most powerfulGPUchip types on the market explicitly designed forAI[reference 564*]. Amazon, Meta, Google, and Microsoft have all recently announced customAIchips to reduce their dependence on theAIchip supply chain, potentially paving the way for more widespread access toGPUs. However, the exceptionally high cost ofGPUs($15,000 for top-tier models such as the H100 at the time of writing) could hinder academic institutions and less wealthy countries from affording this level ofAIinfrastructure. The delegation of lower-levelAIwork to workers in low-income countries has led to a 'ghost work' industry. From content moderation to proofreading to data labelling, a lot of human labour that the typical consumer is usually not aware of - sometimes referred to as 'ghost work' - is necessary for many products of large technology companies[reference 565]. The increasing demand for data to train general-purposeAIsystems, including human feedback to aid in training, has further increased the reliance on ghost work including the creation of firms helping big technology companies to outsource various aspects of data production, including data collection, cleaning, and annotation. This trend has played a significant role in the development of notable machine learning benchmark datasets such as ImageNet[reference 566]. Data production, a crucial aspect of general-purposeAIadvancement, often relies on workers in countries with lower average wages. These workers may face exposure to graphic content, erratic schedules, heavy workloads, and have limited social and economic mobility[references 567, 568, 569, 570]. This can lead to harms against marginalised workers, widening theAIdivide, and increasing the disparity of who reaps the benefits from advanced general-purposeAIdevelopment. The development of state-of-the-artAIsystems currently costs hundreds of millions, or even billions of US dollars. The biggest upfront investments are specialised computational resources,AIexpertise and access to large, often proprietary datasets (see2.3.1 Recent trends in compute, data, and algorithms). The significant costs associated with these inputs are a barrier to entry for new firms[references 571, 572, 573, 574]. Large technology companies are well-positioned thanks to their existing access to the necessary resources and ability to make substantial financial investments. In addition, general-purposeAIsystems benefit from scale. More compute-intensive large-scale models tend to outperform smaller ones[reference 110*], giving rise to economies of scale: large-scale general-purposeAIsystems are in higher demand due to their superior performance, driving down their costs per customer. High user numbers also have network effects: as more users interact with these models, they generate large amounts of additional training data that can be used to improve the models' performance[reference 575]. These tendencies towards market concentration in the general-purposeAIindustry are particularly concerning because of general-purposeAI's potential to enable greater centralisation of decision-making in a few companies than ever before. Since society at large could benefit as well as suffer from these decisions, this raises questions about the appropriate governance of these few large-scale systems. A single general-purposeAImodel could potentially influence decision-making across many organisations and sectors[reference 571]in ways which might be benign, subtle, inadvertent, or deliberately exploited. There is the potential for the malicious use of general-purposeAIas a powerful tool for manipulation, persuasion and control by a few companies or governments. Potentially harmful biases such as demographic, personality traits, and geographical bias, which might be present in any dominant general-purposeAImodel that become embedded in multiple sectors, could propagate widely. For example, popular text-to-image models like DALL-E 2 and Stable Diffusion exhibit various demographic biases across occupations, personality traits, and geographical contexts[reference 576]. The increasing dependence on a fewAIsystems across critical sectors introduces systemic risks. Errors, bugs, or cyberattacks targeting these systems could cause widespread disruption. Different scenarios have been proposed that illustrate potential disruptions. For example, a denial-of-service attack on a widely usedAIAPIcould disrupt critical public infrastructure which relies on that technology. In finance, the adoption of homogeneousAIsystems by multiple institutions could destabilise markets by synchronising participants' decisions[reference 577]: If several banks rely on one model, they may inadvertently make similar choices, creating systemic vulnerabilities[reference 2*]. Comparable risks could potentially arise in domains, like defence or cybersecurity, ifAIsystems with similar functionality are widely deployed (see also4.4. Cross-cutting risk factors). The recent rapid growth in demand for computing power ('compute') used forAI, and particularly general-purposeAI, development and deployment could makeAIa major, and potentially the largest, contributor to data centre electricity consumption in the near future. This is because compute demand is expected to far outpace hardware efficiency improvements. Today, data centres, servers and data transmission networks account for between 1% to 1.5% of global electricity demand[reference 578]; roughly 2% in theEU, 4% in the US, and close to 3% in China[references 69, 579, 580].AIlikely accounts for well under half of data centre electricity consumption currently, but if the rapid growth ofAI's computational requirements continues,AIcould become the primary consumer of data centre electricity over the coming years and increase its share of global electricity demand. In 2023, the largest general-purposeAItraining runs used around 5e25 FLOP[reference 65]. Using H100GPUsrunning at 1400 watts perGPUfor operation and cooling, this consumes around 40 GWh. If this figure were to grow at a rate of 3x/year, then at the end of the decade, the largest training run would consume 90 TWh, over half of total US data centre electricity consumption in 2022. There are several potential mitigations for the increasing energy use of widespread general-purposeAIsystems. SpecialisedAIhardware and other hardware efficiency improvements can enhance the performance-per-watt of machine learning workloads over time[reference 78]. Moreover, new machine learning techniques and architectures can help reduce energy consumption[reference 78]. The energy efficiency of computation generally improves by an estimated 26% annually[reference 68]. However, even with additional optimisation forAI, the growing demand for computing power used forAItraining, which has been increasing by a factor of approximately 4x each year, is so far significantly to outpacing energy efficiency improvements[reference 17]. The CO2 emissions resulting fromAIdevelopment and deployment depend on the extent and sources of its energy consumption as well as several factors. The carbon intensity of the energy source is a key variable, with renewable sources like solar power contributing substantially less CO2 emissions throughout their life cycle compared to fossil fuels[reference 581*].AIfirms often rely on renewable energy[references 76, 78], a significant portion ofAItraining globally still relies on high-carbon sources such as coal or natural gas[reference 581*]. Other important factors affecting CO2 emissions include the geographic location of data centres, their efficiency, and the efficiency of the hardware used. As a result, the actual CO2 emissions for a given amount of energy consumed inAIcan vary considerably. The 'embodied carbon footprint' ofAIhardware, which includes emissions from manufacturing, transportation, the physical building infrastructure, and disposal (as opposed to not running the hardware), contributes a substantial portion to emissions - depending on the location this could be as high as 50%[reference 76]. As hardware efficiency improves, the embodied carbon footprint could become a larger proportion of the total carbon footprint[references 76, 78]. Water consumption might be another noteworthy area of environmental risk fromAI. Given the increases in compute used for training and deploying models, cooling demands increase, too, leading to higher water consumption. Water consumption by current models and the methodology to assess it are still subject to scientific debate, but some researchers predict that water consumption byAIcould ramp up to billions of cubic metres by 2027[references 76, 582]. In the context of concerns around global freshwater scarcity, and assuming no obvious short-term cooling alternatives exist,AIwater footprint might be a substantial cause of environmental concern. General-purposeAIsystems rely on and process vast amounts of personal data, and this could pose significant and potentially wide-reaching privacy risks. Such risks include loss of data confidentiality for people whose data was used to train these systems, loss of transparency and control over how data-driven decisions are made, and new forms of abuse that these systems could enable. Privacy, broadly speaking, refers to a person's right to control others' access to their sensitive or personal information. In the context ofAI, privacy is a complex and multi-faceted concept, which encompasses issues of confidentiality, transparency, and control. Privacy is a challenging concept to define[reference 583]. In the context ofAIit encompasses: General-purposeAIsystems may expose their training data. The training of general-purposeAImodels generally requires large amounts of training data. Academic studies have shown that some of this training data may be memorised by the general-purposeAImodel, or may be extractable using adversarial inputs, enabling users to infer information about individuals whose data was collected[references 588, 589, 590]or even reconstruct entire training examples[references 591, 592, 593, 594]. However, definitions of memorisation vary, so it is challenging to make any concrete claims about the harms that might arise from memorisation[reference 595]. Many systems are trained on publicly available data containing personal information without the knowledge or consent of the individuals it pertains to. This information could then be outputted by a general-purposeAIsystem in undesired contexts. There is a risk that training models on sensitive data containing personal information (such as medical or financial data) could result in serious privacy leaks. It is difficult to assess the likelihood or potential impact of these risks: for example, existing medical general-purposeAIsystems such as Google's Gemini-Med[reference 596*]are only trained on anonymised public patient data, and the rate at which such models regurgitate training data has not yet been studied. General-purposeAIsystems that continuously learn from interactions with users (e.g. chatbots such as ChatGPT) might also leak such interactions to other users, although at the time of writing, there are no well-documented cases of this occurring. General-purposeAIsystems could enable privacy abuse. Some studies have found that general-purposeAIsystems have privacy-relevant capabilities that may be exploited by malicious users of these systems. For example, fine-grained internet-wide search capabilities, such as powerful reverse image search or forms of writing style detection, which allow individuals to be identified and tracked across online platforms, or sensitive personal characteristics to be inferred, further eroding individual privacy[references 597, 598]. Large language models could also enable more efficient and effective search for sensitive information on the internet, or in breached datasets. General-purposeAI-generated content, such as non-consensual deepfakes, could be used to manipulate or harm individuals, raising concerns about the harm caused by the malicious use of personal data and the erosion of trust in online content[references 255, 256, 373, 599]. General-purposeAImodels are usually trained on large data sets sourced online, giving rise to concerns over breaches of copyright, lack of creator compensation, and the potential for economic disruption. Copyright laws aim to protect intellectual property and encourage written and creative expression[references 600, 601]. They grant the creators of original works the exclusive right to copy, distribute, adapt, and perform their own work. However, the third-party use of copyrighted data as training data may be legally permissible in certain circumstances, for instance on the basis of the 'fair use' exception in the US[reference 602], by the 'text and data mining' exception in theEU[reference 603], by the amended Copyright Act in Japan[reference 604], under Israeli copyright law[reference 605], and by the Copyright Act 2021 in Singapore[reference 606]. Beyond copyright, artists and other individuals sometimes feel their style, voice, and likeness are not sufficiently protected, which may implicate other forms of intellectual property such as trademarks and brands. Recent advances in general-purposeAIcapabilities have largely resulted from large-scale web scraping and aggregation of data to train general-purposeAImodels[references 607, 608], often containing copyrighted works, or used without consent from the data's creators. This applies to creative works including text, images, videos, and speech, and other modalities that are increasingly used to develop general-purposeAImodels. The extent to which this is legally permissible is complex and can vary by country. In the US, the fair use exception has been argued for in the case of training general-purposeAImodels[references 222, 609, 610, 611], as well as legally challenged[reference 612]. Many issues related to dataset creation and use across its lifecycle make copyright concerns for trainingAImodels very complicated[reference 613]. These issues include the questions of whether datasets are assembled specifically for machine learning or originally for other purposes[reference 614], whether the infringement analysis applies to model inputs or model outputs[reference 615], and issues of jurisdiction, among others[references 236, 616, 617]. This also presents questions on who is liable for infringement or harmful model outputs[reference 618]. While there are technical strategies for mitigating the risks of copyright infringement from model outputs, these risks are difficult to eliminate entirely[references 619, 620]. As general-purposeAIsystems become more capable, they increasingly have the potential to disrupt labour markets, and in particular creative industries[references 250, 621], (also see4.3.1 Labour market risks). The legal determinations regarding copyright infringement in theAItraining phase will affect the ability for general-purposeAIdevelopers to build powerful and performant models. They may also impact data creators' ability to exert control over their data, which may disincentivize creative expression. An unclear copyright regime disincentivizes general-purposeAIdevelopers from improving data transparency. Transparency about general-purposeAImodel training data is useful for understanding various potential risks and harms of a general-purposeAIsystem[reference 259]. However, this type of transparency is often lacking for major general-purposeAIdevelopers[reference 244]. Fears of legal risk, especially over copyright infringements, may disincentivise these developers from disclosing their training data[reference 622]. The infrastructure to source and filter for legally permissible data is under-developed, making it hard for developers to comply with copyright law. The permissibility of using copyrighted works as part of training data without a licence is an active area of litigation. Tools to source and identify available data without copyright concerns are limited. For instance, recent work shows that ~60% of popular datasets in the most widely used openly accessible dataset repositories have incorrect or missing licence information[reference 236]. Similarly, there are limitations to the current tools for discerning copyright-free data in web scrapes[references 607, 623]. However, practitioners are developing new standards for data documentation and new protocols for data creators to signal their consent for use in trainingAImodels[references 235, 624]. Risk factors are distinct from risks. They are conditions that increase the likelihood and/or impact of risks occurring. This section covers 7 cross-cutting technical risk factors i.e.- factors that each contribute to multiple general-purposeAIrisks. General-purposeAIsystems can be applied in many ways and contexts, making it hard to test and assure their trustworthiness across all possible use cases. The relative safety of general-purposeAIsystems depends on the context in which they are used. General-purposeAIsystems' outputs are often open-ended, such as free-form dialogue or code generation. This makes it difficult to design safe systems because it is not tractable to exhaustively evaluate all possible downstream use cases. Users can also 'jailbreak' general-purposeAImodels to make them comply with potentially harmful requests (see5.2.3 Improving robustness to failures). At present, computer scientists are unable to give guarantees of the form 'System X will not do Y' about general-purposeAIsystems[reference 625]. As discussed in3. Methodology to assess and understand general-purposeAIsystems, assessing the risks of general-purposeAIsystems in real-world applications and making strong assurances against general-purposeAI-related harms is extremely difficult with current methods. General-purposeAIdevelopers have a highly limited understanding of how general-purposeAImodels and systems function internally. A key feature of general-purposeAIsystems is that their capabilities are mainly achieved through learning rather than from top-down design. As a result, unlike most human-engineered systems, state-of-the-art general-purposeAImodels do not come with blueprints, and their structure does not conform to common design principles. This gives rise to concerns around understanding or explaining general-purposeAI, which are used interchangeably in this report to refer to the ability to provide human-understandable accounts of how general-purposeAIarrives at outputs and decisions from inputs and objectives. There are different views around what constitutes a human-understandable account. A thorough discussion of these views lies outside the scope of this report, but key questions include: Currently, scientists' understanding of general-purposeAIsystems is more analogous to that of brains or cells than aeroplanes or power plants. Some researchers believe that it may be possible to develop general-purposeAIsystems that can be proven safe, or are 'safe by design'[reference 626]by focusing the validation on interpretable outputs of the neural networks rather than their internal states, which humans currently cannot understand. However, it has not yet been possible for scientists to achieve such quantitative safety guarantees for state-of-the-art general-purposeAImodels[reference 58]. Research on thoroughly understanding howAIsystems operate has been limited to 'toy' systems that are much smaller and less capable than general-purposeAImodels[references 627, 628, 629, 630], or are unreliable and require major simplifying assumptions[references 631, 632*]. In practice, techniques for interpreting the inner workings of neural networks can be misleading[references 213, 288*, 289, 290, 336], and can fail sanity checks or prove unhelpful in downstream uses[references 218, 297, 298, 299]. As discussed in3. Methodology to assess and understand general-purposeAIsystems, these research methods are being developed, and new improvements might yield further insights, especially with sufficient investment in further research. However, it is unclear yet whether interpreting the inner structures of neural networks will offer sufficient safety assurances. Ensuring that general-purposeAIsystems pursue the goals intended by their developers and users is difficult. Although general-purposeAIsystems can appear to excel at learning what they are 'told' to do, their behaviour may not necessarily be what their designers intended[references 489, 633, 634, 635]. Even subtle differences between a designer's goals and the incentives given to a system can lead to unexpected failures. For example, general-purposeAIchatbots are often trained to produce text that will be rated positively by human evaluators, but user approval is an imperfect proxy for user benefit: widely-used chatbots can learn to pander to users' biases instead of prioritising truth[references 237, 238]. Even when a general-purposeAIsystem receives correct feedback during training, it may still develop a solution that does not generalise well when applied to novel situations during new situations once deployed[references 636, 637, 638]because the training data may not adequately represent real-world scenarios. For example, some researchers have found that chatbots are more likely to comply with harmful requests in languages that are under-represented in their training data[reference 309]. See4.2.3. Loss of controland5.2. Training more trustworthy modelsfor further discussion of these challenges. Because general-purposeAIsystems can proliferate rapidly, like other software, a new fault or harmful capability can rapidly have a global and sometimes irreversible impact. A small number of proprietary and freely available (open-source) general-purposeAImodels reach many millions of users (4.3.3.Market concentration risks and single points of failure). Both proprietary and open-source models can therefore have rapid and global impacts when they are released, although in different ways. Once a model is made available open-source, there is no practical way to erase the model from the market in case it has faults or capabilities that enable malicious use[reference 639](see4.1. Malicious use risks). For model faults, however, open-sourcing a model allows a much greater and more diverse number of practitioners to discover them which can improve the understanding of risks and possible mitigations[reference 640](see3. Methodology to assess and understand general-purposeAIsystems). Developers or others can then repair faults and encourage users to update to a new model version. Furthermore, open-sourcing models allows more actors to customise these models. If one version of the model has flaws, other customised versions may not share the same issues[reference 640]. However, neither repairing model faults nor customising can prevent deliberate malicious use[reference 639]. The risk of deliberate malicious use depends on a model's marginal risk compared to available closed source models and other technologies such as internet search[reference 640](see4.1. Malicious use risks). The above factors are relevant to the specific possibility of rapid, widespread, and irreversible impacts of general-purposeAImodels, but this report does not provide an assessment of the overall impacts of open-source models. Even when a system is not open-sourced, its capabilities can still be accessed by a wide user base. Within 2 months of launch, ChatGPT had over 100 million users and set a record for the fastest-growing user base of any consumer application[reference 641]. Each time a general-purposeAIsystem is updated, a new version of it rapidly reaches a large user base, so any vulnerabilities or harmful tendencies can potentially have a global impact quickly (see also4.3.3. Market concentration and single points of failure.) Despite attempting to debug and diagnose, developers are not able to prevent even overtly harmful behaviours across all circumstances in which general-purposeAIsystems are used. Empirically, harmful behaviours have included revealing private or copyrighted information[references 221, 642*, 643]; generating hate speech[references 225, 644]; spreading social and political biases[references 239, 455, 645]; pandering to user biases[reference 238]; hallucinating inaccurate content[references 46, 47*, 49]; exhibiting vulnerabilities to a variety of attacks on their safety protections[references 108, 208, 209, 210, 211, 309, 646, 647, 648*]; and assisting in overtly harmful tasks[references 368, 649, 650]. Users can circumvent general-purposeAImodel safeguards with relative ease[references 650, 651], for example through 'jailbreaking' techniques (see3. Methodology to assess and understand general-purposeAIsystems). While some have called for safety measures that rule out all overtly harmful behaviours across all situations[reference 626], current general-purposeAIdevelopment fails to meet a lower standard than this: ruling out any specific overtly harmful behaviour across foreseeable situations (such as situations where users try to jailbreak a model.) Today, general-purposeAIsystems are primarily used directly as tools by humans. For example, a human may ask a chatbot to write computer code to help them accomplish a task, which naturally requires a 'human in the loop'. However, developers are increasingly designing systems that allow general-purposeAIsystems to act autonomously, by controlling software tools such as a web browser or controlling the execution of code rather than only writing code. This enables some forms of reasoning about problems, constructing plans, and executing plans step-by-step[references 13, 26, 188, 189, 652, 653, 654, 655*, 656]. Such systems have included web-browsing virtual agents[reference 657*], research assistants[reference 415], and writing, fixing, and running code autonomously[reference 441]. The main purpose of general-purposeAI'agents' is to reduce the need for human involvement and oversight, allowing for faster and cheaper applications of general-purposeAI. This property also reduces human oversight, potentially increasing the risk of accidents (see4.1 Malicious use risks), and allowing the automation workflows for malicious uses (see4.2.1 Risks from product functionality issues use) while also being relevant to the risk of loss of control (see4.2.3 Loss of control)[references 481*, 658]. General-purposeAIagents have early forms of many autonomous capabilities, but lack reliability at performing complex tasks autonomously. Current state-of-the-art general-purposeAIsystems are capable of autonomously executing many simple tasks, but some evaluations have shown that they struggle with more complex ones[references 183, 509]. They are particularly unreliable at performing tasks that involve many steps. As of 2023, general-purposeAIagents scored low in benchmarks designed to measure their performance on complex and economically useful tasks[references 183, 441]. However, given current efforts and growing investment in developing general-purposeAIsystems with greater autonomy, the capabilities of general-purposeAIagents could continue to increase. Although current general-purposeAIagents are unreliable, advancement has been rapid. For example, for 2 challenging benchmarks for general-purposeAIagents that measure general problem-solving[reference 183]and autonomous software engineering[reference 441]accuracy has improved over a period of a few months by factors of 2.2x and 7.1x respectively compared to strong baselines using GPT-4. The performance of the general-purposeAImodels (such asLLMs) which underlie these general-purposeAIagents is key to how reliable they are, so newer generations of general-purposeAImodels typically increase agent capabilities. One potential way to increase agent capabilities could be through combiningLLMswith search and planning methods. In board games like Go and Stratego, combining deep learning with methods like Monte Carlo Tree Search (MCTS) and self-play has led to above-human level performance[references 659, 660]. Early work on combiningLLMswith search has yielded improvements in simple settings[reference 661]. However, careful monitoring of general-purposeAIagent capabilities is needed to assess many of the risks discussed in4. Risks. Risk factors are distinct from risks - they are conditions that increase the likelihood and/or impact of risks occurring. There are societal aspects of general-purposeAIdevelopment and deployment that increase not one but several general-purposeAIrisks. This section discusses these 'cross-cutting societal risk factors'. General-purposeAIdevelopers, who are competing for market share in a dynamic market where getting a product out quickly is vital, may have limited incentives to invest in mitigating risks. The one-time cost of developing a state-of-the-art general-purposeAImodel is very high, while the marginal costs of distributing such a model to (additional) users are relatively low. This can lead to 'winner takes all' dynamics, creating strong incentives for developers to build the most capable model at any cost, because doing so might allow immediately capturing a large market share. In recent years, there has been intense competition between general-purposeAIdevelopers to rapidly build and deploy models. This has raised concern about potential 'race to the bottom' scenarios, where actors compete to develop general-purposeAImodels as quickly as possible while under-investing in measures to ensure safety and ethics[references 662, 663]. This could contribute to situations in which it is challenging for general-purposeAIdevelopers to commit unilaterally to stringent safety standards, as doing so might put them at a competitive disadvantage[reference 664]. Similar dynamics can also occur at the international level regarding regulation efforts. Without global coordination regarding the regulation of general-purposeAI, a regulatory 'race to the bottom' could see countries attempt to attractAIcompanies through lax regulation that might be insufficient for ensuring safety domestically and abroad, a dynamic that has been described for several types of regulation like labour law[reference 665]. As general-purposeAImarkets advance rapidly, regulatory or enforcement efforts can struggle to keep pace. A recurring theme in the discourse on general-purposeAIrisk is the mismatch between the pace of technological innovation and the development of governance structures[reference 666]. While existing legal and governance frameworks apply to some uses of general-purposeAIsystems and several jurisdictions (like the European Union, China, the USA or Canada) have initiated or completed efforts to regulateAIand general-purposeAIspecifically, there often remain regulatory gaps. In a market that is as fast-moving as the general-purposeAImarket currently is, it is very difficult to fill such gaps ex-post, because by the time a regulatory fix is implemented it might already be outdated. Policymakers therefore face the challenge of creating a flexible regulatory environment that ensures the pace of general-purposeAIdevelopment and deployment remains manageable from a public safety perspective. General-purposeAIsystems' inherent lack of transparency makes legal liability hard to determine, potentially hindering governance and enforcement. How current legal frameworks apply in cases where it is suspected that a general-purposeAIsystem caused harm is often unclear. This raises many issues for accountability, liability and justice. In principle, people and corporate entities are held accountable, not the technology, which is why many critical areas maintain a 'human in the loop' policy. However, tracing harm back to the responsible individuals who developed or deployed theAIis very challenging[references 667, 668, 669], as is gathering evidence of error. This accountability issue is compounded by the opaque nature of proprietary general-purposeAImodels, where commercially sensitive training data, methodologies, and decision-making processes are usually not open to public scrutiny and there is a lack of widely shared standard operating procedures for using and interpretingAIsystems[references 291, 292, 293, 294*, 295, 670, 671]. Some also argue that general-purposeAIsystems can exhibit 'emergent' behaviours that were not explicitly programmed or intended by their developers, raising questions about who should be held liable for resulting harm. The distributed nature of general-purposeAIdevelopment, involving multiple actors such as data providers, model trainers, and deployers, also makes it challenging to assign liability to a single entity[reference 669]. It is very difficult to track how general-purposeAImodels and systems are trained, deployed and used. Tracking the use of general-purposeAImodels and systems is not only important for establishing liability for potential harms caused by the use of general-purposeAImodels and systems, but also for monitoring and evidencing malicious use, and noticing malfunctions[references 658, 672, 673]. Comprehensive safety governance is common in safety-critical fields such as automotive, pharmaceuticals, and energy[references 674, 675, 676, 677], but it often relies on broadly accepted standards that are currently missing in general-purposeAIgovernance. This section of the report discusses technical approaches to increase general-purposeAIsafety through mitigating general-purposeAI-related risks: to reduce the scale of harms or the likelihood of their occurrence. This report ultimately finds that, while there are many technical approaches that reduce risk, existing methods are insufficient to prove that systems are safe. The scope of this report does not include nontechnical (political, legal, or social) interventions, but these are equally important for addressing risks. Moreover, technical and nontechnical aspects of managing risks from general-purposeAIare highly intertwined; no technical solutions are implemented in a vacuum. The last months and years have seen increased interest inAIregulation from policymakers, and several jurisdictions (like the European Union, China, the USA or Canada) have initiated or completed efforts to regulateAIand general-purposeAIspecifically. Effective approaches will require the resources and political will to implement technical solutions as well as an interplay between multiple technical and nontechnical safeguards against harms from general-purposeAI. Riskis the combination of the probability of an occurrence of harm and the severity of that harm if it occurs[reference 330]. This technically includes both positive and negative outcomes, but in common usage, the focus is on the negative outcomes. Some (but not all) of the common risks associated with general-purposeAIare described in4. Risks. Risk increases with the severity of the potential harm and the probability of the harm materialising Whether and to what extent the outcomes of a system are considered undesirable has to be conceptualised with respect to contextual human values. Various stakeholders may disagree how undesirable any particular outcome is. Risk surface/exposure: The risk surface of a technology consists of all the ways it can cause harm through accidents or malicious use. The more general-purpose a technology is, the more extensive its risk exposure is expected to be. General-purposeAImodels can be fine-tuned and applied in numerous application domains and used by a wide variety of users (4.4.1. Cross-cutting technical risk factors), leading to extremely broad risk surfaces and exposure, challenging effective risk management. Risk managementconsists of the identification, assessment, and prioritisation of risks and utilising resources to minimise, monitor, and control high-priority risks. System safety engineeringis defined very similarly but with an emphasis on the importance of the interactions of multiple parts of a larger system[reference 678]. In the case ofAI, this approach entails taking into account all the constituent parts of a general-purposeAIsystem, as well as the broader context in which it operates. Industries such as finance, insurance, health, and cybersecurity have well-established risk management practices[reference 679]. TheAIrisk management framework by NIST[reference 680]is among the few prominent recent efforts to come up with a framework of risk management specific toAIsystems. When the scope of applicability and use of anAIsystem is narrow (e.g., consider spam filtering as an example), salient types of risk (e.g., the likelihood of false positives) can be measured with relatively high confidence. In contrast, assessing general-purposeAImodels' risks, such as the generation of toxic language, is much more challenging, in part due to a lack of consensus on what should be considered toxic and the interplay between toxicity and contextual factors (including the prompt and the intention of the user). There is a broad range of risk assessment techniques that could and are already being used with regard to general-purposeAI[references 216, 679], including evaluations, red-teaming, auditing, and qualitative evaluation of general-purposeAIsystems (see3. Methodology to assess and understand general-purposeAIsystems). Other risk methods for assessment, some of which draw on established practices in other fields, include: Current risk assessment methodologies often fail to produce reliable assessments of the risk posed by general-purposeAIsystems. Some of the key challenges of utilising such risk assessment methodologies for highly capable models are: Red teaming, for example, only assesses whether a model can produce some output, not the extent to which it will do so in real-world contexts nor how harmful doing so would be. Instead, they tend to provide qualitative information that informs judgments on what risk the system poses. There are a range of risk assessment techniques that could and are already being used with regard to general-purposeAI(see3. Methodology to assess and understand general-purposeAIsystems). To address the limitations of these existing tools, researchers can look to established practice in risk management in other domains. Some of the common risk management tools in other safety-critical industries are: Similar ideas have been proposed for managing the risks associated with general-purposeAIsystems. Many of these existing approaches are not directly applicable to highly capable general-purposeAImodels, or their efficacy is not well-studied, but efforts are underway to extend existing guidelines to GenerativeAI. Safety and reliability engineering: The practice of safety engineering has a long history in various safety-critical engineering systems, such as the construction of bridges, skyscrapers, aircraft, and nuclear power plants. At a high level, safety engineering assures that a life-critical system acts as intended and with minimal harm, even when certain components of the system fail. Reliability engineering is broader in scope and address non-critical failures as well. These approaches offer several techniques that could be useful for risk assessment in general-purposeAI: However, translating best practices from these fields to general-purposeAIis difficult. Quantitative risk assessment methodologies for general-purposeAIare very nascent and it is not yet clear how quantitative safety guarantees could be obtained. Experience of other risk assessments in general-purposeAIsuggests that many areas of concern may not be amenable to quantification (for example, bias and misinformation). If quantitative risk assessments are too uncertain to be relied on, they may still be an important complement to inform high-stakes decisions, clarify the assumptions used to assess risk levels and evaluate the appropriateness of other decision procedures (e.g. those tied to model capabilities). Further, 'risk' and 'safety' are contentious concepts - for instance, one might ask 'safe to whom?' - which may require the involvement of diverse sets of experts and potentially impacted populations[reference 307]. While there are currently no well-established safety engineering practices for general-purposeAIsystems, the pipeline-aware approach to mitigatingAI's harm takes inspiration from safety engineering and proposes scrutinising numerous design choices made through the general-purposeAIlifecycle, from ideation and problem formulation, to design, development, and deployment, both as individual components and in relation to one another[references 684, 685]. Further work is needed to extend these ideas from traditionalAIto generativeAI. Safety cases: Developers of safety-critical technologies such as aviation, medical devices, and defence software are required to make safety cases, which put the burden of proof on the developer to demonstrate that their product does not exceed maximum risk thresholds set by the regulator[references 686, 687, 688, 689]. A safety case is a structured argument supported by evidence, where the developer identifies hazards, models risk scenarios, and evaluates the mitigations taken. Safety cases would be easier to make for general-purposeAIsystems with limited capabilities since less capable models often pose less risk. Thereby, they are robust to scenarios of both slow and rapid progress in general-purposeAIcapabilities (see2.4 Capability progress in coming years). Safety cases leverage the technical expertise of the technology developer but still require that the regulator (or a suitable third party) has the technical expertise to evaluate safety cases. Safety cases often address only a subset of risks and threat models, leaving out important ones[references 690, 691]. One mitigation to these limitations is to review safety cases alongside risk cases produced by a red team of third-party experts[reference 689]. The 'Swiss-cheese' model for general-purposeAIsafety engineering: The general-purpose, rapidly evolving, and inscrutable nature of highly capable models makes it increasingly difficult to develop, assess, and incentivise systematic risk management practices. Effectively managing the risks of highly capable general-purposeAIsystems might therefore require the involvement of multiple stakeholder groups, including experts from multiple domains and impacted communities, to identify and assess high-priority risks. Further, it suggests that no single line of defence should be relied upon. Instead, multiple independent and overlapping layers of defence against those risks may be advisable, such that if one fails, others will still be effective. This is sometimes referred to as the Swiss Cheese model of defence in depth[reference 692]. Current risk management among general-purposeAIdevelopers: Though not universally adhered to, it is common practice to test models for some dangerous capabilities ahead of release, including via red-teaming and benchmarking, and publishing those results in a 'model card'[reference 257]. Further, some developers have internal decision-making panels that deliberate on how to safely and responsibly release new systems. An increasingly common practice among these developers is to constrain decisions through voluntary pre-defined capabilities thresholds[references 693, 694]. Such thresholds determine that specific model capabilities must be met with specific mitigations that are meant to keep risks to an acceptable level[reference 682]. Such capabilities thresholds have the advantage of being observable in advance of capabilities being developed. However, more work is needed to assess whether adhering to some specific set of thresholds indeed does keep risk to an acceptable level and to assess the practicality of accurately specifying appropriate thresholds in advance. 'AIalignment' refers to the challenge of making general-purposeAIsystems act in accordance with their developer's goals and interests (see5.4 Technical approaches to fairness and representation in general-purposeAIsystemsfor a discussion of the challenges to alignment posed by conflicting values of different stakeholders). There are 2 challenges involved in training aligned general-purposeAIsystems: firstly, ensuring that they are trained with an objective that incentivises the intended goals; and secondly, ensuring that the outputs translate from their training contexts to the real world as intended, especially in high-stakes situations. It is challenging to precisely specify an objective for general-purposeAIsystems in a way that does not unintentionally incentivise undesirable behaviours. Currently, researchers do not know how to specify abstract human preferences and values in a way that can be used to train general-purposeAIsystems. Moreover, given the complex socio-technical relationships embedded in general-purposeAIsystems, it is not clear whether such specification is possible. General-purposeAIsystems are generally trained to optimise for objectives that are imperfect proxies for the developer's true goals[reference 634]. For example,AIchatbots are often trained to produce text that will be rated positively by human evaluators, but user approval is an imperfect proxy for user benefit. Research has shown that several widely-used chatbots sometimes match their stated views to a user's views regardless of truth[reference 238]. This is an ongoing challenge for general-purposeAIand similarAIsystems[references 489, 633, 634, 695*]. Ensuring general-purposeAIsystems learn behaviours that translate from their training contexts to real-world, high-stakes deployment contexts is also highly challenging. Just as general-purposeAIsystems are trained to optimise for imperfect proxy goals, the training context can also fail to adequately represent the real-world situations they will encounter after they are deployed. In such cases, general-purposeAIsystems can still learn to take harmful actions even if they are trained with correct human-provided feedback[references 636, 637, 638]. For example, some researchers have found that chatbots are more likely to take harmful actions in languages that are under-represented in their training data[reference 309]. Using more multilingual data and oversight may be able to more mitigate this type of failure. To elicit the desired behaviours from state-of-the-art general-purposeAIsystems, developers train models using human oversight. Improving performance in real-world contexts benefits from large amounts of data. State-of-the-art alignment techniques rely on feedback or demonstrations from humans and, as such, are constrained by human error and bias. As discussed in2.1. How does General-PurposeAIgain its capabilities?, developers fine-tune state-of-the-art general-purposeAIsystems using a large amount of human involvement. In practice, this involves techniques that leverage human-generated examples of desired actions[reference 18]or human-generated feedback on examples from models[references 19, 20, 21*, 303]. This is done at scale, making it labour-intensive and expensive. However, human attention, comprehension, and trustworthiness are not perfect[reference 303], which limits the quality of the resulting general-purposeAIsystems[references 696, 697*, 698]. Even slight imperfections in feedback from humans can be amplified when used to train highly capable systems with potentially serious consequences (see4.1. Malicious use risksand4.2.3. Loss of control). Improving the quality and quantity of human oversight can help to train more aligned models. Some research has shown that using richer, more detailed forms of feedback from humans can provide a better oversight signal, but at the cost of increased time and effort for data collection[references 699, 700, 701]. To gather larger datasets, leveraging general-purposeAIsystems to partially automate the feedback process can greatly increase the volume of data[references 158, 702]. However, in practice, the amount of explicit human oversight used during finetuning is very small compared to the trillions of data points used in pre-training on internet data and may, therefore, be unable to fully remove harmful knowledge or capabilities from pre-training. Short of rethinking the way state-of-the-art general-purposeAIsystems are trained, improving fine-tuning feedback data is unlikely to be a solution on its own. Maintaining uncertainty over goals can reduce risky actions. Some researchers have proposed methods that involve incorporating uncertainty into the goals that general-purposeAIsystems learn to pursue[references 703, 704, 705, 706*, 707, 708]. By requiring general-purposeAIsystems to act in a way that respects uncertainty about their objectives, these methods can reduce the risk of unexpected actions and encourage information-seeking or deference to humans in response to ambiguity. However, these methods have yet to be incorporated into state-of-the-art, highly capableAI. Some researchers are working toward safe-by-design approaches which might be able to provide quantitative safety guarantees. Similar to the above approaches to estimate uncertainty about goals and predictions, it may be possible to designAIsystems that are constructed to achieve quantified levels of safety[reference 626]. The advantage of mathematical guarantees and bounds is that they may provide safety assurances even outside of the domain where theAIhas been trained and tested, in contrast with empirical trial-and-error methods that are currently the standard for designing deep learning systems. Currently, however, practically-useful, provable guarantees of safety are not possible with general-purposeAImodels and methods, and many open questions however remain in order to achieve those objectives for large-scaleAIsystems. It is unclear if and how humans might be able to oversee general-purposeAIsystems with capabilities exceeding those of humans. Future general-purposeAImodels or systems that surpass the abilities of human experts across many or all domains would pose a particularly difficult challenge (see4.2.3. Loss of control). Some research efforts, especially concentrated in the leadingAIlaboratories, study the extent to which humans might be able to oversee general-purposeAIwith capabilities that exceed the human overseers generally or in a given domain (known as 'scalable oversight'). Some empirical research has studied the ability of less capable general-purposeAIsystems to oversee more capable ones under the assumption that similar dynamics may exist for general-purposeAIexceeding human capabilities[references 709, 710]. Researchers have also proposed theoretical approaches that can, under certain assumptions, allow for stronger assurances than existing methods[references 711, 712, 713]. However, published research on these methods is highly preliminary. The hallucination of falsehoods is a challenge, but it can be reduced. InAI, 'hallucination' refers to the propensity of general-purposeAIsystems to output falsehoods and made-up content. For example, language models commonly hallucinate non-existent citations, biographies, or facts[references 46, 47*, 48, 49, 50], which could pose legal and ethical problems involving the spread of misinformation[reference 714]. It is possible but challenging to reduce general-purposeAIsystems' tendency to hallucinate untrue outputs. Fine-tuning general-purposeAImodels explicitly to make them more truthful - both in the accuracy of their answers and analysis of their competence - is one approach to tackling this challenge[reference 715*]. Additionally, allowing general-purposeAIsystems to access knowledge databases when they are asked to perform tasks helps to improve the reliability of language model generations[references 716, 717]. Alternative approaches attempt to detect hallucinations rather than remove them from the model, and inform the user if the generated output is not to be trusted[reference 718]. However, reducing hallucination remains a very active area of research. Sometimes, unfamiliar inputs a general-purposeAIsystem encounters in deployment can cause unexpected failures[reference 719], and users or attackers can construct inputs that are specifically designed to make a system fail[reference 720]. Adversarial training helps improve robustness in state-of-the-artAIsystems. 'Adversarial training' involves first, constructing 'attacks' designed to make a model act undesirably and second, training the system to handle these attacks appropriately. Attacks againstAIsystems can take many forms and can be either human- or algorithm-generated. Once an adversarial attack has been produced, training on these examples can proceed as usual. Adversarial training has become a principal method by which models are made more robust to failures[references 2, 3, 22, 204, 207, 721]. While adversarial training is a valuable tool, it is not sufficient by itself[reference 219]. Making systems more robust to unforeseen classes of failure modes is a challenging open problem. Adversarial training generally requires examples of a failure to fix it,[references 722*, 723]. These limitations have resulted in ongoing games of 'cat and mouse' in which some developers continually update models in response to newly discovered vulnerabilities. A partial solution to this problem is to simply produce and train on more adversarial examples. Automated methods for generating attacks can help scale up adversarial training[references 203, 210, 237]. However, the exponentially large number of possible inputs for general-purposeAIsystems makes it intractable to thoroughly search for all types of attacks. One way to address this by applying adversarial training to general-purposeAImodels' internal states, instead of inputs, has been proposed[reference 724], but research on this remains preliminary. Mathematical proofs to certify a model's robustness would theoretically be a way to cover all possible attacks[reference 626], but this is not possible with current models and methods. Adversarial training can sometimes harm a model's performance or robustness. In vision models, there is often a trade-off between robustness to adversarial attacks and performance on non-adversarial data[references 725, 726, 727]. Even when adversarial training is helpful to improve worst-case performance, it may not be used when it harms average case performance. Adversarial training can also sometimes make language models less robust to certain attacks that were not trained on[references 722*, 724]. However, some refined approaches to adversarial training may be able to improve trade-offs between performance on clean and adversarial data[references 728, 729, 730]. 'Machine unlearning' can help to remove certain undesirable capabilities from general-purposeAIsystems. For example, removing certain capabilities that could aid malicious users in making explosives, bioweapons, chemical weapons, and cyberattacks would improve safety[reference 408]. Unlearning as a way of negating the influence of undesirable training data was originally proposed as a way to protect privacy and copyright[reference 586]which is discussed in5.5 Privacy methods for general-purposeAIsystems. Unlearning methods to remove hazardous capabilities[references 731, 732]include methods based on fine-tuning[reference 733*]and editing the inner workings of models[reference 408]. Ideally, unlearning should make a model unable to exhibit the unwanted behaviour even when subject to knowledge-extraction attacks, novel situations (e.g. foreign languages), or small amounts of fine-tuning. However, unlearning methods can often fail to perform unlearning robustly and may introduce unwanted side effects[reference 734]on desirable model knowledge. Studying the inner workings of models can help to establish the presence or lack of specific capabilities. One technique that researchers use is to analyse a general-purposeAImodel's internal states to better understand what concepts they reason with and what knowledge they have[references 278, 279, 735]. For example, these approaches have been used to study features related to fairness in visual classifiers[reference 736]and what knowledge language models have[references 737, 738*]. However, methods for assessing a general-purposeAImodel's internal representations are imprecise[references 739, 740, 741]. They are also not currently competitively used over other types of evaluations for understanding a general-purposeAImodel's capabilities (see3. Methodology to assess and understand general-purposeAIsystems). Understanding a model's internal computations might help to investigate whether they have learned trustworthy solutions. 'Mechanistic interpretability' refers to studying the inner workings of state-of-the-artAImodels. However, state-of-the-art neural networks are large and complex, and mechanistic interpretability has not yet been useful and competitive with other ways to analyse models for practical applications. Nonetheless, some researchers have provided thorough investigations of how very small neural networks perform very simple tasks[references 628, 629, 630]. Some recent works have attempted more scalable techniques for designing human-interpretable models[references 282, 631, 632*, 742]. This type of approach cannot be used to rule out the possibility of harmful or unexpected actions, but it could offer a useful lens into how models work that could be useful for understanding how safe a model is. Instead of trying to interpret internal computations of neural networks, one could use neural networks to generate interpretable and verifiable explanations that could yield quantitative safety guarantees[reference 626], although how to do this efficiently remains an open problem. Understanding a model's internal workings can sometimes be used to guide edits to usefully change its behaviour. Despite the difficulty of understanding models' inner workings, some techniques can be used to guide specific edits to them. Compared to finetuning, these methods can sometimes be more compute- or data-efficient ways of modifying their functionality. Researchers have used a variety of methods for this, based on changes to their internal parameters[references 743, 744, 745, 746, 747, 748, 749], and neurons[references 275, 282, 750], or representations[references 281, 751, 752, 753, 754]. However, these techniques are imperfect[reference 299]and typically introduce unintended side effects on model behaviour[reference 755]. They remain an active area of research. Monitoring during general-purposeAIsystem deployment refers to the ongoing identification of risks, inspection of model actions, and evaluation of performance. Interventions prevent potentially harmful outputs. Whereas3. Methodology to assess and understand general-purposeAIsystemsdiscusses how systems are evaluated in order to allow for more informed decisions around their use, this section discusses how some techniques for monitoring and intervention can be built intoAIsystems themselves. Different strategies that researchers are developing for general-purposeAIsystem monitoring and intervention are discussed below, including detectingAI-generated content, detecting risky situations, identifying harmful actions, explaining model actions, and intervening to override or block them. Content generated by general-purposeAIsystems - particularly deepfakes - could have widespread harmful effects[references 756, 757](see4.1. Malicious use risks). The ability to distinguish between genuine and general-purposeAI-generated content to prevent the malicious use of generative models. Some unreliable techniques exist for detecting general-purposeAI-generated content. Just as different humans have unique artistic and writing styles, so do generativeAImodels. Some procedures have been developed to distinguishAI-generated text from human-generated text[references 374, 375, 379, 380, 758]and images[references 759, 760]. Detection methods are typically based on specialised classifiers or assessing how likely it is that a given example was generated by a specific general-purposeAImodel. However, existing methods are limited and are prone to false positives because general-purposeAIsystems tend to memorise examples that appear in their training data, so common text snippets or images of famous objects may be falsely identified as beingAI-generated. As general-purposeAI-generated content becomes even more realistic, it may be even more challenging to detect general-purposeAI-generated content. Watermarks make distinguishingAI-generated content easier, but they can be removed. A 'watermark' refers to a subtle style or motif that can be inserted into a file which is difficult for a human to notice but easy for an algorithm to detect. Watermarks for images typically take the form of imperceptible patterns inserted into image pixels[reference 761], while watermarks for text typically take the form of stylistic or word-choice biases[references 382, 762]. Watermarks are useful, but they are an imperfect strategy for detectingAI-generated content because they can be removed[references 374, 383]. However, this does not mean that they are not useful. As an analogy, fingerprints are easy to avoid or remove, but they are still very useful in forensic science. Watermarks can also be used to indicate genuine content. In contrast to inserting watermarks into general-purposeAI-generated content, a contrasting approach is to put encrypted watermarks in non-AI-generated content[reference 763]. However, this would require changes in the hardware and software of physical recording devices, and it is not clear whether these methods could be bypassed by tampering. Detecting anomalies and attacks on general-purposeAIsystems allows precautions to be taken when they are identified. Some methods have been developed that can help detect unusual inputs or behaviours fromAIsystems[references 764, 765]. Other technical approaches aim to detect when the model outputs for a given input are uncertain, which can indicate that there is an attack or a risk of false outputs[reference 766]. Once detected, these examples can be sent to a fault-handling process or flagged for further investigation. It is also sometimes possible to detect and filter a significant proportion of malicious attacks before they are passed into a general-purposeAImodel[references 723, 767]or detect potentially harmful outputs so that they can be blocked before they are sent to a user[references 768, 769]. Techniques to explain why deployed general-purposeAIsystems act the way they do are nascent and not widely applied yet, but there are some helpful methods. The actions of general-purposeAIsystems can be hard to understand. However, understanding the reasons why models act the way they do is important for evaluation and determining accountability for harms caused by general-purposeAIsystems[references 269, 770]. Unfortunately, simply asking general-purposeAIlanguage models for explanations of their decisions tends to produce misleading answers[reference 771]. To increase the reliability of model explanations, researchers are working on improved prompting and training strategies[references 772, 773]. Other techniques for explaining general-purposeAImodel actions[references 774, 775]have been shown to help with debugging[reference 207]. However, correctly explaining general-purposeAImodel actions is a difficult problem because the size and complexity of general-purposeAIsystems are beyond easy human understanding. State-of-the-art general-purposeAIsystems are trained to produce outputs that are positively reinforced - not to do so for the desired reasons or in a self-consistent way. As discussed in5.1. Risk management and safety engineering, although there is no perfect safety measure, having multiple layers of protective measures and redundant safeguards in place increases the level of assurance. Once detected, interventions can identify and protect against potentially harmful actions from deployed general-purposeAIsystems. Having a human in the loop allows for direct oversight and manual overrides. Humans in the loop are expensive compared to automated systems. However, in high-stakes decision-making situations, they are essential. Instead of teaching general-purposeAIsystems to act on behalf of a human, the human-AIcooperation paradigm aims to combine the skills and strengths of both general-purposeAIsystems and humans and is seen as generally preferable in complex situations with potentially harmful ramifications[references 703, 776, 777*, 778, 779]. However, having a human in the loop is not practical in many situations, when decision-making happens too quickly and cannot be slowed down (such as chat applications with millions of users), when the human does not have sufficient domain knowledge, or when human bias or error can exacerbate risks[reference 780]. As a result, humans in the loop can only be useful in certain situations. Automated processing and filtering methods can offer additional but generally imperfect layers of protection. Some cyber-attacks against general-purposeAIsystems often take the form of subtle, brittle patterns in their inputs. As a result, researchers have developed input pre-processing methods that can remove these patterns[references 723, 781, 782, 783, 784, 785]. Sometimes attempted attacks can be detected and blocked before they are passed to a general-purposeAImodel[references 723, 767], while harmful outputs may be detected before they are sent to a user[references 768, 769]. These methods can significantly reduce risks from some failures, but can be vulnerable to attacks. Secure interfaces can be designed for general-purposeAIsystems with potentially dangerous capabilities. General-purposeAIsystems that can act autonomously and open-endedly on the web or in the physical world pose elevated risks (see4.4.1 Cross-cutting technical risk factors). For general-purposeAIsystems with highly risky capabilities, limiting the ways in which they can directly influence people or objects is a proposed way to reduce potential risks[references 625, 786]. Fairness has no universally agreed-upon definition, and varies according to the cultural, social, and disciplinary contexts[references 333, 787, 788, 789, 790]. Philosophically, fairness entails a deep ethical reflection on principles, values, and the distribution of resources and opportunities, while legal definitions may hinge on constitutional principles and case law. Achieving consensus on fairness proves to be elusive due to its multifaceted nature, necessitating engagement with diverse perspectives and context-specific factors. The rise ofAIapplications brings algorithmic fairness to the forefront as a significant concern. Fairness inAIattempts to correct algorithmic bias in automated decision-making or content generation.AIfairness can be defined and measured in various ways (such as 'individual fairness' versus 'group fairness')[reference 791]which is proved appropriate depending on the context and the specific goals of the application[reference 333], complicating the assessment ofAImodels. A classic example of this type of dilemma from the algorithmic decision-making literature is that the COMPAS software used to predict criminal recidivism is a well-known case of an unfairAImodel. By some measures, COMPAS was found to be biased against African Americans while by other measures it was not[references 788, 790]. When anAImodel is unfair, the actions that it takes are biased, harming individuals or communities. Bias inAIrefers to unjustified favouritism towards or against entities based on their inherent or acquired characteristics[reference 788]. Bias is tightly intertwined with representation and fairness. The effectiveness of data-driven algorithms depends on the quality of the data they utilise. However, datasets often fail to adequately represent minorities, which can be reflected and amplified in theAItrained on these datasets[reference 227]. Biases inAIsystems can lead to social harm, including unequal resource allocation and discriminatory decisions against marginalised groups[reference 792]posing significant challenges across various domains[references 793, 794, 795]. Researchers deploy a variety of methods to mitigate or remove bias and improve fairness in general-purposeAIsystems[references 2, 22], including pre-processing, in-processing, and post-processing techniques[references 796, 797]. Pre-processing techniques analyse and rectify data to remove inherent bias existing in datasets, while in-processing techniques design and employ learning algorithms to mitigate discrimination during the training phase of the system. Post-processing methods adjust general-purposeAIsystem outputs once deployed. There is no single technique that can ensure fairness in every situation or outcome. Consequently, leadingAIcompanies utilise a combination of these approaches to iteratively improve fairness in their general-purposeAIsystems[references 20, 825*]. Importantly, increasing meaningful representation and participation can help to reduce the risk that under-represented groups are excluded. From a societal perspective, theAIalignment problem discussed in5.2.1 Aligning general-purposeAIsystems with developer intentionsis ill-defined. It is not possible to make a general-purposeAIsystem represent the values of everyone in a diverse society where people sometimes disagree[references 239, 307, 332]. Increased participation[reference 826], representation[reference 827], and dialogue[reference 332]have been proposed as ways to reduce the risk that alignment to some people's interests will be harmful to others. However, some forms of participation can fail to be meaningful, and cannot fully solve challenges posed by disagreements between different people[reference 331]. It is debated whether general-purposeAIsystems can ever be completely 'fair'. There are arguments both for and against its feasibility. Mathematical results suggest that it may not be possible to satisfy all aspects of fairness simultaneously under reasonable assumptions[references 828, 829, 830, 831]. This impossibility theorem of fairness is supported by results indicating the complexity of training unbiased general-purposeAImodels[references 832, 833]. Many desirable properties involve trade-offs, such as the 4-way trade-off among system fairness, accuracy, privacy, and efficiency[reference 834]. Studies suggest there is a trade-off between fairness and other values such as privacy[references 821*, 834]and predictive accuracy in general-purposeAIsystems[references 829, 835, 836]. A possible example is Google Gemini, which generated images of indigenous people and women of colour as US senators from the 1800s, and 'ethnically diverse' World War II-era German soldiers. These contents factually misrepresent the history, possibly as a result of an attempt to ensure racial diversity that failed to foresee and adjust to these specific use cases. To avoid prematurely prioritising specific aspects that inadvertently reflect the personal values of the stakeholders, technology can use quantitative and qualitative measures comprehensible to important technologists, helping them make well-informed decisions about trade-offs, which can then be enforced by the developed systems. The counter argument is that, while theoretical limitations exist, practical solutions are attainable[references 837, 838]. Some researchers suggest that fairness definitions can be reconciled with one another practically[references 839, 840], and it is possible to simultaneously satisfy multiple fairness criteria at least to a greater extent than they typically are[reference 841]. Empirical evidence challenges the idea that there is always a non-negligible trade-off between fairness and accuracy, indicating such trade-offs can often be resolved in practice. These studies suggest that reducing disparities in general-purposeAIsystem outputs may not necessarily entail significant drops in accuracy, or require complex methods[references 838, 839, 840]. Despite rigorous and comprehensive training and testing efforts, establishing general-purposeAIsystems that are fair across all measures and across different cultural, social and scientific contexts remains challenging. No existing measure can entirely eliminate all potential risks of bias and unfairness which are inherent in the development of highly capableAIsystems[reference 837]. Nevertheless, there exists the possibility of striving for continual refinement towards fairer systems. Despite all the efforts to remove bias from general-purposeAIsystems, major challenges have remained. Firstly, how fairness should be defined and measured is debated[references 802, 842]. The line between useful and accurate world knowledge, and reinforcing harmful stereotypes, can be difficult to draw, and the perception of bias may vary depending on the situation[references 227, 796]. Secondly, other aspects of ensuring general-purposeAIsystem safety can create or amplify bias: for instance, cleaning data to mitigate toxicity and privacy leakage can change the demographic distribution of the datasets, leading to more bias[reference 843]. Thirdly, some issues such as intersectional bias remain difficult to address[reference 844]; for instance, a general-purposeAIsystem might be fair to Asians and women separately, yet be biased toward Asian women. Finally, mitigation of bias requires ongoing effort throughout the development, deployment, and usage of the general-purposeAIsystems. Bias in its various forms can emerge gradually, requiring specialised detection and mitigation techniques. As described in4.3.5. Risks to privacy, advanced general-purposeAIsystems pose risks to people's privacy, such as loss of data confidentiality, transparency and control over how data is used, and new forms of privacy abuse. Existing technology and policy only partially address these threats. Current privacy-enhancing technologies do not scale to large general-purposeAIModels. While various privacy techniques can be applied toAImodels to protect individual privacy while still allowing for useful insights to be derived from data[references 845, 846], these techniques can significantly impair model accuracy, are hard to scale to large models, and may not be suitable for all use cases, in particular for general-purposeAImodels trained on text[reference 847]. For domains with highly sensitive data (e.g., medical or financial), it may be possible to attain strong privacy guarantees by adapting powerful general-purposeAImodels that are first pre-trained on publicly available data from the internet[references 848, 849], but such techniques have rarely been applied in production, to date. Another solution is using synthetic data to avoid using sensitive data in general-purposeAIsystems training pipelines. However, researchers have demonstrated that there is an important utility/privacy trade-off. If the synthetic data utility is high, then they may carry as much information as the original data and enable mostly the same attacks[references 850, 851, 852]. The confidentiality and data centralisation issues raised by general-purposeAIsystems could, in principle, be addressed using secure computation solutions such as cryptographic approaches[reference 853], federated learning[reference 854], and hardware protections[reference 855]. Existing techniques, however, have not been scaled to the largest and most capable models being trained today. These solutions also all impose costs which might be prohibitive at scale, although researchers are trying to find ways to reduce these costs. Advances in hardware protections for accelerators, a class of specialised hardware designed to accelerateAIapplications such as artificial neural networks and machine vision, could in future provide a practical avenue for training and running general-purposeAImodels without accessing sensitive data. Measures to address the lack of data transparency and control in other areas could be applied to general-purposeAIsystems. Developing better mechanisms for individuals to control and trace their data would promote transparency and accountability in general-purposeAIsystems. This includes providing user-friendly interfaces for managing data permissions, implementing secure data provenance systems to track how data is used and shared, and establishing clear processes for individuals to access, view, correct, and delete their data[reference 856]. The technological means to provide these controls exist and are successfully deployed in other areas (for example, user-controlled dashboards allowing users to decide how various websites or companies collect or use their personal data). It is possible that such and other approaches could be expanded to general-purposeAIsystems. It may also be possible to redistribute the wealth derived from personal data in a more traceable and equitable manner, for example by using economic tools for data valuation[reference 857]. However, providing people with transparency and control over how their public data is used (i.e. information found readily on the Web) is much more challenging. While individual service providers such as social media platforms could prohibit their data from being used by external general-purposeAIsystems, the control is ultimately not in the hands of the end-user and only covers a small portion of the data on the Web. Another challenge is to provide meaningful controls for the use of derived data, or data that is not identified but allows inference about a person. Such cases will likely be common inAIsystems that use personal data. More research is needed to explore ways to reduce the risks of unauthorised data use and sharing. Some forms of privacy abuse are hard to prevent through technical means. In part, due to the lack of data transparency and control, new forms of privacy abuse stemming from general-purposeAI, such as non-consensual deepfakes or stalking, are hard to prevent via technical means. Some legal frameworks aim to hold creators and distributors accountable for malicious use[reference 858], and to provide remedies for individuals whose privacy has been violated. Some recent regulations also call forAIsystems to be developed and deployed in a manner that respects privacy principles, such as by enforcing data minimisation and purpose limitation[references 859, 860], yet how to achieve these properties, or the extent to which they are achievable is questionable[reference 861]. This interim International Scientific Report on the Safety of AdvancedAIfinds that the future trajectory of general-purposeAIis remarkably uncertain. A wide range of possible outcomes appears possible even in the near future, including both very positive and very negative outcomes, as well as anything in between. Among the most promising prospects for general-purposeAIare its potential for education, medical applications, research advances in a wide range of fields, and increased productivity leading to more prosperity. If managed properly, general-purposeAIsystems could substantially improve the lives of people worldwide. But to reap the benefits of this transformative technology safely, researchers and policymakers need to identify and take informed action to mitigate the risks that come with it. Malicious use of general-purposeAIas well as malfunctioning general-purposeAIare already causing harm today, for instance through deepfakes, scams and biased outputs. Depending on the rate of progress of future general-purposeAIcapabilities, the technical methods that developers and regulators employ to mitigate risks, the decisions of governments and societies in relation to general-purposeAI, and the degree of successful global coordination, it is also possible that further risks could emerge. The worst outcomes could see the emergence of risks like large-scale unemployment, general-purposeAI-enabled terrorism, or even humanity losing control over general-purposeAIsystems. There is no consensus among experts about how likely these risks are and when they might occur. This report also examines the factors that make addressing these risks difficult. Despite rapid advances in capabilities, researchers currently cannot generate human-understandable accounts of how general-purposeAImodels and systems arrive at outputs and decisions. This makes it difficult to evaluate or predict what they are capable of, how reliable they are, and obtain assurances on the risks they might pose. There are technical approaches to addressing the risks from general-purposeAI: methods for reducing model bias, improving our understanding of the inner workings of general-purposeAImodels, assessing their capabilities and potential risks, and making them less likely to respond to user requests that could cause harm. There are also complementary techniques for monitoring and mitigating harmful actions from general-purposeAIsystems. However, no existing techniques currently provide quantitative guarantees about the safety of advanced general-purposeAImodels or systems. Nothing about the future ofAIis inevitable. How general-purposeAIgets developed and by whom, which problems it gets designed to solve, whether we will be able to reap general-purposeAI's full economic potential, who benefits from it, and the types of risks we expose ourselves to -- these and many other questions depend on the choices that societies and governments make today and in the future to shape the development of general-purposeAI. Since the impact of general-purposeAIon many aspects of our lives is likely to be profound, and since progress might continue to be rapid, the precautionary principle implies an urgent need to broker consensus and to put resources into understanding and addressing these risks. Constructive scientific and public discussion will be essential for societies and policymakers to make the right choices. For the first time in history, this interim report brought together expert representatives nominated by 30 countries, and theEUand theUN, and several other world-leading experts to provide a shared scientific, evidence-based foundation for these vital discussions. We continue to disagree on several questions, minor and major, around the capabilities, risks, and risk mitigations for general-purposeAI. But we consider this project essential for improving our collective understanding of general-purposeAIand its potential risks, and for moving closer towards consensus and effective risk mitigation to ensure people can enjoy general-purposeAI's benefits safely. The stakes are high. We look forward to continuing this effort. This interim report is the product of extremely rapid collaboration between a diverse and large group ofAIexperts, including an Expert Advisory Panel nominated by 30 countries as well as theEUand theUN. The field ofAIresearch is moving at pace, and on many important questions the field is far from having a consensus view. Against this backdrop, I am particularly impressed by what the 75 international experts contributing their diverse perspectives to this report have achieved in such a short time. Writing a report that discusses the capabilities, risks and potential risks of general purposeAIin a balanced way has been especially important to me. I am very grateful to the experts contributing to the report for the collaborative spirit with which they approached this important project. The short amount of time available for writing this interim report also means that several difficult procedural decisions and decisions about the scope of the report had to be made, leaving many important issues unaddressed or only covered briefly. My goal for the next publication, which will build on this interim report, is for the contributing experts to jointly identify the most important areas of improvement to work on for the next report. This might, for example, include some of the following aspects: I am extremely grateful to all the experts who contributed to this interim report and look forward to working on the next publication. The Chair and Secretariat worked to reach consensus between members of the Expert Advisory Panel on the content of reports. As per the report'sprinciples & procedures, a near-final version of the interim report was shared with the Expert Advisory Panel. Where differing views remained with the Panel, Panel members were offered the option for these to be noted. Below are the views noted. Noted concern that the general tone of the report is excessively negative. Noted that while the report does note that the future ofAIis not predetermined, the language used could create the impression that the outlook for humanity is bleak no matter what steps are taken, and that consequently, the report's impact on policymakers could be undermined. Requested that future iterations of the report should include: The explanations below should all be taken for the use of the term in the context ofAIor general-purposeAI. Adaptivity: The ability to identify patterns, reason, and make decisions in contexts and ways not directly envisioned by human programmers or outside the context of a system's training data. AIagents / agent / autonomous agent:AIsystems that are capable of accomplishing multi-step tasks in pursuit of a high-level goal with little or no human oversight.AIagents may do things like browsing the internet, sending emails, or sending instructions to physical equipment. AIdeployers: Any individual or organisation that supplies or uses anAIsystem to provide a product or service. Deployment can be 'internal', where a system is only used by the developers, or 'external', allowing the public or other non-developer entities to use it. AIdevelopers: Organisations or individuals who design, build, train, adapt, or combineAImodels and applications. AIend user: Any intended or actual individual or organisation that uses or consumes anAI-based product or service as it is deployed. AIlifecycle: All events and processes that relate to anAIsystem's lifespan, from inception to decommissioning, including its design, research, training, development, deployment, integration, operation, maintenance, sale, use, and governance. AIrisks: The combination of the probability of an occurrence of harm arising from the development or deployment ofAImodels or systems, and the severity of that harm. Algorithmic Transparency: The degree to which the factors informing general-purposeAIoutput, e.g. recommendations or decisions, are knowable by various stakeholders. Such factors might include the inner workings of theAImodel, how it has been trained, what data it is trained on, what features of the input affected its output, and what decisions it would have made under different circumstances. Alignment: The process of ensuring anAIsystem's goals and behaviours are in line with its developer's values and intentions. Application Programming Interface (API): A set of rules and protocols that enables integration and communication betweenAIsystems and other software applications. Artificial General Intelligence (AGI): A potential futureAIsystem that equals or surpasses human performance on all or almost all cognitive tasks. A number ofAIcompanies have publicly stated their aim to buildAGI. However, the termAGIhas no universally precisely agreed definition. Autonomy / autonomous: Capable of operating, taking actions, or making decisions without the express intent or oversight of a human. Biological design tools (BDTs): In this report, biological design tools (BDTs) refers toAIsystems trained on biological data that can help design new proteins or other biological agents such as enzymes. Black-box: A system deployed with restrictions such that a user cannot access or analyse its inner workings. See also 'White-box' below. Capabilities: The range of tasks or functions that anAIsystem can perform and the proficiency with which it can perform them. Cloud labs: Remotely controlled automatised biochemical laboratories. Cognitive tasks: Tasks involving a combination of information processing, memory, information recall, planning, reasoning, organisation, problem solving, learning, and goal-oriented decision-making. Compute: Computational resources, required in large amounts to train and run general-purposeAImodels. Mostly provided through clusters of Graphics Processing Units (GPUs). Cross-lingual differences: Discrepancies in how a general-purposeAImodel or system might respond to the same input in different languages. Deep Learning: A set of methods forAIdevelopment that leverages very large amounts of data and compute. Deployment: The process of releasing anAIsystem into a real-world environment, such as a consumer-facingAIsystem. Disinformation: Deliberately false information generated or spread with the intent to deceive or mislead. Ecosystem Audit: A broad evaluation of anAIsystem and its surrounding ecosystem. Ecosystem audits might considerAImodels, their training data, the circumstances of their deployment, and surrounding operational practice. Evaluations: Systematic assessments of anAIsystem's performance, capabilities, or potential impacts. Evaluations can include benchmarking, red-teaming, and audits. FLOPS: 'Floating point operations per second' - a measure of the computing power of a computer. Foundation models: Machine learning models trained on very large amounts of data that can be adapted to a wide range of tasks. FrontierAI: For theAISafety Summit at Bletchley Park, frontier AIs were defined as models that can perform a wide variety of tasks and match or exceed the capabilities present in today's most advanced models. GPU(Graphics Processing Unit): A piece of computer hardware assembled from semiconductors widely used as the central source of computational power for general-purposeAI.GPUswere originally designed for graphics rendering applications. Guardrails: Pre-defined safety constraints or boundaries set up in an attempt to ensure anAIsystem operates within desired parameters and avoids unintended or harmful outcomes. Heuristic: A rule-of-thumb, strategy, or a simplified principle that, in the context of computer science, has been developed to solve problems more efficiently when classic methods are too slow or fail to find an exact solution. Input (to anAIsystem): The data or prompt fed into anAIsystem, often text or an image, which theAIsystem processes before producing an output. Large Language Model (LLMs): Machine learning models trained on large datasets that can recognise, understand, and generate text and other content. Massive Multitask Language Understanding (MMLU): A widely used benchmarkAIresearch that assesses a general-purposeAImodel's performance across a broad range of tasks and subject areas. Misgeneralisation: When anAIsystem trained to perform well in one context fails to perform well in a new context. For instance, if anAItrained mostly on pictures of white cats labels a black cat as a 'dog', it is misgeneralising from its training data. Misinformation: Incorrect or misleading information, potentially generated and spread without harmful intent. Modalities: The types and nature of data that anAImodel can process, such as text, image, sound, or videos. Models might be unimodal, i.e. only able to process one type of data, or multimodal, i.e. able to process multiple types of data. Model Card: A document providing important information on a general-purposeAImodel, such as its purpose, its performance on evaluations and benchmarks, and safety features. NarrowAI: AnAIsystem that only performs well on a single task or narrow set of tasks, like sentiment analysis or playing Chess. Open-ended domains: Scenarios or environments that have a very large set of possible states and inputs to anAIsystem - so that developers cannot anticipate all types of contexts of use and thus cannot test theAI's behaviour in all possible situations. Pre-training: The first stage of developing a modern general-purposeAImodel, in which models learn from large amounts of data. Pre-training is the part of general-purposeAItraining that requires the most data and computational resources. Prompt: An input to anAIsystem, often a text-based question or query, that the system processes before it produces a response. Red-teaming: A method to evaluate the safety and robustness of systems by attempting to design inputs that make them fail. This is often done by developing 'adversarial attacks' or challenging conditions. Red-teaming tries to reveal worst-case behaviours or malicious use opportunities. Risk factors: Elements or conditions that can increase downstream risks. For example, weak guardrails constitute a risk factor that could enable an actor to malicious use anAIsystem to perform a cyber attack (downstream risk). Safety and security: The protection, wellbeing, and autonomy of civil society and the population. In this publication, safety is often used to describe prevention of or protection againstAI-related harms.AIsecurity refers to protectingAIsystems from technical interference such as cyber-attacks or leaks of the code and weights of theAImodel. Scaffold: Additional software that aids in processing inputs and outputs of anAImodel while leaving the model itself unchanged. For example, a scaffold allows GPT-4 to power the autonomousAIagent AutoGPT. The scaffold prompts GPT-4 to break down a high-level task into sub-tasks, assign sub-tasks to other copies of itself, save important information to memory, and browse the internet. Semiconductors: Fundamental material components of modern computer hardware, such asGPUs. Synthetic data: Data, e.g., text, images, etc., that has been generated artificially, for instance by general-purposeAImodels. Synthetic data might be used for training general-purposeAImodels, such as in cases of scarcity of high-quality natural data. System integration: The process of combining different software elements into one cohesive system assembled to perform some function. For instance, system integration might combine a general-purposeAImodel, a content filter, a user interface, and various other components into a chatbot application. Transfer learning: A machine learning technique in which a model's completed training on one task or subject area is used as a starting point for training or using the model on another subject area. Transformer architecture: A deep-learning architecture at the heart of most modern general-purposeAImodels. The transformer architecture has proven particularly efficient at converting increasingly large amounts of training data and computational power into better model performance. Weights: Parameters in a model that are akin to adjustable dials in the algorithm. Training a model means adjusting its parameters to help it make accurate predictions or decisions based on input data, ensuring it learns from patterns it has seen. White-box: A system deployed without restrictions such that a user can access or analyse its inner workings. See also 'Black-box' above. * denotes that the reference was either published by anAIcompany or at least 50% of the authors of a preprint article have anAIcompany as their affiliation. 1. Simmons-Edler, R., Badman, R., Longpre, S., & Rajan, K. (2024).AI-Powered Autonomous Weapons Risk Geopolitical Instability and ThreatenAIResearch. arXiv preprint arXiv:2405.01859. Online at:https://arxiv.org/abs/2405.01859. 2.* OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, R. Avila, I. Babuschkin, S. Balaji, V. Balcom, P. Baltescu, H. Bao, M. Bavarian, J. Belgum, . . . B. Zoph, 'GPT-4 Technical Report' (OpenAI, 2024);http://arxiv.org/abs/2303.08774. 3.* Gemini Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, D. Silver, M. Johnson, I. Antonoglou, J. Schrittwieser, A. Glaese, J. Chen, E. Pitler, T. Lillicrap, A. Lazaridou, . . . O. Vinyals, 'Gemini: A Family of Highly Capable Multimodal Models' (Google DeepMind, 2023);http://arxiv.org/abs/2312.11805. 4.* Anthropic, 'The Claude 3 Model Family: Opus, Sonnet, Haiku' (Anthropic, 2024);www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf. 5.* Qwen Team, J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, B. Hui, L. Ji, M. Li, J. Lin, R. Lin, D. Liu, G. Liu, C. Lu, . . . T. Zhu, Qwen Technical Report, arXiv:2309.16609 [cs.CL] (2023).http://arxiv.org/abs/2309.16609. 6.* Meta, Build the future ofAIwith Meta Llama 3 (2024).https://llama.meta.com/llama3/. 7.* A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. Le Scao, T. Lavril, T. Wang, T. Lacroix, W. El Sayed, 'Mistral 7B' (MistralAI, 2023);https://doi.org/10.48550/arXiv.2310.06825. 8. L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang, B. Cui, M.-H. Yang, Diffusion Models: A Comprehensive Survey of Methods and Applications. ACM Comput. Surv. 56, 1-39 (2023).https://doi.org/10.1145/3626235. 9.* OpenAI, 'DALL*E 3 system card' (OpenAI, 2023);https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf. 10.* Midjourney, Midjourney Documentation (2024).https://docs.midjourney.com/v1/en. 11.* StabilityAI, Stable Diffusion 3 (2024).https://stability.ai/news/stable-diffusion-3. 12.* T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman, C. Ng, R. Wang, A. Ramesh, 'Video generation models as world simulators' (OpenAI, 2024);https://openai.com/research/video-generation-models-as-world-simulators. 13. D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth, S. Levine, V. Vanhoucke, K. Hausman, M. Toussaint, K. Greff, . . . P. Florence, 'PaLM-E: an embodied multimodal language model' in Proceedings of the 40th International Conference on Machine Learning (ICML'23) (PMLR, 2023) vol. 202, pp. 8469-8488.https://dl.acm.org/doi/10.5555/3618408.3618748. 14. J. Abramson, J. Adler, J. Dunger, R. Evans, T. Green, A. Pritzel, O. Ronneberger, L. Willmore, A. J. Ballard, J. Bambrick, S. W. Bodenstein, D. A. Evans, C.-C. Hung, M. O'Neill, D. Reiman, K. Tunyasuvunakool, Z. Wu, A. Zemgulyte, E. Arvaniti, . . . J. M. Jumper, Accurate structure prediction of biomolecular interactions with AlphaFold 3. Nature (2024).https://doi.org/10.1038/s41586-024-07487-w. 15. Y. LeCun, Y. Bengio, G. Hinton, Deep learning. Nature 521, 436-444 (2015).https://doi.org/10.1038/nature14539. 16. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. U. Kaiser, I. Polosukhin, 'Attention is All you Need' in Advances in Neural Information Processing Systems (NIPS 2017) (Curran Associates, Inc., 2017) vol. 30.https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. 17. J. Sevilla, L. Heim, A. Ho, T. Besiroglu, M. Hobbhahn, P. Villalobos, 'Compute Trends Across Three Eras of Machine Learning' in 2022 International Joint Conference on Neural Networks (IJCNN 2022) (2022) pp. 1-8.https://doi.org/10.1109/ijcnn55064.2022.9891914. 18. C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu, S. Zhang, G. Ghosh, M. Lewis, L. Zettlemoyer, O. Levy, 'LIMA: Less Is More for Alignment' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=KBMOKmX2he. 19. R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, C. Finn, 'Direct Preference Optimization: Your Language Model is Secretly a Reward Model' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=HPuSIXJaa9&utm. 20. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Gray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, . . . R. Lowe, 'Training language models to follow instructions with human feedback' in 36th Conference on Neural Information Processing Systems (NeurIPS 2022) (2022).https://openreview.net/forum?id=TG8KACxEON. 21.* Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, T. Hume, . . . J. Kaplan, Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, arXiv:2204.05862 [cs.CL] (2022).https://doi.org/10.48550/arXiv.2204.05862. 22.* H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, . . . T. Scialom, 'Llama 2: Open Foundation and Fine-Tuned Chat Models' (MetaAI, 2023);http://arxiv.org/abs/2307.09288. 23. L. Sharkey, C. Ni Ghuidhir, D. Braun, J. Scheurer, M. Balesni, L. Bushnaq, C. Stix, M. Hobbhahn, A Causal Framework forAIRegulation and Auditing. (2024).https://doi.org/10.20944/preprints202401.1424.v1. 24. J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. V. Le, D. Zhou, 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models' in Advances in Neural Information Processing Systems (NeurIPS 2022) (2022) vol. 35, pp. 24824-24837.https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html. 25. T. Davidson, J.-S. Denain, P. Villalobos, G. Bas, 'AIcapabilities can be significantly improved without expensive retraining' (Epoch, 2023);http://arxiv.org/abs/2312.07413. 26. L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, W. X. Zhao, Z. Wei, J. Wen, A survey on large language model based autonomous agents. Front. Comput. Sci. 18, 186345 (2024).https://doi.org/10.1007/s11704-024-40231-1. 27. R. Bommasani, D. Soylu, T. I. Liao, K. A. Creel, P. Liang, Ecosystem Graphs: The Social Footprint of Foundation Models, arXiv:2303.15772 [cs.LG] (2023).https://doi.org/10.48550/arXiv.2303.15772. 28.* A. Das, W. Kong, R. Sen, Y. Zhou, A decoder-only foundation model for time-series forecasting, arXiv:2310.10688 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2310.10688. 29.* P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, I. Sutskever, 'Jukebox: A Generative Model for Music' (OpenAI, 2020);http://arxiv.org/abs/2005.00341. 30. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, . . . D. Amodei, 'Language Models are Few-Shot Learners' in Advances in Neural Information Processing Systems (Curran Associates, Inc., 2020) vol. 33, pp. 1877-1901.https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. 31.* S. Pichai, D. Hassabis, Our next-generation model: Gemini 1.5. (2024).https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/. 32. Fan, Gokkaya, Harman, Lyubarskiy, Sengupta, Yoo, Zhang, 'Large Language Models for Software Engineering: Survey and Open Problems' in 2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE) (2023) vol. 0, pp. 31-53.https://doi.org/10.1109/ICSE-FoSE59343.2023.00008. 33.* Anthropic, Introducing the next generation of Claude (2024).www.anthropic.com/news/claude-3-family. 34. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N. Houlsby, 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale' in The 9th International Conference on Learning Representations (ICLR 2021) (2021).https://openreview.net/forum?id=YicbFdNTTy. 35. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, I. Sutskever, 'Learning Transferable Visual Models From Natural Language Supervision' in Proceedings of the 38th International Conference on Machine Learning (ICML 2021) (PMLR, 2021) pp. 8748-8763.https://proceedings.mlr.press/v139/radford21a.html. 36.* A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. Dollar, R. Girshick, 'Segment Anything' (MetaAI, 2023);http://arxiv.org/abs/2304.02643. 37.* Meta, V-JEPA: The next step toward advanced machine intelligence (2024).https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/. 38.* OpenAI, Sora: Creating video from text (2023).https://openai.com/sora. 39. B. Ichter, A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz, A. Irpan, E. Jang, R. Julian, D. Kalashnikov, S. Levine, Y. Lu, C. Parada, K. Rao, P. Sermanet, A. T. Toshev, V. Vanhoucke, . . . C. K. Fu, 'Do As I Can, Not As I Say: Grounding Language in Robotic Affordances' in Proceedings of The 6th Annual Conference on Robot Learning (CoRL) (PMLR, 2022) vol. 205.https://openreview.net/forum?id=bdHkMjBJG_w. 40.* Q. Vuong, P. Sanketi, Scaling up learning across many different robot types (2023).https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/. 41. A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis, P. D. Fagan, J. Hejna, M. Itkina, M. Lepert, Y. J. Ma, P. T. Miller, J. Wu, S. Belkhale, S. Dass, . . . C. Finn, DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset, arXiv:2403.12945 [cs.RO] (2024).https://doi.org/10.48550/arXiv.2403.12945. 42. Open X-Embodiment Collaboration, A. O'Neill, A. Rehman, A. Maddukuri, A. Gupta, A. Padalkar, A. Lee, A. Pooley, A. Gupta, A. Mandlekar, A. Jain, A. Tung, A. Bewley, A. Herzog, A. Irpan, A. Khazatsky, A. Rai, A. Gupta, A. Wang, . . . Z. Lin, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864 [cs.RO] (2023).https://doi.org/10.48550/arXiv.2310.08864. 43. A. Madani, B. Krause, E. R. Greene, S. Subramanian, B. P. Mohr, J. M. Holton, J. L. Olmos, C. Xiong, Z. Z. Sun, R. Socher, J. S. Fraser, N. Naik, Large language models generate functional protein sequences across diverse families. Nat. Biotechnol. 41, 1099-1106 (2023).https://doi.org/10.1038/s41587-022-01618-2. 44. P. Bryant, G. Pozzati, A. Elofsson, Improved prediction of protein-protein interactions using AlphaFold2. Nat Commun 13, 1265 (2022).https://doi.org/10.1038/s41467-022-28865-w. 45. X. Hu, J. Chen, X. Li, Y. Guo, L. Wen, P. S. Yu, Z. Guo, Do Large Language Models Know about Facts?, arXiv:2310.05177 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2310.05177. 46. Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, P. Fung, Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55, 1-38 (2023).https://doi.org/10.1145/3571730. 47.* Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen, L. Wang, A. T. Luu, W. Bi, F. Shi, S. Shi, Siren's Song in theAIOcean: A Survey on Hallucination in Large Language Models, arXiv:2309.01219 [cs.CL] (2023).http://arxiv.org/abs/2309.01219. 48. M. Zhang, O. Press, W. Merrill, A. Liu, N. A. Smith, How Language Model Hallucinations Can Snowball, arXiv:2305.13534 [cs.CL] (2023).http://arxiv.org/abs/2305.13534. 49. L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, T. Liu, A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions, arXiv:2311.05232 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2311.05232. 50. V. Rawte, A. Sheth, A. Das, A Survey of Hallucination in Large Foundation Models, arXiv:2309.05922 [cs.AI] (2023).http://arxiv.org/abs/2309.05922. 51. E. Davis, Mathematics, word problems, common sense, and artificial intelligence. Bull. Am. Math. Soc. 61, 287-303 (2024).https://doi.org/10.1090/bull/1828. 52. Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, L. Li, Z. Sui, A Survey on In-context Learning, arXiv:2301.00234 [cs.CL] (2022).http://arxiv.org/abs/2301.00234. 53. W. Zhao, J. T. Chiu, J. D. Hwang, F. Brahman, J. Hessel, S. Choudhury, Y. Choi, X. L. Li, A. Suhr, UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations, arXiv:2311.08469 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2311.08469. 54. J. Liu, W. Wang, D. Wang, N. Smith, Y. Choi, H. Hajishirzi, 'Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements' in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (Association for Computational Linguistics, 2023) pp. 1264-1287.https://doi.org/10.18653/v1/2023.emnlp-main.81. 55. M. Mitchell,AI's challenge of understanding the world. Science 382, eadm8175 (2023).https://doi.org/10.1126/science.adm8175. 56. N. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jiang, B. Y. Lin, S. Welleck, P. West, C. Bhagavatula, R. L. Bras, J. D. Hwang, S. Sanyal, X. Ren, A. Ettinger, Z. Harchaoui, Y. Choi, 'Faith and Fate: Limits of Transformers on Compositionality' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (Curran Associates, 2023).https://openreview.net/forum?id=Fkckkr3ya8. 57. D. Halawi, F. Zhang, C. Yueh-Han, J. Steinhardt, Approaching Human-Level Forecasting with Language Models, arXiv:2402.18563 [cs.LG] (2024).https://doi.org/10.48550/arXiv.2402.18563. 58. U. Anwar, A. Saparov, J. Rando, D. Paleka, M. Turpin, P. Hase, E. S. Lubana, E. Jenner, S. Casper, O. Sourbut, B. L. Edelman, Z. Zhang, M. Gunther, A. Korinek, J. Hernandez-Orallo, L. Hammond, E. Bigelow, A. Pan, L. Langosco, . . . D. Krueger, Foundational Challenges in Assuring Alignment and Safety of Large Language Models, arXiv:2404.09932 [cs.LG] (2024).https://doi.org/10.48550/arXiv.2404.09932. 59. J. Andreas, 'Language Models as Agent Models' in Findings of the Association for Computational Linguistics: EMNLP 2022 (Association for Computational Linguistics, 2022) pp. 5769-5779.https://doi.org/10.18653/v1/2022.findings-emnlp.423. 60. J. S. Park, J. O'Brien, C. J. Cai, M. R. Morris, P. Liang, M. S. Bernstein, 'Generative Agents: Interactive Simulacra of Human Behavior' in Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23) (Association for Computing Machinery, 2023) pp. 1-22.https://doi.org/10.1145/3586183.3606763. 61. J. Wang, Z. Wu, Y. Li, H. Jiang, P. Shu, E. Shi, H. Hu, C. Ma, Y. Liu, X. Wang, Y. Yao, X. Liu, H. Zhao, Z. Liu, H. Dai, L. Zhao, B. Ge, X. Li, T. Liu, . . . S. Zhang, Large Language Models for Robotics: Opportunities, Challenges, and Perspectives, arXiv:2401.04334 [cs.RO] (2024).http://arxiv.org/abs/2401.04334. 62. D. C. Ciresan, U. Meier, L. M. Gambardella, J. Schmidhuber, Deep, Big, Simple Neural Nets for Handwritten Digit Recognition. Neural Comput. 22, 3207-3220 (2010).https://doi.org/10.1162/NECO_a_00052. 63. T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, S. Khudanpur, 'Recurrent neural network based language model' in Proc. Interspeech 2010 (ISCA, 2010) pp. 1045-1048.https://doi.org/10.21437/Interspeech.2010-343. 64. X. Glorot, Y. Bengio, 'Understanding the difficulty of training deep feedforward neural networks' in Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS 2010) (PMLR, 2010) vol. 9, pp. 249-256.https://proceedings.mlr.press/v9/glorot10a.html. 65. Epoch, Parameter, compute and data trends in machine learning (2024).https://epochai.org/data/epochdb/visualization. 66.* InflectionAI, Inflection-2 (2023).https://inflection.ai/inflection-2. 67. D. Coyle, L. Hampton, 21st century progress in computing. Telecomm. Policy 48, 102649 (2024).https://doi.org/10.1016/j.telpol.2023.102649. 68. M. Hobbhahn, L. Heim, G. Aydos, 'Trends in machine learning hardware' (EPOCHAI, 2023);https://epochai.org/blog/trends-in-machine-learning-hardware. 69. G. Li, Z. Sun, Q. Wang, S. Wang, K. Huang, N. Zhao, Y. Di, X. Zhao, Z. Zhu, China's green data center development:Policies and carbon reduction technology path. Environ. Res. 231, 116248 (2023).https://doi.org/10.1016/j.envres.2023.116248. 70.* Anthropic, Research (2023).www.anthropic.com/research. 71.* G. Brockman, I. Sutskever, S. Altman, OpenAI and Microsoft (2016).https://openai.com/blog/openai-and-microsoft. 72.* Anthropic, Anthropic Partners with Google Cloud (2023).www.anthropic.com/news/anthropic-partners-with-google-cloud. 73.* Amazon Staff, Amazon and Anthropic announce strategic collaboration to advance generativeAI(2023).www.aboutamazon.com/news/company-news/amazon-aws-anthropic-ai. 74.* Cohere Team, Cohere Is Available on the Google Cloud Marketplace (2022).https://cohere.com/blog/cohere-is-available-on-the-google-cloud-marketplace. 75.* E. Boyd, Microsoft and MistralAIannounce new partnership to accelerateAIinnovation and introduce Mistral Large first on Azure (2024).https://azure.microsoft.com/en-us/blog/microsoft-and-mistral-ai-announce-new-partnership-to-accelerate-ai-innovation-and-introduce-mistral-large-first-on-azure/. 76. C.-J. Wu, R. Raghavendra, U. Gupta, B. Acun, N. Ardalani, K. Maeng, G. Chang, F. Aga, J. Huang, C. Bai, M. Gschwind, A. Gupta, M. Ott, A. Melnikov, S. Candido, D. Brooks, G. Chauhan, B. Lee, H.-H. Lee, . . . K. Hazelwood, 'SustainableAI: Environmental implications, challenges and opportunities' in Proceedings of the 5th Conference on Machine Learning and Systems (MLSys) (2022) vol. 4, pp. 795-813.https://proceedings.mlsys.org/paper_files/paper/2022/file/462211f67c7d858f663355eff93b745e-Paper.pdf. 77. A. Gardizy, W. Ma, Microsoft ReadiesAIChip as Machine Learning Costs Surge (2023).www.theinformation.com/articles/microsoft-readies-ai-chip-as-machine-learning-costs-surge. 78. D. Patterson, J. Gonzalez, U. Holzle, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild, D. R. So, M. Texier, J. Dean, The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink. Computer 55, 18-28 (2022).https://doi.org/10.1109/mc.2022.3148714. 79.* E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, E. Goffinet, D. Hesslow, J. Launay, Q. Malartic, D. Mazzotta, B. Noune, B. Pannier, G. Penedo, The Falcon Series of Open Language Models, arXiv:2311.16867 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2311.16867. 80.* T. Wei, L. Zhao, L. Zhang, B. Zhu, L. Wang, H. Yang, B. Li, C. Cheng, W. Lu, R. Hu, C. Li, L. Yang, X. Luo, X. Wu, L. Liu, W. Cheng, P. Cheng, J. Zhang, X. Zhang, . . . Y. Zhou, Skywork: A More Open Bilingual Foundation Model, arXiv:2310.19341 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2310.19341. 81. N. Muennighoff, A. Rush, B. Barak, T. Le Scao, N. Tazi, A. Piktus, S. Pyysalo, T. Wolf, C. A. Raffel, 'Scaling Data-Constrained Language Models' in Advances in Neural Information Processing Systems 36 (NeurIPS 2023) Main Conference Track (2023) vol. 36, pp. 50358-50376.https://proceedings.neurips.cc/paper_files/paper/2023/hash/9d89448b63ce1e2e8dc7af72c984c196-Abstract-Conference.html. 82. P. Villalobos, J. Sevilla, L. Heim, T. Besiroglu, M. Hobbhahn, A. Ho, Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning, arXiv:2211.04325 [cs.LG] (2022).http://arxiv.org/abs/2211.04325. 83. M. Marion, A. Ustun, L. Pozzobon, A. Wang, M. Fadaee, S. Hooker, 'When Less is More: Investigating Data Pruning for PretrainingLLMsat Scale' in 1st Workshop on Attributing Model Behavior at Scale (2023).https://openreview.net/forum?id=XUIYn3jo5T. 84. G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, H. Alobeidli, A. Cappelli, B. Pannier, E. Almazrouei, J. Launay, 'The RefinedWeb Dataset for FalconLLM: Outperforming Curated Corpora with Web Data Only' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Datasets and Benchmarks Track (2023).https://openreview.net/pdf?id=kM5eGcdCzq. 85. S. M. Xie, H. Pham, X. Dong, N. Du, H. Liu, Y. Lu, P. Liang, Q. V. Le, T. Ma, A. W. Yu, 'DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=lXuByUeHhd. 86.* M. Mitchell, A. S. Luccioni, N. Lambert, M. Gerchick, A. McMillan-Major, E. Ozoani, N. Rajani, T. Thrush, Y. Jernite, D. Kiela, Measuring Data, arXiv:2212.05129 [cs.AI] (2022).https://doi.org/10.48550/arXiv.2212.05129. 87.* D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, G. Irving, 'Fine-Tuning Language Models from Human Preferences' (OpenAI, 2020);http://arxiv.org/abs/1909.08593. 88.* D. Hernandez, T. B. Brown, Measuring the Algorithmic Efficiency of Neural Networks, arXiv:2005.04305 [cs.LG] (2020).https://doi.org/10.48550/arXiv.2005.04305. 89. A. Ho, T. Besiroglu, E. Erdil, D. Owen, R. Rahman, Z. C. Guo, D. Atkinson, N. Thompson, J. Sevilla, 'Algorithmic progress in language models' (Epoch, 2024);http://arxiv.org/abs/2403.05812. 90. F. E. Dorner, Measuring Progress in Deep Reinforcement Learning Sample Efficiency, arXiv:2102.04881 [cs.LG] (2021).https://doi.org/10.48550/arXiv.2102.04881. 91.* S. Chen, S. Wong, L. Chen, Y. Tian, Extending Context Window of Large Language Models via Positional Interpolation, arXiv:2306.15595 [cs.CL] (2023).http://arxiv.org/abs/2306.15595. 92.* Anthropic, Long context prompting for Claude 2.1 (2023).www.anthropic.com/news/claude-2-1-prompting. 93. A. Gu, T. Dao, Mamba: Linear-Time Sequence Modeling with Selective State Spaces, arXiv:2312.00752 [cs.LG] (2023).https://doi.org/10.48550/arXiv.2312.00752. 94. D. Kiela, M. Bartolo, Y. Nie, D. Kaushik, A. Geiger, Z. Wu, B. Vidgen, G. Prasad, A. Singh, P. Ringshia, Z. Ma, T. Thrush, S. Riedel, Z. Waseem, P. Stenetorp, R. Jia, M. Bansal, C. Potts, A. Williams, 'Dynabench: Rethinking benchmarking in NLP' in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Association for Computational Linguistics, 2021) pp. 4110-4124.https://doi.org/10.18653/v1/2021.naacl-main.324. 95. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, J. Steinhardt, 'Measuring Massive Multitask Language Understanding' in The 9th International Conference on Learning Representations (ICLR 2021) (2021).https://openreview.net/forum?id=d7KBjmI3GmQ. 96. A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, A. Kluska, A. Lewkowycz, A. Agarwal, A. Power, A. Ray, A. Warstadt, A. W. Kocurek, A. Safaya, A. Tazarv, . . . Z. Wu, Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research (2023).https://openreview.net/forum?id=uyTL5Bvosj. 97. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, S. R. Bowman,GPQA: A Graduate-Level Google-Proof Q&A Benchmark, arXiv:2311.12022 [cs.AI] (2023).http://arxiv.org/abs/2311.12022. 98. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, J. Steinhardt, 'Measuring Mathematical Problem Solving With the MATH Dataset' in 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Datasets and Benchmarks Track (Round 2) (2021).https://openreview.net/forum?id=7Bywt2mQsCe. 99.* S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, Y. Zhang, Sparks of Artificial General Intelligence: Early experiments with GPT-4, arXiv:2303.12712 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2303.12712. 100. A. Zhou, K. Wang, Z. Lu, W. Shi, S. Luo, Z. Qin, S. Lu, A. Jia, L. Song, M. Zhan, H. Li, 'Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=c8McWs4Av0. 101. T. R. McIntosh, T. Susnjak, T. Liu, P. Watters, M. N. Halgamuge, Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence, arXiv:2402.09880 [cs.AI] (2024).https://doi.org/10.48550/arXiv.2402.09880. 102. M. Mitchell, A. B. Palmarini, A. K. Moskvichev, 'Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks' in AAAI 2024 Workshop ''Are Large Language Models Simply Causal Parrots?'' (2023).https://openreview.net/forum?id=3rGT5OkzpC. 103. S. Srivastava, M. B. Annarose, P. V. Anto, S. Menon, A. Sukumar, S. T. Adwaith, A. Philipose, S. Prince, S. Thomas, Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap, arXiv:2402.19450 [cs.AI] (2024).https://doi.org/10.48550/arXiv.2402.19450. 104. C. Deng, Y. Zhao, X. Tang, M. Gerstein, A. Cohan, Investigating Data Contamination in Modern Benchmarks for Large Language Models, arXiv:2311.09783 [cs.CL] (2023).http://arxiv.org/abs/2311.09783. 105. O. Sainz, J. Campos, I. Garcia-Ferrero, J. Etxaniz, O. L. de Lacalle, E. Agirre, 'NLP Evaluation in trouble: On the Need to MeasureLLMData Contamination for each Benchmark' in Findings of the Association for Computational Linguistics: EMNLP 2023 (Association for Computational Linguistics, 2023) pp. 10776-10787.https://doi.org/10.18653/v1/2023.findings-emnlp.722. 106. Y. Cao, L. Zhou, S. Lee, L. Cabello, M. Chen, D. Hershcovich, 'Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study' in Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP) (Association for Computational Linguistics, 2023) pp. 53-67.https://doi.org/10.18653/v1/2023.c3nlp-1.7. 107. L. Berglund, M. Tong, M. Kaufmann, M. Balesni, A. C. Stickland, T. Korbak, O. Evans, 'The Reversal Curse:LLMstrained on ""A is B"" fail to learn ""B is A""' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=GPKTIktA0k. 108. J. Geiping, A. Stein, M. Shu, K. Saifullah, Y. Wen, T. Goldstein, 'CoercingLLMsto do and reveal (almost) anything' in ICLR 2024 Workshop on Secure and Trustworthy Large Language Models (SETLLM) (2024).https://openreview.net/forum?id=Y5inHAjMu0. 109.* J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, D. Amodei, Scaling Laws for Neural Language Models, arXiv:2001.08361 [cs.LG] (2020).https://doi.org/10.48550/arXiv.2001.08361. 110.* J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, . . . L. Sifre, Training Compute-Optimal Large Language Models, arXiv:2203.15556 [cs.CL] (2022).http://arxiv.org/abs/2203.15556. 111.* T. Henighan, J. Kaplan, M. Katz, M. Chen, C. Hesse, J. Jackson, H. Jun, T. B. Brown, P. Dhariwal, S. Gray, C. Hallacy, B. Mann, A. Radford, A. Ramesh, N. Ryder, D. M. Ziegler, J. Schulman, D. Amodei, S. McCandlish, Scaling Laws for Autoregressive Generative Modeling, arXiv:2010.14701 [cs.LG] (2020).http://arxiv.org/abs/2010.14701. 112. X. Zhai, A. Kolesnikov, N. Houlsby, L. Beyer, 'Scaling Vision Transformers' in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022) pp. 1204-1213.https://doi.org/10.1109/cvpr52688.2022.01179. 113.* A. L. Jones, Scaling Scaling Laws with Board Games, arXiv:2104.03113 [cs.LG] (2021).https://doi.org/10.48550/arXiv.2104.03113. 114. S. Goyal, P. Maini, Z. C. Lipton, A. Raghunathan, J. Z. Kolter, 'The Science of Data Filtering: Data Curation Cannot Be Compute Agnostic' in ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models (DPFM) (2024).https://openreview.net/forum?id=9wzo4EjEGM. 115.* Y. Bahri, E. Dyer, J. Kaplan, J. Lee, U. Sharma, Explaining Neural Scaling Laws, arXiv:2102.06701 [cs.LG] (2021).https://doi.org/10.48550/arXiv.2102.06701. 116.* A. Maloney, D. A. Roberts, J. Sully, A Solvable Model of Neural Scaling Laws, arXiv:2210.16859 [cs.LG] (2022).http://arxiv.org/abs/2210.16859. 117. U. Sharma, J. Kaplan, Scaling laws from the data manifold dimension. J. Mach. Learn. Res. 23, 343-376 (2022).https://dl.acm.org/doi/abs/10.5555/3586589.3586598. 118. L. Debowski, A Simplistic Model of Neural Scaling Laws: Multiperiodic Santa Fe Processes, arXiv:2302.09049 [cs.IT] (2023).http://arxiv.org/abs/2302.09049. 119. E. J. Michaud, Z. Liu, U. Girit, M. Tegmark, 'The Quantization Model of Neural Scaling' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=3tbTw2ga8K. 120. S. Biderman, U. S. Prashanth, L. Sutawika, H. Schoelkopf, Q. G. Anthony, S. Purohit, E. Raff, 'Emergent and Predictable Memorization in Large Language Models' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=Iq0DvhB4Kf. 121. D. Ganguli, D. Hernandez, L. Lovitt, A. Askell, Y. Bai, A. Chen, T. Conerly, N. Dassarma, D. Drain, N. Elhage, S. El Showk, S. Fort, Z. Hatfield-Dodds, T. Henighan, S. Johnston, A. Jones, N. Joseph, J. Kernian, S. Kravec, . . . J. Clark, 'Predictability and Surprise in Large Generative Models' in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (Association for Computing Machinery, 2022) pp. 1747-1764.https://doi.org/10.1145/3531146.3533229. 122.* Z. Du, A. Zeng, Y. Dong, J. Tang, Understanding Emergent Abilities of Language Models from the Loss Perspective, arXiv:2403.15796 [cs.CL] (2024).http://arxiv.org/abs/2403.15796. 123. J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, W. Fedus, Emergent Abilities of Large Language Models. Transactions on Machine Learning Research (2022).https://openreview.net/forum?id=yzkSU5zdwD. 124. R. Schaeffer, B. Miranda, S. Koyejo, 'Are Emergent Abilities of Large Language Models a Mirage?' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=ITw9edRDlD. 125. I. R. McKenzie, A. Lyzhov, M. M. Pieler, A. Parrish, A. Mueller, A. Prabhu, E. McLean, X. Shen, J. Cavanagh, A. G. Gritsevskiy, D. Kauffman, A. T. Kirtland, Z. Zhou, Y. Zhang, S. Huang, D. Wurgaft, M. Weiss, A. Ross, G. Recchia, . . . E. Perez, Inverse Scaling: When Bigger Isn't Better. Transactions on Machine Learning Research (2023). 126. J. Wei, N. Kim, Y. Tay, Q. Le, 'Inverse Scaling Can Become U-Shaped' in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023) (Association for Computational Linguistics, 2023) pp. 15580-15591.https://doi.org/10.18653/v1/2023.emnlp-main.963. 127. Y. Bengio, G. Hinton, A. Yao, D. Song, P. Abbeel, Y. N. Harari, Y.-Q. Zhang, L. Xue, S. Shalev-Shwartz, G. Hadfield, J. Clune, T. Maharaj, F. Hutter, A. G. Baydin, S. McIlraith, Q. Gao, A. Acharya, D. Krueger, A. Dragan, . . . S. Mindermann, ManagingAIRisks in an Era of Rapid Progress, arXiv:2310.17688 [cs.CY] (2023).http://arxiv.org/abs/2310.17688. 128. J. Pearl, D. Mackenzie, The book of why: the new science of cause and effect (Penguin science, Penguin Books, Harlow, England, 2019), pp. 418. 129. M. Mitchell, 'WhyAIis harder than we think' in Proceedings of the Genetic and Evolutionary Computation Conference (GECCO '21) (Association for Computing Machinery, 2021) pp. 3.https://doi.org/10.1145/3449639.3465421. 130. Y. LeCun, The Power and Limits of Deep Learning: In his IRI Medal address, Yann LeCun maps the development of machine learning techniques and suggests what the future may hold. Res. Technol. Manage. 61, 22-27 (2018).https://doi.org/10.1080/08956308.2018.1516928. 131.* Alphabet, 'Annual Report 2022' (Alphabet, 2023);https://abc.xyz/assets/d4/4f/a48b94d548d0b2fdc029a95e8c63/2022-alphabet-annual-report.pdf. 132.* L. Fan, K. Chen, D. Krishnan, D. Katabi, P. Isola, Y. Tian, Scaling Laws of Synthetic Images for Model Training ... for Now, arXiv:2312.04567 [cs.CV] (2023).https://doi.org/10.48550/arXiv.2312.04567. 133. S. Fu, N. Y. Tamir, S. Sundaram, L. Chai, R. Zhang, T. Dekel, P. Isola, 'DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=DEiNSfh1k7. 134. Y. Tian, L. Fan, P. Isola, H. Chang, D. Krishnan, 'StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=xpjsOQtKqx. 135. S. Alemohammad, J. Casco-Rodriguez, L. Luzi, A. I. Humayun, H. Babaei, D. LeJeune, A. Siahkoohi, R. Baraniuk, 'Self-Consuming Generative Models Go MAD' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=ShjMHfmPs0. 136. I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Papernot, R. Anderson, The Curse of Recursion: Training on Generated Data Makes Models Forget, arXiv:2305.17493 [cs.LG] (2023).http://arxiv.org/abs/2305.17493. 137. H. Abdine, M. Chatzianastasis, C. Bouyioukos, M. Vazirgiannis, 'Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Deep Generative Models for Health Workshop (2023).https://openreview.net/forum?id=EJ7YNgWYFj. 138.* Seamless Communication, L. Barrault, Y.-A. Chung, M. C. Meglioli, D. Dale, N. Dong, P.-A. Duquenne, H. Elsahar, H. Gong, K. Heffernan, J. Hoffman, C. Klaiber, P. Li, D. Licht, J. Maillard, A. Rakotoarison, K. R. Sadagopan, G. Wenzek, E. Ye, . . . S. Wang, 'SeamlessM4T: Massively Multilingual & Multimodal Machine Translation' (MetaAI, 2023);http://arxiv.org/abs/2308.11596. 139. International Energy Agency, 'Electricity 2024: Analysis and forecast to 2026' (IEA, 2024);https://iea.blob.core.windows.net/assets/6b2fd954-2017-408e-bf08-952fdd62118a/Electricity2024-Analysisandforecastto2026.pdf. 140. H. Ritchie, P. Rosado, M. Roser, Energy. Our World in Data (2024).https://ourworldindata.org/energy. 141.* Talen Energy Corporation, Talen Energy Announces Sale of Zero-Carbon Data Center Campus (2024).https://talenenergy.investorroom.com/2024-03-04-Talen-Energy-Announces-Sale-of-Zero-Carbon-Data-Center-Campus. 142. E. Griffith, The Desperate Hunt for the A.I. Boom's Most Indispensable Prize in The New York Times. (2023).www.nytimes.com/2023/08/16/technology/ai-gpu-chips-shortage.html. 143. D. Bragg, N. Caselli, J. A. Hochgesang, M. Huenerfauth, L. Katz-Hernandez, O. Koller, R. Kushalnagar, C. Vogler, R. E. Ladner, The FATE Landscape of Sign LanguageAIDatasets: An Interdisciplinary Perspective. ACM Trans. Access. Comput. 14, 1-45 (2021).https://doi.org/10.1145/3436996. 144. Advanced Electronics Practice, H. Bauer, O. Burkacky, P. Kenevan, S. Lingemann, K. Pototzky, B. Wiseman, 'Semiconductor design and manufacturing: Achieving leading-edge capabilities' (McKinsey & Company, 2020);www.mckinsey.com/industries/industrials-and-electronics/our-insights/semiconductor-design-and-manufacturing-achieving-leading-edge-capabilities#/. 145. J. VerWey, 'No Permits, No Fabs: The Importance of Regulatory Reform for Semiconductor Manufacturing' (Center for Security and Emerging Technology, 2021);https://doi.org/10.51593/20210053. 146. G. Timp, J. Bude, F. Baumann, K. K. Bourdelle, T. Boone, J. Garno, A. Ghetti, M. Green, H. Gossmann, Y. Kim, R. Kleiman, A. Kornblit, F. Klemens, S. Moccio, D. Muller, J. Rosamilia, P. Silverman, T. Sorsch, W. Timp, . . . B. Weir, The relentless march of the MOSFET gate oxide thickness to zero. Microelectron. Reliab. 40, 557-562 (2000).https://doi.org/10.1016/s0026-2714(99)00257-7. 147. M.-L. Chen, X. Sun, H. Liu, H. Wang, Q. Zhu, S. Wang, H. Du, B. Dong, J. Zhang, Y. Sun, S. Qiu, T. Alava, S. Liu, D.-M. Sun, Z. Han, A FinFET with one atomic layer channel. Nat. Commun. 11, 1205 (2020).https://doi.org/10.1038/s41467-020-15096-0. 148. A. Ho, E. Erdil, T. Besiroglu, 'Limits to the Energy Efficiency of CMOS Microprocessors' in 2023 IEEE International Conference on Rebooting Computing (ICRC) (2023) pp. 1-10.https://doi.org/10.1109/icrc60800.2023.10386559. 149. A. Zhou, K. Yan, M. Shlapentokh-Rothman, H. Wang, Y.-X. Wang, 'Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models' in ICLR 2024 Workshop on Large Language Model (LLM) Agents (2024).https://openreview.net/forum?id=2z5dzaqOLp. 150. S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, C. Gan, 'Planning with Large Language Models for Code Generation' in The 11th International Conference on Learning Representations (ICLR 2023) (2022).https://openreview.net/forum?id=Lr8cOOtYbfL. 151. S. Polu, J. M. Han, K. Zheng, M. Baksys, I. Babuschkin, I. Sutskever, 'Formal Mathematics Statement Curriculum Learning' in The 11th International Conference on Learning Representations (ICLR 2023) (2022).https://openreview.net/forum?id=-P7G-8dmSh4. 152. A. Fawzi, M. Balog, A. Huang, T. Hubert, B. Romera-Paredes, M. Barekatain, A. Novikov, F. J. R Ruiz, J. Schrittwieser, G. Swirszcz, D. Silver, D. Hassabis, P. Kohli, Discovering faster matrix multiplication algorithms with reinforcement learning. Nature 610, 47-53 (2022).https://doi.org/10.1038/s41586-022-05172-4. 153. A. Haj-Ali, N. K. Ahmed, T. Willke, Y. S. Shao, K. Asanovic, I. Stoica, 'NeuroVectorizer: end-to-end vectorization with deep reinforcement learning' in Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization (CGO 2020) (Association for Computing Machinery, 2020) pp. 242-255.https://doi.org/10.1145/3368826.3377928. 154. R. Pryzant, D. Iter, J. Li, Y. Lee, C. Zhu, M. Zeng, 'Automatic Prompt Optimization with ""Gradient Descent"" and Beam Search' in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023) (Association for Computational Linguistics, 2023) pp. 7957-7968.https://doi.org/10.18653/v1/2023.emnlp-main.494. 155. S. Zhang, C. Gong, L. Wu, X. Liu, M. Zhou, AutoML-GPT: Automatic Machine Learning with GPT, arXiv:2305.02499 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2305.02499. 156. S. Liu, Z. Lin, S. Yu, R. Lee, T. Ling, D. Pathak, D. Ramanan, Language Models as Black-Box Optimizers for Vision-Language Models, arXiv:2309.05950 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2309.05950. 157. X. Li, P. Yu, C. Zhou, T. Schick, O. Levy, L. Zettlemoyer, J. E. Weston, M. Lewis, 'Self-Alignment with Instruction Backtranslation' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=1oijHJBRsT. 158.* Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li, E. Tran-Johnson, E. Perez, . . . J. Kaplan, ConstitutionalAI: Harmlessness fromAIFeedback, arXiv:2212.08073 [cs.CL] (2022).https://doi.org/10.48550/arXiv.2212.08073. 159.* N. Sachdeva, B. Coleman, W.-C. Kang, J. Ni, L. Hong, E. H. Chi, J. Caverlee, J. McAuley, D. Z. Cheng, How to Train Data-EfficientLLMs, arXiv:2402.09668 [cs.LG] (2024).https://doi.org/10.48550/arXiv.2402.09668. 160. Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang, X. Xie, A Survey on Evaluation of Large Language Models. ACM Trans. Intell. Syst. Technol. 15, 39:1-39:45 (2024).https://doi.org/10.1145/3641289. 161. G. Marcus, E. Davis, S. Aaronson, A very preliminary analysis of DALL-E 2, arXiv:2204.13807 [cs.CV] (2022).https://doi.org/10.48550/arXiv.2204.13807. 162. A. Borji, Pros and cons of GAN evaluation measures: New developments. Comput. Vis. Image Underst. 215 (2022).https://doi.org/10.1016/j.cviu.2021.103329. 163. S. Zhou, M. Gordon, R. Krishna, A. Narcomey, L. F. Fei-Fei, M. Bernstein, 'HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models' in Advances in Neural Information Processing Systems (NeurIPS 2019) (Curran Associates, Inc., 2019) vol. 32.https://proceedings.neurips.cc/paper_files/paper/2019/hash/65699726a3c601b9f31bf04019c8593c-Abstract.html. 164. L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, H. Zhang, J. E. Gonzalez, I. Stoica, 'JudgingLLM-as-a-Judge with MT-Bench and Chatbot Arena' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Datasets and Benchmarks Track (2023).https://openreview.net/forum?id=uccHPGDlao. 165. P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. A. Cosgrove, C. D. Manning, C. Re, D. Acosta-Navas, D. A. Hudson, . . . Y. Koreeda, Holistic Evaluation of Language Models. Transactions on Machine Learning Research (2023).https://openreview.net/forum?id=iO4LZibEqW. 166.* T. Patwardhan, K. Liu, T. Markov, N. Chowdhury, D. Leet, N. Cone, C. Maltbie, J. Huizinga, C. Wainwright, S. f. Jackson, S. Adler, R. Casagrande, A. Madry, 'Building an early warning system forLLM-aided biological threat creation' (OpenAI, 2024);https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation. 167. I. D. Raji, E. Denton, E. M. Bender, A. Hanna, A. Paullada, 'AIand the Everything in the Whole Wide World Benchmark' in 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Datasets and Benchmarks Track (Round 2) (2021).https://openreview.net/forum?id=j6NxpQbREA1. 168. Y. Lecun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied to document recognition. Proc. IEEE Inst. Electr. Electron. Eng. 86, 2278-2324 (1998).https://doi.org/10.1109/5.726791. 169. A. Krizhevsky, 'Learning multiple layers of features from tiny images' (University of Toronto, 2009);www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf. 170. Z. Liu, P. Luo, X. Wang, X. Tang, 'Deep Learning Face Attributes in the Wild' in 2015 IEEE International Conference on Computer Vision (ICCV) (IEEE Computer Society, 2015) pp. 3730-3738.https://doi.org/10.1109/iccv.2015.425. 171. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, L. Fei-Fei, ImageNet Large Scale Visual Recognition Challenge. Int. J. Comput. Vis. 115, 211-252 (2015).https://doi.org/10.1007/s11263-015-0816-y. 172. T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, C. L. Zitnick, 'Microsoft COCO: Common Objects in Context' in Computer Vision - ECCV 2014 (Springer International Publishing, 2014) pp. 740-755.https://doi.org/10.1007/978-3-319-10602-1_48. 173. VQA Consortium, Visual Question Answering (2015).http://visualqa.org. 174. D. A. Hudson, C. D. Manning, 'GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering' in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019) pp. 6693-6702.https://doi.org/10.1109/cvpr.2019.00686. 175. A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, S. Bowman, 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding' in Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (Association for Computational Linguistics, 2018) pp. 353-355.https://doi.org/10.18653/v1/W18-5446. 176. A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, S. Bowman, 'SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems' in Advances in Neural Information Processing Systems (NeurIPS 2019) (Curran Associates, Inc., 2019) vol. 32.https://proceedings.neurips.cc/paper_files/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html. 177.* W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, N. Duan, AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models, arXiv:2304.06364 [cs.CL] (2023).http://arxiv.org/abs/2304.06364. 178. X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, S. Zhang, X. Deng, A. Zeng, Z. Du, C. Zhang, S. Shen, T. Zhang, Y. Su, H. Sun, . . . J. Tang, 'AgentBench: EvaluatingLLMsas Agents' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=zAdUB0aCTQ. 179. Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung, Q. V. Do, Y. Xu, P. Fung, 'A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity' in Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers) (Association for Computational Linguistics, 2023) pp. 675-718.https://doi.org/10.18653/v1/2023.ijcnlp-main.45. 180. S. Lin, J. Hilton, O. Evans, 'TruthfulQA: Measuring How Models Mimic Human Falsehoods' in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (Association for Computational Linguistics, 2022) pp. 3214-3252.https://doi.org/10.18653/v1/2022.acl-long.229. 181. A. Pan, J. S. Chan, A. Zou, N. Li, S. Basart, T. Woodside, H. Zhang, S. Emmons, D. Hendrycks, 'Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the MACHIAVELLI benchmark' in Proceedings of the 40th International Conference on Machine Learning (ICML'23) (JMLR.org, 2023) vol. 202, pp. 26837-26867. 182. N. Mu, S. Chen, Z. Wang, S. Chen, D. Karamardian, L. Aljeraisy, B. Alomair, D. Hendrycks, D. Wagner, CanLLMsFollow Simple Rules?, arXiv:2311.04235 [cs.AI] (2023).https://doi.org/10.48550/arXiv.2311.04235. 183. G. Mialon, C. Fourrier, T. Wolf, Y. LeCun, T. Scialom, 'GAIA: a benchmark for GeneralAIAssistants' in The 12th International Conference on Learning Representations (ICLR 2024) (2024).https://openreview.net/forum?id=fibxvahvs3. 184. T. Liao, R. Taori, D. Raji, L. Schmidt, 'Are We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning' in Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1 (NeurIPS Datasets and Benchmarks 2021) round2 (2021) vol. 1.https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/757b505cfd34c64c85ca5b5690ee5293-Abstract-round2.html. 185. D. Card, P. Henderson, U. Khandelwal, R. Jia, K. Mahowald, D. Jurafsky, 'With Little Power Comes Great Responsibility' in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020) (Association for Computational Linguistics, 2020) pp. 9263-9274.https://doi.org/10.18653/v1/2020.emnlp-main.745. 186. C. G. Northcutt, A. Athalye, J. Mueller, 'Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks' in 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Datasets and Benchmarks Track (Round 1) (2021).https://openreview.net/forum?id=XccDXrDNLek. 187. S. Gehman, S. Gururangan, M. Sap, Y. Choi, N. A. Smith, 'RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models' in Findings of the Association for Computational Linguistics: EMNLP 2020 (Association for Computational Linguistics, 2020) pp. 3356-3369.https://doi.org/10.18653/v1/2020.findings-emnlp.301. 188. T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, T. Scialom, 'Toolformer: Language Models Can Teach Themselves to Use Tools' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=Yacmpz84TH. 189. D. A. Boiko, R. MacKnight, G. Gomes, Emergent autonomous scientific research capabilities of large language models, arXiv:2304.05332 [physics.chem-ph] (2023).https://doi.org/10.48550/arXiv.2304.05332. 190. A. Paullada, I. D. Raji, E. M. Bender, E. Denton, A. Hanna, Data and its (dis)contents: A survey of dataset development and use in machine learning research. Patterns (N. Y.) 2, 100336 (2021).https://doi.org/10.1016/j.patter.2021.100336. 191. S. R. Bowman, G. Dahl, 'What Will it Take to Fix Benchmarking in Natural Language Understanding?' in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2021) (Association for Computational Linguistics, 2021) pp. 4843-4855.https://doi.org/10.18653/v1/2021.naacl-main.385. 192. B. Hutchinson, N. Rostamzadeh, C. Greer, K. Heller, V. Prabhakaran, 'Evaluation Gaps in Machine Learning Practice' in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22) (Association for Computing Machinery, 2022) pp. 1859-1876.https://doi.org/10.1145/3531146.3533233. 193. S. Golchin, M. Surdeanu, 'Time Travel inLLMs: Tracing Data Contamination in Large Language Models' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=2Rwq6c3tvr. 194. Y. Yang, A. Tomar, On the Planning, Search, and Memorization Capabilities of Large Language Models, arXiv:2309.01868 [cs.CL] (2023).http://arxiv.org/abs/2309.01868. 195.* M. R. Morris, J. Sohl-dickstein, N. Fiedel, T. Warkentin, A. Dafoe, A. Faust, C. Farabet, S. Legg, 'Levels ofAGI: Operationalizing Progress on the Path toAGI' (Google DeepMind, 2024);http://arxiv.org/abs/2311.02462. 196. S. Shankar, Y. Halpern, E. Breck, J. Atwood, J. Wilson, D. Sculley, 'No classification without representation: Assessing geodiversity issues in open data sets for the developing world' in 31st Conference on Neural Information Processing Systems (NIPS 2017) Machine learning for the developing world workshop (2017). 197. L. Aroyo, C. Welty, Truth Is a Lie: Crowd Truth and the Seven Myths of Human Annotation.AIMag. 36, 15-24 (2015).https://doi.org/10.1609/aimag.v36i1.2564. 198. M. L. Gordon, K. Zhou, K. Patel, T. Hashimoto, M. S. Bernstein, 'The Disagreement Deconvolution: Bringing Machine Learning Performance Metrics In Line With Reality' in Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI '21) (Association for Computing Machinery, 2021) pp. 1-14.https://doi.org/10.1145/3411764.3445423. 199. C. Firestone, Performance vs. competence in human-machine comparisons. Proc. Natl. Acad. Sci. U. S. A. 117, 26562-26571 (2020).https://doi.org/10.1073/pnas.1905334117. 200. L. Aroyo, A. S. Taylor, M. Diaz, C. M. Homan, A. Parrish, G. Serapio-Garcia, V. Prabhakaran, D. Wang, 'DICES Dataset: Diversity in ConversationalAIEvaluation for Safety' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Datasets and Benchmarks Track (2023).https://openreview.net/forum?id=GjNvvswoUL. 201. H. P. Cowley, M. Natter, K. Gray-Roncal, R. E. Rhodes, E. C. Johnson, N. Drenkow, T. M. Shead, F. S. Chance, B. Wester, W. Gray-Roncal, Author Correction: A framework for rigorous evaluation of human performance in human and machine learning comparison studies. Sci. Rep. 12, 11559 (2022).https://doi.org/10.1038/s41598-022-15857-5. 202. T. Shin, Y. Razeghi, R. L. Logan, IV, E. Wallace, S. Singh, 'AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts' in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020) (Association for Computational Linguistics, 2020) pp. 4222-4235.https://doi.org/10.18653/v1/2020.emnlp-main.346. 203. E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, G. Irving, 'Red Teaming Language Models with Language Models' in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022) (Association for Computational Linguistics, 2022) pp. 3419-3448.https://doi.org/10.18653/v1/2022.emnlp-main.225. 204.* D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann, E. Perez, N. Schiefer, K. Ndousse, A. Jones, S. Bowman, A. Chen, T. Conerly, N. DasSarma, D. Drain, N. Elhage, S. El-Showk, S. Fort, . . . J. Clark, 'Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned' (Anthropic, 2022);http://arxiv.org/abs/2209.07858. 205. S. Casper, J. Lin, J. Kwon, G. Culp, D. Hadfield-Menell, Explore, Establish, Exploit: Red Teaming Language Models from Scratch, arXiv:2306.09442 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2306.09442. 206. S. Tong, E. Jones, J. Steinhardt, 'Mass-Producing Failures of Multimodal Systems with Language Models' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=T6iiOqsGOh. 207. D. Ziegler, S. Nix, L. Chan, T. Bauman, P. Schmidt-Nielsen, T. Lin, A. Scherlis, N. Nabeshima, B. Weinstein-Raun, D. de Haas, B. Shlegeris, N. Thomas, 'Adversarial training for high-stakes reliability' in Advances in Neural Information Processing Systems (NeurIPS 2022) (2022) vol. 35, pp. 9274-9286.https://proceedings.neurips.cc//paper_files/paper/2022/hash/3c44405d619a6920384a45bce876b41e-Abstract-Conference.html. 208. Y. Liu, G. Deng, Z. Xu, Y. Li, Y. Zheng, Y. Zhang, L. Zhao, T. Zhang, K. Wang, Y. Liu, Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study, arXiv:2305.13860 [cs.SE] (2023).http://arxiv.org/abs/2305.13860. 209. A. Wei, N. Haghtalab, J. Steinhardt, 'Jailbroken: How DoesLLMSafety Training Fail?' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=jA235JGM09. 210. A. Zou, Z. Wang, N. Carlini, M. Nasr, J. Zico Kolter, M. Fredrikson, Universal and Transferable Adversarial Attacks on Aligned Language Models, arXiv:2307.15043 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2307.15043. 211. R. Shah, Q. F. Montixi, S. Pour, A. Tagade, J. Rando, 'Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Socially Responsible Language Modelling Research Workshop (SoLaR) (2023).https://openreview.net/forum?id=x3Ltqz1UFg. 212. A. Rao, S. Vashistha, A. Naik, S. Aditya, M. Choudhury, 'TrickingLLMsinto Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks' in 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024) (2024).https://doi.org/10.48550/arXiv.2305.14965. 213. V. Ojewale, R. Steed, B. Vecchione, A. Birhane, I. D. Raji, TowardsAIAccountability Infrastructure: Gaps and Opportunities inAIAudit Tooling, arXiv:2402.17861 [cs.CY] (2024).https://doi.org/10.48550/arXiv.2402.17861. 214. V. Turri, R. Dzombak, 'Why We Need to Know More: Exploring the State ofAIIncident Documentation Practices' in Proceedings of the 2023 AAAI/ACM Conference onAI, Ethics, and Society (AIES '23) (ACM, 2023) pp. 576-583.https://doi.org/10.1145/3600211.3604700. 215. S. Costanza-Chock, I. D. Raji, J. Buolamwini, 'Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem' in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22) (Association for Computing Machinery, 2022) pp. 1571-1583.https://doi.org/10.1145/3531146.3533213. 216. M. Feffer, A. Sinha, Z. C. Lipton, H. Heidari, Red-Teaming for GenerativeAI: Silver Bullet or Security Theater?, arXiv:2401.15897 [cs.CY] (2024).https://doi.org/10.48550/arXiv.2401.15897. 217. A. Birhane, R. Steed, V. Ojewale, B. Vecchione, I. D. Raji, 'SoK:AIAuditing: The Broken Bus on the Road toAIAccountability' in 2nd IEEE Conference on Secure and Trustworthy Machine Learning (2024).https://openreview.net/forum?id=TmagEd33w3. 218. S. Casper, T. Bu, Y. Li, J. Li, K. Zhang, K. Hariharan, D. Hadfield-Menell, 'Red Teaming Deep Neural Networks with Feature Synthesis Tools' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=Od6CHhPM7I. 219. S. Friedler, R. Singh, B. Blili-Hamelin, J. Metcalf, B. J. Chen, 'AIRed-Teaming Is Not a One-Stop Solution toAIHarms: Recommendations for Using Red-Teaming forAIAccountability' (Data & Society, 2023);https://datasociety.net/library/ai-red-teaming-is-not-a-one-stop-solution-to-ai-harms-recommendations-for-using-red-teaming-for-ai-accountability/. 220. D. R. Maffioli, Copyright in GenerativeAItraining: Balancing Fair Use through Standardization and Transparency. (2023).https://doi.org/10.13140/rg.2.2.18478.48961. 221. A. Karamolegkou, J. Li, L. Zhou, A. Sogaard, 'Copyright Violations and Large Language Models' in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023) (Association for Computational Linguistics, 2023) pp. 7403-7412.https://doi.org/10.18653/v1/2023.emnlp-main.458. 222. P. Henderson, X. Li, D. Jurafsky, T. Hashimoto, M. A. Lemley, P. Liang, Foundation Models and Fair Use, arXiv:2303.15715 [cs.CY] (2023).http://arxiv.org/abs/2303.15715. 223. A. Luccioni, J. Viviano, 'What's in the Box? An Analysis of Undesirable Content in the Common Crawl Corpus' in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers) (Association for Computational Linguistics, 2021) pp. 182-189.https://doi.org/10.18653/v1/2021.acl-short.24. 224. A. Birhane, V. Prabhu, S. Han, V. N. Boddeti, A. S. Luccioni, 'Into the LAION's Den: Investigating Hate in Multimodal Datasets' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=6URyQ9QhYv&noteId=6URyQ9QhYv. 225. Y. Qu, X. Shen, X. He, M. Backes, S. Zannettou, Y. Zhang, 'Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models' in Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security (CCS '23) (Association for Computing Machinery, 2023) pp. 3403-3417.https://doi.org/10.1145/3576915.3616679. 226. A. Birhane, V. U. Prabhu, E. Kahembwe, Multimodal datasets: misogyny, pornography, and malignant stereotypes, arXiv:2110.01963 [cs.CY] (2021).http://arxiv.org/abs/2110.01963. 227. N. Shahbazi, Y. Lin, A. Asudeh, H. V. Jagadish, Representation Bias in Data: A Survey on Identification and Resolution Techniques. ACM Comput. Surv. 55, 293:1-293:39 (2023).https://doi.org/10.1145/3588433. 228. D. Thiel, 'Identifying and EliminatingCSAMin Generative ML Training Data and Models' (Stanford Internet Observatory, 2023);https://purl.stanford.edu/kh752sm9123. 229. J. Kreutzer, I. Caswell, L. Wang, A. Wahab, D. van Esch, N. Ulzii-Orshikh, A. Tapo, N. Subramani, A. Sokolov, C. Sikasote, M. Setyawan, S. Sarin, S. Samb, B. Sagot, C. Rivera, A. Rios, I. Papadimitriou, S. Osei, P. O. Suarez, . . . M. Adeyemi, Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets. Trans. Assoc. Comput. Linguist. 10, 50-72 (2022). https://doi.org/10.1162/tacl_a_00447. 230. T. de Vries, I. Misra, C. Wang, L. van der Maaten, 'Does object recognition work for everyone?' in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR) workshops (2019).https://openaccess.thecvf.com/content_CVPRW_2019/papers/cv4gc/de_Vries_Does_Object_Recognition_Work_for_Everyone_CVPRW_2019_paper.pdf. 231. The New York Times Company v. Microsoft Corporation et al. (US District Court for the Southern District of New York, 2023).https://nytco-assets.nytimes.com/2023/12/NYT_Complaint_Dec2023.pdf. 232. J. Dodge, M. Sap, A. Marasovic, W. Agnew, G. Ilharco, D. Groeneveld, M. Mitchell, M. Gardner, 'Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus' in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021) (Association for Computational Linguistics, 2021) pp. 1286-1305.https://doi.org/10.18653/v1/2021.emnlp-main.98. 233. A. S. Luccioni, F. Corry, H. Sridharan, M. Ananny, J. Schultz, K. Crawford, 'A Framework for Deprecating Datasets: Standardizing Documentation, Identification, and Communication' in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22) (Association for Computing Machinery, 2022) pp. 199-212.https://doi.org/10.1145/3531146.3533086. 234. K. Peng, A. Mathur, A. Narayanan, 'Mitigating dataset harms requires stewardship: Lessons from 1000 papers' in Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1 (NeurIPS Datasets and Benchmarks 2021) round2 (2021).https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/077e29b11be80ab57e1a2ecabb7da330-Abstract-round2.html. 235. B. Magagna, D. Goldfarb, P. Martin, M. Atkinson, S. Koulouzis, Z. Zhao, 'Data Provenance' in Towards Interoperable Research Infrastructures for Environmental and Earth Sciences: A Reference Model Guided Approach for Common Challenges, Z. Zhao, M. Hellstrom, Eds. (Springer International Publishing, Cham, 2020), pp. 208-225. 236. S. Longpre, R. Mahari, A. Chen, N. Obeng-Marnu, D. Sileo, W. Brannon, N. Muennighoff, N. Khazam, J. Kabbara, K. Perisetla, X. Wu, E. Shippole, K. Bollacker, T. Wu, L. Villa, S. Pentland, S. Hooker, The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution inAI, arXiv:2310.16787 [cs.CL] (2023).http://arxiv.org/abs/2310.16787. 237. E. Perez, S. Ringer, K. Lukosiute, K. Nguyen, E. Chen, S. Heiner, C. Pettit, C. Olsson, S. Kundu, S. Kadavath, A. Jones, A. Chen, B. Mann, B. Israel, B. Seethor, C. McKinnon, C. Olah, D. Yan, D. Amodei, . . . J. Kaplan, 'Discovering Language Model Behaviors with Model-Written Evaluations' in Findings of the Association for Computational Linguistics: ACL 2023 (Association for Computational Linguistics, 2023) pp. 13387-13434.https://doi.org/10.18653/v1/2023.findings-acl.847. 238. M. Sharma, M. Tong, T. Korbak, D. Duvenaud, A. Askell, S. R. Bowman, E. Durmus, Z. Hatfield-Dodds, S. R. Johnston, S. M. Kravec, T. Maxwell, S. McCandlish, K. Ndousse, O. Rausch, N. Schiefer, D. Yan, M. Zhang, E. Perez, 'Towards Understanding Sycophancy in Language Models' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=tvhaxkMKAn. 239. S. Santurkar, E. Durmus, F. Ladhak, C. Lee, P. Liang, T. Hashimoto, 'Whose opinions do language models reflect?' in Proceedings of the 40th International Conference on Machine Learning (JMLR.org, 2023) vol. 202, pp. 29971-30004.https://dl.acm.org/doi/10.5555/3618408.3619652. 240. D. Go, T. Korbak, G. Kruszewski, J. Rozen, N. Ryu, M. Dymetman, 'Aligning Language Models with Preferences through f-divergence Minimization' in Proceedings of the 40th International Conference on Machine Learning (ICML 2023) (2023) pp. 11546-11583.https://proceedings.mlr.press/v202/go23a.html. 241. Z. Song, T. Cai, J. D. Lee, W. J. Su, 'Reward Collapse in Aligning Large Language Models: A Prompt-Aware Approach to Preference Rankings' in ICML 2023 Workshop The Many Facets of Preference-Based Learning (2023).https://openreview.net/forum?id=dpWxK6aqIK. 242. S. Hooker, Moving beyond 'algorithmic bias is a data problem'. Patterns (N Y) 2, 100241 (2021).https://doi.org/10.1016/j.patter.2021.100241. 243. V. H. Maluleke, N. Thakkar, T. Brooks, E. Weber, T. Darrell, A. A. Efros, A. Kanazawa, D. Guillory, 'Studying Bias in GANs Through the Lens of Race' in Computer Vision - ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XIII (Springer-Verlag, 2022) pp. 344-360.https://doi.org/10.1007/978-3-031-19778-9_20. 244. R. Bommasani, K. Klyman, S. Longpre, S. Kapoor, N. Maslej, B. Xiong, D. Zhang, P. Liang, 'The Foundation Model Transparency Index' (Center for Research on Foundation Models (CRFM) and Institute on Human-Centered Artificial Intelligence (HAI), 2023);http://arxiv.org/abs/2310.12941. 245. V. Lai, C. Chen, A. Smith-Renner, Q. V. Liao, C. Tan, 'Towards a Science of Human-AIDecision Making: An Overview of Design Space in Empirical Human-Subject Studies' in Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT '23) (Association for Computing Machinery, 2023) pp. 1369-1385.https://doi.org/10.1145/3593013.3594087. 246. G. Bansal, B. Nushi, E. Kamar, W. S. Lasecki, D. S. Weld, E. Horvitz, Beyond Accuracy: The Role of Mental Models in Human-AITeam Performance. HCOMP 7, 2-11 (2019).https://doi.org/10.1609/hcomp.v7i1.5285. 247. R. Geirhos, K. Meding, F. A. Wichmann, 'Beyond accuracy: quantifying trial-by-trial behaviour of CNNs and humans by measuring error consistency' in Proceedings of the 34th International Conference on Neural Information Processing Systems (NIPS '20) (Curran Associates Inc., 2020) pp. 13890-13902.https://dl.acm.org/doi/10.5555/3495724.3496889. 248. A. Albright, If you give a judge a risk score: evidence from Kentucky bail decisions. Law, Economics, and Business Fellows' Discussion Paper Series 85, 2019-1 (2019).http://www.law.harvard.edu/programs/olin_center/Prizes/2019-1.pdf. 249. M. Hoffman, L. B. Kahn, D. Li, Discretion in Hiring*. The Quarterly Journal of Economics 133, 765-800 (2018).https://doi.org/10.1093/qje/qjx042. 250. E. Brynjolfsson, D. Li, L. Raymond, 'GenerativeAIat Work' (National Bureau of Economic Research, 2023);https://doi.org/10.3386/w31161. 251. V. Marda, S. Narayan, 'Data in New Delhi's predictive policing system' in Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (Association for Computing Machinery, 2020) pp. 317-324.https://doi.org/10.1145/3351095.3372865. 252. U. Ehsan, R. Singh, J. Metcalf, M. Riedl, 'The Algorithmic Imprint' in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22) (Association for Computing Machinery, 2022) pp. 1305-1317.https://doi.org/10.1145/3531146.3533186. 253. I. D. Raji, P. Xu, C. Honigsberg, D. Ho, 'Outsider Oversight: Designing a Third Party Audit Ecosystem forAIGovernance' in Proceedings of the 2022 AAAI/ACM Conference onAI, Ethics, and Society (Association for Computing Machinery, 2022) pp. 557-571.https://doi.org/10.1145/3514094.3534181. 254. X. Shen, Z. Chen, M. Backes, Y. Shen, Y. Zhang, 'Do Anything Now': Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models, arXiv:2308.03825 [cs.CR] (2023).http://arxiv.org/abs/2308.03825. 255. R. Tolosana, R. Vera-Rodriguez, J. Fierrez, A. Morales, J. Ortega-Garcia, Deepfakes and beyond: A Survey of face manipulation and fake detection. Information Fusion 64, 131-148 (2020).https://doi.org/10.1016/j.inffus.2020.06.014. 256. M. Mustak, J. Salminen, M. Mantymaki, A. Rahman, Y. K. Dwivedi, Deepfakes: Deceptions, mitigations, and opportunities. Journal of Business Research 154, 113368 (2023).https://doi.org/10.1016/j.jbusres.2022.113368. 257. M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchinson, E. Spitzer, I. D. Raji, T. Gebru, 'Model Cards for Model Reporting' in Proceedings of the Conference on Fairness, Accountability, and Transparency (ACM, 2019) pp. 220-229.https://doi.org/10.1145/3287560.3287596. 258. W. Liang, N. Rajani, X. Yang, E. Ozoani, E. Wu, Y. Chen, D. S. Smith, J. Zou, What's documented inAI? Systematic Analysis of 32KAIModel Cards, arXiv:2402.05160 [cs.SE] (2024).https://doi.org/10.48550/arXiv.2402.05160. 259. E. M. Bender, B. Friedman, Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science. Transactions of the Association for Computational Linguistics 6, 587-604 (2018).https://doi.org/10.1162/tacl_a_00041. 260. B. Hutchinson, A. Smart, A. Hanna, E. Denton, C. Greer, O. Kjartansson, P. Barnes, M. Mitchell, 'Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure' in Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21) (Association for Computing Machinery, 2021) pp. 560-575.https://doi.org/10.1145/3442188.3445918. 261. T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. D. Iii, K. Crawford, Datasheets for datasets. Commun. ACM 64, 86-92 (2021).https://doi.org/10.1145/3458723. 262. M. Arnold, R. K. E. Bellamy, M. Hind, S. Houde, S. Mehta, A. Mojsilovic, R. Nair, K. N. Ramamurthy, A. Olteanu, D. Piorkowski, D. Reimer, J. Richards, J. Tsay, K. R. Varshney, FactSheets: Increasing trust inAIservices through supplier's declarations of conformity. IBM Journal of Research and Development 63, 6:1-6:13 (2019).https://doi.org/10.1147/jrd.2019.2942288. 263. F. Gursoy, I. A. Kakadiaris, System Cards forAI-Based Decision-Making for Public Policy, arXiv:2203.04754 [cs.CY] (2022).https://doi.org/10.48550/arXiv.2203.04754. 264. I. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, J. Smith-Loud, D. Theron, P. Barnes, 'Closing theAIaccountability gap: defining an end-to-end framework for internal algorithmic auditing' in Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* '20) (Association for Computing Machinery, 2020) pp. 33-44.https://doi.org/10.1145/3351095.3372873. 265. C. Chen, O. Li, D. Tao, A. Barnett, C. Rudin, J. K. Su, 'This Looks Like That: Deep Learning for Interpretable Image Recognition' in Advances in Neural Information Processing Systems (NeurIPS 2019) (Curran Associates, Inc., 2019) vol. 32.https://proceedings.neurips.cc/paper_files/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html. 266. M. Danilevsky, K. Qian, R. Aharonov, Y. Katsis, B. Kawas, P. Sen, 'A Survey of the State of ExplainableAIfor Natural Language Processing' in Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing (AACL 2020) (Association for Computational Linguistics, 2020) pp. 447-459.https://aclanthology.org/2020.aacl-main.46. 267. A. Das, P. Rad, Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey, arXiv:2006.11371 [cs.CV] (2020).https://doi.org/10.48550/arXiv.2006.11371. 268. D. Minh, H. X. Wang, Y. F. Li, T. N. Nguyen, Explainable artificial intelligence: a comprehensive review. Artif Intell Rev 55, 3503-3568 (2022).https://doi.org/10.1007/s10462-021-10088-y. 269. J. Gryz, M. Rojszczak, Black box algorithms and the rights of individuals: no easy solution to the 'explainability' problem. Internet Pol. Rev. 10 (2021).https://policyreview.info/articles/analysis/black-box-algorithms-and-rights-individuals-no-easy-solution-explainability. 270. T. Ploug, S. Holm, 'Right to ContestAIDiagnostics' in Artificial Intelligence in Medicine, N. Lidstromer, H. Ashrafian, Eds. (Springer International Publishing, Cham, 2022), pp. 227-238. 271. M. C. Buiten, L. A. Dennis, M. Schwammberger, 'A Vision on What Explanations of Autonomous Systems are of Interest to Lawyers' in 2023 IEEE 31st International Requirements Engineering Conference Workshops (REW) (2023) pp. 332-336.https://doi.org/10.1109/rew57809.2023.00062. 272. M. Wortsman, V. Ramanujan, R. Liu, A. Kembhavi, M. Rastegari, J. Yosinski, A. Farhadi, 'Supermasks in Superposition' in Advances in Neural Information Processing Systems (NeurIPS 2020) (Curran Associates, Inc., 2020) vol. 33, pp. 15173-15184.https://proceedings.neurips.cc/paper/2020/hash/ad1f8bb9b51f023cdc80cf94bb615aa9-Abstract.html. 273. D. Bau, B. Zhou, A. Khosla, A. Oliva, A. Torralba, 'Network Dissection: Quantifying Interpretability of Deep Visual Representations' in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017) pp. 3319-3327.https://doi.org/10.1109/cvpr.2017.354. 274. C. Olah, A. Mordvintsev, L. Schubert, Feature Visualization. Distill 2, 10.23915/distill.00007 (2017).https://doi.org/10.23915/distill.00007. 275. A. Ghorbani, J. Y. Zou, 'Neuron Shapley: Discovering the Responsible Neurons' in Advances in Neural Information Processing Systems (NeurIPS 2020) (Curran Associates, Inc., 2020) vol. 33, pp. 5922-5932.https://proceedings.neurips.cc/paper/2020/hash/41c542dfe6e4fc3deb251d64cf6ed2e4-Abstract.html. 276. C. Olah, N. Cammarata, L. Schubert, G. Goh, M. Petrov, S. Carter, Zoom In: An Introduction to Circuits. Distill (2020).https://doi.org/10.23915/distill.00024.001. 277. A. Conmy, A. N. Mavor-Parker, A. Lynch, S. Heimersheim, A. Garriga-Alonso, 'Towards Automated Circuit Discovery for Mechanistic Interpretability' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=89ia77nZ8u. 278. B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas, R. Sayres, 'Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)' in Proceedings of the 35th International Conference on Machine Learning (PMLR, 2018) pp. 2668-2677.https://proceedings.mlr.press/v80/kim18d.html. 279. Y. Belinkov, Probing Classifiers: Promises, Shortcomings, and Advances. Comput. Linguist. Assoc. Comput. Linguist. 48, 207-219 (2022).https://doi.org/10.1162/coli_a_00422. 280. S. Black, L. Sharkey, L. Grinsztajn, E. Winsor, D. Braun, J. Merizian, K. Parker, C. R. Guevara, B. Millidge, G. Alfour, C. Leahy, Interpreting Neural Networks through the Polytope Lens, arXiv:2211.12312 [cs.LG] (2022).http://arxiv.org/abs/2211.12312. 281. A. Zou, L. Phan, S. Chen, J. Campbell, P. Guo, R. Ren, A. Pan, X. Yin, M. Mazeika, A.-K. Dombrowski, S. Goel, N. Li, M. J. Byun, Z. Wang, A. Mallen, S. Basart, S. Koyejo, D. Song, M. Fredrikson, . . . D. Hendrycks, Representation Engineering: A Top-Down Approach toAITransparency, arXiv:2310.01405 [cs.LG] (2023).https://doi.org/10.48550/arXiv.2310.01405. 282. S. Marks, C. Rager, E. J. Michaud, Y. Belinkov, D. Bau, A. Mueller, Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models, arXiv:2403.19647 [cs.LG] (2024).https://doi.org/10.48550/arXiv.2403.19647. 283. S. Carter, Z. Armstrong, L. Schubert, I. Johnson, C. Olah, Activation Atlas. Distill 4, 10.23915/distill.00015 (2019).https://doi.org/10.23915/distill.00015. 284. J. Mu, J. Andreas, 'Compositional Explanations of Neurons' in Advances in Neural Information Processing Systems (NeurIPS 2020) (Curran Associates, Inc., 2020) vol. 33, pp. 17153-17163.https://proceedings.neurips.cc/paper/2020/hash/c74956ffb38ba48ed6ce977af6727275-Abstract.html. 285. E. Hernandez, S. Schwettmann, D. Bau, T. Bagashvili, A. Torralba, J. Andreas, 'Natural Language Descriptions of Deep Visual Features' in The 10th International Conference on Learning Representations (2022).https://openreview.net/forum?id=NudBMY-tzDr. 286. S. Casper, M. Nadeau, D. Hadfield-Menell, G. Kreiman, 'Robust Feature-Level Adversaries are Interpretability Tools' in 36th Conference on Neural Information Processing Systems (NeurIPS 2022) (2022).https://openreview.net/forum?id=lQ-doSB2o. 287. M. Geva, J. Bastings, K. Filippova, A. Globerson, 'Dissecting Recall of Factual Associations in Auto-Regressive Language Models' in The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023) (2023) pp. 11116-12235.https://openreview.net/forum?id=F1G7y94K02. 288.* T. Bolukbasi, A. Pearce, A. Yuan, A. Coenen, E. Reif, F. Viegas, M. Wattenberg, An Interpretability Illusion for BERT, arXiv:2104.07143 [cs.CL] (2021).https://doi.org/10.48550/arXiv.2104.07143. 289. A. Makelov, G. Lange, A. Geiger, N. Nanda, 'Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=Ebt7JgMHv1. 290. M. Ananny, K. Crawford, Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability. New Media Soc. 20, 973-989 (2018).https://doi.org/10.1177/1461444816676645. 291. T. Miller, Explanation in artificial intelligence: Insights from the social sciences. Artif. Intell. 267, 1-38 (2019).https://doi.org/10.1016/j.artint.2018.07.007. 292. F. Doshi-Velez, B. Kim, Towards A Rigorous Science of Interpretable Machine Learning, arXiv:1702.08608 [stat.ML] (2017).https://doi.org/10.48550/arXiv.1702.08608. 293. Z. C. Lipton, The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery. ACM Queue 16, 31-57 (2018).https://doi.org/10.1145/3236386.3241340. 294.* M. L. Leavitt, A. Morcos, Towards falsifiable interpretability research, arXiv:2010.12016 [cs.CY] (2020).http://arxiv.org/abs/2010.12016. 295. T. Rauker, A. Ho, S. Casper, D. Hadfield-Menell, Toward TransparentAI: A Survey on Interpreting the Inner Structures of Deep Neural Networks, arXiv:2207.13243 [cs.LG] (2022).http://arxiv.org/abs/2207.13243. 296. M. Krishnan, Against Interpretability: a Critical Examination of the Interpretability Problem in Machine Learning. Philos. Technol. 33, 487-502 (2020).https://doi.org/10.1007/s13347-019-00372-9. 297. J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt, B. Kim, 'Sanity Checks for Saliency Maps' in Advances in Neural Information Processing Systems (NeurIPS 2018) (Curran Associates, Inc., 2018) vol. 31.https://proceedings.neurips.cc/paper_files/paper/2018/hash/294a8ed24b1ad22ec2e7efea049b8737-Abstract.html. 298. J. Adebayo, M. Muelly, I. Liccardi, B. Kim, 'Debugging Tests for Model Explanations' in Advances in Neural Information Processing Systems (NeurIPS 2020) (Curran Associates, Inc., 2020) vol. 33, pp. 700-712.https://proceedings.neurips.cc/paper/2020/hash/075b051ec3d22dac7b33f788da631fd4-Abstract.html. 299. P. Hase, M. Bansal, B. Kim, A. Ghandeharioun, 'Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=EldbUlZtbd. 300. S. Casper, C. Ezell, C. Siegmann, N. Kolt, T. L. Curtis, B. Bucknall, A. Haupt, K. Wei, J. Scheurer, M. Hobbhahn, L. Sharkey, S. Krishna, M. Von Hagen, S. Alberti, A. Chan, Q. Sun, M. Gerovitch, D. Bau, M. Tegmark, . . . D. Hadfield-Menell, Black-Box Access is Insufficient for RigorousAIAudits, arXiv:2401.14446 [cs.CY] (2024).http://arxiv.org/abs/2401.14446. 301. B. S. Bucknall, R. F. Trager, 'Structured access for third-party research on frontier Ai models: Investigating researchers' model access requirements' (Oxford Martin School, University of Oxford and Center for the Governance ofAI, 2023);www.ceris.be/wp-content/uploads/2024/03/Structured-Access-for-Third-Party-Research-Robert-Trager.pdf. 302. R. Ashmore, R. Calinescu, C. Paterson, Assuring the machine learning lifecycle. ACM Comput. Surv. 54, 1-39 (2022).https://doi.org/10.1145/3453444. 303. S. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer, J. Rando, R. Freedman, T. Korbak, D. Lindner, P. Freire, T. T. Wang, S. Marks, C.-R. Segerie, M. Carroll, A. Peng, P. Christoffersen, M. Damani, S. Slocum, U. Anwar, . . . D. Hadfield-Menell, Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. Transactions on Machine Learning Research (2023).https://openreview.net/forum?id=bx24KpJ4Eb. 304. S. Longpre, S. Kapoor, K. Klyman, A. Ramaswami, R. Bommasani, B. Blili-Hamelin, Y. Huang, A. Skowron, Z.-X. Yong, S. Kotha, Y. Zeng, W. Shi, X. Yang, R. Southen, A. Robey, P. Chao, D. Yang, R. Jia, D. Kang, . . . P. Henderson, A Safe Harbor forAIEvaluation and Red Teaming, arXiv:2403.04893 [cs.AI] (2024).https://doi.org/10.48550/arXiv.2403.04893. 305. T. Shevlane, Structured access: an emerging paradigm for safeAIdeployment, arXiv:2201.05159 [cs.AI] (2022).https://doi.org/10.48550/arXiv.2201.05159. 306. M. Dolata, S. Feuerriegel, G. Schwabe, A sociotechnical view of algorithmic fairness. Inf. Syst. J. 32, 754-818 (2022).https://doi.org/10.1111/isj.12370. 307. S. Lazar, A. Nelson,AIsafety on whose terms? Science 381, 138 (2023).https://doi.org/10.1126/science.adi8982. 308. Y. Wang, Y. Zhu, C. Kong, S. Wei, X. Yi, X. Xie, J. Sang, CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models, arXiv:2311.16421 [cs.CL] (2023).http://arxiv.org/abs/2311.16421. 309. Z. X. Yong, C. Menghini, S. Bach, 'Low-Resource Languages Jailbreak GPT-4' in NeurIPS Workshop on Socially Responsible Language Modelling Research (SoLaR) (2023).https://openreview.net/forum?id=pn83r8V2sv. 310. Y. Jin, M. Chandra, G. Verma, Y. Hu, M. De Choudhury, S. Kumar, Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries, arXiv:2310.13132 [cs.CL] (2023).http://arxiv.org/abs/2310.13132. 311.* A. Ustun, V. Aryabumi, Z.-X. Yong, W.-Y. Ko, D. D'souza, G. Onilude, N. Bhandari, S. Singh, H.-L. Ooi, A. Kayid, F. Vargus, P. Blunsom, S. Longpre, N. Muennighoff, M. Fadaee, J. Kreutzer, S. Hooker, Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model, arXiv:2402.07827 [cs.CL] (2024).http://arxiv.org/abs/2402.07827. 312. Y. Xu, H. Cao, W. Du, W. Wang, A Survey of Cross-lingual Sentiment Analysis: Methodologies, Models and Evaluations. Data Science and Engineering 7, 279-299 (2022).https://doi.org/10.1007/s41019-022-00187-3. 313.* W. Wang, Z. Tu, C. Chen, Y. Yuan, J.-T. Huang, W. Jiao, M. R. Lyu, All Languages Matter: On the Multilingual Safety of Large Language Models, arXiv:2310.00905 [cs.CL] (2023).http://arxiv.org/abs/2310.00905. 314. A. D. Selbst, An Institutional View Of Algorithmic Impact Assessments. 35 (2021).https://jolt.law.harvard.edu/assets/articlePDFs/v35/Selbst-An-Institutional-View-of-Algorithmic-Impact-Assessments.pdf. 315. S. L. Blodgett, S. Barocas, H. Daume, III, H. Wallach, 'Language (Technology) is Power: A Critical Survey of ""Bias"" in NLP' in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020) (Association for Computational Linguistics, 2020) pp. 5454-5476.https://doi.org/10.18653/v1/2020.acl-main.485. 316. A. Hagerty, I. Rubinov, GlobalAIEthics: A Review of the Social Impacts and Ethical Implications of Artificial Intelligence, arXiv:1907.07892 [cs.CY] (2019).https://doi.org/10.48550/arXiv.1907.07892. 317. M. M. Maas, 'AligningAIRegulation to Sociotechnical Change' in The Oxford Handbook ofAIGovernance, J. B. Bullock, Y.-C. Chen, J. Himmelreich, V. M. Hudson, A. Korinek, M. M. Young, B. Zhang, Eds. (Oxford University Press, 2022). 318.* L. Weidinger, J. Barnhart, J. Brennan, C. Butterfield, S. Young, W. Hawkins, L. A. Hendricks, R. Comanescu, O. Chang, M. Rodriguez, J. Beroshi, D. Bloxwich, L. Proleev, J. Chen, S. Farquhar, L. Ho, I. Gabriel, A. Dafoe, W. Isaac, 'Holistic Safety and Responsibility Evaluations of AdvancedAIModels' (Google Deepmind, 2024);http://arxiv.org/abs/2404.14068. 319. M. Raghavan, The Societal Impacts of Algorithmic Decision-Making (Association for Computing Machinery, New York, NY, USA, ed. 1, 2023), vol. 53, pp. 366. 320. S. Kapoor, R. Bommasani, K. Klyman, S. Longpre, A. Ramaswami, P. Cihon, A. Hopkins, K. Bankston, S. Biderman, M. Bogen, R. Chowdhury, A. Engler, P. Henderson, Y. Jernite, S. Lazar, S. Maffulli, A. Nelson, J. Pineau, A. Skowron, . . . A. Narayanan, 'On the Societal Impact of Open Foundation Models' (Stanford Institute for Human-Centered Artificial Intelligence, 2024);https://crfm.stanford.edu/open-fms/paper.pdf. 321. E. Moss, E. A. Watkins, R. Singh, M. C. Elish, J. Metcalf, 'Assembling Accountability: Algorithmic Impact Assessment for the Public Interest' (Data & Society, 2021);https://datasociety.net/library/assembling-accountability-algorithmic-impact-assessment-for-the-public-interest/. 322.* I. Solaiman, Z. Talat, W. Agnew, L. Ahmad, D. Baker, S. L. Blodgett, H. Daume, III, J. Dodge, E. Evans, S. Hooker, Y. Jernite, A. S. Luccioni, A. Lusoli, M. Mitchell, J. Newman, M.-T. Png, A. Strait, A. Vassilev, Evaluating the Social Impact of GenerativeAISystems in Systems and Society, arXiv:2306.05949 [cs.CY] (2023).http://arxiv.org/abs/2306.05949. 323.* L. Weidinger, M. Rauh, N. Marchal, A. Manzini, L. A. Hendricks, J. Mateos-Garcia, S. Bergman, J. Kay, C. Griffin, B. Bariach, I. Gabriel, V. Rieser, W. Isaac, 'Sociotechnical Safety Evaluation of GenerativeAISystems' (Google Deepmind, 2023);http://arxiv.org/abs/2310.11986. 324. A. D. Selbst, D. Boyd, S. A. Friedler, S. Venkatasubramanian, J. Vertesi, 'Fairness and Abstraction in Sociotechnical Systems' in Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* '19) (Association for Computing Machinery, 2019) pp. 59-68.https://doi.org/10.1145/3287560.3287598. 325. E. Moss, J. Metcalf, 'Ethics Owners: A New Model of Organizational Responsibility in Data-Driven Technology Companies' (Data & Society, 2020);https://datasociety.net/wp-content/uploads/2020/09/Ethics-Owners_20200923-DataSociety.pdf. 326. M. Feffer, M. Skirpan, Z. Lipton, H. Heidari, 'From Preference Elicitation to Participatory ML: A Critical Survey & Guidelines for Future Research' in Proceedings of the 2023 AAAI/ACM Conference onAI, Ethics, and Society (AIES '23) (ACM, 2023) pp. 38-48.https://doi.org/10.1145/3600211.3604661. 327. F. Delgado, S. Yang, M. Madaio, Q. Yang, 'The Participatory Turn inAIDesign: Theoretical Foundations and the Current State of Practice' in Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO '23) (Association for Computing Machinery, 2023) pp. 1-23.https://doi.org/10.1145/3617694.3623261. 328. A. Birhane, W. Isaac, V. Prabhakaran, M. Diaz, M. C. Elish, I. Gabriel, S. Mohamed, 'Power to the People? Opportunities and Challenges for ParticipatoryAI' in Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO '22) (Association for Computing Machinery, 2022) pp. 1-8.https://doi.org/10.1145/3551624.3555290. 329. J. Metcalf, E. Moss, E. A. Watkins, R. Singh, M. C. Elish, 'Algorithmic Impact Assessments and Accountability: The Co-construction of Impacts' in Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21) (Association for Computing Machinery, 2021) pp. 735-746.https://doi.org/10.1145/3442188.3445935. 330. D. Martin, Jr, V. Prabhakaran, J. Kuhlberg, A. Smart, W. S. Isaac, 'Participatory Problem Formulation for Fairer Machine Learning Through Community Based System Dynamics' in ICLR Workshop on Machine Learning in Real Life (2020).https://doi.org/10.48550/arXiv.2005.07572. 331. M. Sloane, E. Moss, O. Awomolo, L. Forlano, 'Participation Is not a Design Fix for Machine Learning' in Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO '22) (Association for Computing Machinery, 2022) pp. 1-6.https://doi.org/10.1145/3551624.3555285. 332. R. I. J. Dobbe, T. K. Gilbert, Y. Mintz, 'Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments (AIES '20)' in Proceedings of the AAAI/ACM Conference onAI, Ethics, and Society (Association for Computing Machinery, 2020) pp. 242.https://doi.org/10.1145/3375627.3375861. 333. S. A. Friedler, C. Scheidegger, S. Venkatasubramanian, The (Im)possibility of fairness: different value systems require different mechanisms for fair decision making. Commun. ACM 64, 136-143 (2021).https://doi.org/10.1145/3433949. 334. N. Guha, C. M. Lawrence, L. A. Gailmard, K. T. Rodolfa, F. Surani, R. Bommasani, I. D. Raji, M.-F. Cuellar, C. Honigsberg, P. Liang, D. E. Ho,AIRegulation Has Its Own Alignment Problem: The Technical and Institutional Feasibility of Disclosure, Registration, Licensing, and Auditing. The George Washington Law Review 92 (2024).https://dho.stanford.edu/wp-content/uploads/AI_Regulation.pdf. 335. U. Bhatt, A. Xiang, S. Sharma, A. Weller, A. Taly, Y. Jia, J. Ghosh, R. Puri, J. M. F. Moura, P. Eckersley, 'Explainable machine learning in deployment' in Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* '20) (Association for Computing Machinery, 2020) pp. 648-657.https://doi.org/10.1145/3351095.3375624. 336. K. Kaye, P. Dixon, 'Risky Analysis: Assessing and ImprovingAIGovernance Tools An international review ofAIGovernance Tools and suggestions for pathways forward' (World Privacy Forum, 2023);www.worldprivacyforum.org/wp-content/uploads/2023/12/WPF_Risky_Analysis_December_2023_fs.pdf. 337. D. Slack, S. Hilgard, E. Jia, S. Singh, H. Lakkaraju, 'Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods' in Proceedings of the AAAI/ACM Conference onAI, Ethics, and Society (AIES '20) (Association for Computing Machinery, 2020) pp. 180-186.https://doi.org/10.1145/3375627.3375830. 338. I. D. Raji, J. Yang, 'ABOUT ML: Annotation and Benchmarking on Understanding and Transparency of Machine Learning Lifecycles' in NeurIPS 2019 Human-Centric Machine Learning Workshop (2019).https://doi.org/10.48550/arXiv.1912.06166. 339. Project Management Institute (PMI), 'A guide to the project management body of knowledge (PMBOK Guide)' (PMI, 2000). 340. P. V. Falade, Decoding the Threat Landscape : ChatGPT, FraudGPT, and WormGPT in Social Engineering Attacks. int. j. sci. res. comput. sci. eng. inf. technol. 9, 185-198 (2023).https://doi.org/10.32628/cseit2390533. 341. Y. Yao, J. Duan, K. Xu, Y. Cai, Z. Sun, Y. Zhang, A Survey on Large Language Model (LLM) Security and Privacy: The Good, The Bad, and The Ugly. High-Confidence Computing 4, 100211 (2024).https://doi.org/10.1016/j.hcc.2024.100211. 342. N. Begou, J. Vinoy, A. Duda, M. Korczynski, 'Exploring the dark side ofAI: Advanced phishing attack design and deployment using ChatGPT' in 2023 IEEE Conference on Communications and Network Security (CNS) (IEEE, 2023).https://doi.org/10.1109/cns59707.2023.10288940. 343.* M. Heinemeyer, Cyber Security Threats - Email Compromise With GenerativeAI. (2023).https://es.darktrace.com/blog/tackling-the-soft-underbelly-of-cyber-security-email-compromise. 344. O. Bendel, The synthetization of human voices.AISoc. 34, 83-89 (2019).https://doi.org/10.1007/s00146-017-0748-x. 345. R. Gil, J. Virgili-Goma, J.-M. Lopez-Gil, R. Garcia, Deepfakes: evolution and trends. Soft Computing 27, 11295-11318 (2023).https://doi.org/10.1007/s00500-023-08605-y. 346. A. F. Gambin, A. Yazidi, A. Vasilakos, H. Haugerud, Y. Djenouri, Deepfakes: current and future trends. Artificial Intelligence Review 57, 64 (2024).https://doi.org/10.1007/s10462-023-10679-x. 347. V. Ciancaglini, C. Gibson, D. Sancho, O. McCarthy, M. Eira, P. Amann, A. Klayn, 'Malicious uses and abuses of artificial intelligence' (European Union Agency for Law Enforcement Cooperation, 2020). 348. R. Umbach, N. Henry, G. Beard, C. Berryessa, Non-Consensual Synthetic Intimate Imagery: Prevalence, Attitudes, and Knowledge in 10 Countries, arXiv:2402.01721 [cs.CY] (2024).https://doi.org/10.48550/arXiv.2402.01721. 349. M. B. Kugler, C. Pace, Deepfake Privacy: Attitudes and Regulation. Northwest. Univ. Law Rev. 116, 611-680 (2021).https://scholarlycommons.law.northwestern.edu/nulr/vol116/iss3/1. 350. M. Viola, C. Voto, Designed to abuse? Deepfakes and the non-consensual diffusion of intimate images. Synthese 201, 30 (2023).https://doi.org/10.1007/s11229-022-04012-2. 351. D. M. J. Lazer, M. A. Baum, Y. Benkler, A. J. Berinsky, K. M. Greenhill, F. Menczer, M. J. Metzger, B. Nyhan, G. Pennycook, D. Rothschild, M. Schudson, S. A. Sloman, C. R. Sunstein, E. A. Thorson, D. J. Watts, J. L. Zittrain, The science of fake news. Science 359, 1094-1096 (2018).https://doi.org/10.1126/science.aao2998. 352. G. Spitale, N. Biller-Andorno, F. Germani,AImodel GPT-3 (dis)informs us better than humans. Sci Adv 9, eadh1850 (2023).https://doi.org/10.1126/sciadv.adh1850. 353. M. Jakesch, J. T. Hancock, M. Naaman, Human heuristics forAI-generated language are flawed. Proc. Natl. Acad. Sci. U. S. A. 120, e2208839120 (2023).https://doi.org/10.1073/pnas.2208839120. 354. K.-C. Yang, F. Menczer, Anatomy of anAI-powered malicious social botnet, arXiv:2307.16336 [cs.CY] (2023).https://doi.org/10.48550/arXiv.2307.16336. 355. M. Masood, M. Nawaz, K. M. Malik, A. Javed, A. Irtaza, H. Malik, Deepfakes generation and detection: state-of-the-art, open challenges, countermeasures, and way forward. Applied Intelligence 53, 3974-4026 (2023).https://doi.org/10.1007/s10489-022-03766-z. 356. D. Cooke, A. Edwards, S. Barkoff, K. Kelly, As Good As A Coin Toss: Human detection ofAI-generated images, videos, audio, and audiovisual stimuli, arXiv:2403.16760 [cs.HC] (2024).http://arxiv.org/abs/2403.16760. 357. S. J. Nightingale, H. Farid,AI-synthesized faces are indistinguishable from real faces and more trustworthy. Proc. Natl. Acad. Sci. U. S. A. 119 (2022).https://doi.org/10.1073/pnas.2120481119. 358. S. C. Matz, J. D. Teeny, S. S. Vaid, H. Peters, G. M. Harari, M. Cerf, The potential of generativeAIfor personalized persuasion at scale. Sci. Rep. 14, 4692 (2024).https://doi.org/10.1038/s41598-024-53755-0. 359. H. Bai, J. G. Voelkel, J. C. Eichstaedt, R. Willer, Artificial Intelligence Can Persuade Humans on Political Issues. (2023).https://doi.org/10.31219/osf.io/stakv. 360. K. Hackenburg, L. Ibrahim, B. M. Tappin, M. Tsakiris, Comparing the persuasiveness of role-playing large language models and human experts on polarized U.S. political issues. (2023).https://doi.org/10.31219/osf.io/ey8db. 361. K. Hackenburg, H. Margetts, Evaluating the persuasive influence of political microtargeting with large language models. (2023).https://doi.org/10.31219/osf.io/wnt8b. 362. A. Simchon, M. Edwards, S. Lewandowsky, The persuasive effects of political microtargeting in the age of generative artificial intelligence. PNAS Nexus 3, gae035 (2024).https://doi.org/10.1093/pnasnexus/pgae035. 363. B. M. Tappin, C. Wittenberg, L. B. Hewitt, A. J. Berinsky, D. G. Rand, Quantifying the potential persuasive returns to political microtargeting. Proc. Natl. Acad. Sci. U. S. A. 120, e2216261120 (2023).https://doi.org/10.1073/pnas.2216261120. 364. F. Salvi, M. H. Ribeiro, R. Gallotti, R. West, On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial, arXiv:2403.14380 [cs.CY] (2024).https://doi.org/10.48550/arXiv.2403.14380. 365. T. H. Costello, G. Pennycook, D. G. Rand, Durably reducing conspiracy beliefs through dialogues withAI. (2024).https://doi.org/10.31234/osf.io/xcwdn. 366. P. S. Park, S. Goldstein, A. O'Gara, M. Chen, D. Hendrycks,AIdeception: A survey of examples, risks, and potential solutions. PATTER 5 (2024).https://doi.org/10.1016/j.patter.2024.100988. 367.* M. Phuong, M. Aitchison, E. Catt, S. Cogan, A. Kaskasoli, V. Krakovna, D. Lindner, M. Rahtz, Y. Assael, S. Hodkinson, H. Howard, T. Lieberum, R. Kumar, M. A. Raad, A. Webson, L. Ho, S. Lin, S. Farquhar, M. Hutter, . . . T. Shevlane, 'Evaluating Frontier Models for Dangerous Capabilities' (Google Deepmind, 2024);https://doi.org/10.48550/arXiv.2403.13793. 368. M. Burtell, T. Woodside, Artificial Influence: An Analysis OfAI-Driven Persuasion, arXiv:2303.08721 [cs.CY] (2023).https://doi.org/10.48550/arXiv.2303.08721. 369. S. Kapoor, A. Narayanan, 'How to prepare for the deluge of generativeAIon social media: A grounded analysis of the challenges and opportunities' (Knight First Amendment Institute at Columbia University., 2023);https://s3.amazonaws.com/kfai-documents/documents/a566f4ded5/How-to-Prepare-for-the-Deluge-of-Generative-AI-on-Social-Media.pdf. 370. M. Hameleers, Cheap Versus Deep Manipulation: The Effects of Cheapfakes Versus Deepfakes in a Political Setting. Int J Public Opin Res 36 (2024).https://doi.org/10.1093/ijpor/edae004. 371. S. Zuboff, The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power (PublicAffairs, 2019), pp. 704. 372. S. Vosoughi, D. Roy, S. Aral, The spread of true and false news online. Science 359, 1146-1151 (2018).https://doi.org/10.1126/science.aap9559. 373. D. Citron, R. Chesney, Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security. Calif. Law Rev. 107, 1753 (2019).https://scholarship.law.bu.edu/faculty_scholarship/640. 374. V. S. Sadasivan, A. Kumar, S. Balasubramanian, W. Wang, S. Feizi, CanAI-Generated Text be Reliably Detected?, arXiv:2303.11156 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2303.11156. 375. S. S. Ghosal, S. Chakraborty, J. Geiping, F. Huang, D. Manocha, A. Bedi, A Survey on the Possibilities & Impossibilities ofAI-generated Text Detection. Transactions on Machine Learning Research (2023).https://openreview.net/pdf?id=AXtFeYjboj. 376. J. Luo, G. Nan, D. Li, Y. Tan,AI-Generated Review Detection. (2023).https://doi.org/10.2139/ssrn.4610727. 377. L. Frohling, A. Zubiaga, Feature-based detection of automated language models: tackling GPT-2, GPT-3 and Grover. PeerJ Comput Sci 7, e443 (2021).https://doi.org/10.7717/peerj-cs.443. 378. S. Gehrmann, H. Strobelt, A. Rush, 'GLTR: Statistical Detection and Visualization of Generated Text' in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations (Association for Computational Linguistics, 2019) pp. 111-116.https://doi.org/10.18653/v1/P19-3019. 379. D. M. Markowitz, J. T. Hancock, J. N. Bailenson, Linguistic Markers of Inherently FalseAICommunication and Intentionally False Human Communication: Evidence From Hotel Reviews. J. Lang. Soc. Psychol. 43, 63-82 (2024).https://doi.org/10.1177/0261927x231200201. 380. T. Berber Sardinha,AI-generated vs human-authored texts: A multidimensional comparison. Applied Corpus Linguistics 4, 100083 (2024).https://doi.org/10.1016/j.acorp.2023.100083. 381. Y. Xie, A. Rawal, Y. Cen, D. Zhao, S. K. Narang, S. Sushmita, MUGC: Machine Generated versus User Generated Content Detection, arXiv:2403.19725 [cs.CL] (2024).https://doi.org/10.48550/arXiv.2403.19725. 382. M. Christ, S. Gunn, O. Zamir, Undetectable Watermarks for Language Models, arXiv:2306.09194 [cs.CR] (2023).https://doi.org/10.48550/arXiv.2306.09194. 383. H. Zhang, B. L. Edelman, D. Francati, D. Venturi, G. Ateniese, B. Barak, Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models, arXiv:2311.04378 [cs.LG] (2023).https://doi.org/10.48550/arXiv.2311.04378. 384. C. R. Leibowicz, S. McGregor, A. Ovadya, 'The Deepfake Detection Dilemma: A Multistakeholder Exploration of Adversarial Dynamics in Synthetic Media' in Proceedings of the 2021 AAAI/ACM Conference onAI, Ethics, and Society (AIES '21) (Association for Computing Machinery, 2021) pp. 736-744.https://doi.org/10.1145/3461702.3462584. 385. L. Zhong, Z. Wang, CanLLMReplace Stack Overflow? A Study on Robustness and Reliability of Large Language Model Code Generation. AAAI 38, 21841-21849 (2024).https://doi.org/10.1609/aaai.v38i19.30185. 386.* H. Khlaaf, P. Mishkin, J. Achiam, G. Krueger, M. Brundage, 'A Hazard Analysis Framework for Code Synthesis Large Language Models' (OpenAI, 2022);http://arxiv.org/abs/2207.14157. 387. H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, R. Karri, 'Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions' in 2022 IEEE Symposium on Security and Privacy (SP) (IEEE Computer Society, 2022) pp. 754-768.https://doi.org/10.1109/sp46214.2022.9833571. 388. G. Deng, Y. Liu, V. Mayoral-Vilches, P. Liu, Y. Li, Y. Xu, T. Zhang, Y. Liu, M. Pinzger, S. Rass, PentestGPT: AnLLM-empowered Automatic Penetration Testing Tool, arXiv:2308.06782 [cs.SE] (2023).http://arxiv.org/abs/2308.06782. 389.* J. Xu, J. W. Stokes, G. McDonald, X. Bai, D. Marshall, S. Wang, A. Swaminathan, Z. Li, AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks, arXiv:2403.01038 [cs.CR] (2024).http://arxiv.org/abs/2403.01038. 390. A. Happe, A. Kaplan, J. Cito,LLMsas Hackers: Autonomous Linux Privilege Escalation Attacks, arXiv:2310.11409 [cs.CR] (2023).https://doi.org/10.48550/arXiv.2310.11409. 391. R. Fang, R. Bindu, A. Gupta, Q. Zhan, D. Kang,LLMAgents can Autonomously Hack Websites, arXiv:2402.06664 [cs.CR] (2024).https://doi.org/10.48550/arXiv.2402.06664. 392. M. Shao, B. Chen, S. Jancheska, B. Dolan-Gavitt, S. Garg, R. Karri, M. Shafique, An Empirical Evaluation ofLLMsfor Solving Offensive Security Challenges, arXiv:2402.11814 [cs.CR] (2024).https://doi.org/10.48550/arXiv.2402.11814. 393. R. Raman, P. Calyam, K. Achuthan, ChatGPT or Bard: Who is a better Certified Ethical Hacker? Computers & Security 140, 103804 (2024).https://doi.org/10.1016/j.cose.2024.103804. 394. Center for Security and Emerging Technology, B. Buchanan, J. Bansemer, D. Cary, J. Lucas, M. Musser, 'Automating Cyber Attacks: Hype and Reality' (Center for Security and Emerging Technology, 2020);https://doi.org/10.51593/2020ca002. 395. National Cyber Security Centre (NCSC), 'The near-term impact ofAIon the cyber threat' (GOV.UK, 2024);www.ncsc.gov.uk/report/impact-of-ai-on-cyber-threat. 396.* B. Berabi, A. Gronskiy, V. Raychev, G. Sivanrupan, V. Chibotaru, M. Vechev, DeepCodeAIFix: Fixing Security Vulnerabilities with Large Language Models, arXiv:2402.13291 [cs.CR] (2024).https://doi.org/10.48550/arXiv.2402.13291. 397. R. Meng, M. Mirchev, M. Bohme, A. Roychoudhury, 'Large language model guided protocol fuzzing' in Proceedings of the 31st Annual Network and Distributed System Security Symposium (NDSS) (2024).www.ndss-symposium.org/wp-content/uploads/2024-556-paper.pdf. 398. Y. Ding, Y. Fu, O. Ibrahim, C. Sitawarin, X. Chen, B. Alomair, D. Wagner, B. Ray, Y. Chen, Vulnerability Detection with Code Language Models: How Far Are We?, arXiv:2403.18624 [cs.SE] (2024).http://arxiv.org/abs/2403.18624. 399. H. Pearce, B. Tan, B. Ahmad, R. Karri, B. Dolan-Gavitt, 'Examining Zero-Shot Vulnerability Repair with Large Language Models' in 2023 IEEE Symposium on Security and Privacy (SP) (2023) pp. 2339-2356.https://doi.org/10.1109/sp46215.2023.10179324. 400.* A. Shestov, R. Levichev, R. Mussabayev, E. Maslov, A. Cheshkov, P. Zadorozhny, Finetuning Large Language Models for Vulnerability Detection, arXiv:2401.17010 [cs.CR] (2024).https://doi.org/10.48550/arXiv.2401.17010. 401. N. Risse, M. Bohme, 'Uncovering the Limits of Machine Learning for Automatic Vulnerability Detection' in USENIX Security Symposium 2024 (2024) pp. 19. 402.* Artificial Intelligence Cyber Challenge, Artificial Intelligence Cyber Challenge (2024).https://aicyberchallenge.com/. 403. S. Ullah, M. Han, S. Pujar, H. Pearce, A. Coskun, G. Stringhini, 'LLMsCannot Reliably Identify and Reason About Security Vulnerabilities (Yet?): A Comprehensive Evaluation, Framework, and Benchmarks' in IEEE Symposium on Security and Privacy (2024).https://research.ibm.com/publications/llms-cannot-reliably-identify-and-reason-about-security-vulnerabilities-yet-a-comprehensive-evaluation-framework-and-benchmarks. 404. S. Rose, C. Nelson, 'UnderstandingAI-Facilitated Biological Weapon Development' (Centre for Long-Term Resilience, 2023);www.longtermresilience.org/post/report-launch-examining-risks-at-the-intersection-of-ai-and-bio. 405. J. B. Sandbrink, Artificial intelligence and biological misuse: Differentiating risks of language models and biological design tools, arXiv:2306.13952 [cs.CY] (2023).https://doi.org/10.48550/arXiv.2306.13952. 406. E. H. Soice, R. Rocha, K. Cordova, M. Specter, K. M. Esvelt, Can large language models democratize access to dual-use biotechnology?, arXiv:2306.03809 [cs.CY] (2023).https://doi.org/10.48550/arXiv.2306.03809. 407. S. R. Carter, N. Wheeler, S. Chwalek, C. Isaac, J. M. Yassif, 'The Convergence of Artificial Intelligence and the Life Sciences: Safeguarding Technology, Rethinking Governance, and Preventing Catastrophe' (Nuclear Threat Initiative, 2023);www.nti.org/wp-content/uploads/2023/10/NTIBIO_AI_FINAL.pdf. 408. N. Li, A. Pan, A. Gopal, S. Yue, D. Berrios, A. Gatti, J. D. Li, A.-K. Dombrowski, S. Goel, L. Phan, G. Mukobi, N. Helm-Burger, R. Lababidi, L. Justen, A. B. Liu, M. Chen, I. Barrass, O. Zhang, X. Zhu, . . . D. Hendrycks, The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning, arXiv:2403.03218 [cs.LG] (2024).https://doi.org/10.48550/arXiv.2403.03218. 409. S. Batalis, 'AIand Biorisk: An Explainer' (CSET, 2023);https://cset.georgetown.edu/publication/ai-and-biorisk-an-explainer/. 410. G. Lewis, P. Millett, A. Sandberg, A. Snyder-Beattie, G. Gronvall, Information Hazards in Biotechnology. Risk Anal. 39, 975-981 (2019).https://doi.org/10.1111/risa.13235. 411. C. A. Mouton, C. Lucas, E. Guest, 'The Operational Risks ofAIin Large-Scale Biological Attacks: Results of a Red-Team Study' (RAND Corporation, 2024);www.rand.org/pubs/research_reports/RRA2977-2.html. 412. S. Ben Ouagrham-Gormley, Barriers to Bioweapons: The Challenges of Expertise and Organization for Weapons Development (Cornell University Press, 2014), pp. 220. 413. J. Revill, C. Jefferson, Tacit knowledge and the biological weapons regime. Sci. Public Policy 41, 597-610 (2014).https://doi.org/10.1093/scipol/sct090. 414. E. National Academies of Sciences, and Medicine, Biodefense in the Age of Synthetic Biology (National Academies Press, Washington, DC, USA, 2018), pp. 188. 415. A. M. Bran, S. Cox, O. Schilter, C. Baldassari, A. White, P. Schwaller, 'Augmenting large language models with chemistry tools' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023)AIfor Science Workshop (2023).https://openreview.net/forum?id=wdGIL6lx3l. 416. K. H. Sumida, R. Nunez-Franco, I. Kalvet, S. J. Pellock, B. I. M. Wicky, L. F. Milles, J. Dauparas, J. Wang, Y. Kipnis, N. Jameson, A. Kang, J. De La Cruz, B. Sankaran, A. K. Bera, G. Jimenez-Oses, D. Baker, Improving Protein Expression, Stability, and Function with ProteinMPNN. J. Am. Chem. Soc. 146, 2054-2061 (2024).https://doi.org/10.1021/jacs.3c10941. 417. Z. Wu, S. B. J. Kan, R. D. Lewis, B. J. Wittmann, F. H. Arnold, Machine learning-assisted directed protein evolution with combinatorial libraries. Proc. Natl. Acad. Sci. U. S. A. 116, 8852-8858 (2019).https://doi.org/10.1073/pnas.1901979116. 418. A. H.-W. Yeh, C. Norn, Y. Kipnis, D. Tischer, S. J. Pellock, D. Evans, P. Ma, G. R. Lee, J. Z. Zhang, I. Anishchenko, B. Coventry, L. Cao, J. Dauparas, S. Halabiya, M. DeWitt, L. Carter, K. N. Houk, D. Baker, De novo design of luciferases using deep learning. Nature 614, 774-780 (2023).https://doi.org/10.1038/s41586-023-05696-3. 419. J. L. Watson, D. Juergens, N. R. Bennett, B. L. Trippe, J. Yim, H. E. Eisenach, W. Ahern, A. J. Borst, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, N. Hanikel, S. J. Pellock, A. Courbet, W. Sheffler, J. Wang, P. Venkatesh, I. Sappington, S. V. Torres, . . . D. Baker, De novo design of protein structure and function with RFdiffusion. Nature 620, 1089-1100 (2023).https://doi.org/10.1038/s41586-023-06415-8. 420. T. Blaschke, J. Arus-Pous, H. Chen, C. Margreitter, C. Tyrchan, O. Engkvist, K. Papadopoulos, A. Patronov, REINVENT 2.0: AnAITool for De Novo Drug Design. J. Chem. Inf. Model. 60, 5918-5922 (2020).https://doi.org/10.1021/acs.jcim.0c00915. 421. N. N. Thadani, S. Gurev, P. Notin, N. Youssef, N. J. Rollins, D. Ritter, C. Sander, Y. Gal, D. S. Marks, Learning from prepandemic data to forecast viral escape. Nature 622, 818-825 (2023).https://doi.org/10.1038/s41586-023-06617-0. 422. F. Urbina, F. Lentzos, C. Invernizzi, S. Ekins, Dual Use of Artificial Intelligence-powered Drug Discovery. Nat Mach Intell 4, 189-191 (2022).https://doi.org/10.1038/s42256-022-00465-9. 423.* A. Elnaggar, H. Essam, W. Salah-Eldin, W. Moustafa, M. Elkerdawy, C. Rochereau, B. Rost, Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling, arXiv:2301.06568 [cs.LG] (2023).https://doi.org/10.48550/arXiv.2301.06568. 424. T. Inagaki, A. Kato, K. Takahashi, H. Ozaki, G. N. Kanda,LLMscan generate robotic scripts from goal-oriented instructions in biological laboratory automation, arXiv:2304.10267 [q-bio.QM] (2023).http://arxiv.org/abs/2304.10267. 425. J. N. Acosta, G. J. Falcone, P. Rajpurkar, E. J. Topol, Multimodal biomedicalAI. Nat. Med. 28, 1773-1784 (2022).https://doi.org/10.1038/s41591-022-01981-2. 426. M. Moor, O. Banerjee, Z. S. H. Abad, H. M. Krumholz, J. Leskovec, E. J. Topol, P. Rajpurkar, Foundation models for generalist medical artificial intelligence. Nature 616, 259-265 (2023). https://doi.org/10.1038/s41586-023-05881-4. 427.* 310.ai, GenAI + BIO: Nature didn't have time, we haveGPUs(2024).https://310.ai/. 428. National Security Commission on Emerging Biotechnology (NSCEB), 'Risks of AIxBio' (NSCEB, 2024);www.biotech.senate.gov/wp-content/uploads/2024/01/NSCEB_AIxBio_WP3_Risks.pdf. 429. C. A. Mouton, C. Lucas, E. Guest, 'The Operational Risks ofAIin Large-Scale Biological Attacks: A Red-Team Approach' (RAND Corporation, 2023);www.rand.org/pubs/research_reports/RRA2977-1.html. 430. I. D. Raji, I. E. Kumar, A. Horowitz, A. Selbst, 'The Fallacy ofAIFunctionality' in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22) (Association for Computing Machinery, 2022) pp. 959-972.https://doi.org/10.1145/3531146.3533158. 431. A. Mallen, A. Asai, V. Zhong, R. Das, D. Khashabi, H. Hajishirzi, 'When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories' in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (Association for Computational Linguistics, 2023) pp. 9802-9822.https://doi.org/10.18653/v1/2023.acl-long.546. 432. A. Perlman, The Implications of ChatGPT for Legal Services and Society in The Practice. (Center on the Legal Profession, Harvard Law School, 2023).https://clp.law.harvard.edu/knowledge-hub/magazine/issues/generative-ai-in-the-legal-profession/the-implications-of-chatgpt-for-legal-services-and-society/. 433. E. Martinez, Re-evaluating GPT-4's bar exam performance. Artificial Intelligence and Law (2024).https://doi.org/10.1007/s10506-024-09396-9. 434. J. Tan, H. Westermann, K. Benyekhlef, 'ChatGPT as an Artificial Lawyer?' in Workshop on Artificial Intelligence for Access to Justice (AI4AJ 2023) (CEUR Workshop Proceedings, 2023).https://ceur-ws.org/Vol-3435/short2.pdf. 435. J. A. Omiye, J. C. Lester, S. Spichak, V. Rotemberg, R. Daneshjou, Large language models propagate race-based medicine. npj Digit. Med. 6, 1-4 (2023).https://doi.org/10.1038/s41746-023-00939-z. 436. K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl, P. Payne, M. Seneviratne, P. Gamble, C. Kelly, A. Babiker, N. Scharli, A. Chowdhery, P. Mansfield, D. Demner-Fushman, . . . V. Natarajan, Large language models encode clinical knowledge. Nature 620, 172-180 (2023).https://doi.org/10.1038/s41586-023-06291-2. 437. T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon, C. Elepano, M. Madriaga, R. Aggabao, G. Diaz-Candido, J. Maningo, V. Tseng, Performance of ChatGPT on USMLE: Potential forAI-assisted medical education using large language models. PLOS Digit Health 2, e0000198 (2023).https://doi.org/10.1371/journal.pdig.0000198. 438. A. Ettinger, What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models. Trans. Assoc. Comput. Linguist. 8, 34-48 (2020).https://doi.org/10.1162/tacl_a_00298. 439. Y. Zhang, M. Yasunaga, Z. Zhou, J. Z. HaoChen, J. Zou, P. Liang, S. Yeung, 'Beyond Positive Scaling: How Negation Impacts Scaling Trends of Language Models' in Findings of the Association for Computational Linguistics: ACL 2023 (Association for Computational Linguistics, 2023) pp. 7479-7498.https://doi.org/10.18653/v1/2023.findings-acl.472. 440.* M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, . . . W. Zaremba, Evaluating Large Language Models Trained on Code, arXiv:2107.03374 [cs.LG] (2021).http://arxiv.org/abs/2107.03374. 441. C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, K. R. Narasimhan, 'SWE-bench: Can Language Models Resolve Real-world Github Issues?' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=VTF8yNQM66. 442. R. Pan, A. R. Ibrahimzada, R. Krishna, D. Sankar, L. P. Wassi, M. Merler, B. Sobolev, R. Pavuluri, S. Sinha, R. Jabbarvand, 'Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code' in Proceedings of the IEEE/ACM 46th International Conference on Software Engineering (ICSE '24) (Association for Computing Machinery, 2024) pp. 1-13.https://doi.org/10.1145/3597503.3639226. 443. F. Cassano, L. Li, A. Sethi, N. Shinn, A. Brennan-Jones, J. Ginesin, E. Berman, G. Chakhnashvili, A. Lozhkov, C. J. Anderson, A. Guha, Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions, arXiv:2312.12450 [cs.SE] (2023).http://arxiv.org/abs/2312.12450. 444. S. Nguyen, H. M. Babe, Y. Zi, A. Guha, C. J. Anderson, M. Q. Feldman, 'How Beginning Programmers and CodeLLMs(Mis)read Each Other' in Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI '24) (Association for Computing Machinery, 2024) pp. 1-26.https://doi.org/10.1145/3613904.3642706. 445. P. Darke, T. Dhar, C. B. Weinberg, X. Zeng, Truth, Lies and Advertising: Understanding Market and Individual Causes of False Advertising Claims. (2022).https://doi.org/10.2139/ssrn.4140673. 446. M. Rauh, J. F. J. Mellor, J. Uesato, P.-S. Huang, J. Welbl, L. Weidinger, S. Dathathri, A. Glaese, G. Irving, I. Gabriel, W. Isaac, L. A. Hendricks, 'Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models' in 36th Conference on Neural Information Processing Systems (NeurIPS 2022) (2022).https://openreview.net/forum?id=u46CbCaLufp. 447. O. Bohdal, T. Hospedales, P. H. S. Torr, F. Barez, 'Fairness inAIand Its Long-Term Implications on Society' in Intersections, Reinforcements, Cascades: Proceedings of the 2023 Stanford Existential Risks Conference (Stanford Existential Risks Initiative, 2023) pp. 171-186.https://doi.org/10.25740/pj287ht2654. 448. D. Acemoglu, 'Harms ofAI' in The Oxford Handbook ofAIGovernance, J. B. Bullock, Y.-C. Chen, J. Himmelreich, V. M. Hudson, A. Korinek, M. M. Young, B. Zhang, Eds. (Oxford University Press, 2023). 449. I. Ajunwa, The Quantified Worker: Law and Technology in the Modern Workplace (Cambridge University Press, Cambridge, 2023). 450. Z. Obermeyer, B. Powers, C. Vogeli, S. Mullainathan, Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 447-453 (2019).https://doi.org/10.1126/science.aax2342. 451. A. Wong, E. Otles, J. P. Donnelly, A. Krumm, J. McCullough, O. DeTroyer-Cooley, J. Pestrue, M. Phillips, J. Konye, C. Penoza, M. Ghous, K. Singh, External Validation of a Widely Implemented Proprietary Sepsis Prediction Model in Hospitalized Patients. JAMA Intern Med 181, 1065-1070 (2021).https://doi.org/10.1001/jamainternmed.2021.2626. 452. J. Buolamwini, T. Gebru, 'Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification' in Proceedings of the 1st Conference on Fairness, Accountability and Transparency (FAT/MM '19) (PMLR, 2018) pp. 77-91.https://proceedings.mlr.press/v81/buolamwini18a.html. 453. J. Dressel, H. Farid, The accuracy, fairness, and limits of predicting recidivism. Sci Adv 4, eaao5580 (2018).https://doi.org/10.1126/sciadv.aao5580. 454. M. A. Gianfrancesco, S. Tamang, J. Yazdany, G. Schmajuk, Potential Biases in Machine Learning Algorithms Using Electronic Health Record Data. JAMA Intern. Med. 178, 1544-1547 (2018).https://doi.org/10.1001/jamainternmed.2018.3763. 455. Y. Wan, G. Pu, J. Sun, A. Garimella, K.-W. Chang, N. Peng, '""Kelly is a Warm Person, Joseph is a Role Model"": Gender Biases inLLM-Generated Reference Letters' in Findings of the Association for Computational Linguistics: EMNLP 2023 (Association for Computational Linguistics, 2023) pp. 3730-3748.https://doi.org/10.18653/v1/2023.findings-emnlp.243. 456. D. van Niekerk, M. Perez-Ortiz, J. Shawe-Taylor, D. Orlic, I. Drobnjak, J. Kay, N. Siegel, K. Evans, N. Moorosi, T. Eliassi-Rad, L. M. Tanczer, W. Holmes, M. P. Deisenroth, I. Straw, M. Fasli, R. Adams, N. Oliver, D. Mladenic, U. Aneja, . . . M. Janicky, 'Challenging Systematic Prejudices: An Investigation into Bias Against Women and Girls in Large Language Models' (UNESCO, IRCAI, 2024);https://ircai.org/project/challenging-systematic-prejudices/. 457. M. Vlasceanu, D. M. Amodio, Propagation of societal gender inequality by internet search algorithms. Proceedings of the National Academy of Sciences 119, e2204529119 (2022).https://doi.org/10.1073/pnas.2204529119. 458. US Equal Employment Opportunity Commission, EEOC Sues iTutorGroup for Age Discrimination (2022).www.eeoc.gov/newsroom/eeoc-sues-itutorgroup-age-discrimination. 459. M. Diaz, I. Johnson, A. Lazar, A. M. Piper, D. Gergle, 'Addressing Age-Related Bias in Sentiment Analysis' in Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI-19) (International Joint Conferences on Artificial Intelligence Organization, 2019) pp. 6146-6150.https://doi.org/10.24963/ijcai.2019/852. 460. J. Stypinska,AIageism: a critical roadmap for studying age discrimination and exclusion in digitalized societies.AISoc. 38, 665-677 (2023).https://doi.org/10.1007/s00146-022-01553-5. 461. P. Rucker, M. Miller, D. Armstrong, How Cigna Saves Millions by Having Its Doctors Reject Claims Without Reading Them in ProPublica. (2023).www.propublica.org/article/cigna-pxdx-medical-health-insurance-rejection-claims. 462. A. Mack, R. Qadri, R. Denton, S. K. Kane, C. L. Bennett, '""They only care to show us the wheelchair"": Disability representation in text-to-imageAImodels' in Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI '24) (ACM, 2024).https://dl.acm.org/doi/10.1145/3613904.3642166. 463. P. N. Venkit, M. Srinath, S. Wilson, 'Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models' in Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023) (Association for Computational Linguistics, 2023) pp. 26-34.https://doi.org/10.18653/v1/2023.trustnlp-1.3. 464. I. A. Adeyanju, O. O. Bello, M. A. Adegboye, Machine learning methods for sign language recognition: A critical review and analysis. Intelligent Systems with Applications 12, 200056 (2021).https://doi.org/10.1016/j.iswa.2021.200056. 465. S. Gueuwou, K. Takyi, M. Muller, M. S. Nyarko, R. Adade, R.-M. O. M. Gyening, 'AfriSign: Machine Translation for African Sign Languages' in 4th Workshop on African Natural Language Processing (AfricaNLP 2023) (2023).https://openreview.net/forum?id=EHldk3J2xk. 466. W. Guo, A. Caliskan, 'Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases' in Proceedings of the 2021 AAAI/ACM Conference onAI, Ethics, and Society (AIES '21) (Association for Computing Machinery, 2021) pp. 122-133.https://doi.org/10.1145/3461702.3462536. 467. A. A. Cabrera, W. Epperson, F. Hohman, M. Kahng, J. Morgenstern, D. H. Chau, 'FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning' in 2019 IEEE Conference on Visual Analytics Science and Technology (VAST) (2019) pp. 46-56.https://doi.org/10.1109/vast47406.2019.8986948. 468. L. Magee, L. Ghahremanlou, K. Soldatic, S. Robertson, Intersectional Bias in Causal Language Models, arXiv:2107.07691 [cs.CL] (2021).https://doi.org/10.48550/arXiv.2107.07691. 469. A. Ovalle, A. Subramonian, V. Gautam, G. Gee, K.-W. Chang, 'Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality inAIFairness' in Proceedings of the 2023 AAAI/ACM Conference onAI, Ethics, and Society (AIES '23) (Association for Computing Machinery, 2023) pp. 496-511.https://doi.org/10.1145/3600211.3604705. 470. J. S. Park, M. S. Bernstein, R. N. Brewer, E. Kamar, M. R. Morris, 'Understanding the Representation and Representativeness of Age inAIData Sets' in Proceedings of the 2021 AAAI/ACM Conference onAI, Ethics, and Society (AIES '21) (Association for Computing Machinery, 2021) pp. 834-842.https://doi.org/10.1145/3461702.3462590. 471. R. Kamikubo, L. Wang, C. Marte, A. Mahmood, H. Kacorri, 'Data Representativeness in Accessibility Datasets: A Meta-Analysis' in Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '22) (Association for Computing Machinery, 2022) pp. 1-15.https://doi.org/10.1145/3517428.3544826. 472.* L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang, M. Cheng, M. Glaese, B. Balle, A. Kasirzadeh, Z. Kenton, S. Brown, W. Hawkins, T. Stepleton, C. Biles, A. Birhane, J. Haas, L. Rimell, L. A. Hendricks, . . . I. Gabriel, 'Ethical and social risks of harm from Language Models' (Google DeepMind, 2021);http://arxiv.org/abs/2112.04359. 473. J. Nwatu, O. Ignat, R. Mihalcea, 'Bridging the Digital Divide: Performance Variation across Socio-Economic Factors in Vision-Language Models' in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023) (Association for Computational Linguistics, 2023) pp. 10686-10702.https://doi.org/10.18653/v1/2023.emnlp-main.660. 474. S. Ghosh, A. Caliskan, 'ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages' in Proceedings of the 2023 AAAI/ACM Conference onAI, Ethics, and Society (AIES '23) (Association for Computing Machinery, 2023) pp. 901-912.https://doi.org/10.1145/3600211.3604672. 475. G. Vardi, On the Implicit Bias in Deep-Learning Algorithms. Commun. ACM 66, 86-93 (2023).https://doi.org/10.1145/3571070. 476. F. Bianchi, P. Kalluri, E. Durmus, F. Ladhak, M. Cheng, D. Nozza, T. Hashimoto, D. Jurafsky, J. Zou, A. Caliskan, 'Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale' in Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT '23) (Association for Computing Machinery, 2023) pp. 1493-1504.https://doi.org/10.1145/3593013.3594095. 477. J. Hartmann, J. Schwenzow, M. Witte, The political ideology of conversationalAI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation, arXiv:2301.01768 [cs.CL] (2023).http://arxiv.org/abs/2301.01768. 478. T. Kaufmann, S. Ball, J. Beck, E. Hullermeier, F. Kreuter, 'On the Challenges and Practices of Reinforcement Learning from Real Human Feedback' in First Workshop on Hybrid Human-Machine Learning and Decision Making (HLDM'23) (2023). 479. A. Siththaranjan, C. Laidlaw, D. Hadfield-Menell, 'Distributional Preference Learning: Understanding and Accounting for Hidden Context inRLHF' in The 12th International Conference on Learning Representations (ICLR) (2024).https://openreview.net/forum?id=0tWTxYYPnW. 480.* OpenAI, ChatGPT plugins (2023).https://openai.com/blog/chatgpt-plugins. 481.* I. Gabriel, A. Manzini, G. Keeling, L. A. Hendricks, V. Rieser, H. Iqbal, N. Tomasev, I. Ktena, Z. Kenton, M. Rodriguez, S. El-Sayed, S. Brown, C. Akbulut, A. Trask, E. Hughes, A. Stevie Bergman, R. Shelby, N. Marchal, C. Griffin, . . . J. Manyika, 'The Ethics of AdvancedAIAssistants' (Google DeepMind, 2024);http://arxiv.org/abs/2404.16244. 482. A. Chan, R. Salganik, A. Markelius, C. Pang, N. Rajkumar, D. Krasheninnikov, L. Langosco, Z. He, Y. Duan, M. Carroll, M. Lin, A. Mayhew, K. Collins, M. Molamohammadi, J. Burden, W. Zhao, S. Rismani, K. Voudouris, U. Bhatt, . . . T. Maharaj, 'Harms from Increasingly Agentic Algorithmic Systems' in Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT '23) (Association for Computing Machinery, 2023) pp. 651-666.https://doi.org/10.1145/3593013.3594033. 483. A. M. Turing, Intelligent Machinery, A Heretical Theory*. Philos. Math. 4, 256-260 (1996).https://doi.org/10.1093/philmat/4.3.256. 484. I. J. Good, 'Speculations Concerning the First Ultraintelligent Machine' in Advances in Computers, F. L. Alt, M. Rubinoff, Eds. (Elsevier, 1966), vol. 6, pp. 31-88. 485. N. Wiener, Some Moral and Technical Consequences of Automation. Science 131, 1355-1358 (1960).https://doi.org/10.1126/science.131.3410.1355. 486. Center forAISafety, Statement onAIRisk:AIexperts and public figures express their concern aboutAIrisk (2024).www.safe.ai/work/statement-on-ai-risk. 487. Y. Bengio, Written Statement of Professor Yoshua Bengio Before the US SenateAIInsight Forum. (2023).www.schumer.senate.gov/imo/media/doc/Yoshua%20Benigo%20-%20Statement.pdf. 488. K. Grace, H. Stewart, J. F. Sandkuhler, S. Thomas, B. Weinstein-Raun, J. Brauner, Thousands ofAIAuthors on the Future ofAI, arXiv:2401.02843 [cs.CY] (2024).https://doi.org/10.48550/arXiv.2401.02843. 489. A. Pan, K. Bhatia, J. Steinhardt, 'The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models' in The 10th International Conference on Learning Representations (ICLR 2022) (2021).https://openreview.net/forum?id=JYtwGwIL7ye. 490. R. Ngo, L. Chan, S. Mindermann, 'The Alignment Problem from a Deep Learning Perspective' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=fh8EYKFKns. 491. J. Ji, T. Qiu, B. Chen, B. Zhang, H. Lou, K. Wang, Y. Duan, Z. He, J. Zhou, Z. Zhang, F. Zeng, K. Y. Ng, J. Dai, X. Pan, A. O'Gara, Y. Lei, H. Xu, B. Tse, J. Fu, . . . W. Gao,AIAlignment: A Comprehensive Survey, arXiv:2310.19852 [cs.AI] (2023).https://doi.org/10.48550/arXiv.2310.19852. 492. D. Hendrycks, X. Liu, E. Wallace, A. Dziedzic, R. Krishnan, D. Song, 'Pretrained Transformers Improve Out-of-Distribution Robustness' in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020) (Association for Computational Linguistics, 2020) pp. 2744-2751.https://doi.org/10.18653/v1/2020.acl-main.244. 493. M. K. Cohen, M. Hutter, M. A. Osborne, Advanced artificial agents intervene in the provision of reward.AIMag. 43, 282-293 (2022).https://doi.org/10.1002/aaai.12064. 494. T. Everitt, M. Hutter, R. Kumar, V. Krakovna, Reward tampering problems and solutions in reinforcement learning: a causal influence diagram perspective. Synthese 198, 6435-6467 (2021).https://doi.org/10.1007/s11229-021-03141-4. 495.* R. Kumar, J. Uesato, R. Ngo, T. Everitt, V. Krakovna, S. Legg, REALab: An Embedded Perspective on Tampering, arXiv:2011.08820 [cs.LG] (2020).http://arxiv.org/abs/2011.08820. 496. D. Hadfield-Menell, A. D. Dragan, P. Abbeel, S. Russell, 'The Off-Switch Game' in The 31st AAAI Conference on Artificial Intelligence (2017).http://aaai.org/ocs/index.php/WS/AAAIW17/paper/view/15156. 497.* V. Krakovna, J. Kramar, Power-seeking can be probable and predictive for trained agents, arXiv:2304.06528 [cs.AI] (2023).https://doi.org/10.48550/arXiv.2304.06528. 498. A. Turner, L. Smith, R. Shah, A. Critch, P. Tadepalli, 'Optimal Policies Tend To Seek Power' in 35th Conference on Neural Information Processing Systems (NeurIPS 2021) (Curran Associates, Inc., 2021) vol. 34.https://proceedings.neurips.cc/paper/2021/hash/c26820b8a4c1b3c2aa868d6d57e14a79-Abstract.html. 499. A. Turner, P. Tadepalli, 'Parametrically retargetable decision-makers tend to seek power' in Advances in Neural Information Processing Systems 35 (NeurIPS 2022) Main Conference Track (2022) vol. abs/2206.13477.https://doi.org/10.48550/arXiv.2206.13477. 500. J. D. Gallow, Instrumental divergence. Philos. Stud. (2024).https://doi.org/10.1007/s11098-024-02129-3. 501. J. Scheurer, M. Balesni, M. Hobbhahn, 'Large Language Models can Strategically Deceive their Users when Put Under Pressure' in ICLR 2024 Workshop on Large Language Model (LLM) Agents (2024).https://doi.org/10.48550/arXiv.2311.07590. 502. E. Brynjolfsson, The Turing Trap: The Promise & Peril of Human-Like Artificial Intelligence. Daedalus 151, 272-287 (2022).https://doi.org/10.1162/daed_a_01915. 503. M. Kuziemski, G. Misuraca,AIgovernance in the public sector: Three tales from the frontiers of automated decision-making in democratic settings. Telecomm. Policy 44, 101976 (2020).https://doi.org/10.1016/j.telpol.2020.101976. 504. L. Mitrou, M. Janssen, E. Loukis, 'Human control and discretion inAI-driven decision-making in government' in 14th International Conference on Theory and Practice of Electronic Governance (ICEGOV 2021) (ACM, 2021).https://doi.org/10.1145/3494193.3494195. 505. I. Taylor, Justice by Algorithm: The Limits ofAIin Criminal Sentencing. Crim. Justice Ethics 42, 193-213 (2023).https://doi.org/10.1080/0731129x.2023.2275967. 506. J.-P. Rivera, G. Mukobi, A. Reuel, M. Lamparth, C. Smith, J. Schneider, Escalation Risks from Language Models in Military and Diplomatic Decision-Making, arXiv:2401.03408 [cs.AI] (2024).http://arxiv.org/abs/2401.03408. 507. D. Hendrycks, M. Mazeika, T. Woodside, An Overview of CatastrophicAIRisks, arXiv:2306.12001 [cs.CY] (2023).https://doi.org/10.48550/arXiv.2306.12001. 508.* T. Shevlane, S. Farquhar, B. Garfinkel, M. Phuong, J. Whittlestone, J. Leung, D. Kokotajlo, N. Marchal, M. Anderljung, N. Kolt, L. Ho, D. Siddarth, S. Avin, W. Hawkins, B. Kim, I. Gabriel, V. Bolina, J. Clark, Y. Bengio, . . . A. Dafoe, 'Model evaluation for extreme risks' (Google DeepMind, 2023);http://arxiv.org/abs/2305.15324. 509. M. Kinniment, L. J. K. Sato, H. Du, B. Goodrich, M. Hasin, L. Chan, L. H. Miles, T. R. Lin, H. Wijk, J. Burget, A. Ho, E. Barnes, P. Christiano, Evaluating Language-Model Agents on Realistic Autonomous Tasks, arXiv:2312.11671 [cs.CL] (2023).http://arxiv.org/abs/2312.11671. 510. P. J. Denning, The Science of Computing: The Internet Worm. Am. Sci. 77, 126-128 (1989).www.jstor.org/stable/27855650. 511. A. Critch, S. Russell, TASRA: a Taxonomy and Analysis of Societal-Scale Risks fromAI, arXiv:2306.06924 [cs.AI] (2023).http://arxiv.org/abs/2306.06924. 512. G. Marcus, Deep Learning: A Critical Appraisal, arXiv:1801.00631 [cs.AI] (2018).https://doi.org/10.48550/arXiv.1801.00631. 513. D. Acemoglu, D. Autor, 'Skills, Tasks and Technologies: Implications for Employment and Earnings*' in Handbook of Labor Economics, D. Card, O. Ashenfelter, Eds. (Elsevier, 2011), vol. 4, pp. 1043-1171. 514. D. H. Autor, 'The ""task approach"" to labor markets: an overview'. J. Labour Mark. Res. 46, 185-199 (2013).https://doi.org/10.1007/s12651-013-0128-z. 515. D. Acemoglu, P. Restrepo, Automation and New Tasks: How Technology Displaces and Reinstates Labor. J. Econ. Perspect. 33, 3-30 (2019).https://doi.org/10.1257/jep.33.2.3. 516. D. Autor, 'ApplyingAIto Rebuild Middle Class Jobs' (National Bureau of Economic Research, 2024);https://doi.org/10.3386/w32140. 517. D. H. Autor, Why Are There Still So Many Jobs? The History and Future of Workplace Automation. J. Econ. Perspect. 29, 3-30 (2015).https://doi.org/10.1257/jep.29.3.3. 518. A. Georgieff, R. Hyee, Artificial Intelligence and Employment: New Cross-Country Evidence. Front Artif Intell 5, 832736 (2022).https://doi.org/10.3389/frai.2022.832736. 519. M. Cazzaniga, F. Jaumotte, L. Li, G. Melina, A. J. Panton, C. Pizzinelli, E. J. Rockall, M. M. Tavares, 'Gen-AI: Artificial Intelligence and the Future of Work' (International Monetary Fund, 2024);www.imf.org/en/Publications/Staff-Discussion-Notes/Issues/2024/01/14/Gen-AI-Artificial-Intelligence-and-the-Future-of-Work-542379. 520. F. Dell'Acqua, E. McFowland, III, E. R. Mollick, H. Lifshitz-Assaf, K. Kellogg, S. Rajendran, L. Krayer, F. Candelon, K. R. Lakhani, 'Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects ofAIon Knowledge Worker Productivity and Quality' (Harvard Business School, 2023);www.hbs.edu/ris/Publication%20Files/24-013_d9b45b68-9e74-42d6-a1c6-c72fb70c7282.pdf. 521.* S. Peng, E. Kalliamvakou, P. Cihon, M. Demirer, The Impact ofAIon Developer Productivity: Evidence from GitHub Copilot, arXiv:2302.06590 [cs.SE] (2023).https://doi.org/10.48550/arXiv.2302.06590. 522. E. D. Yilmaz, I. Naumovska, V. A. Aggarwal, 'AI-Driven Labor Substitution: Evidence from Google Translate and ChatGPT' (INSEAD, 2023);https://sites.insead.edu/facultyresearch/research/doc.cfm?did=70468. 523. X. Hui, O. Reshef, L. Zhou, 'The Short-Term Effects of Generative Artificial Intelligence on Employment: Evidence from an Online Labor Market' (CESifo Working Paper, 2023);www.econstor.eu/handle/10419/279352. 524. L. Belzner, T. Gabor, M. Wirsing, 'Large Language Model Assisted Software Engineering: Prospects, Challenges, and a Case Study' in Bridging the Gap BetweenAIand Reality (AISoLA 2023) (Springer Nature Switzerland, 2024) pp. 355-374.https://doi.org/10.1007/978-3-031-46002-9_23. 525. J. Bessen, 'Automation and Jobs: When Technology Boosts Employment' (Boston University School of Law, 2019);https://scholarship.law.bu.edu/faculty_scholarship/815. 526. D. Autor, C. Chin, A. Salomons, B. Seegmiller, 'New Frontiers: The Origins and Content of New Work, 1940-2018' (National Bureau of Economic Research, 2022);https://doi.org/10.3386/w30389. 527. A. Agrawal, J. S. Gans, A. Goldfarb, Artificial Intelligence: The Ambiguous Labor Market Impact of Automating Prediction. J. Econ. Perspect. 33, 31-50 (2019).https://doi.org/10.1257/jep.33.2.31. 528. B. Moll, L. Rachel, P. Restrepo, Uneven Growth: Automation's Impact on Income and Wealth Inequality. Econometrica 90, 2645-2683 (2022).https://doi.org/10.3982/ecta19417. 529. M. Beraja, N. Zorzi, 'Inefficient Automation' (National Bureau of Economic Research, 2022);https://doi.org/10.3386/w30154. 530. D. Acemoglu, P. Restrepo, 'The Wrong Kind ofAI? Artificial Intelligence and the Future of Labor Demand' (National Bureau of Economic Research, 2019);https://doi.org/10.3386/w25682. 531. S. G. Benzell, L. J. Kotlikoff, G. LaGarda, J. D. Sachs, 'Robots Are Us: Some Economics of Human Replacement' (National Bureau of Economic Research, 2015);https://doi.org/10.3386/w20941. 532. A. Korinek, D. Suh, 'Scenarios for the Transition toAGI' (National Bureau of Economic Research, 2024);https://doi.org/10.3386/w32255. 533. E. Ilzetzki, S. Jain, The impact of artificial intelligence on growth and employment in VoxEU - CEPR's policy portal. (2023).https://cepr.org/voxeu/columns/impact-artificial-intelligence-growth-and-employment. 534. M. N. Baily, E. Brynjolfsson, A. Korinek, 'Machines of mind: The case for anAI-powered productivity boom' (Brookings, 2023);www.brookings.edu/articles/machines-of-mind-the-case-for-an-ai-powered-productivity-boom/. 535. A. K. Agrawal, J. S. Gans, A. Goldfarb, 'The Turing Transformation: Artificial Intelligence, Intelligence Augmentation, and Skill Premiums' (National Bureau of Economic Research, 2023);https://doi.org/10.3386/w31767. 536. D. Acemoglu, P. Restrepo, The Race between Man and Machine: Implications of Technology for Growth, Factor Shares, and Employment. Am. Econ. Rev. 108, 1488-1542 (2018).https://doi.org/10.1257/aer.20160696. 537. D. Acemoglu, P. Restrepo, 'Artificial Intelligence, Automation and Work' (National Bureau of Economic Research, 2018);https://doi.org/10.3386/w24196. 538. I. Cockburn, R. Henderson, S. Stern, 'The Impact of Artificial Intelligence on Innovation' (National Bureau of Economic Research, 2018);https://doi.org/10.3386/w24449. 539. W. D. Nordhaus, Are We Approaching an Economic Singularity? Information Technology and the Future of Economic Growth. Am. Econ. J. Macroecon. 13, 299-332 (2021).https://doi.org/10.1257/mac.20170105. 540. OECD, 'UsingAIin the workplace: Opportunities, risks and policy responses' (OECD, 2024);https://doi.org/10.1787/73d417f9-en. 541. D. Acemoglu, P. Restrepo, Tasks, Automation, and the Rise in U.S. Wage Inequality. Econometrica 90, 1973-2016 (2022).https://doi.org/10.3982/ecta19815. 542. O. Afonso, R. Forte, Routine and non-routine sectors, tasks automation and wage polarization. Appl. Econ. (2023).www.tandfonline.com/doi/abs/10.1080/00036846.2023.2280461. 543. D. Acemoglu, J. Loebbing, 'Automation and Polarization' (National Bureau of Economic Research, 2022);https://doi.org/10.3386/w30528. 544. L. Karabarbounis, 'Perspectives on the Labor Share' (National Bureau of Economic Research, 2023);https://doi.org/10.3386/w31854. 545. M. Ranaldi, Income Composition Inequality. Rev. Income Wealth 68, 139-160 (2022).https://doi.org/10.1111/roiw.12503. 546. T. Piketty, A. Goldhammer, Capital in the twenty-first century (The Belknap Press of Harvard University Press, Cambridge Massachusetts, 2014), pp. 685. 547. A. Korinek, J. E. Stiglitz, 'Artificial Intelligence, Globalization, and Strategies for Economic Development' (National Bureau of Economic Research, 2021);https://doi.org/10.3386/w28453. 548. Association for Progressive communications, Article 19, Swedish International Development Cooperation Agency, 'Global Information Society Watch 2019: Artificial intelligence: Human rights, social justice and development' (APC, 2019). 549. N. Sambasivan, E. Arnesen, B. Hutchinson, T. Doshi, V. Prabhakaran, 'Re-imagining Algorithmic Fairness in India and Beyond' in Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21) (Association for Computing Machinery, 2021) pp. 315-328.https://doi.org/10.1145/3442188.3445896. 550. C. T. Okolo,AIin the Global South: Opportunities and challenges towards more inclusive governance (2023).www.brookings.edu/articles/ai-in-the-global-south-opportunities-and-challenges-towards-more-inclusive-governance/. 551. M.-T. Png, 'At the Tensions of South and North: Critical Roles of Global South Stakeholders inAIGovernance' in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22) (Association for Computing Machinery, 2022) pp. 1434-1445.https://doi.org/10.1145/3531146.3533200. 552. S. L. Harlan, D. N. Pellow, J. T. Roberts, S. E. Bell, W. G. Holt, J. Nagel, 'Climate Justice and Inequality' in Climate Change and Society, R. E. Dunlap, R. J. Brulle, Eds. (Oxford University Press, 2015), pp. 127-163. 553. F. Sultana, Critical climate justice. Geogr. J. 188, 118-124 (2022).https://doi.org/10.1111/geoj.12417. 554. R. Zwetsloot, B. Zhang, N. Dreksler, L. Kahn, M. Anderljung, A. Dafoe, M. C. Horowitz, 'Skilled and Mobile: Survey Evidence ofAIResearchers' Immigration Preferences' in Proceedings of the 2021 AAAI/ACM Conference onAI, Ethics, and Society (AIES '21) (Association for Computing Machinery, 2021) pp. 1050-1059.https://doi.org/10.1145/3461702.3462617. 555. R. Jurowetzki, D. Hain, J. Mateos-Garcia, K. Stathoulopoulos, The Privatization ofAIResearch(-ers): Causes and Potential Consequences - From university-industry interaction to public research brain-drain?, arXiv:2102.01648 [cs.CY] (2021).http://arxiv.org/abs/2102.01648. 556. N. Ahmed, M. Wahed, The De-democratization ofAI: Deep Learning and the Compute Divide in Artificial Intelligence Research, arXiv:2010.15581 [cs.CY] (2020).https://doi.org/10.48550/arXiv.2010.15581. 557. T. Besiroglu, S. A. Bergerson, A. Michael, L. Heim, X. Luo, N. Thompson, The Compute Divide in Machine Learning: A Threat to Academic Contribution and Scrutiny?, arXiv:2401.02452 [cs.CY] (2024).https://doi.org/10.48550/arXiv.2401.02452. 558. S. Teleanu, J. Kurbalija, 'Stronger digital voices from Africa: Building African digital foreign policy and diplomacy' (Diplo, 2022);www.diplomacy.edu/resource/report-stronger-digital-voices-from-africa/. 559. International Telecommunication Union (ITU), 'Internet use' in Measuring digital development: Facts and Figures 2023 (ITU Publications, 2023), pp. 1-2. 560. E. Panos, M. Densing, K. Volkart, Access to electricity in the World Energy Council's global energy scenarios: An outlook for developing regions until 2030. Energy Strategy Reviews 9, 28-49 (2016).https://doi.org/10.1016/j.esr.2015.11.003. 561. H. Ritchie, P. Rosado, M. Roser, Access to energy in Our World in Data. (2024).https://ourworldindata.org/energy-access. 562. N. Maslej, L. Fattorini, R. Perrault, V. Parli, A. Reuel, E. Brynjolfsson, J. Etchemendy, K. Ligett, T. Lyons, J. Manyika, J. C. Niebles, Y. Shoham, R. Wald, J. Clark, 'TheAIIndex 2024 Annual Report' (AIIndex Steering Committee, Institute for Human-CenteredAI, Stanford University, 2024);https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024.pdf. 563. N. Ahmed, M. Wahed, N. C. Thompson, The growing influence of industry inAIresearch. Science 379, 884-886 (2023).https://doi.org/10.1126/science.ade2420. 564.* M. Sukumaran, A. Lewis, Server Market Analysis - 2H23 (2023).https://omdia.tech.informa.com/om033795/server-market-analysis-2h23. 565. M. L. Gray, S. Suri, Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass (Houghton Mifflin Harcourt, 2019), pp. 297. 566. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, 'ImageNet: A large-scale hierarchical image database' in 2009 IEEE Conference on Computer Vision and Pattern Recognition (2009) pp. 248-255.https://doi.org/10.1109/cvpr.2009.5206848. 567. A. Arora, M. Barrett, E. Lee, E. Oborn, K. Prince, Risk and the future ofAI: Algorithmic bias, data colonialism, and marginalization. Inf. Organ. 33 (2023).https://doi.org/10.1016/j.infoandorg.2023.100478. 568. C. T. Okolo, 'Addressing global inequity inAIdevelopment' in Handbook of Critical Studies of Artificial Intelligence (Edward Elgar Publishing, 2023), pp. 378-389. 569. D. Wang, S. Prabhat, N. Sambasivan, 'WhoseAIDream? In search of the aspiration in data annotation' in CHI Conference on Human Factors in Computing Systems (CHI '22) (ACM, 2022) pp. 1-16.https://doi.org/10.1145/3491102.3502121. 570. M. Miceli, T. Yang, A. Alvarado Garcia, J. Posada, S. M. Wang, M. Pohl, A. Hanna, Documenting Data Production Processes: A Participatory Approach for Data Work. Proc. ACM Hum. Comput. Interact. 6, 1-34 (2022).https://doi.org/10.1145/3555623. 571. E. Brynjolfsson, A. Ng, 'BigAIcan centralize decision-making and power, and that'sa problem' in Missing links inAIgovernance (UNESCO/Mila, 2023), pp. 65-87. 572. R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. Chatterji, A. Chen, K. Creel, J. Q. Davis, D. Demszky, . . . P. Liang, On the Opportunities and Risks of Foundation Models, arXiv:2108.07258 [cs.LG] (2021).https://doi.org/10.48550/arXiv.2108.07258. 573. J. Vipra, A. Korinek, 'Market concentration implications of foundation models: The Invisible Hand of ChatGPT' (Brookings Institution, 2023). 574. Competition and Markets Authority, 'AIFoundation Models: Initial report' (CMA, 2023);www.gov.uk/government/publications/ai-foundation-models-initial-report. 575. A. Goldfarb, D. Trefler, 'Artificial Intelligence and International Trade' in The Economics of Artificial Intelligence: An Agenda (University of Chicago Press, 2018), pp. 463-492. 576. R. Naik, B. Nushi, 'Social Biases through the Text-to-Image Generation Lens' in Proceedings of the 2023 AAAI/ACM Conference onAI, Ethics, and Society (AIES '23) (Association for Computing Machinery, 2023) pp. 786-808.https://doi.org/10.1145/3600211.3604711. 577. J. Danielsson, R. Macrae, A. Uthemann, Artificial intelligence and systemic risk. J. Bank. Financ. 140, 106290 (2022).https://doi.org/10.1016/j.jbankfin.2021.106290. 578. International Energy Agency, 'Tracking Clean Energy Progress 2023' (IEA, 2023);www.iea.org/reports/tracking-clean-energy-progress-2023. 579. European Commission Joint Research Centre, G. Kamiya, P. Bertoldi, Energy consumption in data centres and broadband communication networks in theEU(Publications Office of the European Union, 2024). 580. E. Halper, Amid explosive demand, America is running out of power in Washington Post. (2024).www.washingtonpost.com/business/2024/03/07/ai-data-centers-power/. 581.* A. S. Luccioni, A. Hernandez-Garcia, Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning, arXiv:2302.08476 [cs.LG] (2023).http://arxiv.org/abs/2302.08476. 582. P. Li, J. Yang, M. A. Islam, S. Ren, MakingAILess 'Thirsty': Uncovering and Addressing the Secret Water Footprint ofAIModels, arXiv:2304.03271 [cs.LG] (2023).http://arxiv.org/abs/2304.03271. 583. D. J. Solove, Understanding privacy (Harvard University Press, London, England, ed. First, 2009), pp. 257. 584. B. Smith, Developing and deployingAIresponsibly: Elements of an effective legislative framework to regulateAI. (2023).https://blogs.microsoft.com/on-the-issues/2023/09/12/developing-and-deploying-ai-responsibly-elements-of-an-effective-legislative-framework-to-regulate-ai/. 585. H. Nissenbaum, Privacy in Context: Technology, Policy, and the Integrity of Social Life (Stanford University Press, Palo Alto, CA, 2009), pp. 304. 586. L. Bourtoule, V. Chandrasekaran, C. A. Choquette-Choo, H. Jia, A. Travers, B. Zhang, D. Lie, N. Papernot, 'Machine Unlearning' in 2021 IEEE Symposium on Security and Privacy (SP) (IEEE, 2021) pp. 141-159.https://doi.org/10.1109/sp40001.2021.00019. 587. D. J. Solove, Artificial Intelligence and Privacy. Florida Law Review (2025).https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4713111. 588. R. Shokri, M. Stronati, C. Song, V. Shmatikov, 'Membership Inference Attacks Against Machine Learning Models' in 2017 IEEE Symposium on Security and Privacy (SP) (IEEE, 2017) pp. 3-18.https://doi.org/10.1109/sp.2017.41. 589. M. Fredrikson, S. Jha, T. Ristenpart, 'Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures' in Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security (CCS '15) (Association for Computing Machinery, 2015) pp. 1322-1333.https://doi.org/10.1145/2810103.2813677. 590. G. Chen, Y. Zhang, F. Song, 'SLMIA-SR: Speaker-level membership inference attacks against speaker recognition systems' in Proceedings 2024 Network and Distributed System Security Symposium (NDSS 2024) (Internet Society, 2024).https://doi.org/10.14722/ndss.2024.241323. 591. N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson, A. Oprea, C. Raffel, 'Extracting training data from large language models' in 30th USENIX security symposium (USENIX security 21) (USENIX Association, 2021) pp. 2633-2650.www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting. 592. N. Carlini, J. Hayes, M. Nasr, M. Jagielski, V. Sehwag, F. Tramer, B. Balle, D. Ippolito, E. Wallace, 'Extracting training data from diffusion models' in 32nd USENIX security symposium (USENIX security 23) (USENIX Association, 2023) pp. 5253-5270.www.usenix.org/conference/usenixsecurity23/presentation/carlini. 593. W. Shi, A. Ajith, M. Xia, Y. Huang, D. Liu, T. Blevins, D. Chen, L. Zettlemoyer, 'Detecting Pretraining Data from Large Language Models' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=zWqr3MQuNs. 594. N. Lukas, A. Salem, R. Sim, S. Tople, L. Wutschitz, S. Zanella-Beguelin, 'Analyzing Leakage of Personally Identifiable Information in Language Models' in 2023 IEEE Symposium on Security and Privacy (SP) (IEEE, 2023) pp. 346-363.https://doi.org/10.1109/sp46215.2023.10179300. 595. N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, C. Zhang, 'Quantifying Memorization Across Neural Language Models' in The 11th International Conference on Learning Representations (ICLR 2023) (2022).https://openreview.net/forum?id=TatRHT_1cK. 596.* K. Saab, T. Tu, W.-H. Weng, R. Tanno, D. Stutz, E. Wulczyn, F. Zhang, T. Strother, C. Park, E. Vedadi, J. Z. Chaves, S.-Y. Hu, M. Schaekermann, A. Kamath, Y. Cheng, D. G. T. Barrett, C. Cheung, B. Mustafa, A. Palepu, . . . V. Natarajan, 'Capabilities of Gemini Models in Medicine' (Google Deepmind, 2024);http://arxiv.org/abs/2404.18416. 597. K. Hill, The Secretive Company That Might End Privacy as We Know It in The New York Times. (2020).www.nytimes.com/2020/01/18/technology/clearview-privacy-facial-recognition.html. 598. R. Staab, M. Vero, M. Balunovic, M. Vechev, 'Beyond Memorization: Violating Privacy via Inference with Large Language Models' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=kmn0BhQk7p. 599. M. Sharma, M. Kaur, 'A Review of Deepfake Technology: An EmergingAIThreat' in Soft Computing for Security Applications (Springer Singapore, 2022) pp. 605-619.https://doi.org/10.1007/978-981-16-5301-8_44. 600. P. Burger, The Berne Convention: Its History and Its Key Role in the Future. J.L. & Tech. 3, 1-70 (1988).https://heinonline.org/HOL/LandingPage?handle=hein.journals/jlawtecy3&div=4&id=&page=. 601. L. Patterson, Copyright in 1791: An Essay Concerning the Founers' View of the Copyright Power Granted to Congress in Article I, Section 8, Clause 8 of the US Constitution. Emory L. J. (2003).https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/emlj52&section=25. 602. The Office of the Law Revision Counsel of the United States House of Representatives, 'Limitations on exclusive rights: Fair use. Sec. 107' in United States Code, 2006 Edition, Supplement 4, Title 17 - Copyrights (US Government Publishing Office, ed. 2010, 2010). 603. E. Parliament, D.-G. f. I. P. o. t. Union, E. Rosati, The exception for text and data mining (TDM) in the proposed Directive on Copyright in the Digital Single Market - Technical aspects (European Parliament, 2018). 604. Japanese Law Translation Database System, 'Zhu Zuo Quan Fa (Yi Bu Wei Shi Xing )Copyright Act (Partially unenforced)' (Ministry of Justice, Japan, 2024);www.japaneselawtranslation.go.jp/en/laws/view/4207. 605. Israeli Ministry of Justice, Opinion: Uses of Copyrighted Materials for Machine Learning. (2022).www.gov.il/BlobFolder/legalinfo/machine-learning/he/18-12-2022.pdf. 606. Intellectual Property Office of Singapore, 'Copyright: Factsheet on Copyright Act 2021' (IPOS, 2022);www.ipos.gov.sg/docs/default-source/resources-library/copyright/copyright-act-factsheet.pdf. 607. L. Soldaini, R. Kinney, A. Bhagia, D. Schwenk, D. Atkinson, R. Authur, B. Bogin, K. Chandu, J. Dumas, Y. Elazar, V. Hofmann, A. H. Jha, S. Kumar, L. Lucy, X. Lyu, N. Lambert, I. Magnusson, J. Morrison, N. Muennighoff, . . . K. Lo, Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research, arXiv:2402.00159 [cs.CL] (2024).http://arxiv.org/abs/2402.00159. 608. L. Tiedrich, TheAIdata scraping challenge: How can we proceed responsibly? (2024).https://oecd.ai/en/wonk/data-scraping-responsibly. 609. B. L. W. Sobel, Artificial Intelligence's Fair Use Crisis. JLA 41, 45-97 (2018).https://doi.org/10.7916/jla.v41i1.2036. 610. M. A. Lemley, B. Casey, Fair Learning. Tex. Law Rev. 99, 743-786 (2020).https://texaslawreview.org/fair-learning/. 611. P. Samuelson, GenerativeAImeets copyright. Science 381, 158-161 (2023).https://doi.org/10.1126/science.adi0656. 612. Tremblay v. OpenAI, Inc. (3:23-cv-03223) Document 1. (District Court, N.D. California, 2023).https://storage.courtlistener.com/recap/gov.uscourts.cand.414822/gov.uscourts.cand.414822.1.0_1.pdf. 613. D. Zhang, B. Xia, Y. Liu, X. Xu, T. Hoang, Z. Xing, M. Staples, Q. Lu, L. Zhu, 'Privacy and Copyright Protection in GenerativeAI: A Lifecycle Perspective' in 3rd International Conference onAIEngineering - Software Engineering forAI(CAIN) (2024).http://arxiv.org/abs/2311.18252. 614. R. Mahari, S. Longpre, 'Discit ergo est: Training Data Provenance And Fair Use' in Dynamics of GenerativeAI, T. Schrepel, V. Stocker, Eds. (Network Law Review, 2023). 615. K. Lee, A. F. Cooper, J. Grimmelmann, 'Talkin' 'BoutAIGeneration: Copyright and the Generative-AISupply Chain (The Short Version)' in Proceedings of the Symposium on Computer Science and Law (CSLAW '24) (Association for Computing Machinery, 2024) pp. 48-63.https://doi.org/10.1145/3614407.3643696. 616. J. Grimmelmann, Copyright for Literate Robots. Iowa Law Rev. 101, 657-682 (2015).https://scholarship.law.cornell.edu/cgi/viewcontent.cgi?article=2615&context=facpub. 617. K. Lee, A. F. Cooper, J. Grimmelmann, D. Ippolito,AIand Law: The Next Generation. (2023).https://doi.org/10.2139/ssrn.4580739. 618. L. Tiedrich, WhenAIgenerates work, standard contractual terms can help generate value and clarity in OECD.AIPolicy Observatory. (2024).https://oecd.ai/en/wonk/contractual-terms. 619. M. Sag, Copyright Safety for GenerativeAI. Houst. Law Rev. 61, 295-347 (2023).https://houstonlawreview.org/article/92126-copyright-safety-for-generative-ai. 620. N. Vyas, S. M. Kakade, B. Barak, 'On Provable Copyright Protection for Generative Models' in Proceedings of the 40th International Conference on Machine Learning (ICML 2023) (PMLR, 2023).https://proceedings.mlr.press/v202/vyas23b.html. 621. K. McElheran, J. F. Li, E. Brynjolfsson, Z. Kroff, E. Dinlersoz, L. Foster, N. Zolas,AIadoption in America: Who, what, and where. J. Econ. Manag. Strategy 33, 375-415 (2024).https://doi.org/10.1111/jems.12576. 622. R. Mahari, L. Shayne, L. Donewald, A. Polozov, A. Pentland, A. Lipsitz, Comment to US Copyright Office on Data Provenance and Copyright. (US Copyright Office, 2023).https://dspace.mit.edu/handle/1721.1/154171?show=full?show=full. 623. S. Min, S. Gururangan, E. Wallace, W. Shi, H. Hajishirzi, N. A. Smith, L. Zettlemoyer, 'SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore' in NeurIPS 2023 Workshop on Distribution Shifts (DistShift) (2023).https://openreview.net/forum?id=z03bW0doni. 624. S. Longpre, R. Mahari, N. Obeng-Marnu, W. Brannon, T. South, J. Kabbara, S. Pentland, Data authenticity, consent, and provenance forAIare all broken: What will it take to fix them? An MIT Exploration of GenerativeAI(2024).https://doi.org/10.21428/e4baedd9.a650f77d. 625. S. G. Patil, T. Zhang, V. Fang, N. C., R. Huang, A. Hao, M. Casado, J. E. Gonzalez, R. A. Popa, I. Stoica, GoEX: Perspectives and Designs Towards a Runtime for AutonomousLLMApplications, arXiv:2404.06921 [cs.CL] (2024).https://doi.org/10.48550/arXiv.2404.06921. 626. D. Dalrymple, J. Skalse, Y. Bengio, S. Russell, M. Tegmark, S. Seshia, S. Omohundro, C. Szegedy, B. Goldhaber, N. Ammann, A. Abate, J. Halpern, C. Barrett, D. Zhao, T. Zhi-Xuan, J. Wing, J. Tenenbaum, Towards Guaranteed SafeAI: A Framework for Ensuring Robust and ReliableAISystems, arXiv:2405.06624 [cs.AI] (2024).http://arxiv.org/abs/2405.06624. 627. N. Cammarata, S. Carter, G. Goh, C. Olah, M. Petrov, L. Schubert, Thread: Circuits. Distill 5, 10.23915/distill.00024 (2020).https://doi.org/10.23915/distill.00024. 628. N. Nanda, L. Chan, T. Lieberum, J. Smith, J. Steinhardt, 'Progress measures for grokking via mechanistic interpretability' in The 11th International Conference on Learning Representations (ICLR 2023) (2022).https://openreview.net/forum?id=9XFSbDPmdW. 629. Z. Zhong, Z. Liu, M. Tegmark, J. Andreas, 'The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=S5wmbQc1We. 630. E. J. Michaud, I. Liao, V. Lad, Z. Liu, A. Mudide, C. Loughridge, Z. C. Guo, T. R. Kheirkhah, M. Vukelic, M. Tegmark, Opening theAIblack box: program synthesis via mechanistic interpretability, arXiv:2402.05110 [cs.LG] (2024).https://doi.org/10.48550/arXiv.2402.05110. 631. R. Huben, H. Cunningham, L. R. Smith, A. Ewart, L. Sharkey, 'Sparse Autoencoders Find Highly Interpretable Features in Language Models' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=F76bwRSLeK. 632.* T. Bricken, A. Templeton, J. Batson, B. Chen, A. Jermyn, T. Conerly, N. Turner, C. Anil, C. Denison, A. Askell, R. Lasenby, Y. Wu, S. Kravec, N. Schiefer, T. Maxwell, N. Joseph, Z. Hatfield-Dodds, A. Tamkin, K. Nguyen, . . . C. Olah, Towards monosemanticity: Decomposing language models with dictionary learning, Transformer Circuits Thread (2023).https://transformer-circuits.pub/2023/monosemantic-features. 633. L. Gao, J. Schulman, J. Hilton, 'Scaling Laws for Reward Model Overoptimization' in Proceedings of the 40th International Conference on Machine Learning (PMLR, 2023) pp. 10835-10866.https://proceedings.mlr.press/v202/gao23h.html. 634. J. M. V. Skalse, N. H. R. Howe, D. Krasheninnikov, D. Krueger, 'Defining and Characterizing Reward Gaming' in 36th Conference on Neural Information Processing Systems (NeurIPS 2022) (2022).https://openreview.net/forum?id=yb3HOXO3lX2. 635. P. Singhal, T. Goyal, J. Xu, G. Durrett, A Long Way to Go: Investigating Length Correlations inRLHF, arXiv:2310.03716 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2310.03716. 636. J. Tien, J. Z.-Y. He, Z. Erickson, A. Dragan, D. S. Brown, 'Causal Confusion and Reward Misidentification in Preference-Based Reward Learning' in The 11th International Conference on Learning Representations (ICLR 2023) (2022).https://openreview.net/forum?id=R0Xxvr_X3ZA. 637. L. E. McKinney, Y. Duan, D. Krueger, A. Gleave, 'On The Fragility of Learned Reward Functions' in 36th Conference on Neural Information Processing Systems (NeurIPS 2022) Deep Reinforcement Learning Workshop (2022).https://openreview.net/forum?id=9gj9vXfeS-y. 638. L. L. D. Langosco, J. Koch, L. D. Sharkey, J. Pfau, D. Krueger, 'Goal Misgeneralization in Deep Reinforcement Learning' in Proceedings of the 39th International Conference on Machine Learning (PMLR, 2022) vol. 162, pp. 12004-12019.https://proceedings.mlr.press/v162/langosco22a.html. 639. E. Seger, N. Dreksler, R. Moulange, E. Dardaman, J. Schuett, K. Wei, C. Winter, M. Arnold, S. O. hEigeartaigh, A. Korinek, M. Anderljung, B. Bucknall, A. Chan, E. Stafford, L. Koessler, A. Ovadya, B. Garfinkel, E. Bluemke, M. Aird, . . . A. Gupta, 'Open-Sourcing Highly Capable Foundation Models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives' (Centre for the Governance ofAI, 2023);http://arxiv.org/abs/2311.09227. 640. S. Kapoor, R. Bommasani, K. Klyman, S. Longpre, A. Ramaswami, P. Cihon, A. Hopkins, K. Bankston, S. Biderman, M. Bogen, R. Chowdhury, A. Engler, P. Henderson, Y. Jernite, S. Lazar, S. Maffulli, A. Nelson, J. Pineau, A. Skowron, . . . A. Narayanan, On the Societal Impact of Open Foundation Models, arXiv:2403.07918 [cs.CY] (2024).https://doi.org/10.48550/arXiv.2403.07918. 641. K. Hu, ChatGPT sets record for fastest-growing user base - analyst note in Reuters. (2023).www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/. 642.* M. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. Feder Cooper, D. Ippolito, C. A. Choquette-Choo, E. Wallace, F. Tramer, K. Lee, Scalable Extraction of Training Data from (Production) Language Models, arXiv:2311.17035 [cs.LG] (2023).https://doi.org/10.48550/arXiv.2311.17035. 643. H. Li, D. Guo, W. Fan, M. Xu, J. Huang, F. Meng, Y. Song, 'Multi-step Jailbreaking Privacy Attacks on ChatGPT' in The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023) (2023).https://openreview.net/forum?id=ls4Pfsl2jZ. 644. A. Deshpande, V. Murahari, T. Rajpurohit, A. Kalyan, K. Narasimhan, 'Toxicity in chatgpt: Analyzing persona-assigned language models' in Findings of the Association for Computational Linguistics: EMNLP 2023 (Association for Computational Linguistics, 2023) pp. 1236-1270.https://doi.org/10.18653/v1/2023.findings-emnlp.88. 645. V. Hofmann, P. R. Kalluri, D. Jurafsky, S. King, Dialect prejudice predictsAIdecisions about people's character, employability, and criminality, arXiv:2403.00742 [cs.CL] (2024).https://doi.org/10.48550/arXiv.2403.00742. 646. E. Choi, Y. Jo, J. Jang, J. Jang, M. Seo, 'Fixed Input Parameterization for Efficient Prompting' in Findings of the Association for Computational Linguistics: ACL 2023 (Association for Computational Linguistics, 2023) pp. 8428-8441.https://doi.org/10.18653/v1/2023.findings-acl.533. 647. E. Shayegani, M. A. Al Mamun, Y. Fu, P. Zaree, Y. Dong, N. Abu-Ghazaleh, Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks, arXiv:2310.10844 [cs.CL] (2023).http://arxiv.org/abs/2310.10844. 648.* C. Anil, E. Durmus, M. Sharma, J. Benton, S. Kundu, J. Batson, N. Rimsky, M. Tong, J. Mu, D. Ford, F. Mosconi, R. Agrawal, R. Schaeffer, N. Bashkansky, S. Svenningsen, M. Lambert, A. Radhakrishnan, C. Denison, E. J. Hubinger, . . . D. Duvenaud, 'Many-shot Jailbreaking' (Anthropic, 2024). 649. Y. M. Pa Pa, S. Tanizaki, T. Kou, M. van Eeten, K. Yoshioka, T. Matsumoto, 'An Attacker's Dream? Exploring the Capabilities of ChatGPT for Developing Malware' in Proceedings of the 16th Cyber Security Experimentation and Test Workshop (CSET '23) (Association for Computing Machinery, 2023) pp. 10-18.https://doi.org/10.1145/3607505.3607513. 650. Q. Zhan, R. Fang, R. Bindu, A. Gupta, T. Hashimoto, D. Kang, 'RemovingRLHFProtections in GPT-4 via Fine-Tuning' in 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (2024).https://doi.org/10.48550/arXiv.2311.05553. 651. X. Qi, Y. Zeng, T. Xie, P.-Y. Chen, R. Jia, P. Mittal, P. Henderson, 'Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=hTEGyKf0dZ. 652. Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou, R. Zheng, X. Fan, X. Wang, L. Xiong, Y. Zhou, W. Wang, C. Jiang, Y. Zou, X. Liu, . . . T. Gui, The Rise and Potential of Large Language Model Based Agents: A Survey, arXiv:2309.07864 [cs.AI] (2023).https://doi.org/10.48550/arXiv.2309.07864. 653. Y. Tian, X. Yang, J. Zhang, Y. Dong, H. Su, Evil Geniuses: Delving into the Safety ofLLM-based Agents, arXiv:2311.11855 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2311.11855. [ 654. Z. Wu, C. Han, Z. Ding, Z. Weng, Z. Liu, S. Yao, T. Yu, L. Kong, OS-Copilot: Towards Generalist Computer Agents with Self-Improvement, arXiv:2402.07456 [cs.AI] (2024).http://arxiv.org/abs/2402.07456. 655.* SIMA Team, M. A. Raad, A. Ahuja, C. Barros, F. Besse, A. Bolt, A. Bolton, B. Brownfield, G. Buttimore, M. Cant, S. Chakera, S. C. Y. Chan, J. Clune, A. Collister, V. Copeman, A. Cullum, I. Dasgupta, D. de Cesare, J. Di Trapani, . . . N. Young, 'Scaling Instructable Agents Across Many Simulated Worlds' (Google Deepmind, 2024);http://arxiv.org/abs/2404.10179. 656. Q. Lu, L. Zhu, X. Xu, Z. Xing, S. Harrer, J. Whittle, Towards Responsible GenerativeAI: A Reference Architecture for Designing Foundation Model based Agents, arXiv:2311.13148 [cs.AI] (2023).http://arxiv.org/abs/2311.13148. 657.* R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight, B. Chess, J. Schulman, 'WebGPT: Browser-assisted question-answering with human feedback' (OpenAI, 2021);http://arxiv.org/abs/2112.09332. 658. A. Chan, C. Ezell, M. Kaufmann, K. Wei, L. Hammond, H. Bradley, E. Bluemke, N. Rajkumar, D. Krueger, N. Kolt, L. Heim, M. Anderljung, Visibility intoAIAgents, arXiv:2401.13138 [cs.CY] (2024).https://doi.org/10.48550/arXiv.2401.13138. 659. D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, . . . D. Hassabis, Mastering the game of Go with deep neural networks and tree search. Nature 529, 484-489 (2016).https://doi.org/10.1038/nature16961. 660. M. Mazeika, L. Phan, X. Yin, A. Zou, Z. Wang, N. Mu, E. Sakhaee, N. Li, S. Basart, B. Li, D. Forsyth, D. Hendrycks, HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal, arXiv:2402.04249 [cs.LG] (2024).http://arxiv.org/abs/2402.04249. 661. S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, K. R. Narasimhan, 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=5Xc1ecxO1h. 662. T. A. Han, L. M. Pereira, T. Lenaerts, 'Modelling and Influencing theAIBidding War: A Research Agenda' in Proceedings of the 2019 AAAI/ACM Conference onAI, Ethics, and Society (AIES '19) (2019) pp. 5-11.https://doi.org/10.1145/3306618.3314265. 663. T. Cimpeanu, F. C. Santos, L. M. Pereira, T. Lenaerts, T. A. Han, Artificial intelligence development races in heterogeneous settings. Sci. Rep. 12, 1723 (2022).https://doi.org/10.1038/s41598-022-05729-3. 664. S. Armstrong, N. Bostrom, C. Shulman, Racing to the precipice: a model of artificial intelligence development.AISoc. 31, 201-206 (2016).https://doi.org/10.1007/s00146-015-0590-y. 665. M. Menashe, The Race to the Bottom Revisited: International Labour Law, Global Trade and Evolutionary Game Theory. Oxf. J. Leg. Stud. 40, 53-81 (2020).https://doi.org/10.1093/ojls/gqz029. 666. M. M. Maas, 'Artificial intelligence governance under change: Foundations, facets, frameworks', thesis, University of Copenhagen (2020). 667. L. Collina, M. Sayyadi, M. Provitera, Critical Issues About A.I. Accountability Answered. California Management Review Insights (2023).https://cmr.berkeley.edu/2023/11/critical-issues-about-a-i-accountability-answered/. 668. A. T. da Fonseca, E. Vaz de Sequeira, L. Barreto Xavier, 'Liability forAIDriven Systems' in Multidisciplinary Perspectives on Artificial Intelligence and the Law, H. Sousa Antunes, P. M. Freitas, A. L. Oliveira, C. Martins Pereira, E. Vaz De Sequeira, L. Barreto Xavier, Eds. (Springer International Publishing, Cham, 2024), pp. 299-317. 669. M. Buiten, A. de Streel, M. Peitz, The law and economics ofAIliability. Comput. Law Secur. Rep. 48, 105794 (2023).https://doi.org/10.1016/j.clsr.2023.105794. 670. M. Busuioc, Accountable Artificial Intelligence: Holding Algorithms to Account. Public Adm. Rev. 81, 825-836 (2021).https://doi.org/10.1111/puar.13293. 671. F. Doshi-Velez, M. Kortz, R. Budish, C. Bavitz, S. J. Gershman, D. O'Brien, K. Scott, S. Shieber, J. Waldo, D. Weinberger, A. Weller, A. Wood, 'Accountability ofAIUnder the Law: The Role of Explanation' (Berkman Klein Center Working Group on Explanation and the Law, 2017);http://nrs.harvard.edu/urn-3:HUL.InstRepos:34372584. 672. N. Kolt, M. Anderljung, J. Barnhart, A. Brass, K. Esvelt, G. K. Hadfield, L. Heim, M. Rodriguez, J. B. Sandbrink, T. Woodside, Responsible Reporting for FrontierAIDevelopment, arXiv:2404.02675 [cs.CY] (2024).https://doi.org/10.48550/arXiv.2404.02675. 673. M. Anderljung, J. Barnhart, A. Korinek, J. Leung, C. O'Keefe, J. Whittlestone, S. Avin, M. Brundage, J. Bullock, D. Cass-Beggs, B. Chang, T. Collins, T. Fist, G. Hadfield, A. Hayes, L. Ho, S. Hooker, E. Horvitz, N. Kolt, . . . K. Wolf, FrontierAIRegulation: Managing Emerging Risks to Public Safety, arXiv:2307.03718 [cs.CY] (2023).https://doi.org/10.48550/arXiv.2307.03718. 674. R. Palin, I. Habli, 'Assurance of Automotive Safety - A Safety Case Approach' in Computer Safety, Reliability, and Security (SAFECOMP 2010) (Springer, 2010) pp. 82-96.https://doi.org/10.1007/978-3-642-15651-9_7. 675. M. L. Cummings, Rethinking the Maturity of Artificial Intelligence in Safety-Critical Settings.AIMag. 42, 6-15 (2021).https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/7394. 676. World Health Organization, Health products policy and standards (2024).www.who.int/teams/health-product-and-policy-standards/standards-and-specifications/norms-and-standards-for-pharmaceuticals/guidelines. 677. I. I. Livshitz, P. A. Lontsikh, N. P. Lontsikh, E. Y. Golovina, O. M. Safonova, 'A Study of Modern Risk Management Methods for Industrial Safety Assurance in the Fuel and Energy Industry' in 2021 International Conference on Quality Management, Transport and Information Security, Information Technologies (IT&QM&IS) (2021) pp. 165-167.https://doi.org/10.1109/itqmis53292.2021.9642791. 678. H. E. Roland, B. Moriarty, System safety engineering and management (Wiley, New York, ed. 2nd ed, 1990), pp. 367. 679. L. Koessler, J. Schuett, Risk assessment atAGIcompanies: A review of popular risk assessment techniques from other safety-critical industries, arXiv:2307.08823 [cs.CY] (2023).https://doi.org/10.48550/arXiv.2307.08823. 680. National Institute of Standards and Technology,AIRisk Management Framework. NIST (2021).www.nist.gov/itl/ai-risk-management-framework. 681. D. Khodyakov, S. Grant, J. Kroger, M. Bauman, 'RAND methodological guidance for conducting and critically appraising Delphi panels' (RAND Corporation, 2023);https://doi.org/10.7249/tla3082-1. 682. ISO, ISO 31000: Risk management (2021).https://www.iso.org/iso-31000-risk-management.html. 683. US Nuclear Regulatory Commission, Risk Metrics for Operating New Reactors. (2009).www.nrc.gov/docs/ML0909/ML090910608.pdf. 684. E. Black, R. Naidu, R. Ghani, K. Rodolfa, D. Ho, H. Heidari, 'Toward Operationalizing Pipeline-aware ML Fairness: A Research Agenda for Developing Practical Guidelines and Tools' in Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO '23) (Association for Computing Machinery, 2023) pp. 1-11.https://doi.org/10.1145/3617694.3623259. 685. S. Rismani, R. Shelby, A. Smart, E. Jatho, J. Kroll, A. Moon, N. Rostamzadeh, 'From Plane Crashes to Algorithmic Harm: Applicability of Safety Engineering Frameworks for Responsible ML' in Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23) (Association for Computing Machinery, 2023) pp. 1-18.https://doi.org/10.1145/3544548.3581407. 686. T. Kelly, A Systematic Approach to Safety Case Management. SAE Trans. J. Mater. Manuf. 113, 257-266 (2004).http://www.jstor.org/stable/44699541. 687. M. Stein, C. Dunlop, 'Safe before sale: Learnings from the FDA's model of life sciences oversight for foundation models' (Ada Lovelace Institute, 2023);www.adalovelaceinstitute.org/wp-content/uploads/2023/12/2023_12_ALI_Safe-before-sale_Discussion_paper.pdf. 688. T. Raz, D. Hillson, A Comparative Review of Risk Management Standards. Risk Manage.: Int. J. 7, 53-66 (2005).https://doi.org/10.1057/palgrave.rm.8240227. 689. J. Clymer, N. Gabrieli, D. Krueger, T. Larsen, Safety Cases: How to Justify the Safety of AdvancedAISystems, arXiv:2403.10462 [cs.CY] (2024).http://arxiv.org/abs/2403.10462. 690. C. Haddon-Cave, The Nimrod Review: an independent review into the broader issues surrounding the loss of the RAF Nimrod MR2 aircraft XV230 in Afghanistan in 2006, report (Stationery Office, 2009). 691. N. G. Leveson, Applying systems thinking to analyze and learn from events. Saf. Sci. 49, 55-64 (2011).https://doi.org/10.1016/j.ssci.2009.12.021. 692. D. Hendrycks, Introduction toAISafety, Ethics, and Society (Taylor & Francis). 693.* Anthropic, Anthropic's Responsible Scaling Policy, Version 1.0. (2023). 694.* OpenAI, 'Preparedness Framework (Beta)' (OpenAI, 2023);https://cdn.openai.com/openai-preparedness-framework-beta.pdf. 695.* Z. Kenton, T. Everitt, L. Weidinger, I. Gabriel, V. Mikulik, G. Irving, 'Alignment of Language Agents' (Google DeepMind, 2021);http://arxiv.org/abs/2103.14659. 696. M. Wu, A. F. Aji, Style Over Substance: Evaluation Biases for Large Language Models, arXiv:2307.03025 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2307.03025. 697.* N. Lambert, R. Calandra, The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback, arXiv:2311.00168 [cs.LG] (2023).https://doi.org/10.48550/arXiv.2311.00168. 698. H. Bansal, J. Dang, A. Grover, 'Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=dKl6lMwbCy. 699. J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, I. Higgins, 'Solving math word problems with process- and outcome-based feedback' (Google Deepmind, 2022);https://doi.org/10.48550/arXiv.2211.14275. 700. H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, K. Cobbe, 'Let's Verify Step by Step' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=v8L0pN6EOi. 701. Z. Wu, Y. Hu, W. Shi, N. Dziri, A. Suhr, P. Ammanabrolu, N. A. Smith, M. Ostendorf, H. Hajishirzi, 'Fine-Grained Human Feedback Gives Better Rewards for Language Model Training' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=CSbGXyCswu. 702.* H. Lee, S. Phatale, H. Mansoor, T. Mesnard, J. Ferret, K. Lu, C. Bishop, E. Hall, V. Carbune, A. Rastogi, S. Prakash, RLAIF: Scaling Reinforcement Learning from Human Feedback withAIFeedback, arXiv:2309.00267 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2309.00267. 703. D. Hadfield-Menell, A. Dragan, P. Abbeel, S. Russell, 'Cooperative inverse reinforcement learning' in Proceedings of the 30th International Conference on Neural Information Processing Systems (NIPS'16) (Curran Associates Inc., 2016) pp. 3916-3924. 704. D. Hadfield-Menell, S. Milli, P. Abbeel, S. Russell, A. D. Dragan, 'Inverse reward design' in Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17) (Curran Associates Inc., 2017) pp. 6768-6777. 705. X. Liang, K. Shu, K. Lee, P. Abbeel, 'Reward Uncertainty for Exploration in Preference-based Reinforcement Learning' in The 10th International Conference on Learning Representations (ICLR 2022) (2021).https://openreview.net/forum?id=OWZVD-l-ZrC. 706.* A. Gleave, G. Irving, Uncertainty Estimation for Language Reward Models, arXiv:2203.07472 [cs.CL] (2022).https://doi.org/10.48550/arXiv.2203.07472. 707. A. Rame, G. Couairon, C. Dancette, J.-B. Gaya, M. Shukor, L. Soulier, M. Cord, 'Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=lSbbC2VyCu. 708. T. Coste, U. Anwar, R. Kirk, D. Krueger, 'Reward Model Ensembles Help Mitigate Overoptimization' in The 12th International Conference on Learning Representations (ICLR 2024) (2024).https://openreview.net/forum?id=dcjtMYkpXx. 709.* S. R. Bowman, J. Hyun, E. Perez, E. Chen, C. Pettit, S. Heiner, K. Lukosiute, A. Askell, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Olah, D. Amodei, D. Amodei, D. Drain, D. Li, E. Tran-Johnson, . . . J. Kaplan, Measuring Progress on Scalable Oversight for Large Language Models, arXiv:2211.03540 [cs.HC] (2022).http://arxiv.org/abs/2211.03540. 710.* C. Burns, P. Izmailov, J. H. Kirchner, B. Baker, L. Gao, L. Aschenbrenner, Y. Chen, A. Ecoffet, M. Joglekar, J. Leike, I. Sutskever, J. Wu, Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision, arXiv:2312.09390 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2312.09390. 711. J. Michael, S. Mahdi, D. Rein, J. Petty, J. Dirani, V. Padmakumar, S. R. Bowman, Debate Helps Supervise Unreliable Experts, arXiv:2311.08702 [cs.AI] (2023).http://arxiv.org/abs/2311.08702. 712.* J. Leike, D. Krueger, T. Everitt, M. Martic, V. Maini, S. Legg, Scalable agent alignment via reward modeling: a research direction, arXiv:1811.07871 [cs.LG] (2018).http://arxiv.org/abs/1811.07871. 713.* A. Khan, J. Hughes, D. Valentine, L. Ruis, K. Sachan, A. Radhakrishnan, E. Grefenstette, S. R. Bowman, T. Rocktaschel, E. Perez, Debating with More PersuasiveLLMsLeads to More Truthful Answers, arXiv:2402.06782 [cs.AI] (2024).http://arxiv.org/abs/2402.06782. 714. Z. Li, The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic Parrots and Hallucination, arXiv:2304.14347 [cs.CY] (2023).http://arxiv.org/abs/2304.14347. 715.* A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. Brown, J. Clark, . . . J. Kaplan, A General Language Assistant as a Laboratory for Alignment, arXiv:2112.00861 [cs.CL] (2021).http://arxiv.org/abs/2112.00861. 716. P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W.-T. Yih, T. Rocktaschel, S. Riedel, D. Kiela, 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks' in 34th Conference on Neural Information Processing Systems (NeurIPS 2020) (Curran Associates, Inc., 2020) vol. 33, pp. 9459-9474.https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html. 717. K. Shuster, S. Poff, M. Chen, D. Kiela, J. Weston, 'Retrieval Augmentation Reduces Hallucination in Conversation' in Findings of the Association for Computational Linguistics: EMNLP 2021 (Association for Computational Linguistics, 2021) pp. 3784-3803.https://doi.org/10.18653/v1/2021.findings-emnlp.320. 718. L. Kuhn, Y. Gal, S. Farquhar, 'Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation' in The 11th International Conference on Learning Representations (ICLR 2023) (2023).https://openreview.net/forum?id=VD-AYtP0dve. 719. D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli, M. Guo, D. Song, J. Steinhardt, J. Gilmer, 'The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization' in 2021 IEEE/CVF International Conference on Computer Vision (ICCV) (2021) pp. 8320-8329.https://doi.org/10.1109/iccv48922.2021.00823. 720. N. [Carlini, M. Nasr, C. A. Choquette-Choo, M. Jagielski, I. Gao, P. W. Koh, D. Ippolito, F. Tramer, L. Schmidt, 'Are aligned neural networks adversarially aligned?' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=OQQoD8Vc3B. 721. A. Madry, A. Makelov, L. Schmidt, D. Tsipras, A. Vladu, 'Towards Deep Learning Models Resistant to Adversarial Attacks' in The 6th International Conference on Learning Representations (ICLR 2018) (2018).https://openreview.net/forum?id=rJzIBfZAb. 722.* E. Hubinger, C. Denison, J. Mu, M. Lambert, M. Tong, M. MacDiarmid, T. Lanham, D. M. Ziegler, T. Maxwell, N. Cheng, A. Jermyn, A. Askell, A. Radhakrishnan, C. Anil, D. Duvenaud, D. Ganguli, F. Barez, J. Clark, K. Ndousse, . . . E. Perez, Sleeper Agents: Training DeceptiveLLMsthat Persist Through Safety Training, arXiv:2401.05566 [cs.CR] (2024).https://doi.org/10.48550/arXiv.2401.05566. 723. N. Jain, A. Schwarzschild, Y. Wen, G. Somepalli, J. Kirchenbauer, P.-Y. Chiang, M. Goldblum, A. Saha, J. Geiping, T. Goldstein, Baseline Defenses for Adversarial Attacks Against Aligned Language Models, arXiv:2309.00614 [cs.LG] (2023).https://doi.org/10.48550/arXiv.2309.00614. 724. S. Casper, L. Schulze, O. Patel, D. Hadfield-Menell, Defending Against Unforeseen Failure Modes with Latent Adversarial Training, arXiv:2403.05030 [cs.CR] (2024).https://doi.org/10.48550/arXiv.2403.05030. 725. D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, A. Madry, 'Robustness May Be at Odds with Accuracy' in The 7th International Conference on Learning Representations (ICLR 2019) (2018).https://openreview.net/forum?id=SyxAb30cY7. 726. H. Zhang, Y. Yu, J. Jiao, E. Xing, L. E. Ghaoui, M. Jordan, 'Theoretically Principled Trade-off between Robustness and Accuracy' in Proceedings of the 36th International Conference on Machine Learning (2019) pp. 7472-7482.https://proceedings.mlr.press/v97/zhang19p.html. 727. Y.-Y. Yang, C. Rashtchian, H. Zhang, R. R. Salakhutdinov, K. Chaudhuri, 'A Closer Look at Accuracy vs. Robustness' in Advances in Neural Information Processing Systems (NeurIPS 2020) (Curran Associates, Inc., 2020) vol. 33, pp. 8588-8601.https://proceedings.neurips.cc/paper/2020/hash/61d77652c97ef636343742fc3dcf3ba9-Abstract.html. 728. Y. Balaji, T. Goldstein, J. Hoffman, Instance adaptive adversarial training: Improved accuracy tradeoffs in neural nets, arXiv:1910.08051 [cs.LG] (2019).https://doi.org/10.48550/arXiv.1910.08051. 729. Y. Wang, D. Zou, J. Yi, J. Bailey, X. Ma, Q. Gu, 'Improving Adversarial Robustness Requires Revisiting Misclassified Examples' in The 8th International Conference on Learning Representations (ICLR 2020) (2019).https://openreview.net/forum?id=rklOg6EFwS. 730. R. Rade, S.-M. Moosavi-Dezfooli, 'Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off' in The 10th International Conference on Learning Representations (ICLR 2022) (2021).https://openreview.net/forum?id=Azh9QBQ4tR7. 731. Z. Liu, G. Dou, Z. Tan, Y. Tian, M. Jiang, Towards Safer Large Language Models through Machine Unlearning, arXiv:2402.10058 [cs.CL] (2024).https://doi.org/10.48550/arXiv.2402.10058. 732. S. Liu, Y. Yao, J. Jia, S. Casper, N. Baracaldo, P. Hase, X. Xu, Y. Yao, H. Li, K. R. Varshney, M. Bansal, S. Koyejo, Y. Liu, Rethinking Machine Unlearning for Large Language Models, arXiv:2402.08787 [cs.LG] (2024).https://doi.org/10.48550/arXiv.2402.08787. 733.* R. Eldan, M. Russinovich, Who's Harry Potter? Approximate Unlearning inLLMs, arXiv:2310.02238 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2310.02238. 734. A. Lynch, P. Guo, A. Ewart, S. Casper, D. Hadfield-Menell, Eight Methods to Evaluate Robust Unlearning inLLMs, arXiv:2402.16835 [cs.CL] (2024).https://doi.org/10.48550/arXiv.2402.16835. 735. G. Alain, Y. Bengio, 'Understanding intermediate layers using linear classifier probes' in 5th International Conference on Learning Representations, ICLR 2017 (2017).https://openreview.net/forum?id=HJ4-rAVtl. 736. P. Goyal, A. R. Soriano, C. Hazirbas, L. Sagun, N. Usunier, 'Fairness Indicators for Systematic Assessments of Visual Feature Extractors' in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22) (Association for Computing Machinery, 2022) pp. 70-88.https://doi.org/10.1145/3531146.3533074. 737. W. Gurnee, M. Tegmark, 'Language Models Represent Space and Time' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=jE8xbmvFin. 738.* S. Marks, M. Tegmark, The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets, arXiv:2310.06824 [cs.AI] (2023).https://doi.org/10.48550/arXiv.2310.06824. 739. A. Ravichander, Y. Belinkov, E. Hovy, 'Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?' in Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume (EACL 2021) (Association for Computational Linguistics, 2021) pp. 3363-3377.https://doi.org/10.18653/v1/2021.eacl-main.295. 740. Y. Elazar, S. Ravfogel, A. Jacovi, Y. Goldberg, Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals. Trans. Assoc. Comput. Linguist. 9, 160-175 (2021).https://doi.org/10.1162/tacl_a_00359. 741. O. Antverg, Y. Belinkov, 'On the Pitfalls of Analyzing Individual Neurons in Language Models' in The 10th International Conference on Learning Representations (ICLR 2022) (2021).https://openreview.net/forum?id=8uz0EWPQIMu. 742. T. Lieberum, M. Rahtz, J. Kramar, N. Nanda, G. Irving, R. Shah, V. Mikulik, Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla, arXiv:2307.09458 [cs.LG] (2023).http://arxiv.org/abs/2307.09458. 743. E. Mitchell, C. Lin, A. Bosselut, C. D. Manning, C. Finn, 'Memory-Based Model Editing at Scale' in Proceedings of the 39th International Conference on Machine Learning (PMLR, 2022) pp. 15817-15831.https://proceedings.mlr.press/v162/mitchell22a.html. 744. K. Meng, D. Bau, A. J. Andonian, Y. Belinkov, 'Locating and Editing Factual Associations in GPT' in 36th Conference on Neural Information Processing Systems (NeurIPS 2022) (2022).https://openreview.net/forum?id=-h6WAS6eE4. 745. K. Meng, A. S. Sharma, A. J. Andonian, Y. Belinkov, D. Bau, 'Mass-Editing Memory in a Transformer' in The 11th International Conference on Learning Representations (ICLR 2023) (2022).https://openreview.net/forum?id=MkbcAHIYgyS. 746. Y. Gandelsman, A. A. Efros, J. Steinhardt, 'Interpreting CLIP's Image Representation via Text-Based Decomposition' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=5Ca9sSzuDp. 747. C. Tan, G. Zhang, J. Fu, 'Massive Editing for Large Language Models via Meta Learning' in The 12th International Conference on Learning Representations (ICLR 2024) (2023).https://openreview.net/forum?id=L6L1CJQ2PE. 748. S. Wang, Y. Zhu, H. Liu, Z. Zheng, C. Chen, J. Li, Knowledge Editing for Large Language Models: A Survey, arXiv:2310.16218 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2310.16218. 749. Z. Han, C. Gao, J. Liu, J. Zhang, S. Q. Zhang, Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey, arXiv:2403.14608 [cs.LG] (2024).https://doi.org/10.48550/arXiv.2403.14608. 750. X. Wu, J. Li, M. Xu, W. Dong, S. Wu, C. Bian, D. Xiong, 'DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models' in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023) (Association for Computational Linguistics, 2023) pp. 2875-2886.https://doi.org/10.18653/v1/2023.emnlp-main.174. 751. K. Li, O. Patel, F. Viegas, H. Pfister, M. Wattenberg, 'Inference-Time Intervention: Eliciting Truthful Answers from a Language Model' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=aLLuYpn83y. 752. A. M. Turner, L. Thiergart, D. Udell, G. Leech, U. Mini, M. MacDiarmid, Activation Addition: Steering Language Models Without Optimization, arXiv:2308.10248 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2308.10248. 753. N. Belrose, D. Schneider-Joseph, S. Ravfogel, R. Cotterell, E. Raff, S. Biderman, 'LEACE: Perfect linear concept erasure in closed form' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=awIpKpwTwF&noteId=Ju4XcafMir. 754. E. Hernandez, B. Z. Li, J. Andreas, Inspecting and Editing Knowledge Representations in Language Models, arXiv:2304.00740 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2304.00740. 755. D. Brown, C. Godfrey, C. Nizinski, J. Tu, H. Kvinge, 'Robustness of Edited Neural Networks' in ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models (ME-FoMo 2023) (2023).https://openreview.net/forum?id=JAjH6VANZ4. 756. D. Gamage, J. Chen, K. Sasahara, 'The Emergence of Deepfakes and its Societal Implications: A Systematic Review' in Proceedings of the 2021 Truth and Trust Online Conference (Hacks Hackers, 2021) pp. 28-39.www.researchgate.net/publication/355583941_The_Emergence_of_Deepfakes_and_its_Societal_Implications_A_Systematic_Review. 757. A. Kaushal, A. Mina, A. Meena, T. H. Babu, 'The societal impact of Deepfakes: Advances in Detection and Mitigation' in 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT) (2023) pp. 1-7.https://doi.org/10.1109/icccnt56998.2023.10307353. 758. R. Tang, Y.-N. Chuang, X. Hu, The Science of DetectingLLM-Generated Text. Commun. ACM 67, 50-59 (2024).https://doi.org/10.1145/3624725. 759. R. Corvi, D. Cozzolino, G. Zingarini, G. Poggi, K. Nagano, L. Verdoliva, 'On The Detection of Synthetic Images Generated by Diffusion Models' in ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2023) pp. 1-5.https://doi.org/10.1109/icassp49357.2023.10095167. 760. U. Ojha, Y. Li, Y. J. Lee, 'Towards Universal Fake Image Detectors that Generalize Across Generative Models' in 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (IEEE Computer Society, 2023) pp. 24480-24489.https://doi.org/10.1109/cvpr52729.2023.02345. 761. Y. Zhao, T. Pang, C. Du, X. Yang, N.-M. Cheung, M. Lin, A Recipe for Watermarking Diffusion Models, arXiv:2303.10137 [cs.CV] (2023).https://doi.org/10.48550/arXiv.2303.10137. 762. J. Kirchenbauer, J. Geiping, Y. Wen, J. Katz, I. Miers, T. Goldstein, 'A Watermark for Large Language Models' in Proceedings of the 40th International Conference on Machine Learning (PMLR, 2023) pp. 17061-17084.https://proceedings.mlr.press/v202/kirchenbauer23a.html. 763. A. Knott, D. Pedreschi, R. Chatila, T. Chakraborti, S. Leavy, R. Baeza-Yates, D. Eyers, A. Trotman, P. D. Teal, P. Biecek, S. Russell, Y. Bengio, GenerativeAImodels should include detection mechanisms as a condition for public release. Ethics Inf. Technol. 25, 55 (2023).https://doi.org/10.1007/s10676-023-09728-4. 764. G. Pang, C. Shen, L. Cao, A. Van Den Hengel, Deep Learning for Anomaly Detection: A Review. ACM Comput. Surv. 54, 38:1-38:38 (2021).https://doi.org/10.1145/3439950. 765. T. Ali, P. Kostakos, HuntGPT: Integrating Machine Learning-Based Anomaly Detection and ExplainableAIwith Large Language Models (LLMs), arXiv:2309.16021 [cs.CR] (2023).https://doi.org/10.48550/arXiv.2309.16021. 766. J. Geng, F. Cai, Y. Wang, H. Koeppl, P. Nakov, I. Gurevych, A Survey of Confidence Estimation and Calibration in Large Language Models, arXiv:2311.08298 [cs.CL] (2023).http://arxiv.org/abs/2311.08298. 767. A. Aldahdooh, W. Hamidouche, S. A. Fezza, O. Deforges, Adversarial example detection for DNN models: a review and experimental comparison. Artif. Intell. Rev. 55, 4403-4462 (2022).https://doi.org/10.1007/s10462-021-10125-w. 768. M. Phute, A. Helbling, M. D. Hull, S. Peng, S. Szyller, C. Cornelius, D. H. Chau, 'LLMSelf Defense: By Self Examination,LLMsKnow They Are Being Tricked' in The Second Tiny Papers Track at ICLR 2024 (2024).https://openreview.net/forum?id=YoqgcIA19o. 769. R. Greenblatt, B. Shlegeris, K. Sachan, F. Roger,AIControl: Improving Safety Despite Intentional Subversion, arXiv:2312.06942 [cs.LG] (2023).https://doi.org/10.48550/arXiv.2312.06942. 770. T. Ploug, S. Holm, 'Right to ContestAIDiagnostics Defining Transparency and Explainability Requirements from a Patient's Perspective' in Artificial Intelligence in Medicine (Springer Publishing Company, 2022), pp. 227-238. 771. M. Turpin, J. Michael, E. Perez, S. R. Bowman, 'Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=bzs4uPLXvi. 772.* A. Radhakrishnan, K. Nguyen, A. Chen, C. Chen, C. Denison, D. Hernandez, E. Durmus, E. Hubinger, J. Kernion, K. Lukosiute, N. Cheng, N. Joseph, N. Schiefer, O. Rausch, S. McCandlish, S. El Showk, T. Lanham, T. Maxwell, V. Chandrasekaran, . . . E. Perez, Question Decomposition Improves the Faithfulness of Model-Generated Reasoning, arXiv:2307.11768 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2307.11768. 773.* J. Chua, E. Rees, H. Batra, S. R. Bowman, J. Michael, E. Perez, M. Turpin, Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought, arXiv:2403.05518 [cs.CL] (2024).https://doi.org/10.48550/arXiv.2403.05518. 774. A. Saranya, R. Subhashini, A systematic review of Explainable Artificial Intelligence models and applications: Recent developments and future trends. Decision Analytics Journal 7, 100230 (2023).https://doi.org/10.1016/j.dajour.2023.100230. 775. H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang, D. Yin, M. Du, Explainability for large language models: A survey. ACM Trans. Intell. Syst. Technol. 15, 1-38 (2024).https://doi.org/10.1145/3639372. 776. I. Seeber, E. Bittner, R. O. Briggs, T. de Vreede, G.-J. de Vreede, A. Elkins, R. Maier, A. B. Merz, S. Oeste-Reiss, N. Randrup, G. Schwabe, M. Sollner, Machines as teammates: A research agenda onAIin team collaboration. Inf. Manag. 57, 103174 (2020).https://doi.org/10.1016/j.im.2019.103174. 777.* A. Dafoe, E. Hughes, Y. Bachrach, T. Collins, K. R. McKee, J. Z. Leibo, K. Larson, T. Graepel, Open Problems in CooperativeAI, arXiv:2012.08630 [cs.AI] (2020).https://doi.org/10.48550/arXiv.2012.08630. 778. R. Shah, P. Freire, N. Alex, R. Freedman, D. Krasheninnikov, L. Chan, M. D. Dennis, P. Abbeel, A. Dragan, S. Russell, Benefits of Assistance over Reward Learning. (2020).https://openreview.net/forum?id=DFIoGDZejIB. 779. A. Dafoe, Y. Bachrach, G. Hadfield, E. Horvitz, K. Larson, T. Graepel, CooperativeAI: machines must learn to find common ground. Nature 593, 33-36 (2021).https://doi.org/10.1038/d41586-021-01170-0. 780. X. Wu, L. Xiao, Y. Sun, J. Zhang, T. Ma, L. He, A survey of human-in-the-loop for machine learning. Future Gener. Comput. Syst. 135, 364-381 (2022).https://doi.org/10.1016/j.future.2022.05.014. 781. J. Cohen, E. Rosenfeld, Z. Kolter, 'Certified Adversarial Robustness via Randomized Smoothing' in Proceedings of the 36th International Conference on Machine Learning (PMLR, 2019) pp. 1310-1320.https://proceedings.mlr.press/v97/cohen19c.html. 782. N. Carlini, F. Tramer, K. D. Dvijotham, L. Rice, M. Sun, J. Z. Kolter, '(Certified!!) Adversarial Robustness for Free!' in The 11th International Conference on Learning Representations (ICLR 2023) (2022).https://openreview.net/forum?id=JLg5aHHv7j. 783. W. Nie, B. Guo, Y. Huang, C. Xiao, A. Vahdat, A. Anandkumar, 'Diffusion Models for Adversarial Purification' in Proceedings of the 39th International Conference on Machine Learning (PMLR, 2022) pp. 16805-16827.https://proceedings.mlr.press/v162/nie22a.html. 784. A. Kumar, C. Agarwal, S. Srinivas, A. J. Li, S. Feizi, H. Lakkaraju, CertifyingLLMSafety against Adversarial Prompting, arXiv:2309.02705 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2309.02705. 785. A. Zhou, B. Li, H. Wang, 'Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks' in ICLR 2024 Workshop on Secure and Trustworthy Large Language Models (2024).https://openreview.net/forum?id=cSPXIO7min. 786. J. Babcock, J. Kramar, R. V. Yampolskiy, 'Guidelines for Artificial Intelligence Containment' in Next-Generation Ethics: Engineering a Better Society, A. E. Abbas, Ed. (Cambridge University Press, Cambridge, 2019), pp. 90-112. 787. J. Banja, J. W. Gichoya, N. Martinez-Martin, L. A. Waller, G. D. Clifford, Fairness as an afterthought: An American perspective on fairness in model developer-clinician user collaborations. PLOS Digit Health 2, e0000386 (2023).https://doi.org/10.1371/journal.pdig.0000386. 788. N. A. Saxena, K. Huang, E. DeFilippis, G. Radanovic, D. C. Parkes, Y. Liu, 'How Do Fairness Definitions Fare? Examining Public Attitudes Towards Algorithmic Definitions of Fairness' in Proceedings of the 2019 AAAI/ACM Conference onAI, Ethics, and Society (AIES '19) (Association for Computing Machinery, 2019) pp. 99-106.https://doi.org/10.1145/3306618.3314248. 789. W. Fleisher, 'What's Fair about Individual Fairness?' in Proceedings of the 2021 AAAI/ACM Conference onAI, Ethics, and Society (AIES '21) (Association for Computing Machinery, 2021) pp. 480-490.https://doi.org/10.1145/3461702.3462621. 790. N. A. Saxena, 'Perceptions of Fairness' in Proceedings of the 2019 AAAI/ACM Conference onAI, Ethics, and Society (AIES '19) (Association for Computing Machinery, 2019) pp. 537-538.https://doi.org/10.1145/3306618.3314314. 791. R. Binns, 'On the apparent conflict between individual and group fairness' in Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* '20) (Association for Computing Machinery, 2020).https://doi.org/10.1145/3351095.3372864. 792. N. Lee, Y. Bang, H. Lovenia, S. Cahyawijaya, W. Dai, P. Fung, Survey of Social Bias in Vision-Language Models, arXiv:2309.14381 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2309.14381. 793. N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, A. Galstyan, A Survey on Bias and Fairness in Machine Learning. ACM Comput. Surv. 54, 1-35 (2021).https://doi.org/10.1145/3457607. 794. X. Ferrer, T. van Nuenen, J. M. Such, M. Cote, N. Criado, Bias and Discrimination inAI: A Cross-Disciplinary Perspective. IEEE Technol. Soc. Mag. 40, 72-80 (2021).https://doi.org/10.1109/mts.2021.3056293. 795. R. Schwartz, A. Vassilev, K. Greene, L. Perine, A. Burt, P. Hall, 'NIST Special Publication 1270: Towards a standard for identifying and managing bias in artificial intelligence' (National Institute of Standards and Technology (U.S.), 2022);https://doi.org/10.6028/nist.sp.1270. 796. R. Navigli, S. Conia, B. Ross, Biases in Large Language Models: Origins, Inventory, and Discussion. J. Data and Information Quality 15, 1-21 (2023).https://doi.org/10.1145/3597307. 797. Y. Li, M. Du, R. Song, X. Wang, Y. Wang, A Survey on Fairness in Large Language Models, arXiv:2308.10149 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2308.10149. 798. M. Georgopoulos, J. Oldfield, M. A. Nicolaou, Y. Panagakis, M. Pantic, Mitigating Demographic Bias in Facial Datasets with Style-Based Multi-attribute Transfer. Int. J. Comput. Vis. 129, 2288-2307 (2021).https://doi.org/10.1007/s11263-021-01448-w. 799. S. Yucer, S. Akcay, N. Al-Moubayed, T. P. Breckon, 'Exploring racial bias within face recognition via per-subject adversarially-enabled data augmentation' in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (2020).https://doi.org/10.1109/cvprw50498.2020.00017. 800. D. Jin, Z. Jin, Z. Hu, O. Vechtomova, R. Mihalcea, Deep Learning for Text Style Transfer: A Survey. Comput. Linguist. 48, 155-205 (2022).https://doi.org/10.1162/coli_a_00426. 801. A. V. Nadimpalli, A. Rattani, 'GBDF: Gender Balanced DeepFake Dataset Towards Fair DeepFake Detection' in Pattern Recognition, Computer Vision, and Image Processing. ICPR 2022 International Workshops and Challenges (Springer-Verlag, 2023) pp. 320-337.https://doi.org/10.1007/978-3-031-37742-6_25. 802. O. Parraga, M. D. More, C. M. Oliveira, N. S. Gavenski, L. S. Kupssinsku, A. Medronha, L. V. Moura, G. S. Simoes, R. C. Barros, Fairness in Deep Learning: A Survey on Vision and Language Research. ACM Comput. Surv. (2023).https://doi.org/10.1145/3637549. 803. T. Wang, J. Zhao, M. Yatskar, K.-W. Chang, V. Ordonez, 'Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias in Deep Image Representations' in 2019 IEEE/CVF International Conference on Computer Vision (ICCV) (IEEE, 2019) pp. 5309-5318.https://doi.org/10.1109/iccv.2019.00541. 804. C.-Y. Chuang, Y. Mroueh, 'Fair Mixup: Fairness via Interpolation' in The 9th International Conference on Learning Representations (ICLR 2021) (2021).https://openreview.net/forum?id=DNl5s5BXeBn. 805. V. S. Lokhande, A. K. Akash, S. N. Ravi, V. Singh, 'FairALM: Augmented Lagrangian Method for Training Fair Models with Little Regret' in Computer Vision - ECCV 2020 (ECCV 2020) (Springer-Verlag, 2020) pp. 365-381.https://doi.org/10.1007/978-3-030-58610-2_22. 806. M. Kearns, S. Neel, A. Roth, Z. S. Wu, 'Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness' in Proceedings of the 35th International Conference on Machine Learning (PMLR, 2018).https://proceedings.mlr.press/v80/kearns18a.html. 807. N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. D. Laroussilhe, A. Gesmundo, M. Attariyan, S. Gelly, 'Parameter-Efficient Transfer Learning for NLP' in Proceedings of the 36th International Conference on Machine Learning (PMLR, 2019).https://proceedings.mlr.press/v97/houlsby19a.html. 808. Z. Fatemi, C. Xing, W. Liu, C. Xiong, 'Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting' in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (Association for Computational Linguistics, 2023) pp. 1249-1262.https://doi.org/10.18653/v1/2023.acl-short.108. 809. M. Tomalin, B. Byrne, S. Concannon, D. Saunders, S. Ullmann, The practical ethics of bias reduction in machine translation: why domain adaptation is better than data debiasing. Ethics Inf. Technol. 23, 419-433 (2021).https://doi.org/10.1007/s10676-021-09583-1. 810. L. Cheng, A. Mosallanezhad, P. Sheth, H. Liu, 'Causal learning for socially responsibleAI' in Proceedings of the 30th International Joint Conference on Artificial Intelligence IJCAI-21 (International Joint Conferences on Artificial Intelligence Organization, 2021).https://doi.org/10.24963/ijcai.2021/598. 811. A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, L. Campbell-Gillingham, J. Uesato, P.-S. Huang, R. Comanescu, F. Yang, A. See, S. Dathathri, R. Greig, C. Chen, . . . G. Irving, 'Improving alignment of dialogue agents via targeted human judgements' (Google Deepmind, 2022);https://doi.org/10.48550/arXiv.2209.14375. 812. Z. Yang, L. Li, K. Lin, J. Wang, C.-C. Lin, Z. Liu, L. Wang, The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision), arXiv:2309.17421 [cs.CV] (2023).https://doi.org/10.48550/arXiv.2309.17421. 813. Y. Li, C. Zhang, G. Yu, Z. Wang, B. Fu, G. Lin, C. Shen, L. Chen, Y. Wei, StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data, arXiv:2308.10253 [cs.CV] (2023).https://doi.org/10.48550/arXiv.2308.10253. 814. W. Dai, J. Li, D. Li, A. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, S. Hoi, 'InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning' in 37th Conference on Neural Information Processing Systems (NeurIPS 2023) (2023).https://openreview.net/forum?id=vvoWPYqZJA. 815. P. Delobelle, B. Berendt, 'FairDistillation: Mitigating Stereotyping in Language Models' in Machine Learning and Knowledge Discovery in Databases (ECML PKDD 2022) (Springer-Verlag, 2023) pp. 638-654.https://doi.org/10.1007/978-3-031-26390-3_37. 816. K. Wang, H. Machiraju, O.-H. Choung, M. Herzog, P. Frossard, CLAD: A Contrastive Learning based Approach for Background Debiasing, arXiv:2210.02748 [cs.CV] (2022).https://doi.org/10.48550/arXiv.2210.02748. 817. M. Zhang, C. R'e, Contrastive adapters for foundation model group robustness. Adv. Neural Inf. Process. Syst. abs/2207.07180 (2022).https://doi.org/10.48550/arXiv.2207.07180. 818. U. Gupta, J. Dhamala, V. Kumar, A. Verma, Y. Pruksachatkun, S. Krishna, R. Gupta, K.-W. Chang, G. Ver Steeg, A. Galstyan, 'Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal' in Findings of the Association for Computational Linguistics: ACL 2022 (Association for Computational Linguistics, 2022) pp. 658-678.https://doi.org/10.18653/v1/2022.findings-acl.55. 819. S. Park, K. Choi, H. Yu, Y. Ko, 'Never Too Late to Learn: Regularizing Gender Bias in Coreference Resolution' in Proceedings of the 16th ACM International Conference on Web Search and Data Mining (WSDM '23) (Association for Computing Machinery, 2023) pp. 15-23.https://doi.org/10.1145/3539597.3570473. 820.* J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, W. Manassra, P. Dhariwal, C. Chu, Y. Jiao, A. Ramesh, 'Improving image generation with better captions' (OpenAI, 2023). 821.* A. Xiang, Being 'Seen' vs. 'Mis-Seen': Tensions between Privacy and Fairness in Computer Vision. Harvard Journal of Law & Technology 36 (2022).https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4068921. 822. G. Pleiss, M. Raghavan, F. Wu, J. Kleinberg, K. Q. Weinberger, 'On Fairness and Calibration' in 31st Conference on Neural Information Processing Systems (NIPS 2017) (Curran Associates, Inc., 2017) vol. 30.https://papers.nips.cc/paper_files/paper/2017/hash/b8b9c74ac526fffbeb2d39ab038d1cd7-Abstract.html. 823. Z. Shi, Y. Wang, F. Yin, X. Chen, K.-W. Chang, C.-J. Hsieh, Red teaming language model detectors with language models. Trans. Assoc. Comput. Linguist. 12, 174-189 (2024).https://doi.org/10.1162/tacl_a_00639. 824. Y. Liu, K. Zhang, Y. Li, Z. Yan, C. Gao, R. Chen, Z. Yuan, Y. Huang, H. Sun, J. Gao, L. He, L. Sun, Sora: A review on background, technology, limitations, and opportunities of large vision models, arXiv:2402.17177 [cs.CV] (2024).https://doi.org/10.48550/arXiv.2402.17177. 825.* S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, A. Awadallah, Orca: Progressive Learning from Complex Explanation Traces of GPT-4, arXiv:2306.02707 [cs.CL] (2023).https://doi.org/10.48550/arXiv.2306.02707. 826. T. Sorensen, J. Moore, J. Fisher, M. Gordon, N. Mireshghallah, C. M. Rytting, A. Ye, L. Jiang, X. Lu, N. Dziri, T. Althoff, Y. Choi, A Roadmap to Pluralistic Alignment, arXiv:2402.05070 [cs.AI] (2024).http://arxiv.org/abs/2402.05070. 827. M. Shur-Ofry, Multiplicity as anAIGovernance Principle. (2023).https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4444354. 828. A. Chouldechova, Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments. Big Data 5, 153-163 (2017).https://doi.org/10.1089/big.2016.0047. 829. J. Kleinberg, 'Inherent Trade-Offs in Algorithmic Fairness' in Abstracts of the 2018 ACM International Conference on Measurement and Modeling of Computer Systems (SIGMETRICS '18) (Association for Computing Machinery, 2018) pp. 40.https://doi.org/10.1145/3219617.3219634. 830. Q. Zhang, J. Liu, Z. Zhang, J. Wen, B. Mao, X. Yao, Mitigating Unfairness via Evolutionary Multiobjective Ensemble Learning. IEEE Trans. Evol. Comput. 27, 848-862 (2023).https://doi.org/10.1109/tevc.2022.3209544. 831. M. Hardt, E. Price, E. Price, N. Srebro, 'Equality of Opportunity in Supervised Learning' in 30th Conference on Neural Information Processing Systems (NIPS 2016) (Curran Associates, Inc., 2016) vol. 29.https://proceedings.neurips.cc/paper_files/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html. 832. A. Grover, J. Song, A. Kapoor, K. Tran, A. Agarwal, E. J. Horvitz, S. Ermon, 'Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting' in 33rd Conference on Neural Information Processing Systems (NeurIPS 2019) (Curran Associates, Inc., 2019) vol. 32.https://proceedings.neurips.cc/paper/2019/hash/d76d8deea9c19cc9aaf2237d2bf2f785-Abstract.html. 833. S. Arora, A. Risteski, Y. Zhang, 'Do GANs learn the distribution? Some Theory and Empirics' in 6th International Conference on Learning Representations (ICLR) (2018).https://openreview.net/forum?id=BJehNfW0-. 834. D. Zhang, S. Pan, T. Hoang, Z. Xing, M. Staples, X. Xu, L. Yao, Q. Lu, L. Zhu, To be forgotten or to be fair: unveiling fairness implications of machine unlearning methods.AIand Ethics 4, 83-93 (2024).https://doi.org/10.1007/s43681-023-00398-y. 835. H. Nilforoshan, J. D. Gaebler, R. Shroff, S. Goel, 'Causal Conceptions of Fairness and their Consequences' in Proceedings of the 39th International Conference on Machine Learning (ICML 2022) (PMLR, 2022).https://proceedings.mlr.press/v162/nilforoshan22a.html. 836. N. Konstantinov, C. H. Lampert, 'On the Impossibility of Fairness-Aware Learning from Corrupted Data' in Algorithmic Fairness through the Lens of Causality and Robustness workshop (AFCR 2021) (PMLR, 2021).https://proceedings.mlr.press/v171/konstantinov22a.html. 837. M. Brcic, R. V. Yampolskiy, Impossibility Results inAI: A Survey. ACM Comput. Surv. 56, 1-24 (2023).https://doi.org/10.1145/3603371. 838. K. T. Rodolfa, H. Lamba, R. Ghani, Empirical observation of negligible fairness-accuracy trade-offs in machine learning for public policy. Nature Machine Intelligence 3, 896-904 (2021).https://doi.org/10.1038/s42256-021-00396-x. 839. B. Green, Escaping the Impossibility of Fairness: From Formal to Substantive Algorithmic Fairness. Philos. Technol. 35, 90 (2022).https://doi.org/10.1007/s13347-022-00584-6. 840. A. Bell, L. Bynum, N. Drushchak, T. Zakharchenko, L. Rosenblatt, J. Stoyanovich, 'The Possibility of Fairness: Revisiting the Impossibility Theorem in Practice' in Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT '23) (Association for Computing Machinery, 2023) pp. 400-422.https://doi.org/10.1145/3593013.3594007. 841. C. Hertweck, T. Raz, Gradual (In)Compatibility of Fairness Criteria. AAAI 36, 11926-11934 (2022).https://doi.org/10.1609/aaai.v36i11.21450. 842. S. Caton, C. Haas, Fairness in Machine Learning: A Survey. ACM Comput. Surv. 56, 1-38 (2024).https://doi.org/10.1145/3616865. 843. S. Guha, F. A. Khan, J. Stoyanovich, S. Schelter, 'Automated Data Cleaning Can Hurt Fairness in Machine Learning-based Decision Making' in 2023 IEEE 39th International Conference on Data Engineering (ICDE) (IEEE, 2023) pp. 3747-3754.https://doi.org/10.1109/icde55515.2023.00303. 844. B. Ghai, M. N. Hoque, K. Mueller, 'WordBias: An Interactive Visual Tool for Discovering Intersectional Biases Encoded in Word Embeddings' in Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems (CHI EA '21) (Association for Computing Machinery, 2021) pp. 1-7.https://doi.org/10.1145/3411763.3451587. 845. C. Dwork, F. McSherry, K. Nissim, A. Smith, 'Calibrating Noise to Sensitivity in Private Data Analysis' in Theory of Cryptography, S. Halevi, T. Rabin, Eds. (Lecture Notes in Computer Science, Springer, Berlin, Heidelberg, 2006), vol. 3876. 846. M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, L. Zhang, 'Deep Learning with Differential Privacy' in Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (CCS '16) (Association for Computing Machinery, 2016) pp. 308-318.https://doi.org/10.1145/2976749.2978318. 847. H. Brown, K. Lee, F. Mireshghallah, R. Shokri, F. Tramer, 'What Does it Mean for a Language Model to Preserve Privacy?' in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22) (Association for Computing Machinery, 2022) pp. 2280-2292.https://doi.org/10.1145/3531146.3534642. 848. S. De, L. Berrada, J. Hayes, S. L. Smith, B. Balle, 'Unlocking High-Accuracy Differentially Private Image Classification through Scale' (Google Deepmind, 2022);http://arxiv.org/abs/2204.13650. 849. X. Li, F. Tramer, P. Liang, T. Hashimoto, 'Large Language Models Can Be Strong Differentially Private Learners' in International Conference on Learning Representations 2022 (2022).https://openreview.net/forum?id=bVuP3ltATMz. 850. T. Stadler, B. Oprisanu, C. Troncoso, Synthetic data - anonymisation groundhog day. (USENIX Association, 2022), pp. 1451-1468.www.usenix.org/conference/usenixsecurity22/presentation/stadler. 851. M. Meeus, F. Guepin, A.-M. Cretu, Y.-A. de Montjoye, Achilles' Heels: Vulnerable Record Identification in Synthetic Data Publishing. (Springer Nature Switzerland, 2024), pp. 380-399.https://doi.org/10.1007/978-3-031-51476-0_19. 852. G. Ganev, E. De Cristofaro, On the Inadequacy of Similarity-based Privacy Metrics: Reconstruction Attacks against 'Truly Anonymous Synthetic Data', arXiv:2312.05114 [cs.CR] (2023).https://doi.org/10.48550/arXiv.2312.05114. 853. P. Mohassel, Y. Zhang, SecureML: A System for Scalable Privacy-Preserving Machine Learning. (IEEE Computer Society, 2017), pp. 19-38.https://doi.org/10.1109/sp.2017.12. 854. B. McMahan, E. Moore, D. Ramage, S. Hampson, B. A. y. Arcas, Communication-Efficient Learning of Deep Networks from Decentralized Data. (PMLR, 2017), pp. 1273-1282.https://proceedings.mlr.press/v54/mcmahan17a.html. 855. O. Ohrimenko, F. Schuster, C. Fournet, A. Mehta, S. Nowozin, K. Vaswani, M. Costa, Oblivious Multi-Party machine learning on trusted processors. (USENIX Association, 2016), pp. 619-636.https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/ohrimenko. 856. T. Li, E. F. Villaronga, P. Kieseberg, Humans Forget, Machines Remember: Artificial Intelligence and the Right to Be Forgotten. Computer Law & Security Review 34, 304 (2018).https://scholarship.law.bu.edu/faculty_scholarship/817. 857. A. Ghorbani, J. Zou, 'Data Shapley: Equitable Valuation of Data for Machine Learning' in Proceedings of the 36th International Conference on Machine Learning (ICML 2019) (PMLR, 2019) vol. 97, pp. 2242-2251.https://proceedings.mlr.press/v97/ghorbani19c.html. 858. M. ElBaih, The Role of Privacy Regulations inAIDevelopment (A Discussion of the Ways in Which Privacy Regulations Can Shape the Development ofAI). (2023).https://doi.org/10.2139/ssrn.4589207. 859. A. Cavoukian, Privacy by design: The 7 foundational principles. (2009). 860. European Parliament, Artificial Intelligence Act: deal on comprehensive rules for trustworthyAI(2023).www.europarl.europa.eu/news/en/press-room/20231206IPR15699/artificial-intelligence-act-deal-on-comprehensive-rules-for-trustworthy-ai. 861. T. Stadler, B. Kulynych, N. Papernot, M. Gastpar, C. Troncoso, The Fundamental Limits of Least-Privilege Learning. (arXiv, 2024).https://doi.org/10.48550/arXiv.2402.12235. Bengio, Y., Mindermann, S., Privitera, D., Besiroglu, T., Bommasani, R., Casper, S., Choi, Y., Goldfarb, D., Heidari, H., Khalatbari, L., Longpre, S., Mavroudis, V., Mazeika, M., Ng, K. Y., Okolo, C. T., Raji, D., Skeadas, T., Tramer, F., Fox, B., de Leon Ferreira de Carvalho, A. C. P., Nemer, M., Pezoa Rivera, R., Zeng, Y., Heikkila, J., Avrin, G., Kruger, A., Ravindran, B., Riza, H., Seoighe, C., Katzir, Z., Monti, A., Kitano, H., Kerema, M., Lopez Portillo, J. R., Sheikh, H., Jolly, G., Ajala, O., Ligot, D., Lee, K. M., Hatip, A. H., Rugege, C., Albalawi, F., Wong, D., Oliver, N., Busch, C., Molchanovskyi, O., Alserkal, M., Khan, S. M., McLean, A., Gill, A., Adekanmbi, B., Christiano, P., Dalrymple, D., Dietterich, T. G., Felten, E., Fung, P., Gourinchas, P.-O., Jennings, N., Krause, A., Liang, P., Ludermir, T., Marda, V., Margetts, H., McDermid, J. A., Narayanan, A., Nelson, A., Oh, A., Ramchurn, G., Russell, S., Schaake, M., Song, D., Soto, A., Tiedrich, L., Varoquaux, G., Yao, A., & Zhang, Y.-Q. (2024).International Scientific Report on the Safety of AdvancedAI: Interim Report.",2023
govuk_010,govuk,Ai Procurement 0,"A summary of best practice addressing specific challenges of acquiring Artificial Intelligence technologies in government. HTML PDF,11.1 MB,20 pages This file may not be suitable for users of assistive technology. Artificial Intelligence is a technology that has the potential to greatly improve our public services by reducing costs, enhancing quality, and freeing up valuable time of frontline staff. Recognising this, the UK Government published the Data Ethics Framework and A Guide to using AI in the Public Sector to enable public bodies to adopt AI systems in a way that works for everyone in society. These new procurement guidelines will help inform and empower buyers in the public sector, helping them to evaluate suppliers, then confidently and responsibly procure AI technologies for the benefit of citizens.",2023
govuk_022,govuk,Automated Decision-Making 2,"This 7 point framework will help government departments with the safe, sustainable and ethical use of automated or algorithmic decision-making systems. This 7 point framework will help government departments with the safe, sustainable and ethical use of automated or algorithmic decision-making systems. It has been developed in line with guidance from government (such as theData Ethics Framework) and industry, as well as relevant legislation. It supports the priorities of the Central Digital and Data Office, and aligns with wider cross- government strategies in the digital, data and technology space. Departments should use the framework with existing organisational guidance and processes. Read the guidance on Ethics, Transparency and Accountability Framework for Automated Decision-Making",2023
govuk_000,govuk,Ai Ethics 0,"Understand how to use artificial intelligence ethically and safely This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. The Office for Artificial Intelligence (OAI) and the Government Digital Service (GDS) have produced the following chapter's guidance in partnership with The Alan Turing Institute'spublic policy programme. This chapter is a summary ofThe Alan Turing Institute's detailed guidance, and readers should refer to the full guidance when implementing these recommendations. AI has the potential to make a substantial impact for individuals, communities, and society. To make sure the impact of your AI project is positive and does not unintentionally harm those affected by it, you and your team should make considerations of AI ethics and safety a high priority. This section introduces AI ethics and provides a high-level overview of the ethical building blocks needed for the responsible delivery of an AI project. The following guidance is designed to complement and supplement theData Ethics Framework. The Data Ethics Framework is a tool that should be used in any project. This guidance is for everyone involved in the design, production, and deployment of an AI project such as: Ethical considerations will arise at every stage of your AI project. You should use the expertise and active cooperation of all your team members to address them. AI ethics is a set of values, principles, and techniques that employ widely accepted standards to guide moral conduct in the development and use of AI systems. The field of AI ethics emerged from the need to address the individual and societal harms AI systems might cause. These harms rarely arise as a result of a deliberate choice - most AI developers do not want to build biased or discriminatory applications or applications which invade users' privacy. The main ways AI systems can cause involuntary harm are: The field of AI ethics mitigates these harms by providing project teams with the values, principles, and techniques needed to produce ethical, fair, and safe AI applications. The guidance summarised in this chapter and presented at length inThe Alan Turing Institute's further guidance on AI ethics and safetyis as comprehensive as possible. However, not all issues discussed will apply equally to each project using AI. An AI model which filters out spam emails, for example, will present fewer ethical challenges than one which identifies vulnerable children. You and your team should formulate governance procedures and protocols for each project using AI, following a careful evaluation of social and ethical impacts. You should establish ethical building blocks for the responsible delivery of your AI project. This involves building a culture of responsible innovation as well as a governance architecture to bring the values and principles of ethical, fair, and safe AI to life. To build and maintain a culture of responsibility you and your team should prioritise 4 goals as you design, develop, and deploy your AI project. In particular, you should make sure your AI project is: Prioritising these goals will help build a culture of responsible innovation. To make sure they are fully incorporated into your project you should establish a governance architecture consisting of a: You should understand the framework of ethical values which support, underwrite, and motivate the responsible design and use of AI. The Alan Turing Institute calls these 'the SUM Values': These values: You can read further guidance on SUM Values inThe Alan Turing Institute's comprehensive guidance on AI ethics and safety. While the SUM Values can help you consider the ethical permissibility of your AI project, they are not specifically catered to the particularities of designing, developing, and implementing an AI system. AI systems increasingly perform tasks previously done by humans. For example, AI systems can screen CVs as part of a recruitment process. However, unlike human recruiters, you cannot hold an AI system directly responsible or accountable for denying applicants a job. This lack of accountability of the AI system itself creates a need for a set of actionable principles tailored to the design and use of AI systems. The Alan Turing Institute calls these the 'FAST Track Principles': Carefully reviewing the FAST Track Principles helps you: If your AI system processes social or demographic data, you should design it to meet a minimum level of discriminatory non-harm. To do this you should: You should design your AI system to be fully answerable and auditable. To do this you should: The technical sustainability of these systems ultimately depends on their safety, including their accuracy, reliability, security, and robustness. You should make sure designers and users remain aware of: Designers and implementers of AI systems should be able to: To assess these criteria in depth,you should consult The Alan Turing Institute's guidance on AI ethics and safety. The final method to make sure you use AI ethically, fairly, and safely is building a process-based governance framework. The Alan Turing Institute calls it a 'PBG Framework'. Its primary purpose is to integrate the SUM Values and the FAST Track Principles across the implementation of AI within a service. Building a good PBG Framework for your AI project will provide your team with an overview of: You may find it useful toconsider further guidance on allocating responsibility and governance for AI projects.",2023
govuk_027,govuk,Responsible Ai 2,"Case study from EY. Our client, a global biopharmaceutical company, had previously conducted an internal AI audit which helped identify several gaps. The most pressing was the absence of an AI governance framework. The firm understood that plugging this gap would be a vital step in harnessing the opportunities of AI, while successfully identifying and managing AI risks. Our client subsequently developed a comprehensive AI governance framework embracing responsible AI principles like transparency, fairness, and human-centricity. However, the leadership required assurance that it was moving in the right direction and went in search of an independent partner to validate their efforts. EY teams worked with the client on a review of its AI governance programme, to support the business in maintaining its organizational values, preparing a solid foundation for forthcoming EU regulation, as well as building employee, patient, and clinician trust, without impeding innovation. We leveraged our global Responsible AI framework to help the client optimize their approach to AI governance, mitigate AI risks, and protect stakeholders. The global Responsible AI framework is a flexible set of guiding principles connected to risk areas, and practical actions that enable assurance over AI products and overarching governance programmes. A multi-disciplinary team consisting of digital ethicists, IT risk practitioners, data scientists and subject-matter resources harnessed EY's Responsible AI framework to evaluate how well the biopharma's responsible AI principles had been rolled out and understood across the business. The policies and procedures outlining how AI systems and underlying data are secured from unauthorised access, corruption or adversarial attack were evaluated as part of this audit. Transparency and explainability figured heavily into this audit, given the use of AI products and services in the context of health and medical care impacting patients and clinicians. Understanding how the organisation was assessing and subsequently demonstrate transparency and explainability in a consistent and reproducible manner across AI projects was key to our findings and subsequent recommendations. The makeup of development teams and the logic underlying the creation, procurement, and/or utilisation of AI products and services throughout the organisation and across geographies were considered to gauge the potential introduction of bias. Our detailed review helped the biopharma appreciate the need for major changes in its approach to AI governance, which have subsequently been translated into a roadmap to guide the future of the organisation's investment and strategy regarding AI governance and risk management. We adopted a principles-focused approach in this audit to gauge the gap between their aspirational ethical behaviour and the day-to-day realities of how aspirations had been practically operationalised. In adopting a consultative audit style focused on the decision and documentation processes surrounding the AI lifecycle, we were able to widen the set of stakeholders with which we engaged, as opposed to limiting our interviews to technical staff. The practice of putting technical, commercial, and risk-oriented stakeholders in conversation with one another across from our own multi-disciplinary team enabled a holistic understanding of the biopharma's approach to AI governance and the nature of the challenges that they faced in implementing said governance. Recommendations resulting from our review added value for the organisation by informing the creation of their future AI governance roadmap. Our detailed review helped the biopharma appreciate the need for major changes to its AI governance methodology, including the introduction of an improved third-party AI risk assessment and a new central AI inventory. ""The EY audit highlighted a number of gaps in our approach, allowing us to set minimum requirements for business teams working with AI, which we're already working toward,"" says the biopharma company's AI Governance Lead. Due to the qualitative nature of the audit, stakeholders must be engaged and forthcoming during interviews in order for audit teams to make an informed judgement regarding governance gaps and subsequent recommendations. We utilised EY Responsible AI Framework, which was developed from various AI principles and areas for AI evaluation from academia and various sectors available at the time, to identify and assess areas relevant for AI Assurance. However, as the engagement was delivered before foundation models and current AI standards exist, we did not consider specific universal test metrics, generative AI risks, minimum documentation standards, or other areas of evaluation from AI risk management standards that are available today (i.e. ISO, NIST RMF, etc.). Case study:How a global biopharma became a leader in ethical AI | EY UK EY's responsible AI framework:How do you teach ai the value of trust",2023
govuk_014,govuk,Ai Procurement 4,"Published 25 March 2024 (c) Crown copyright 2024 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visitnationalarchives.gov.uk/doc/open-government-licence/version/3or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:psi@nationalarchives.gov.uk. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned. This publication is available at https://www.gov.uk/government/publications/ppn-0224-improving-transparency-of-ai-use-in-procurement/ppn-0224-improving-transparency-of-ai-use-in-procurement-html 1. AI systems, tools and products are part of a rapidly growing and evolving market, and as such, there may be increased risks associated with their adoption. Care should be taken to ensure that AI is used appropriately, and with due regard to risks and opportunities. As the Government increases its adoption of AI, it is essential to take steps to identify and manage associated risks and opportunities, as part of the Government's commercial activities. 2. The contents of this Procurement Policy Note (PPN) apply to all Central Government Departments, their Executive Agencies and Non-Departmental Public Bodies, and are referred to in this PPN as 'In-Scope Organisations'. 3. Please circulate this PPN within your organisation, particularly to those with a commercial, procurement and/or contract management role. 4. Other public sector contracting authorities may wish to apply the approach set out in this PPN. 5. In-scope organisations should note the provisions of this PPN. 6. There is a range of guidance available to support commercial teams to understand AI as a subject area, the appropriate use of AI within public services and how AI products should be procured. References to some of this guidance can be found in Annex A. 7. There are potential benefits to suppliers using AI to develop their bids, enabling them to bid for a greater number of public contracts. It is important to note that suppliers' use of AI is not prohibited during the commercial process but steps should be taken to understand the risks associated with the use of AI tools in this context, as would be the case if a bid writer has been used by the bidder. This may include: 8. In certain procurements where there are national security concerns in relation to use of AI by suppliers, there may be additional considerations and risk mitigations that are required. In such instances, commercial teams should engage with their Information Assurance and Security colleagues, before launching the procurement, to ensure proportionate risk mitigations are implemented. 9. Commercial teams should take note of existing guidance[footnote 1]when purchasing AI services, however they should also be aware that AI and Machine Learning is becoming increasingly prevalent in the delivery of ""non-AI"" services. Where AI is likely to be used in the delivery of a service, commercial teams may wish to require suppliers to declare this, and provide further details (see Example Disclosure Question 3). This will enable commercial teams to consider any additional due diligence or contractual amendments to manage the impact of AI as part of the service delivery. A Contracting Authority is seeking to implement a new video conferencing suite in their offices. Key considerations for the functionality of this service may include the number of concurrent users, audio and visual call quality, integration with existing software and services. In such instances, a supplier may be able to provide a service that includes meeting transcription, or live translation using Generative AI. Additional consideration should therefore be given to how these meeting records will be used (e.g. for training of further AI models) and whether the information gathered is subject to Data Classification policies. In this example, departments will likely want to ensure that proposed contractual terms make it clear that data retrieved from the videoconference is appropriately managed, e.g. not used for training purposes unless this has been specifically agreed and approved in writing by the client. 10. Generative AI is a form of Artificial Intelligence (AI) - a broad field which aims to use computers to emulate the products of human intelligence - or to build capabilities which go beyond human intelligence. Unlike previous forms of AI, Generative AI produces new content, such as images, text or music. 11. AI systems, tools and products are part of a rapidly growing and evolving market, and as such, there may be increased risks associated with their adoption. Care should be taken to ensure that the use of AI is restricted to use cases where risks can be effectively understood and managed. 12. AI has potential to accelerate and support decision making processes, especially through the use of large data sets. It is essential to ensure that decisions are made with the support of AI systems, not a reliance upon them, in accordance with the principles outlined in the Government'sData Ethics Frameworkand guidance onUnderstanding Artificial Intelligence Ethics and Safety. 13. Content created with the support of Large Language Models (LLMs) may include inaccurate or misleading statements; where statements, facts or references appear plausible, but are in fact false. LLMs are trained to predict a ""statistically plausible"" string of text, however statistical plausibility does not necessarily mean that the statements are factually accurate. As LLMs do not have a contextual understanding of the question they are being asked, or the answer they are proposing, they are unable to identify or correct any errors they make in their response. Care must be taken both in the use of LLMs, and in assessing returns that have used LLMs, in the form of additional due diligence. Enquiries about this PPN should be directed to the Crown Commercial Service Helpdesk (telephone 0345 410 2222, emailinfo@crowncommercial.gov.uk). Government Digital Service (GDS) and the Office for Artificial Intelligence (OAI) have published jointguidance[footnote 2]for anyone responsible for choosing technology in a public sector organisation, and will help you determine if the use of AI is appropriate for your specific use case. Cabinet Office, Office for Artificial Intelligence (OAI) and the World Economic Forum have published jointguidance[footnote 3]to provide a set of guiding principles on how to buy AI technology, as well as insights on tackling challenges that may arise during procurement. Government Digital Service (GDS) and the Office for Artificial Intelligence (OAI) have published jointguidance[footnote 4]on how to build and use AI in the public sector. This guidance covers how: You can contactai-guide@digital.cabinet-office.gov.ukfor further information about using AI in the public sector. Generative Artificial Intelligence (Gen AI) is a subset of AI that focuses on creating new data. Unlike traditional machine learning models that are designed for specific tasks, Gen AI models are capable of generating new content, such as images, text, music, and more. CDDO has developed internalguidance[footnote 5]for the use of AI and Large Language Models (LLMs) across the Civil Service. Central Digital and Data Office (CDDO) have published further, more detailedguidance[footnote 6]on Generative AI. The Generative AI Framework for HMG is guidance on using Generative AI safely and securely for civil servants and people working in government organisations. The white paperA pro-innovation approach to AI regulation, sets out five principles to guide and inform AI development in all sectors. This framework builds on those principles to create ten core principles for generative AI use in government and public sector organisations. Office for Artificial Intelligence (OAI) publishedguidance[footnote 7]that provided advice on understanding how to manage a project which uses artificial intelligence. The Equality and Human Rights Commission publishedguidance[footnote 8]on procuring, commissioning, building or adapting AI for the workplace or the services they provide. Guidance[footnote 9]for public sector organisations on how to use data appropriately and responsibly when planning, implementing, and evaluating a new policy or service. The Office for Artificial Intelligence (OAI) and the Government Digital Service (GDS) have producedGuidance[footnote 10]in partnership with The Alan Turing Institute. This guidance will help to ensure your project is fair and prevent bias or discrimination and to safeguard public trust in your project's capacity to deliver safe and reliable AI through the use of the 'FAST Track Principles'; Fairness, Accountability, Sustainability and Transparency. There are a range of training resources related to AI available through the Government Campus and Civil Service Learning. Government Campus: Artificial Intelligence[footnote 11] Civil Service Learning: Artificial Intelligence[footnote 12] In order to establish whether AI has been used by suppliers to develop responses to questions in the procurement, a disclosure question can be added to the Invitation to Tender. This requires suppliers to disclose their use of AI when responding to the tender questions, or as part of their proposed delivery of the service. There are a series of example disclosure questions set-out below which may be helpful. The Questions included in this annex should not be scored or taken into account in tender evaluation, and should be used for information only. Contracting authorities can however continue to ask and evaluate any further relevant questions about use of AI as part of their award process which are specific to their requirements and are compliant with procurement law. Whether, and if so how, any questions are to be scored should be set out in the procurement documents. These questions may be helpful in informing wider commercial strategy, defining how to undertake meaningful due diligence and manage risk. Note: It is important that contracting authorities do not discriminate against particular suppliers in the use of these questions or in the interpretation of supplier responses. AI tools can be used to improve the efficiency of your bid writing process, however they may also introduce an increased risk of misleading statements via 'hallucination'. Have you used AI or machine learning tools, including large language models, to assist in any part of your tender submission? This may include using these tools to support the drafting of responses to Award questions. Yes/No Please provide details: .................. Where AI tools have been used to support the generation of Tender responses, please confirm that they have been checked and verified for accuracy: Yes/No AI tools can be used to improve the efficiency of your bid writing process, however they may also introduce an increased risk of misleading statements via 'hallucination'. Please detail any instances where AI or machine learning tools, including large language models have been used to generate written content, or support your bid submission. Please provide details: .................. Where AI tools have been used to support the generation of Tender responses, please confirm that they have been checked and verified for accuracy: Yes/No Are AI or machine learning technologies used as part of the products/services you intend to provide to [Insert Contracting Authority Name]? Yes/No If Yes: Please describe how AI technologies are integrated into your service offerings. Please provide details: .................. Guidelines for AI Procurement- Assessing if artificial intelligence is the right solution- Guidelines for AI Procurement- A guide to using artificial intelligence in the public sector- Guidance to civil servants on use of generative AI- Generative AI Framework for HMG- Managing your artificial intelligence project- Artificial intelligence in public services- Data Ethics Framework- Understanding AI Ethics and Safety- Government Campus AI Courses- Civil Service Learning AI Courses-",2023
govuk_016,govuk,Algorithm Transparency 1,"Updated 23 September 2022 (c) Crown copyright 2022 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visitnationalarchives.gov.uk/doc/open-government-licence/version/3or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:psi@nationalarchives.gov.uk. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned. This publication is available at https://www.gov.uk/government/publications/findings-from-the-drcf-algorithmic-processing-workstream-spring-2022/the-benefits-and-harms-of-algorithms-a-shared-perspective-from-the-four-digital-regulators Note: This discussion paper is intended to foster debate and discussion among our stakeholders. It should not be taken as an indication of current or future policy by any of the member regulators of the Digital Regulation Cooperation Forum (DRCF). Every day, we use a wide variety of automated systems that collect and process data. Such ""algorithmic processing"" is ubiquitous and often beneficial, underpinning many of the products and services we use in everyday life. From detecting fraudulent activity in financial services to connecting us with friends online or translating languages at the click of a button, these systems have become a core part of modern society. However, algorithmic systems, particularly modern Machine Learning (ML) approaches, pose significant risks if deployed and managed without due care. They can amplify harmful biases that lead to discriminatory decisions or unfair outcomes that reinforce inequalities. They can be used to mislead consumers and distort competition. Further, the opaque and complex nature by which they collect and process large volumes of personal data can put people's privacy rights in jeopardy. It is important for regulators to understand and articulate the nature and severity of these risks. In doing so, they can help empower businesses to develop and deploy algorithmic processing systems in safe and responsible ways that are pro-innovation and pro-consumer. When it comes to addressing these risks, regulators have a variety of options available, such as producing instructive guidance, undertaking enforcement activity and, where necessary, issuing financial penalties for unlawful conduct and mandating new practices. Over the past year, the Digital Regulation Co-operation Forum (DRCF) has enabled our 4 regulatory bodies (CMA, FCA, ICO and Ofcom) to collaborate in defining common areas of interest and concern. From this foundation, we can act more effectively in this space, identifying potential initiatives for individual regulators while recognising areas where joint initiatives and collaboration may have significantly more impact than individual interventions. This paper is one of 2 initial publications by theDRCF on algorithmic processing[footnote 1]. In this paper we set out 6 common areas of focus among the DRCF members: transparency, fairness, access to information, resilience of infrastructure, individual autonomy and healthy competition. These areas were developed by DRCF members in conjunction with stakeholders from academia, civil society, government, industry, public sector and consumer groups. We then outline the current and potential harms and some of the current and future benefits of algorithmic processing that relate to our focus areas. Finally, we explore possible roles for UK regulators, the DRCF in particular, and outline suggestions for future work. The key takeaways from this paper are: As the DRCF continues to evolve, there are opportunities for members to co-ordinate and collaborate in a manner that would enable greater impact than individual regulatory action. These could include: Through the process of researching and writing these papers, we have developed a better mutual understanding of members' capabilities, remits and powers. This includes perceived areas of tension, such as those that between pro-privacy and pro-competition activities. We believe that through continued collaboration and co-ordination we can continue to resolve some of these tensions and have greater positive impact than acting solely as individual regulatory bodies.[footnote 3] In the next financial year, we intend to undertake further activity in the field of algorithmic processing. We are now launching a call for input alongside the publication of these 2 papers to inform the future work of the DRCF, and we welcome and encourage all interested parties to engage with us in helping shape our agenda. This discussion paper examines the benefits and harms posed by algorithmic processing. Here, we understand algorithmic processing as the processing of data (both personal and non-personal) by automated systems. This includes artificial intelligence (AI) applications, such as those powered by machine learning (ML) techniques, but also simpler statistical models. Our interest covers the processing of data, as well as the context in which that processing occurs, such as the means used to collect and store that data, and the ways humans interact with the results of any processing. We are also interested in both the positive and negative impacts on individuals and society that algorithms cause, as well as how different algorithmic systems interact with each other. Algorithmic processing can be used both to produce an output (for example video or text content) and to make or inform decisions that have a direct bearing on individuals. It is already being woven into many digital products and services, resulting in efficiency gains across the public and private sectors. It can and does enable innovation and can unlock significant benefits for individuals, consumers, businesses, public services, and society at large. Examples of the benefits it provides include: However, algorithmic processing can also be a source of harm if not managed responsibly. It may, for example, produce biased outputs/predictions, leading some groups to be treated less favourably than others (for example algorithms used in CV screening software have the potential to unfairly discriminate against job applicants of one gender over another if not deployed or managed with due care).[footnote 4]Algorithmic processing could also lead to society-wide harms (for example by promoting disinformation through social media recommender systems). Algorithmic harms can emerge as an outcome of use, as in the case of the above examples, or through the development of these systems (for example on account of the energy costs of training an AI model[footnote 5]). These examples represent a small fraction of harms that can be caused. Algorithmic processing can pose significant risks for several reasons. It can be used: The CMA, FCA, ICO and Ofcom collectively believe that we can, and should, play a role in identifying and mitigating these risks within the industries we regulate. In terms of algorithmic processing, for example, the data protection legislation the ICO oversees includes provisions that restrict the circumstances in which organisations can make solely automated decisions that have legal or significant effects on individuals. While the remits and powers of members vary[footnote 6], between us we have the ability to produce advice and guidance, set standards in the form of codes of practice, and commend responsible behavior. Our independence means we can provide robust oversight, scrutinising both the public and private sectors. The DRCF is able to provide a coordinated regulatory approach to algorithmic processing. Collaboration is particularly important for addressing issues that cut across our regulatory remits, such as the use of personal data for real-time bidding in the advertising industry, and financial scams on social media.[footnote 7]The DRCF is the first forum in the world where 4 regulators, representing a range of perspectives, can pool insights and expertise on algorithmic processing, as well as conduct joint initiatives on topics of common interest. Working together will allow us to develop consistent messaging, and provide regulatory clarity for those using algorithmic systems. This discussion paper provides an initial assessment of the benefits and harms that can arise from the use of algorithmic processing in the delivery of digital services. Our goal is to better understand how algorithmic processing takes place to help organisations achieve the benefits without causing the harms, laying the groundwork for future action in DRCF's 2022 to 23 workplan. The paper covers the following topics: While we look here at the harms and benefits associated with all types of algorithmic processing, much of the research and stakeholder comments we cite relate specifically to the use of machine learning (ML) algorithms. This reflects the fact that ML-trained algorithms pose novel and sometimes more significant risks, which are only beginning to be understood. For example, they can surface, reward and amplify underlying harmful patterns that did not scale in the past. It is important, however, not to discount the impact of algorithmic systems built using conventional statistical methods, for example the Ofqual algorithm used to decide A-level grades for students in 2020.[footnote 8]In addition, this discussion paper will largely, although not exclusively, focus on the direct harms and benefits caused by the use and development of algorithms as they relate to our regulatory remits. This discussion paper is being published at a time of growing interest - both domestically and internationally - in the effects of algorithmic processing, particularly that powered by AI and ML methods. In 2021 the UK government published a National AI Strategy[footnote 9], setting out its ambition to position the UK as an AI 'superpower'. Among its commitments were to launch a National AI Research and Innovation Programme, an AI Standards Hub, and a new visa regime to attract AI talent to the UK. The government is expected to follow up with a separate AI White Paper later this year that sets out a national position for governing AI, as well as a National AI Strategy for Health and Social Care and a Defence AI Strategy. The government has also put forward a series of proposals to amend the UK data protection framework, including aspects that relate to AI.[footnote 10] Other governments around the world have issued similar policy blueprints, including France, Germany, Canada, the US and China. So too have international and supranational bodies, among them the Ad Hoc Committee on AI at the Council of Europe (CAHAI), which is working on a legal framework for the development, design and application of AI. The European Commission, meanwhile, has proposed its own Artificial Intelligence Act, which - as presently conceived - would introduce new rules such as mandatory ""conformity assessments"" for high-risk applications of algorithmic processing.[footnote 11] Regulators at home and overseas have also begun to examine the impact of algorithmic processing, with some issuing new directives to support the responsible use of algorithms in their sectors: The rest of this paper should be read with this wider context in mind. Indeed, there is much that DRCF members can learn from the research and experiences of other regulators and policymakers, particularly where they have successfully addressed the harms we outline in the following pages. However, this paper is unique in that it captures perspectives from 4 different digital regulators. The production of this discussion paper involved 3 stages. As digital regulation is a complex landscape that cuts across the remit of various regulators, it is important to provide clarity on the priorities that will guide the DRCF's work in this area. To do this the DRCF convened a series of internal workshops which led us to identify the following high-level regulatory priorities: In the second stage, DRCF members grouped a range of algorithmic harms and benefits into several shared areas of focus that are of mutual interest. The identification of these areas helped us to consider if and where a co-regulatory, collaborative approach may be effective in mitigating the potential harms arising from algorithmic processing. It should be noted that we see accountability as an overarching concept that is a key motivator for DRCF members. It was not specifically identified as a shared focus area since it is fundamental to all the areas, particularly transparency. The main areas of focus are: The relationship between the working group priorities and shared areas of focus is shown in the chart on the following page. Each area is explained in detail inCurrent and Potential Harms of Algorithmic Processing In the third stage, DRCF members produced a summary report to share with stakeholders from academia, civil society, government, industry, public sector and consumer groups, as well as a list of questions for their consideration. Stakeholders provided feedback in a series of bilateral engagements with DRCF members. This discussion paper reflects the DRCF's foundational thinking in the first 2 stages and the feedback we received from the list of stakeholders in the third stage. Stakeholders generally agreed that the working group priorities and 6 shared areas of focus were important. In addition, several other potential shared areas of focus were suggested which are discussed later in the paper. Working Group Priorities and Shared areas of focus in algorithmic processing systems In this section we explain each of the shared areas and their importance. We then give examples of harms that can occur within each of these areas, however we acknowledge that some of these harms can affect more than one area. Transparency refers to the act of providing information about how and where algorithmic processing takes place. This information could relate to the technical features of the algorithm, including the data used to train it and the type of outputs it generates. Or the information could relate to the wider context in which the algorithm is deployed, such as the protocols and procedures that govern its use, whether it is overseen by a human operator, and whether there are any mechanisms through which people can seek redress. Transparency serves a number of purposes: The areas in which algorithmic processing should be transparent, include: Algorithmic processing often involves multiple parties, each playing a different role in the journey from the creation of an algorithm through to its deployment. One party may collect data, another may label and clean it, and another still may use it to train an algorithm. There is concern that the number of players involved in algorithmic supply chains is leading to confusion over who is accountable for their proper development and use. A study looking at business-to-business AI services, for example, found that the roles of data ""processor"" and ""controller"" as expressed in data protection legislation are not always clearly identified, meaning those building, selling and using algorithms may not be fulfilling their obligations under the UK GDPR.[footnote 31]This confusion also means that citizens and consumers are left unsure of where they should turn for support in cases where they feel algorithmic processing is being misused. Stakeholders were particularly concerned about the potential for confusion where organisations purchase algorithms ""off the shelf"", and stressed that developers and deployers must be clear on their responsibilities at the point of procurement. Vendors of algorithmic systems should also inform customers of the limitations and risks associated with their products. While there is often a lack of transparency regarding who is accountable for the outcomes of algorithmic processing, on occasion there is also a lack of transparency about the very use of those algorithms. So-called ""invisible processing""[footnote 32]describes circumstances where personal data is obtained and processed without the direct participation or knowledge of individuals. Indeed, there are many reported cases of personal data being collected for one purpose but then being used for another. The ICO, for example, has taken enforcement action against a number of credit reference agencies, which were processing customer data for purposes that were beyond those originally agreed, including to build marketing products that help commercial firms predict people's ability to afford different goods and services.[footnote 33]The ICO has also identified invisible processing in the advertising technology (adtech) ecosystem, where personal data (including behavioural data) has been used to build intricate profiles of internet users, often without their knowledge.[footnote 34] While data subjects may technically provide consent for the reuse of their data, they may not always understand what this means in practice (hence it is not informed consent). As well as being potentially unfair, this lack of transparency makes it more difficult for individuals to exercise their rights in relation to the processing of their personal data. Under UK GDPR, this includes the right to rectify any errors in personal data, the right for personal data to be erased (also known as the right to be ""forgotten""), the right to obtain and reuse personal data for their own purposes, and the right to object to the processing of data under certain circumstances. Without knowing that an algorithm is processing their personal data, individuals are also unable to seek redress for any harms that may have occurred as a result of that processing. They may not even be aware that they are being harmed. For example, those who face unlawful discrimination by a pricing algorithm may not realise they are paying a higher price than someone with similar circumstances in a different demographic group (for example a customer of one ethnicity paying more than a customer of another). Even when individuals are aware that their personal data is being processed, and have made a decision to raise a complaint, they may not know where to turn to begin this process. While the ICO allows people to raise concerns about how an organisation is using their data, the stakeholders we spoke with suggested that public awareness of this option was low. Some stakeholders also believed that stronger measures of redress were required, such as introducing an easier route for people to seek financial compensation where their data protection rights have been infringed, in a manner akin to the small claims court system. Others we spoke with emphasised the need to lower the cost to civil society actors and private individuals of bringing legal action against those misusing algorithmic systems. In general, it is important that regulators work together to simplify the process of raising complaints, enabling people to seek redress without having to navigate separate regulatory systems. In some cases, it is not enough simply to know that an algorithm is present and is processing data. It may also be important to understand how that algorithm has arrived at a particular decision or output (for example to know why someone has received a poor credit score, or why a photo posted on social media has been flagged as inappropriate). Indeed, the ability of individuals to have access to the 'logic' of a system is a requirement under UK data protection law for solely automated decisions that significantly affect them (with certain exceptions). However, the complexity and dynamic nature of some algorithmic systems - particularly those developed using machine learning techniques - can make it difficult to acquire an explanation. By definition, machine learning algorithms are not programmed by human hand but rather learn from data, which can result in models that are difficult to interrogate. This in turn makes it harder for individuals and consumers to understand why an algorithm has made the recommendation or decision it has, and therefore what they should do differently in future to achieve a different result. It also makes it more difficult for those interpreting the results of an algorithm - for example a social media content moderator - to properly act on its outputs, which in turn undermines the quality of decision-making. This is especially the case where operators lack technical expertise. Two of the academic stakeholders we spoke with stressed the importance of ""justifiability"" in the context of an explanation - the idea that those on the receiving end of an algorithmic decision or output should understand the rationale behind that decision, as well as why it was appropriate to use an algorithm in that context. A lack of transparency regarding where and how algorithmic systems operate can also lead to harmful behaviour in a population. Members of the public may not know, for example, that what they are seeing and reading online is in fact produced by, or being recommended by, an algorithmic system, leading them to be less discerning about that content than they should be. One example is the use of algorithms by so-called troll farms to produce fake social media posts during elections - posts which are then shared between real users, facilitating the spread of disinformation. Another example is the use of algorithms to facilitate high-speed trading, which can result in ""herding"" behaviour where individual traders unknowingly mimic the actions of automated trading tools. This can in turn lead to erratic and unstable movements in financial markets. Another issue falling under the banner of algorithmic transparency is the production of ""synthetic media"". Synthetic media describes audio and visual content that either replicates the behaviours and characteristics of real people, or which alters how real people and environments are presented.[footnote 35]This type of content has long been used in the entertainment industry, including to enhance films in post-production, however there are growing concerns that it is now being used to deliberately mislead the public, who are unaware that the content is fabricated by an algorithmic system. A number of stakeholders flagged the risks posed by ""deepfake"" videos on social media, which falsely portray individuals as doing and saying things that are embarrassing, offensive, or in some other way inappropriate. As well as damaging personal reputations[footnote 36], synthetic media also risks undermining user trust in online content of all kinds, making it more difficult for the public to distinguish what is true from what is false.[footnote 37]This in turn could undermine democratic institutions, including news outlets and criminal and civil courts that rely on audio, visual and text-based media as evidence.[footnote 38] For algorithmic systems to win the trust of consumers and citizens, they need to be shown as operating fairly. To some, fairness means that people experience the same outcomes, while to others it means that people are treated in the same way, even if that results in different outcomes for different groups. What counts as ""fair"" in the context of algorithmic processing varies from context to context, and can even vary within a single industry.[footnote 39]However, fairness is not just a subjective ethical value, it is also a legal requirement. The UK GDPR for example mandates that organisations only process personal data fairly and in a transparent manner. Separately, the Equality Act prohibits organisations from discriminating against people on the basis of protected characteristics, including in cases where they are subject to algorithmic processing. The Consumer Rights Act, meanwhile, includes a ""fairness test"", whereby a contract term will be unfair if ""contrary to the requirement of good faith, it causes a significant imbalance in the parties' rights and obligations to the detriment of the consumer"". This applies to contracts between traders and consumers, including those which involve algorithmic processing. With a small number of exceptions, most observers agree it is unfair to discriminate against people on the basis of sensitive characteristics, such as their socio-economic status or accent.[footnote 40]Indeed, discrimination on the basis of protected characteristics (for example age, sexual orientation or race) is prohibited in specific contexts, such as employment or education, under the Equality Act.[footnote 41]It is therefore concerning that a number of algorithmic systems have been shown to produce biased or discriminatory results, from facial recognition technology that is better at recognising male and white faces,[footnote 42]to recruitment screening software that penalises job applications from female candidates.[footnote 43]Researchers have differentiated between 2 main categories of harm caused by biased algorithms: allocative and representational.[footnote 44]Allocative harms are those where particular groups are denied access to important goods and services. Representational harms occur when systems reinforce the subordination of groups through stereotyping, under-representation, and denigration. Few of those who build and use algorithms deliberately set out to unfairly discriminate against people. However, there are many ways that bias can be inadvertently embedded within algorithms. One of these is by using training data that reflects historical bias. For example, if a predictive policing model is trained on the arrest data of police forces that have historically discriminated against black residents, that model is likely to reproduce those same biases in its patrol recommendations. These historical biases can also result in feedback loops, with biased models leading to biased outcomes, which are subsequently fed back into model training exercises. Other sources of bias include model optimisation, human interpretation, and even how a problem has been framed.[footnote 45]It is important to emphasise that those deploying algorithms do not need to intend to discriminate for their conduct to be unlawful. Those building and deploying algorithms often try to address bias by removing information about sensitive characteristics from their data (a technique known as ""fairness through unawareness""). However, other information can act as a proxy for sensitive or protected characteristics, such as postcode acting as a proxy for ethnicity because of the correlation between those 2 variables. That means that depending on the context, simply removing sensitive or protected characteristics may not be the solution. These proxies as correlations are conceptually different to proxies intentionally used in model design when what you want to measure is not observable. For instance, in order to measure individuals' risk of re-offending (unobserved quality), developers building recidivism models often use a score based on past arrests as a proxy since that has been recorded. The validity-reliability of these proxies for unobserved information in model design can affect the fairness of the outcome. A good example of how this can play out unexpectedly comes from the US healthcare system, where an algorithm used to refer patients to specialist healthcare programmes was recently found to systematically discriminate against black people.[footnote 46]The researchers investigating the algorithm found that healthcare costs accrued in a year were being used as a proxy for patient risk scores that would inform referral decisions. However, because healthcare costs were on average lower for black people than for white people with the same chronic conditions, black patients were less likely to be referred to specialist care than white patients, despite having the same support needs. In some situations, however, there may be an operational need to use data points that happen to also correlate with sensitive attributes when building and running an algorithm. To take another example from the insurance industry, car engine size is known to be correlated with gender, yet it is also a material factor in determining the premiums of customers, given that larger engines are more costly to replace and that they result in more serious incidents.[footnote 47]Organisations may benefit from regulatory guidance to understand what counts as a legitimate use of proxy data, particularly in circumstances where they are under an obligation to treat their stakeholders fairly. This is the case, for instance, in the financial services industry, where the FCA has asked firms to demonstrate that ""fair treatment of customers is at the heart of their business model"".[footnote 48] In addition to these issues of demographic discrimination, several stakeholders highlighted how algorithmic processing could be used to discriminate against people on the basis of their purchasing power or willingness to pay. ""Price personalisation"" is not a new activity; many types of business have long attempted to set prices according to what individual customers are willing and able to pay, from cars to holidays, to household goods. However, algorithmic processing could amplify the ability of sellers to predict what people are willing to pay, drawing on data about those individuals which has not previously been available. In theory, this type of practice could be described as fair, since it may lead to lower income customers paying less and higher income customers paying more, possibly resulting in more people being able to access those goods and services. This practice may not be perceived as fair across the board, however. Additionally, others may view this practice as inherently unfair regardless of the outcome, as it would mean sellers are scrutinising the behaviour and characteristics of buyers without their knowledge.[footnote 49] Another reason personalisation might be considered unfair is because it could result in people being penalised for circumstances outside of their control. People living in less affluent areas, for example, may be more likely to be the victim of a burglary, and therefore could face higher premiums for their home insurance - a pricing practice that one of our stakeholders described as a ""poverty premium"".[footnote 50]This example also highlights the difficulty of defining fairness, as it could also be argued that the practice is fair with regards to the insurer in terms of increased premium for increased risk. Less affluent and more vulnerable consumers may also be unaware that some businesses engage in this type of pricing strategy, leaving them more exposed to its effects. Indeed, qualitative research undertaken by Ofcom in 2020 found that participants had very limited awareness and knowledge of personalised pricing, which is consistent with findings in the wider literature, that many consumers are surprised that their online behaviour might influence the prices they are offered for products and services.[footnote 51]The study also found that, with the exception of lower prices for low-income households, consumers were sceptical of the benefits of personalisation, with some saying that the practice was ""disempowering"". By its nature personalised pricing is difficult to spot, and the extent and nature of this practice outside insurance and credit markets is not clearly understood. Another practice that can sometimes result in unfair outcomes is the repurposing of algorithms. While some organisations are able to develop bespoke models that are attuned to their specific needs, others must rely on ""off the shelf"" systems purchased from third party vendors, which may have been trained in a very different context. These models can suffer from lower accuracy levels, and may harm individuals whose data is being analysed.[footnote 52]For example, an algorithm that has been developed to identify hate speech in one region of the world is likely to perform worse when deployed in another region, owing to differences in common parlance and cultural attitudes. The dangers of repurposing algorithms have also been well documented in the world of healthcare, where hospitals (notably in the US) have unsuccessfully sought to export their in-house diagnostic models to other settings.[footnote 53]Those procuring algorithms may be able to work with vendors to retrain systems to suit their own context, however this depends on the buyer having sufficient resources and bargaining power. From targeted job adverts to recommendation systems on social media sites, algorithmic processing is transforming how consumers and citizens access information, products and services online. By enabling a degree of personalisation, they are helping people to both seek out and be exposed to content and opportunities that are more relevant to their interests and appropriate for their needs. However, algorithms also pose several risks in this context, potentially closing people off from alternative viewpoints, as well as depriving some groups from seeing valuable economic opportunities. The use of algorithms to target content online, particularly on social media platforms, could result in internet users being repeatedly exposed to the same type of information. As has been well documented, many of today's platforms deploy sophisticated recommendation algorithms, which adapt as they learn more about the type of content users tend to engage with. In many cases, this results in users being presented with more of the same innocuous content, such as a favourite TV show or a friend's social media posts. However, in other cases this type of targeting can result in people being repeatedly shown content that is misleading or damaging, such as antivaxx conspiracy theories, or even violent content. Toxic online environments, polarisation or online aggressive behaviour may result from exposing internet users to this kind of emotionally charged content, potentially leading to physical harm in the real world.[footnote 54]This phenomenon can affect both individuals (for example if a person acts on misleading health information) and society (for example with so-called online echo chambers fostering political and cultural polarisation). As well as changing the type of content people see online, algorithms can also shape the economic opportunities that internet users are exposed to. Many businesses today use targeted adverts to channel their products and services at desired audiences, saving them time and money, and benefiting consumers who want to see those goods. However, not everyone who has an interest in seeing those adverts is shown them. Researchers from Northeastern University, for example, ran an experiment on Facebook in 2019 which suggested that some online adverts for housing opportunities were being shown to black and white users at differing levels of frequency.[footnote 55]A separate study, also looking at Facebook, found that online job adverts for STEM careers were less frequently displayed to women.[footnote 56]The researchers hypothesised that this was partly because the underlying algorithms were designed to optimise for cost, and women tend to be more costly to advertise to (in part because they are seen as more likely to make a purchase). This is linked to how algorithmic processing can lead to unfair outcomes for some demographic groups, as was explained in [Fairness for individuals affected by algorithmic processing](https://www.gov.uk/government/publications/findings-from-the-drcf-algorithmic-processing-workstream-spring-2022/the-benefits-and-harms-of-algorithms-a-shared-perspective-from-the-four-digital-regulators#Fairness-for-individuals-affected-by-algorithmic-processing] Resilience refers to the capability of algorithmic systems to withstand shocks and perform consistently when exposed to different conditions. This includes being able to cope with adversarial attacks, such as when bad actors seek to ""poison"" datasets or extract personal information from an organisation's training datasets. Algorithms can themselves be weaponised in order to inflict damage, for example by automating spear phishing[footnote 57]activity and scaling up denial of service (DoS) operations. These are issues of concern to all DRCF members. As algorithmic processing has become more important to the functioning of public services and industry, so too has it become an increasingly attractive target for those eager to cause disruption. There are many ways that algorithmic systems can be undermined. One of these is by poisoning training data, resulting in models with lower levels of accuracy. Cyber criminals could, for example, seek to corrupt the training data used to build a bank's fraud detection model, making it less likely that fraudulent activity is noticed. Another way criminals can wrongfoot algorithmic systems is by deploying ""adversarial examples"".[footnote 58]This is where inputs to a model are deliberately manipulated in order to be misclassified or unrecognised, even if that manipulation is imperceptible to the human eye. Terrorist organisations, for instance, could try to evade the content moderation algorithms of social media platforms by making minute changes to the pixel patterns of their images and videos. While these are cases of algorithms being manipulated in order to cause mistakes, cybersecurity experts have also highlighted how algorithms can be manipulated in order to leak sensitive information.[footnote 59]One practice of particular concern is ""model inversion"", where personal information can be inferred about individuals who are featured in training datasets. A report from the US-based Center for Security and Emerging Technology highlights the example of a facial recognition system, where a model's attackers start with a randomly generated image of a face, and make repeated edits to that image until they arrive at a version that the model matches to the name of their target individual.[footnote 60] Stakeholders also raised more general concerns about the ability of organisations to safely manage the data they collect to train and run algorithmic systems. Despite the secure processing of personal data being a key principle under the UK GDPR[footnote 61], a DCMS Cyber Security Breaches Survey produced in 2021 found that 4 in 10 businesses experienced a ""cyber security breach or attack"" in the last 12 months.[footnote 62]The survey also suggested that businesses found it harder to implement cyber security measures during the pandemic, with fewer businesses now deploying security monitoring tools or undertaking any form of user monitoring than was the case a year ago. Just as bad actors can seek to undermine algorithmic systems, so too can they weaponise them for their own purposes. A number of cyber security experts have documented how machine learning algorithms can be used to scale up criminal activity online.[footnote 63]This includes by automating and improving the quality of spear phishing attacks, which are personalised messages designed to extract information or money from their victims. In a recent experiment in Singapore, researchers from the Government Technology Agency used a deep learning natural language model in conjunction with other AI-as-a-service tools to craft bespoke phishing emails tailored to people's backgrounds and personality traits.[footnote 64]Sending these emails to colleagues at the Government Technology Agency as an experiment, the researchers say they were impressed by the quality of the synthetic messages and the rate of click-throughs they were able to generate, when compared to messages that were drafted by humans. As well as scaling up existing threats, algorithms could be used to introduce new ones. In a first of its kind incident, it was reported in 2019 that fraudsters used deepfake technology to mimic the voice of a senior executive from a German energy company, allowing them to request a transfer of several hundred thousand pounds from its UK subsidiary.[footnote 65]A report produced by a consortium of organisations including the Universities of Oxford and Cambridge predicts that novel cyber threats such as these will continue to emerge over the coming years.[footnote 66]This includes the use of algorithms to predict which individuals are most likely to respond to scams, thereby improving target selection.[footnote 67]The report also argues that the advent of machine learning tools is lowering the barriers to entry for cyber criminals, for instance by enabling attackers to perform phishing attacks in multiple languages with little additional effort required. These developments raise questions about the future resilience of traditional cyber security tools used by organisations. Individual autonomy is about citizens and consumers having control over their lives, which includes being able to make informed choices about what they buy, the media they consume, and the people they interact with online. As we have already seen, algorithmic processing is allowing firms to target information, products and services with increasing precision, as well as to build more sophisticated ""choice architectures"" that determine how options are presented to users online. These practices offer tremendous benefits, yet when deployed inappropriately they can also undermine people's autonomy, encouraging them to do things, buy things and believe things that are damaging to themselves and/or wider society. They can also impact on people's freedom to determine their identity, including how they choose to present themselves to the world.[footnote 68]Vulnerable people are especially exposed to these risks. Targeting (for example via the use of recommender systems) is essential in helping people to navigate the large volume of content online; without it, it would be impossible for search engines to function or for music and video streaming sites to serve up the content we want to see and hear. Yet targeting can sometimes err into manipulation, resulting in people making decisions and altering their beliefs in a way they would otherwise not, given time, space and more options at their disposal. The Centre for Data Ethics and Innovation's Online Targeting Review argued that targeting, when left unchecked, could exploit people's impulses and emotions. The CDEI also expressed concern that targeting could be a driver of ""internet addiction"", with recommender systems being designed to maximise endless engagement and clicks. Several stakeholders suggested these risks were greater when people had a ""false sense of control"" over their online interactions, since those who are unaware that targeting is taking place are also less likely to scrutinise what they are seeing and why. Stakeholders also expressed concern that algorithmic targeting may be making consumer preferences more 'sticky'. At any one point in time, consumers will have an affinity for a particular set of brands, products and services, which would typically be expected to change over time as societal tastes evolve, new goods arrive on the market, and brands launch new advertising campaigns. However, algorithmic recommendations (for example those provided via search results or targeted adverts) may serve to limit people's exposure to alternative goods, potentially hardening their preferences. The extent to which people are aware of this practice, consent to it and can escape it in order to access wider options, will determine how much it impacts on individual autonomy. Some groups in society are particularly vulnerable to the effects of targeting. This includes children, older people, people with learning disabilities, and people with addictions. An investigation by the Gambling Commission, for instance, found that 45% of online gamblers were encouraged to spend money on gambling activity due to the adverts they saw.[footnote 69]While there is no formula for determining what types of targeting are harmful, it is clear that user manipulation is a present risk online and one regulators will need to pay close attention to. Given the technical sophistication of some algorithms and the fact they may be deployed behind the scenes in ways that individuals affected may not appreciate, conceptions of who is 'vulnerable' in this context may be broader than when thinking about vulnerability in other regulatory dimensions. A broader grouping of online practices that can undermine the autonomy of citizens and consumers is the use of harmful ""choice architectures"".[footnote 70]User experience and interaction designers, content designers and marketers can be thought of as choice architects, and the design of the environment they create is the choice architecture.[footnote 71]Common examples of choice architecture include how products are ordered in search results, browsing buttons available to users on social media platforms, the number of steps needed to cancel a subscription, or whether an application is selected by default for tasks on mobile devices. Choice architecture is a neutral term. A well-designed website, app or digital service built with users' interests in mind will help consumers choose between suitable products, make transactions faster, and result in suggestions for more relevant products and services. Websites and platforms often spend significant time and resources refining their choice architectures, resulting in a better user experience and reduced friction for users. However, a CMA study identified that firms can design their user interfaces utilising algorithms in a manner that goes against users' interests by exploiting their innate biases, such as loss aversion, inertia and their tendency to choose default options.[footnote 72]Some websites, for example, present consumers with potentially misleading scarcity messages, which aim to convince them that there is only so much time to buy a particular product, or that there is more limited stock than there is in reality.[footnote 73]Furthermore, both search algorithms and personalisation underpinned by algorithms can drive the choice architecture encountered by users.[footnote 74] The CMA has also highlighted the practice of firms using algorithms to predict the likely rating that users would give to their service. Makers of apps, for example, have been shown to use algorithms to determine when users are more likely to leave positive reviews - a tactic that some fear is leading to ""ratings inflation"".[footnote 75] Researchers have coined new phrases to describe particularly harmful forms of choice architecture. These include: dark patterns,[footnote 76]a set of (deliberate) manipulative practices identified by user experience (UX) designers; sludge[footnote 77], which makes it hard for consumers to take action in their interests; and dark nudges[footnote 78], which make it easy for consumers to take action that is not in their interests. Dark patterns have also been observed in the ""consent management platforms"" that are used by websites and apps to acquire consent from internet users to collect, share and sell their personal data. Some of these practices are unlikely to be compliant with data protection regulation. Analysis of choice architecture is already central to some areas of regulatory intervention. For example, qualitative and quantitative analysis of the choice architecture of default applications are key parts of the CMA's recent interim report on mobile eco-systems.[footnote 79]As we go on to discuss in the section below, choice architecture is also highly relevant to the control of personal data. The complexity of algorithmic systems' supply chains (including data collection and annotation) and how they operate across domains may lead to some loss of user control in how and where personal data is shared. In its Online Platforms and Digital Advertising Market Study, the CMA recommended that the new Digital Markets Unit (DMU) has the power to compel platforms to give consumers more control over their data.[footnote 80]Under this arrangement, the DMU would have the ability to introduce a ""choice requirement"", which would require platforms to give consumers the choice to receive non-personalised advertising;[footnote 81]as well as to introduce a ""fairness by design"" duty, which would set out for platforms the choice architecture they should utilise to present effective choices to consumers. Furthermore, the CMA recommended the government consider giving the DMU powers to ask platforms to trial how such choices are presented to consumers given this is a complex area where unintended impacts are possible. The government is currently consulting on empowering the CMA to order trialing of potential remedies in the course of a Market Investigation Reference (MIR),[footnote 82]as well as giving similar powers to the Digital Markets Unit which could support its approach to implementing codes of conduct and pro-competitive interventions.[footnote 83] Strong competition helps to push down costs and prices, drive up service standards and quality, and increase access to products and services. It also creates incentives for innovation, productivity and economic growth. Effective competition means that markets are open to new firms that can offer better deals and products, while firms that cannot keep up either have to change or go out of business. Promoting competition is a priority statutory objective shared by the FCA, Ofcom and the CMA. The ICO is committed to supporting innovation and economic growth which is one aspect of competition.[footnote 84] Recommender systems or ranking systems may be designed to promote a platform's own products, content or services above those of its competitors. Self-preferencing can also occur where companies exploit default effects or saliency, such as where their own products and services are shown as the default option to consumers, rather than in a list of options. The CMA's 2021 report on algorithms outlined this issue and the risks it poses to competition.[footnote 85] Where own-brand content is recommended on video-on-demand services, there is a risk that the diversity of content is reduced for viewers, and public service material, for example, may become less prominent in search results. Algorithms that are used for information retrieval and ranking in search engines may be designed to up-rank certain sponsored links and own-brand content. Some users may be unaware of this preferencing and be unwittingly steered towards products or services that are more profitable to the company. A good example of where this practice has been shown to play out is the online hotel booking industry, where an investigation by the CMA between 2017 to 19 found that search results on some booking sites were affected by the amount of commission a hotel pays to the site.[footnote 86]Such practices may in turn impede competition, for example the ability of challengers to compete in concentrated markets such as search engines, as well as fairness concerns for consumers. Algorithms and the infrastructure around them are evolving and becoming increasingly complex, often with a multitude of interacting components, which could make it hard to explain or reverse engineer the output. Interconnectedness of algorithms developed by multiple organizations can also pose a risk. They could propagate and amplify issues within a system and make it challenging to isolate the root cause(s). For example, the ""Flash Crash""[footnote 87]on 6 May 2010 has highlighted the risks of automated algorithmic trading.[footnote 88] Some developers have also highlighted the challenges of integrating dynamic machine learning models with software that has been programmed using conventional methods. This is because the behaviour of the ML model will change as it is re-trained, which can cause issues with downstream applications. Connected algorithmic processing systems could also facilitate collusion and lead to higher prices for consumers. A firm might develop a certain part of their product or service in-house and source other algorithmic components from third parties, for example to set prices, through which they could exchange information. There are concerns that algorithms could also lead to new forms of tacit collusion - where there is no explicit agreement between businesses to collude, but where pricing algorithms effectively deliver the same result.[footnote 89]At the extreme end, pricing algorithms drawing on ML technology could autonomously learn to collude.[footnote 90]They can be used to automatically detect and respond to price deviations by competitors, which could make explicit collusion between firms more stable, as there is less incentive for those involved to cheat or defect from the cartel. An example of this was the CMA's Trod/GB eye decision in the online posters markets, where the parties agreed that they would not undercut each other's prices for posters and frames sold on Amazon's UK website. They implemented the agreement by using automated repricing software that they each configured to give effect to the illegal cartel.[footnote 91]A possible avenue to address concerns about autonomous learning by algorithms may be increased transparency from businesses, both around pricing behaviour and their rationale for using price matching algorithms. Online platforms and search engines collect individuals' data to train their algorithmic systems, allowing content to be personalised to user interests and needs. This personalisation of content can in turn drive more engagement on those platforms and engines, resulting in the collection of even more personal data with which to further refine their algorithms. This dynamic leads to network effects, with these products or services gaining additional value as more people use them. While this is in one sense a consequence of an effective and attractive service offering, it can also result in barriers to new entrants, who often lack the necessary user data to train comparable algorithmic systems. A study undertaken by the CMA found that this was a particular barrier to new entrants in digital advertising, with Google and Facebook benefiting from rich data sources that are well beyond those available to smaller companies in this market.[footnote 93]For example, a dominant search engine can use its volume and variety of activity to develop a deeper understanding of consumer interests than a competitor with lower market share. This allows the engine to provide more effective search advertising services as well as opportunities for advertisers to target niche search terms. Additionally, mass personal data collection also potentially violates the principle in UK data protection law that requires firms to minimise the amount of personal data they collect. AI development has exacerbated the issue because it creates a heightened demand for data, including personal data. Organisations with data power accumulate granular information on individuals across their online journey that they then use to personalise their offerings, which can exacerbate information asymmetry between consumers and service providers. In addition to the 6 shared areas discussed above, stakeholders suggested the following 3 topics for the DRCF to consider. Stakeholders drew attention to the important role played by human practitioners who operate and interpret the results of algorithmic systems. These practitioners - who range from social media content moderators to the employees of financial services firms - are often seen as providing an additional line of defence against the potential harms that might be caused by algorithms. Applying common sense and more contextual knowledge, they may be able to spot, for example, where a social media post has been mistakenly flagged as containing hate speech, or where a financial transaction has been wrongly interpreted as being fraudulent. However, a growing number of commentators are cautioning against viewing human involvement as a foolproof safeguard. Specialists in human computer interaction (HCI) have highlighted the problem of ""automation bias"", where practitioners uncritically accept the recommended decision of an algorithm, rather than meaningfully engage with that output.[footnote 94]This is a concern the ICO has also identified in its AI guidance, as data protection law prohibits solely automated decisions that significantly impact individuals without a meaningful human review. Practitioners could also become distracted while in command of a system, or be unable to interpret its technical outputs, for example the different types of statistical accuracy indicators that a facial recognition model might produce when it flags a positive match. For these reasons, it is important that users of algorithmic systems implement a wider set of oversight and governance arrangements. This includes establishing effective decision-making procedures for approving new systems, as well as regularly reviewing the accuracy of those systems once they are live. There is intense ongoing debate about the potential impact of algorithmic systems including AI and machine learning on climate change. There are several ways in which AI can help with reducing climate change, however the computational resources required for developing and maintaining this technology can also have a negative impact. Research shows that AI may act as an enabler on 134 targets (79%) across all Sustainable Development Goals developed by United Nations (UN).[footnote 95]For example, machine learning could help optimize energy supply and demand in real time, with increased efficiency. AI can also help retailers reduce their environmental footprint through waste reduction, better optimization of their supply chain to improve how they respond to market demands.[footnote 96]At a consumer level, algorithms can play a positive role by helping users make sustainable choices. The potential of AI to combat climate change is an active topic of research that has been explored by various organisations.[footnote 97]Several agencies are calling upon governments to develop appropriate policies to tap into the full potential of these technologies.[footnote 98] But despite AI's promise, research suggests 35%[footnote 99]of targets across all Sustainable Development Goals may experience a negative impact from its development.[footnote 100]Algorithmic systems, especially advanced ML systems, require very high computational resources, particularly in their training and development phases. For example, research estimated that the carbon footprint of training a single big Natural Language Processing (NLP) model is equal to around 300,000 kg of carbon dioxide emission.[footnote 101] Stakeholders told us that regulators should pay close attention to the way organisations handle data, given that the quality of data is a major determinant in shaping how an algorithmic system performs. Incomplete or outdated training datasets, for example, are likely to result in poorly performing models. Unrepresentative datasets, meanwhile, could result in models that are less accurate when processing the data of particular demographic groups, whether that is for the purposes of screening CVs or targeting consumer adverts. A recent business survey undertaken by Ipsos MORI for the Centre for Data Ethics and Innovation revealed that 23% of UK firms using ""AI and data-driven technology"" saw challenges in accessing quality data.[footnote 102]Of these, 74% cited the problem of collating data from fragmented data sources, while 32% said there was a lack of historical data available on which to train their systems. Even when organisations have access to high quality data, they may not be aware of how to store that data responsibly. Depending on the nature of the data, good data governance may mean storing data in a standardised format, creating metadata to ensure other users understand how it should be used, putting security controls around who has access to that data, and keeping a record of who is using that data and how. Some organisations have taken to creating ""data catalogues"" to monitor their data inventory and ensure its proper use. In the same CDEI-Ipsos MORI survey, 86% of firms who use AI and data-driven technology said they felt able to ""store and manage data responsibly through well-defined governance and data protection protocols"".[footnote 103]While this is reassuring, the survey also identifies room for improvement in several areas. This includes the ability of firms to handle unstructured data (for example videos and images), with only 45% of respondents saying they do this well. The UK government has documented and sought to address a number of these issues in its National Data Strategy, which highlights the recent creation of a government Data Quality Hub to promote best practice methods for maintaining data quality.[footnote 104]Effective algorithmic auditing may also be a way to help address these issues in some settings, with auditors looking not just at how algorithms perform but also how organisations are managing the datasets that underpin them. Auditing of this nature could potentially serve a related purpose of assuring that datasets have been developed responsibly, for example that the data they contain has been labelled by individuals who have been adequately compensated for their time. Algorithmic processing has the potential to bring about huge, positive impacts on people's lives. Some examples include: In this section we provide a discussion about how algorithmic processing can provide benefits within our 6 shared focus areas, both now and in the near future. Algorithms are often discussed as being difficult (or impossible) to interpret or explain, especially when more complex machine learning such as neural networks or deep learning are used. However, algorithms themselves can sometimes be used to assist in creating valuable interpretations and explanations. There is growing interest in 'counterfactual algorithms' that can generate explanations based on what could have happened if the input of a model was different.[footnote 110]For example, an algorithm could inform an individual who had been rejected for a loan that if their income was higher, or their level of debt was lower, that their loan application would have been accepted. Understanding how to achieve a better decision can help foster greater trust. Algorithms can also be used for dimension reduction in models,[footnote 111]removing excessive and irrelevant features from machine learning models and thus making them simpler and potentially more explainable and interpretable. This could lead to better human-computer interactions and making AI accessible to a broader audience. Future benefits could see algorithms assist with better decision-making. New developments in interpretable AI and visualisation of AI are making it easier for human experts to put complex data together to draw actionable insights. For example, in a medical research context, AI-assisted summarisation could one day help clinicians see the most important information and patterns about a patient leading to better treatment.[footnote 112] Algorithms can also be used to detect bias and discrimination. Some suggest that models and so-called 'causal graphs' (graphical representations of the causal relationship between features and outputs) can be used to detect and explain the causal pathways that lead to potential unfairness.[footnote 113]Research in this area is evolving and tools are being developed to assist in detecting unfair bias and discriminatory decision-making. Algorithmic processing can assist in widening access to digital markets. For example, price personalisation can enable certain customers to access goods or services by lowering the price and thereby widening access.[footnote 114]Browser plug-ins enable users to control their browsing data and help them to understand how they are being tracked and what is informing the recommendations being made to them.[footnote 115]This may help empower users and increase user inclusion in online services. In credit underwriting, there may be opportunities to improve the efficiency and inclusiveness of lending if some algorithmic systems can help assess the creditworthiness of customers with limited credit histories ('thin-files').[footnote 116]There is also an opportunity to empower consumers with unique insights into their financial needs, reducing matching frictions and supporting effective decision-making. There may be ways to promote legal inclusion too, such as through automated advice - also known as robo-justice. For example, individuals can receive automated advice on whether they are eligible for legal aid.[footnote 117] As well as creating and exacerbating security risks, algorithmic processing can be used to enhance resilience of infrastructure and users to cyber threats, scams and fraud. For example, algorithms are used for triaging, monitoring and blocking spam/fraudulent activity, which supports consumers, benefits business, and helps to avert data breaches. AI can be used to flag erroneous commercial transactions, and to train systems that detect synthetic media content designed to mimic real individuals. They can be deployed in the financial markets for Anti-Money Laundering (AML) purposes, fraud detection and countering the financing of terrorism purposes. They can also assist in anti-corruption efforts; for example, Microsoft announced its Anti-Corruption Technology and Solutions initiative in late 2020, which will leverage technologies to enhance transparency and to detect and deter corruption. Early applications of this initiative have helped to bring greater transparency to how the use of COVID-19 economic stimulus funds has been spent.[footnote 118] Algorithms can be used to enhance the user experience and enable individuals to make better choices via specific design choices on social media platforms. This can be done at multiple stages of the user journey and allows users to consciously control how and when they share their personal data, or determine what they see (such as filtering results in recommender systems). Context-aware recommender systems[footnote 119]may be used to provide information and services that take into account the users' needs and context. A future trend could see greater personalisation of how individuals interact with algorithmic systems. For example, if a user is partially sighted, an algorithm could adjust the size or font of some text automatically to enable greater autonomy. Algorithmic processing can foster competition by helping customers connect with a greater number of providers, as well as helping firms to access consumers, hence reducing the barrier to entry in some markets. Search engines, for example, are algorithmic systems that allow people to find hundreds if not thousands of products that match their search terms. Price comparison websites use similar techniques to collate information and present consumers with up-to-date prices on a range of goods and services, from flights to car insurance to broadband. Algorithmic processing has also helped to power the growth of the sharing economy, including ride-hailing and home rental services. P2P platforms have opened up more choice for consumers and increased pressure on traditional industries to improve their offerings (for example with Airbnb disrupting the traditional hotel industry). There are strong indications that the increase in use of algorithmic systems will lead to economic growth and efficiency optimisation. It has been predicted that AI, for example, could deliver a 22% boost to the UK economy by 2030.[footnote 120]More economic growth, driven by algorithmic processing, could mean better incentives to invest in the sector experiencing growth. In turn, this leads to greater incentives for new organisations to enter the market, creating greater competition amongst firms. This could produce benefits for consumers as they will have more choices. Implementing algorithmic systems could also reduce the supply costs of goods and services, with savings passed on to customers in the form of lower prices. The DRCF was established to build on the strong working relationships between its members and to enhance this cooperation and the effectiveness of individual regulatory approaches, given the unique challenges posed by the regulation of digital services and products. This discussion paper illustrates some of the benefits and harms that could arise from the current use of algorithmic processing, as well as how these issues might evolve in the near future. We have integrated the views of different agencies to help firms and other stakeholders understand common concerns. Our findings suggest many areas where there is shared interest and therefore opportunities for a greater level of cooperation. We recognise the influential role we can play to shape the algorithmic processing landscape to benefit individuals, consumers, businesses, and society more broadly. The DRCF is pioneering in that it can address issues from 4 different perspectives. We can be inspired by the interventions that individual regulators have made to think of ways of collaborating in the future. Through guidance and thought leadership, we can provide greater clarity for organisations so they can confidently innovate. For example, the ICO and The Alan Turing Institute's co-badged guidance on 'Explaining Decisions Made with AI' guides organisations in ways to make their use of AI systems more transparent. As DRCF members, we may consider ways to build and expand on this to provide further clarity to the organisations we regulate to ensure they are transparent about who is accountable and what the allocation of accountability within the AI pipeline entails. We can also explore ways of clarifying the similarities and differences over the concept of transparency across the different DRCF members. A more hands-on cooperative intervention could be achieved through the increased use of regulatory sandboxes. The FCA's regulatory sandbox allows firms to test products and services in a controlled environment, and to reduce the time-to-market at potentially lower cost. The ICO is an active mentor in the FCA Digital Sandbox, and also runs its own regulatory sandbox programme on a rolling basis. These sandboxes are not exclusively open to organisations developing algorithms, although many of the entrants do use them. We could explore ways of running sandboxes where 2 or more DRCF members can (subject to their particular powers) offer advice and the ability to test products and services that use algorithmic processing in a controlled environment. As well as interventions that are targeted at organisations during the pre-deployment stages, regulators can exercise their powers to take enforcement action against actors who have not complied with the law and caused harm. Appropriate enforcement action can be a powerful tool to deter organisations from ignoring compliance issues. We can explore ways to collaborate in investigations where algorithmic processing is causing harms that span the mandate of more than one regulator. There may also be opportunities for valuable joint work on supporting individuals and consumers in seeking redress over harms they believe they have incurred. The DRCF could also establish greater consistency in the way we engage with citizens about algorithms to enable them to better understand what algorithms are, where they're used, and the choices available to consumers. This includes consistency about the language and terminology we use, as this can easily create or increase confusion. Cooperation can be wider than just between DRCF members, it can include other regulators as well as wider society. For example, engaging with the Equality and Human Rights Commission when we conduct further work on algorithmic processing and fairness. We can also engage with technology providers and professional users (for example media organisations, retail firms, and public services) to better understand how algorithmic processing takes place and how to achieve the benefits while minimising harms. Finally, not every issue that is identified in the context of algorithmic processing will require joint action from DRCF members, and regulatory approaches may well vary in important aspects, reflecting the specific regulatory context and mandate. Many potential benefits and harms related to algorithms are also context dependent and require a tailored approach from an individual regulator that is sensitive to the specifics of a particular sector. Although the 4 regulators within the DRCF have different remits, there are overlapping areas of mutual interest. The DRCF have identified the following 6 cross-cutting focus areas in the context of algorithmic processing: One aim of the DRCF is that future regulatory guidance and thought leadership in these areas is approached in a more joined up way. This approach is important for businesses - particularly in terms of guidance and standard setting. Algorithmic processing systems have the potential to deliver many benefits and harms as identified in this document. We will work together where appropriate to ensure that the harms are mitigated in a proportionate way, and help businesses to innovate so that they can realise the benefits. There was a broad set of answers when stakeholders were asked to identify which area should be prioritised: transparency received the most support with fairness and resilience coming joint second. Some stakeholders also suggested that pursuing some priorities may require balance with others: for example, the pandemic has shown that there can be perceived tensions between protecting individuals from harm and protecting individual rights. There may also be perceived tensions between the aims of competition law and data protection, although these tensions can be resolved. We believe that the ICO and CMA's joint statement provides a blueprint for how tensions or unintended effects across different types of digital regulation can be negotiated between regulators and allow synergies to emerge.[footnote 121]Where firms make ""privacy preserving"" claims in the context of defending their exclusive access to large volumes of data flows, regulators may test those claims as substantial access to data may be a source of market power. Going forward there are a number of potential areas we could focus on, and, of these, transparency and fairness have been shown to be particularly significant. Similarly, actively supporting access to redress is important, as is recognising the role DRCF members can play in helping citizens/users better understand what algorithms are, where they're used and the choices available to them. Many of the issues identified are exacerbated in situations where there are multiple parties involved in the development, deployment and use of systems, for example in AI-as-a-Service tools We have identified the following points as the key takeaways from our work. 1 . Algorithms offer many benefits for individuals and society and these benefits can increase with continued responsible innovation. Companies that innovate responsibly can use algorithms to create benefits for individuals and society in a virtuous cycle. When consumers see evidence of and/or experience benefits they trust and support firms facilitating those benefits. This can create and stimulate markets and drive economic growth. Benefits may include increased productivity; the development of tools for disadvantaged groups; and improved methods of summarising, organising and finding information and content. DRCF members could (where appropriate) work together to identify best practice in different areas of algorithmic design, testing and governance, and disseminate these lessons to help industry innovate responsibly for the benefit of all. There may also be opportunities to help businesses demonstrate compliance where they deploy algorithms, making sure this process is as simple and cost-effective as possible. 2 . Harms can occur both intentionally and inadvertently As explained in this paper, algorithmic processing can be deliberately used to inflict damage, whether that is by automating spear phishing attacks or enabling the creation of subversive deepfake content. Yet much of the harm that results from the use of algorithmic processing may be inadvertent, perhaps caused not by malice but by insufficient understanding on the part of those who deploy these systems. Some users may not appreciate, for example, that harmful bias can be embedded within algorithms, nor that some algorithms may affect vulnerable users differently to the wider population. Thus, it may not be appropriate for DRCF members to assume that organisations understand the risks of algorithmic processing, nor that they are aware of methods to mitigate those risks. DRCF members, as well as producing clear guidance and policies, could therefore look at ways of improving industry's baseline knowledge of the impact algorithms can have on individuals and society. 3 . Those procuring and/or using algorithms often know little about their origins and limitations Those purchasing algorithmic systems often do so with little knowledge of how they have been built and how they perform in different contexts. This makes it more difficult for purchasers to identify and mitigate risks (for example algorithmic bias), and to ascertain whether the systems they are using were developed responsibly (for example built with the support of data labelers who were adequately compensated for their work). DRCF members could support the development of algorithmic auditing practices, and consider appropriate minimum standards in relation to some areas of algorithmic deployment. We could, for example, set standards for third party auditors, or investigate the merits of tools like bias tests. Algorithmic auditing is the subject of another DRCF paper[footnote 122]being published alongside this one. 4 . The lack of visibility in algorithmic processing can undermine accountability Algorithmic processing may take place without the knowledge of those affected by it (for example someone rejected for a credit card may not realise their application was processed by an algorithm, just as those viewing videos on a streaming site may not realise that content has been personalised by an algorithm). In some cases this lack of transparency may make it more difficult for people to exercise their rights - including those under the GDPR. It may also mean algorithmic systems face insufficient scrutiny in some areas (for example from the public, the media and researchers). DRCF members could help organisations communicate more information to consumers about where and how algorithms are being deployed. This could mean issuing new transparency guidelines, such as the ICO's Explaining Decisions Made with AI guidance[footnote 123]or the government's algorithmic transparency standard for the use of high impact algorithms by public bodies. We could also explore the costs and benefits of ""algorithmic registers"", which serve as a public log that anyone can access. 5 . A ""human in the loop"" is not a foolproof safeguard against harms Having a human review the outcomes of an algorithmic system has been suggested by some AI commentators to be an essential safeguard, and indeed data protection law includes specific protections for individuals from being subject to decisions made by solely automated means. Yet research suggests human operators often struggle to interpret the results of algorithmic processing, with some misunderstanding the different ways that accuracy can be measured. Some also place too much faith in the effectiveness of algorithmic processing, insufficiently scrutinising their outputs (for example that of a positive match provided by a content moderation tool used by a social media platform). DRCF members could further investigate the concept a ""human in the loop"" and explore opportunities to help firms understand better the strengths and limitations of this and other approaches to risk mitigation. Appropriate human oversight and accountability will be essential to mitigate potential harms, whatever the technology deployed. DRCF members may find that further engagement with researchers in the field of ""human-computer interaction"" (HCI) is valuable in deepening collective understanding of potential issues related to human oversight and may wish to share emerging insights in this space with the industries they regulate. 6 . There are limitations to DRCF members' current understanding of the risks associated with algorithmic processing Recent years have seen a spate of innovations in algorithmic processing, from the arrival of powerful language models like GPT-3, to the proliferation of facial recognition technology in commercial and consumer apps.[footnote 124]As the number of use cases for algorithmic processing grows, so too will the number of questions concerning the impact of algorithmic processing on society. Already there are many gaps in our knowledge of this technology, with myths and misconceptions commonplace. DRCF members could conduct or commission further research on algorithmic processing where appropriate, and otherwise draw the attention of external researchers to important open questions. There may be additional opportunities to liaise with organisations funding research, like UK Research and Innovation, to help inform their funding priorities. We may also consider using futures methodologies (for example horizon scanning and scenario planning) to identify emerging trends in the development and adoption of algorithms and work through the implications of these. Having presented our view on the most prominent risks and benefits associated with algorithmic processing, we are eager to hear views from a wide range of stakeholders on these matters. The DRCF is therefore launching a call for input on the findings of this report and our related paper on algorithmic auditing. We are particularly interested in hearing the views of stakeholders on the questions set out in Annex A. The call for input will last until Wednesday 8th June. Stakeholders can submit views via email atdrcf.algorithms@cma.gov.uk. We would welcome views from stakeholders on the following questions: DRCF (2022)Auditing algorithms: the existing landscape, role of regulators and future outlook.- Ibid.- See for example ICO and CMA (2021) 'CMA-ICO Joint Statement on Competition and Data Protection Law'.- Reuters (2018), 'Amazon scraps secret AI recruiting tool that showed bias against women'. 11 October.- Taddeo, M., Tsamados, A., Cowls, J., and Floridi, L., (2021) 'Artificial intelligence and the climate emergency: Opportunities, challenges, and recommendation', One Earth, Vol 4, Issue 6, pp.776-779. 18 June.- Such powers may include analysing systems at code-level; interrogating existing policies; interviewing stakeholders; issuing fines and taking other enforcement action where they find unlawful activity involving algorithmic processing.- Real-time bidding is an automated digital auction process that allows advertisers to bid on ad space from publishers.- BBC (2020) 'A-levels: Ofqual's 'cheating' algorithm under review'. 20 August.- Office for AI, DCMS and BEIS (2021) 'National AI Strategy'. 22 September.- Department for Digital, Culture, Media and Sport (2021), 'Data: a new direction'. 10 September. See also: ICO (2021), 'ICO response to DCMS consultation ""Data: a new direction""'. 7 October.- European Commission (2021), 'Laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain union legislative acts'. 21 April ; See the ICO's response: ICO (2021), 'Proposal for a regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence act) and amending certain union legislative acts'. 6 August.- Federal Trade Commission (2020), 'Using Artificial Intelligence and Algorithms'. 8 April.- Federal Trade Commission (2018) 'FTC Hearing #7: The Competition and Consumer Protection Issues of Algorithms, Artificial Intelligence, and Predictive Analytics' 13 to 14 November.- Slaughter, R. K., Kopec, J., and Batal, M., (2021), 'Algorithms and Economic Justice: A Taxonomy of Harms and a Path Forward for the Federal Trade Commission', Yale Journal of Law and Technology, Special Publication. August.- CNIL (2018), 'Algorithms and artificial intelligence: CNIL's report on the ethical issues'. 25 May.- Office of the Australian Information Commissioner (2018), 'Guide to data analytics and the Australian Privacy Principles'. 21 March.- CMA (2021), 'Algorithms: how they can reduce competition and harm consumers'. 19 January. See also CMA (2018) 'Pricing algorithms: Economic working paper on the use of algorithms to facilitate collusion and personalised pricing'. 8 October.- ICO and The Alan Turing Institute (2020), 'Explaining decisions made with AI'. No date.- Ofcom (2019), 'Use of AI in online content moderation'. 18 July.- ICO (2020), 'Guidance on AI and data protection'. No date.- ICO (2021), 'AI and data protection risk toolkit'. No date.- ICO (2021), 'Regulatory Policy Methodology Framework'. 5 May.- The FCA has an outcomes-focused, technology neutral approach to regulation and sets clear expectations around accountability for FSMA authorised firms through the Senior Managers and Certification Regime. Accountability for the outcomes of algorithmic decisions remains with the Senior Manager accountable for the relevant business activity whatever technology is deployed. For instance, where firms use 'robo advisory' services, the Senior Manager accountable for advice would be accountable for the advice given and the end outcomes for consumers. Senior Managers should ensure that there are adequate systems and controls around the use of an algorithm.- Mueller, H., and Ostmann, F., (2020), 'AI transparency in financial services'. 18 February.- Examples include Bracke, P., Croxson K., and Jung, C. (2019) 'Explaining why the computer says no', FCA Insight, and Bono, T., Croxson, K., and Giles, A (2021) `Algorithmic fairness in Credit Scoring', Oxford Review of Economic Policy.- FCA (2018) Algorithmic Trading Compliance in Wholesale Markets.- Bank of England and FCA (2022)The AI Public-Private Forum: Final Report- The Board of the International Organization of Securities Commissions (2020): The use of artificial intelligence and machine learning by market intermediaries and asset managers: Consultation Report- OECD (2022) Public consultation on draft proposed revisions to the Recommendation on G20/OECD High-Level Principles on Financial Consumer Protection- Tsamados, A., Aggarwal, N., Cowls, J., Morley, J., Roberts, H., Taddeo, M., and Floridi, L. (2022) 'The ethics of algorithms: key problems and solutions', AI & Soc.- Cobbe, J. and Singh, J. (2021), 'Artificial Intelligence as a Service: Legal Responsibilities, Liabilities, and Policy Challenges'. Forthcoming in Computer Law & Security Review. 9 June.- ICO, 'When do we need to do a DPIA?'.- ICO (2020), 'ICO takes enforcement action against Experian after data broking investigation'.27 October.- ICO (2019) 'Update report into adtech and real time bidding'.20 June.- The Royal Society (2022), 'The online information environment: Understanding how the internet shapes people's engagement with scientific information'. January.- MIT Technology Review (2021), 'A horrifying new AI app swaps women into porn videos with a click'. 13 September.- The Royal Society (2022), 'The online information environment: Understanding how the internet shapes people's engagement with scientific information'. January. See also, Paris, B. and Donovan, J. (2019), 'Deepfakes and cheap fakes: The manipulation of audio and visual evidence'. Data and Society. 18 September.- Paris, B. and Donovan, J. (2019), 'Deepfakes and cheap fakes: The manipulation of audio and visual evidence'. Data & Society. 18 September.- Binns, R. (2018) 'Fairness in Machine Learning: Lessons from Political Philosophy'. Proceedings of Machine Learning Research. See also CDEI (2020) 'Review into bias in algorithmic decision-making'. 27 November.- For example, while the law deems it fair for insurers to discriminate against people on the basis of their age (with older drivers often paying lower premiums than younger ones), it does not allow discrimination on the basis of gender or ethnicity- UK Government (2010), 'Equality Act 2010'.1 October.- Goode, L. (2018), 'Facial recognition software is biased towards white men, researcher finds'.The Verge. 11 February.- Reuters (2018), 'Amazon scraps secret AI recruiting tool that showed bias against women'. 11 October.- Kiat, L. S. (unknown), 'Machines gone wrong'. No date.- Bias from model optimisation occurs when models are designed to take into account features (e.g. price) which result in some groups being unfavourably treated. For example, MIT and London Business School researchers found in 2018 that online job adverts for STEM careers were less frequently displayed to women, in part because the underlying algorithms were designed to optimise for cost, and women tend to be more costly to advertise to. Bias from model generalisation occurs when organisations fail to use a single model to produce reliable results from multiple groups. In healthcare, for example, symptoms and biomedical markers for some diseases (for example diabetes) can vary by ethnic group, meaning that multiple models may be required to support diagnosis in the population.- Nature (2019), 'Millions of black people affected by racial bias in health-care algorithms'. 26 October.- CDEI (2019), 'Snapshot Paper - AI and Personal Insurance'. 12 September.- FCA (2021), 'Fair treatment of customers'. 24 March. See also: FCA (2021), 'FCA to introduce new Consumer DUty to drive a fundamental shift in industry mindset'. 7 December.- CMA (2021), 'Algorithms: how they can reduce competition and harm consumers'.19 January.- See for example Pandey, A., and Caliskan, A. (2021) 'Disparate Impact of Artificial Intelligence Bias in Ridehailing Economy's Price Discrimination Algorithms'. Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. July.- Ofcom (2020), 'Personalised pricing for communications: Making data work for consumers'. 4 August.- Danks, D., and London, A.J. (2017) 'Algorithmic Bias in Autonomous Systems'. Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI 2017).- Yu, A. C. and Eng, J. (2020), 'One algorithm may not fit all: How selection bias affects machine learning performance'. RadioGraphics, 40 (7). 25 September.- Hao, K. (2021) 'How Facebook got addicted to spreading misinformation MIT Technology Review'. 11 March.- M. Ali et al. (2019), 'Discrimination through optimization: How Facebook's ad delivery can lead to skewed outcomes'. Proceedings of the ACM on Human-Computer Interaction, Volume 3, Issue CSCW, November 2019, Article No.: 199, pp 1.- Lambrecht, A and Tucker, C E (2019) 'Algorithmic Bias? An Empirical Study of Apparent Gender-Based Discrimination in the Display of STEM Career Ads'. Management Science, 65 (7). pp. 2966-2981.- Spear phishing is an email or electronic communications scam targeted towards a specific individual, organisation or business. Although often intended to steal data for malicious purposes, cybercriminals may also intend to install malware on a targeted user's computer.- Belfer Center for Science and International Affairs (2019), 'Attacking Artificial Intelligence: AI's Security Vulnerability and What Policymakers Can Do About It'. August.- ICO (2019), 'Privacy attacks on AI models'. 12 August.- CSET(2020), 'Hacking AI - A Primer for Policymakers on Machine Learning Cybersecurity'. December.- ICO, 'Guide to the UK General Data Protection Regulation (UK GDPR) - Security'- DCMS (2021), 'Cyber Security Breaches Survey 2021'. 24 March.- CSER (2018), 'The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation'. 21 February.- WIRED (2021), 'AI Wrote Better Phishing Emails Than Humans in a Recent Test'. 7 August.- WSJ (2019), 'Fraudsters Used AI to Mimic CEO's Voice in Unusual Cybercrime Case'. 30 August.- CSER (2018), 'The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation'. 21 February.- These algorithms might equally in future be used by anti-fraud agencies to identify those most likely to be targeted by fraudsters, allowing to provide advance warning to these individuals that they are at risk.- ICO (2021), 'Data protection and privacy expectations for online advertising proposals'. 25 November.- CDEI (2020), 'Online targeting: Final report and recommendations'. 4 February.- CMA (2022) 'Online Choice Architecture: How digital design can harm competition and consumers'. 5th April.- Thaler, R. H., Sunstein, C. R., & Balz, J. P. (2013), 'Choice architecture.The behavioral foundations of public policy', Princeton University Press. (pp. 428-439); Johnson, E. (2022). The Elements of Choice: Why the Way We Decide Matters. Oneworld Publications.- Competition and Markets Authority (2021), 'Algorithms: How they can Reduce Competition and Harm Consumers.' 19 January.- For example, the CMA discussed online hotel booking websites which used a combination of partitioned pricing, reference pricing, paid for ranking and scarcity claims to influence customer decision-making. Fung, S. S., Haydock, J., Moore, A., Rutt, J., Ryan, R., Walker, M., & Windle, I. (2019).Recent Developments at the CMA: 2018-2019.Review of Industrial Organization, 55(4), 579-605.- CMA (2021), 'Algorithms: How they can reduce competition and harm consumers'. 19 January.)- FT (2020), 'Apple: how app developers manipulate your mood to boost ranking'. 7 September.- The term ""dark patterns"" was coined by Harry Brignull: for examples of dark patterns, seeWhat are Dark Patterns?.- Sunstein, C. R. (2020), 'Sludge audits'. Behavioural Public Policy, 1-20.- Campione, Chiara (A.A. 2018/2019), 'The dark nudge era: Cambridge analytica, digital manipulation in politics, and the fragmentation of society'. Tesi di Laurea in Nudging: behavioral insights for regulation and public policy, Luiss Guido Carli, , Luiss Guido Carli, relatore Giacomo Sillari, pp. 55. [Bachelor's Degree Thesis] Giacomo Sillari, pp. 55. [Bachelor's Degree Thesis]- Competition and Markets Authority (2021), 'Mobile ecosystems market study'. 15 June.- Competition and Markets Authority (2019), 'Online platforms and digital advertising market study'. 3 July.- Such consumer choice has now been implemented in China. See Vernotti, C. (2022), 'Digital policy experts weigh in on China's new algorithm regulation'. Technode. 5 April.- BEIS (2021), 'Reforming Competition and Consumer Policy'. 1 October.- DCMS (2021), 'A new pro-competition regime for digital markets.' 20 July. Separately, the ICO's Age Appropriate Design Code requires that ""information society services"" set the highest privacy settings as default for child users. See: ICO (2019), 'Age-appropriate design: a code of practice for online services'.- The ICO has a statutory duty under the Deregulation Act 2015 to take into account the desirability of promoting economic growth.- CMA (2021) 'Algorithms: How they can reduce competition and harm consumers'. 19 January.- CMA (2019). 'Online hotel booking'. 13 September.- The flash crash was a United States trillion-dollar stock market crash, which lasted for approximately 360 minutes.- Buchanan, Bonnie. (2019), 'Artificial intelligence in finance'.2 April.- CMA (2021) 'Algorithms: How they can reduce competition and harm consumers'. 18 June.- HM Treasury (2019), 'Unlocking digital competition, Report of the Digital Competition Expert Panel'. 13 March.- CMA (2016), 'Online seller admits breaking competition law'. 21 July.- Lynskey, Orla (2019), 'Grappling with ""data power"": normative nudges from data protection and privacy'. Theoretical Inquiries in Law, 20 (1). 189 - 220.- CMA (2019), 'Online platforms and digital advertising market study'. 3 July.- Strauss, Stefan. 2021. ""Deep Automation Bias: How to Tackle a Wicked Problem of AI?"" Big Data and Cognitive Computing 5, no. 2: 18.- Vinuesa, R., Azizpour, H., Leite, I. et al. (2020), 'The role of artificial intelligence in achieving the Sustainable Development Goals'. Nature Communications 11, Article number 233.- Australian Retail Association(2020), 'How AI and ML can enhance sustainability for fresh retailers'. 13 January.- The Royal Society (2020), 'Digital technology and the planet Harnessing computing to achieve net zero'. December.- GPAI (2021), 'Climate change and AI: Recommendations for government action'. November.- The positive and negative impacts do not sum to 100% as AI could have both a positive and a negative impact on some of the targets depending on the scenario.- Vinuesa R., Azizpour H., Leite I. et al.(2020), 'The role of artificial intelligence in achieving the Sustainable Development Goals'. Nature Communications, 11, 233.- Emma S., Ananya G., Andrew M. (2019), 'Energy and Policy Considerations for Deep Learning in NLP'. 57th Annual Meeting of the Association for Computational Linguistics (ACL).- CDEI (2021), 'AI Barometer 2021'. 17 December.- CDEI (2021), 'AI Barometer 2021'. 17 December.- DCMS (2019), 'National Data Strategy'.8 July.- Graetzer, SN , Barker, J, Cox, TJ , Akeroyd, M, Culling, JF, Naylor, G, Porter, E and Viveros Munoz, R (2021), 'Clarity-2021 challenges : machine learning challenges for advancing hearing aid processing'. Interspeech 2021, 30th August - 3rd September.- CDEI (2019), 'Snapshot Paper - Deepfakes and Audiovisual Disinformation'. 12 September.- Michael L. Littman, Ifeoma Ajunwa, Guy Berger, Craig Boutilier, Morgan Currie, Finale Doshi-Velez, Gillian Hadfield, Michael C. Horowitz, Charles Isbell, Hiroaki Kitano, Karen Levy, Terah Lyons, Melanie Mitchell, Julie Shah, Steven Sloman, Shannon Vallor, and Toby Walsh (2021). ""Gathering Strength, Gathering Storms: The One Hundred Year Study on Artificial Intelligence(AI100) 2021 Study Panel Report."" Stanford University, Stanford, CA, September.- SmartCitiesWorld (2021), 'Manchester uses artificial intelligence to gain more insight into active travel'. 13 August.- PLANNING (2021), 'Birmingham Council to use artificial intelligence to help it find more housing land'. 30 July.- Verma, S., Dickerson, J., & Hines, K. (2020). 'Counterfactual explanations for machine learning: A review'. arXiv preprint. October.- VentureBeat (2021), 'Understanding dimensionality reduction in machine learning models'. 16 May.- Michael L. Littman, Ifeoma Ajunwa, Guy Berger, Craig Boutilier, Morgan Currie, Finale Doshi-Velez, Gillian Hadfield, Michael C. Horowitz, Charles Isbell, Hiroaki Kitano, Karen Levy, Terah Lyons, Melanie Mitchell, Julie Shah, Steven Sloman, Shannon Vallor, and Toby Walsh (2021). ""Gathering Strength, Gathering Storms: The One Hundred Year Study on Artificial Intelligence (AI100) 2021 Study Panel Report."" Stanford University, Stanford, CA, September.- Nature (2020), 'The long road to fairer algorithms'. 04 February.- HM Treasury (2019), 'Unlocking digital competition'. 13 March.- Such as plug-ins that block ads and trackers likePymk Inspector - Open Source AgendaandGhostery.- Ostmann, F., and Dorobantu C. (2021), 'AI in financial services'. The Alan Turing Institute.- Zeleznikow, J. (2017), 'Don't fear robo-justice. Algorithms could help more people access legal advice'. The Conversation. 23 October.- Microsoft (2020), 'Microsoft launches Anti-Corruption Technology and Solutions (ACTS)'. 9 December.- Adomavicius, G., Mobasher, B., Ricci, F., & Tuzhilin, A. (2011). 'Context-Aware Recommender Systems'. AI Magazine, 32(3), 67-80.- McKinsey Global Institute (2019), 'Artificial intelligence in the United Kingdom: Prospects and challenges'. 10 June.- CMA&ICO (2021), 'Competition and data protection in digital markets: a joint statement between the CMA and the ICO'. 19 May.- DRCF (2022) 'Auditing algorithms: the existing landscape, role of regulators and future outlook'. [INSERT HYPERLINK TO OTHER PAPER]- ICO and The Alan Turing Institute (2020), 'Explaining decisions made with AI'.No date.- GPT-3 is a language model that performs a wide variety of natural language tasks, including autocompleting sentences-",2023
govuk_012,govuk,Ai Procurement 2,"This PPN provides optional questions to help identify the use of Artificial Intelligence in procurements, and in the delivery of government services. PDF,291 KB,8 pages This file may not be suitable for users of assistive technology. HTML This PPN has been updated to reflect new terminology introduced by the Procurement Act 2023 and the Procurement Regulations 2024. For procurements commenced and contracts awarded before this date, please refer to PPN 02/24.",2023
govuk_018,govuk,Algorithm Transparency 3,"Updated 8 May 2025 (c) Crown copyright 2025 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visitnationalarchives.gov.uk/doc/open-government-licence/version/3or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:psi@nationalarchives.gov.uk. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned. This publication is available at https://www.gov.uk/government/publications/guidance-for-organisations-using-the-algorithmic-transparency-recording-standard/algorithmic-transparency-recording-standard-guidance-for-public-sector-bodies This guidance explains what the Algorithmic Transparency Recording Standard (ATRS) is, why it matters and how public sector organisations should use it. It includes section-by-section guidance for completing theATRStemplate. The Algorithmic Transparency Recording Standard (ATRS) enables public sector organisations to publish information about the algorithmic tools they are using and why they are using them. It consists of a template for organisations to fill in with key information about their algorithmic tools. This information is then published on the GOV.UK repository in the form of anATRSrecord. By using theATRS, public sector organisations can: TheATRSis a core part of the government'sBlueprint for Modern Digital Government, in particular the promise to 'Commit to transparency, drive accountability'. An algorithmic tool is a product, application, or device that supports or solves a specific problem using complex algorithms. We use 'algorithmic tool' as an intentionally broad term that covers different applications of artificial intelligence (AI), statistical modelling and complex algorithms. An algorithmic tool might often incorporate a number of different component models integrated as part of a broader digital tool. TheATRSis mandatory for certain organisations, and certain algorithmic tools within those organisations. It is mandatory for all government departments, and for ALBs which deliver public or frontline services, or directly interact with the general public. Within those organisations, theATRSis mandatory for algorithmic tools which have a significant influence on a decision-making process with public effect, or directly interact with the general public. This scope is designed to emphasise context, focusing on situations where a tool is influencing specific operational decisions about individuals, organisations or groups, not where a tool is an analytical model supporting broad government policymaking. Further detail, including examples of algorithmic tools in and out of mandatory scope, can be found in the scope and exemptions policy . If your organisation is within the mandatory scope of theATRSpolicy, it should have a single point of contact (SPOC) whose role is to coordinate with theATRSteam on identifying in-scope algorithmic tools, drafting and publishing records. You can email theATRSteam onalgorithmic-transparency@dsit.gov.ukif you are unsure who yourSPOCis. However, theATRSis recommended by the Data Standards Authority for use across the entire public sector and we have welcomedATRSrecords from local government, police forces and other broader public sector organisations. If you are from such an organisation, you can complete anATRStemplate and email it toalgorithmic-transparency@dsit.gov.ukdirectly. We recommend assigning a lead at your organisation to collate the relevant information from internal teams (and third-party providers, if applicable), to oversee the drafting and completion of the record, and to manage contact with theATRSteam. As outlined above, if your organisation falls within the mandatory scope of theATRS, aSPOCwill have been assigned. You should contact yourSPOCbefore beginning work on anATRSrecord. Email us onalgorithmic-transparency@dsit.gov.ukif you are unsure who yourSPOCis. If your supplier holds information that you need to complete a record, we encourage you to ask your commercial contact for the relevant details, explaining why you are asking for this information and why algorithmic transparency is important in the public sector. If your organisation and the tool is within mandatory scope of theATRSpolicy, you should highlight this. If the supplier is reluctant to share some information with you based on concerns around potentially revealing intellectual property, it can help to walk the supplier through the questions asked in the template, explain how they are designed to provide only a high-level picture of the tool. TheATRShas been designed to minimise possibly security or intellectual property risks that could arise from publications. Thescope and exemptions policy, modelled on theFOIAct, provides a detailed framework for exempting information from individualATRSrecords, or entireATRSrecords from publication. In general, publishing anATRSrecord and redacting certain fields with a brief explanation of why this has been done is preferable to not publishing anATRSrecord at all, particularly when partial information about the algorithmic tool is already in the public domain. Considerations for limiting the information in certain fields include: TheATRStemplate is available in two formats: an Excel version and a Google Sheets version for browser.Both can be downloaded here. Please do not alter or change the format of the template as this may affect our ability to process and publish yourATRSrecord. TheATRStemplate is divided into 2 tiers. Tier 2, whilst still accessible to the general public, is aimed at specialist audiences such as civil society, journalists, academic researchers and other public sector organisations wishing to learn from their peers. TheATRSaims to deliver meaningful transparency around public sector uses of algorithmic tools. This means not just acknowledging the existence of such tools, but providing an intelligible explanation of how and why they are being used. You should aim to complete theATRStemplate in full sentences, in clear and simple language. You may consider sharing the draft record with teams who are not connected to the algorithmic tool to check for understandability. For examples of existingATRSrecords which may help you complete the template, consult therepository. Fictional examples are also included in the guidance below. Tier 1 asks for basic, high-level information about the algorithmic tool aimed at a general audience without technical knowledge. All fields should be completed. The tool name will also appear in the title of yourATRSrecord, and will help people navigate theATRSrepository. It should be clear, concise and consistent throughout. Your description should be brief and clear, focusing on what the tool is and why it is being used (rather than technical detail of how it works, which comes later in the record). Remember that theATRSaims to show the public when and why algorithmic tools are being used in processes that affect them. Ideally the description should be no more than two or three sentences. Not all algorithmic tools will have a relevant website. If providing one, please ensure it is live and publicly accessible - otherwise, enter 'N/A'. The email address you provide should be that of the team responsible for the tool, not an individual, for business continuity and security purposes. When an individual leaves the organisation but the wider team remains, the email address will still be up to date. LeaseSureAIuses machine learning to analyse council housing rent accounts and create a prioritised caseload of rent arrears for housing officers. The tool is designed to work alongside existing housing management systems within the council to help improve arrears management. This section focuses on accountability for the development and deployment of the tool. All fields should be completed, with 'N/A' where necessary. TheSROshould be a role title, not a named individual, for business continuity and security purposes. It should be the role which is ultimately accountable for the tool in an operational context. This may be the policy or service owner, for example. Third parties include commercial suppliers and other public sector organisations who may, for example, have developed an in-house algorithmic tool which they are sharing with your organisation. A procured tool can involve multiple companies at different places in the supply chain. For instance, a public body could procure a tool from a company, which in turn procured the model and data from another company before integrating the model into a customisable tool. Ideally, you should describe those different supplier relationships as clearly and concisely as possible, detailing which organisation was or is responsible for which part of the final tool that you are deploying. Yes Sulentra Dynamics Ltd. 813004659779X Sulentra Dynamic has provided LeaseSureAIfor a six-month pilot. Proof-of-concept pilot (a formal procurement process will follow if the tool demonstrates measurable benefit after the trial period). Sulentra Dynamics Ltd. has been provided with controlled, read-only access to renting accounts data in the council's MundioTenancy platform, but it does not integrate with other systems. This has been done in compliance with data protection legislation and all Sulentra Dynamics Ltd. staff with access to the data have been subject to appropriate vetting checks. Access to the data is only granted for the limited period of time while the tool is developed. This section expands on the high-level description given in Tier 1, with more granular detail about the algorithmic tool, its scope and the justification for its use. In contrast to the basic description of the tool in Tier 1, which focuses on what the tool is and why it is being used, the Tier 2 detailed description here aims to explain how the algorithmic tool works. As such, you should describe the tool's purpose, its intended users, key aspects and functions at a more granular level. You should also include the tool's scope, as well as limitations or context where it does not apply. Whilst the amount of information provided here will vary between algorithmic tools, we typically expect a paragraph or two of text. LeaseSureAImonitors tenant payment patterns and predicts financial distress using models like Logistic Regression and Random Forest Classifier on historical rent data. It utilises the Logistic Regression (LR) model first to analyse changes in rent payment patterns (e.g. type and date of payment) and predicts the probability of falling into arrears. TheLRmodel produces a list of 'at risk' accounts, which is then analysed further by the Random Forest Classifier (RFC) model. Based on features such as payment trends and arrears duration (e.g. '30-Day Arrear', '60-Day Arrear', '90-Day Arrear', etc.), it classifies accounts into 'Low', 'Medium', and 'High' risk. The output of the tool is a weekly prioritised caseload that is integrated into the council's existing MundioTenancy platform for housing officers to review and action. You may choose to provide a list of individual benefits or a few sentences of description. Where possible, try to explain how and why the tool should deliver the benefit. For example, rather than just stating 'improved customer experience', explain how and why the tool should achieve this. Your algorithmic tool may have replaced a legacy tool or a manual process. In either case, you should provide a brief description of what it replaced. If your algorithmic tool is part of a brand-new process (for example, delivering a programme which did not previously exist), you should make this clear. Briefly explain why no alternatives were considered. For example, you may be using an algorithmic tool provided by a central government department for others to use. This section should help people understand how the algorithmic tool ultimately helps to deliver an operational process or service, and how humans are involved in this delivery. We typically expect a paragraph or two of text. It can be helpful to frame the answer around the output that the tool produces and how this is then used, for example to determine the outcome of an application process, or to deliver a public service. You should aim to make clear the degree of automation that the tool delivers within the broader process. LeaseSureAIdoes not automate decisions. Instead, it provides a recommended list of priority cases, which are categorised as 'Low', 'Medium' or 'High', based on their risk of falling into payment arrears. Each case on the list includes reasons, such as 'Escalating 60-Day Arrears', 'Payment Arrangement Broken', or 'Benefit Reduction Detected', that then help housing officers interpret the tool's outputs effectively. Housing officers review the list weekly and record any actions taken (e.g. sending notifications of payment arrears to tenants) directly in the council's tenancy management platform, MundioTenancy. You should consider both the outputs of the algorithmic tool itself and whether they can be challenged or appealed, and the outputs of the broader operational process and whether they can be challenged or appealed. This may involve providing a link to a public appeal or contact form. If no appeals or review process is necessary or relevant for your tool, include a short sentence explaining why you are not completing this section. You should also be aware of Article 22 UKGDPRwhich states that 'The data subject shall have the right not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her or similarly significantly affects him or her'. If your algorithmic tool falls within the scope of these provisions, you must complete this field. Further information can be found via theICO's guidance on 'Rights related to automated decision making, including profiling'. This section should detail the technical specifications of the algorithmic tool. As outlined above, the level of detail here should not infringe on a supplier's intellectual property rights, or generate cybersecurity risks. You should broadly describe how your tool is organised and provide context for the rest of the technical section. System representations such as AWS diagrams are ideal for conveying this type of information in a concise way - they capture the primary components of your technology stack and how they interact. You should think about the end-to-end process by which your tool processes inputs, the digital services and resources that it uses and the environments in which system processes occur. Any models that you consider later should be mentioned in this field. You can see a helpful example of the diagram of system architecture provided by the Department for Health and Social Care in their algorithmic transparency report for theQCovid tool here. For tools that consist of multiple machine learning models, this will be the primary input into or output from the system as a whole. For tools that consist of only one machine learning model, the system-level input and output, and the model input and output, may be the same. These fields should include the expected formats and data types. Both historical and near real-time structured, tabular tenancy-related and financial data such as rent transaction history, payment type and date history, payment due dates, broken promise amounts, rent status, etc. The tool's output is a prioritised caseload of accounts that are ranked by risk of non-payment - i.e. 'High', 'Medium' and 'Low'. The output list is delivered to, and integrated with, the council's interactive MundioTenancy platform in the form of an interactive caseload, with the option to export it in CSV or Excel file formats for reporting and audit purposes. This section should detail the model or models used within the algorithmic tool. Should your tool consist of more than one model, please duplicate this sheet and complete a separate sheet for each individual model. N.B. For off-the-shelf models that your organisation has not trained, validated, tested, or applied any refinement techniques to (e.g. Web UI-based LLMs), please leave both the Model Specification (2.4.2) and following Development Data Specification (2.4.3) sections blank and move straight to the Operational Data Specification (2.4.4) section instead. For tools that consist of more than one model, please make and complete copies of the Model Specification (2.4.2) section in the template for each model. As a minimum, the fields in this section should include the type of model. If using a pre-trained model, please also specify the name of the API provider, where applicable, or mention if it is 'self-hosted'. Using Logistic Regression from the scikit-learn library in Python, which has pre-defined parameters. Further details can be found on the scikit-learn website Whereas the system architecture refers to how the model is integrated into the broader technical architecture, while model architecture describes the internal structure of the model - i.e. how it works or how it transforms an input into an output. At a minimum, you should enter the type of model used (e.g. Logistic Regression, Decision tree, Random Forest Classifier, Convolutional Neural Network, Rule-Based System, etc.). If the model has been designed such that certain features or inputs are given more priority over others, and where this has significant bearing on the model's output, then indicate what those features are. For rule-based systems, describe how the rules are structured and indicate if any rules are weighted or prioritised over others. You may also provide a publicly accessible link to further resources. For security, do not include details of the network architecture to which the tool is connected. If it aids understanding of the model, you are also encouraged to provide further details or provide a link to publicly available resources that offer further information. Using Logistic Regression from scikit-learn library in Python, which has pre-defined parameters. NV-Administration is an optimisation-based automated planning model. The model consists of: Performance metrics will differ based on what type of method and tool you are developing or deploying. Useful metrics to consider may include accuracy metrics such as precision, recall or F1 scores, metrics related to privacy, and metrics related to computational efficiency. You should also describe any bias and fairness evaluation you have undertaken (i.e. model performance over subgroups within the dataset), and any measures taken to address issues you identify. For more information about setting performance metrics, you may find thisGOV.UK Service Manual guidancehelpful. Useful resources may include the government'sAIPlaybookand the former Centre for Data Ethics and Innovation'sReview into bias in algorithmic decision-making, especially chapters 1 and 2. For more information about bias in algorithmic decision-making, see theRTAU(formerly Centre for Data Ethics and Innovation) review into bias in algorithmic decision-making, especially Chapters 1 and 2. For more information about how to mitigate bias in algorithmic decision making, you may find it helpful to review theRTAU's repository of bias mitigation techniques which can be found here. This section aims to expand on '2.4.2.8 Datasets and their purposes' in the Model Specification section. It focuses on the data used to train, validate or test your model(s). Provided you have not trained, validated, tested or applied any refinement techniques to an off-the-shelf model (e.g. Web UI-based LLMs), you should leave the Development Data Specification section blank and move straight to the (Operational Data Specification) section. The aim of this field is to describe all of the datasets used for developing the tool as a whole. Where possible, please provide publicly accessible links to these datasets. (This differs from the datasets and their purposes field in the Model Specification, which simply asks for a list and specification of what each dataset was used for). The purpose of the 'data quantities' field is to sense-check proportionality of data in relation to the model task and complexity. Where a learning algorithm is applied to data, small datasets with few samples are more likely to yield underfitting models, while large datasets with numerous attributes may cause overfitting. In addition, too few samples may indicate insufficient representation of a target population, and too many attributes may indicate increased data security risks (such as de-identification). While we don't prescribe a specific definition of 'sensitive', we encourage you to consider: In certain cases, it might not be feasible to disclose all the sensitive attributes in the data. At a minimum, you should disclose the fact that you are processing sensitive data and add as much detail as appropriate. It is unlikely that theATRSrecord will lead to individuals being made identifiable as you are only being asked to provide a general description of the types of variables being used. If you are considering making the dataset you are using openly accessible and linking to it, you should comply with the relevant data protection legislation to prevent individuals from being made identifiable from the dataset. This should also be considered as part of a Data Protection Impact Assessment (DPIA). For further guidance on completingDPIAs, please refer to theICO'sguidance. You may also find it helpful to consult theICO'sAIand data protection risk toolkit. This section focuses on the data used or produced in your algorithmic tool's real-world operation, such as user inputs, retrieved documents, system-generated logs and other data generated during use. See above. This section should provide information on impact assessments conducted, identified risks, and mitigation efforts. No, there is no need to provide a summary if you are providing an openly accessible link to the full assessment. Categories of risk likely to be relevant include: This list is not exhaustive and there may be additional categories of risk that are helpful to include. The Government Finance Function'sOrange Bookprovides further guidance on conducting risk assessments for public sector projects. You may also find it helpful to consult theICO'sAIand data protection risk toolkit. Review and feedback Email your completedATRStemplate toalgorithmic-transparency@dsit.gov.uk(or send to yourSPOC, if your organisation has one). TheATRSteam will check for readability and provide feedback or suggested amendments if necessary. Before finalising your record, you should consider the possibility that publishing information on your algorithmic tool may invite attention and scrutiny from the public and media. This is particularly true for more high-profile use cases and where the use of an algorithmic tool has not been publicly disclosed before. You can help to mitigate these risks by ensuring you provide clear information and an appropriate level of detail in your record. You should also ensure that your organisation's communications team or press office is aware of the plan to publish, has reviewed the record and has prepared to respond to media requests if deemed necessary. If your organisation has aSPOC, you should ask them to coordinate this. You may wish to consider publishing supplementary information, for example a blog post explaining what the algorithmic tool is, or a link to theATRSrecord on the relevant service or policy pages on your website. TheATRSteam requires written confirmation that yourATRSrecord has gone through all appropriate internal signoff procedures before publishing it to the GOV.UK repository. At a minimum, this should include clearance by: In certain high-profile instances it may be appropriate to seek ministerial clearance. Should substantive details change in relation to yourATRStool, you should update theATRStemplate, go through internal clearance again, and send the updated template toalgorithmic-transparency@dsit.gov.ukasking for your record to be updated accordingly. Substantive changes might include a pilot tool moving to production, new datasets being used to train or refine the tool, or a change to the broader operational process of which the tool is part. Should you decommission an algorithmic tool for which you have published anATRSrecord, contact the team onalgorithmic-transparency@dsit.gov.uk.",2023
govuk_002,govuk,Ai Ethics 2,"Understand how to use artificial intelligence ethically and safely. This guidance introduces AI ethics and provides a high-level overview of the ethical building blocks needed for the responsible delivery of an AI project. It is part of awider collection about using artificial intelligence (AI) in the public sector. AI has the potential to make a substantial impact for individuals, communities, and society. To make sure the impact of your AI project is positive and does not unintentionally harm those affected by it, you and your team should make considerations of AI ethics and safety a high priority. The following guidance is designed to complement and supplement the Data Ethics Framework. The Data Ethics Framework is a tool that should be used in any project. Read this guidance on Understanding artificial intelligence ethics and safety",2023
govuk_005,govuk,Ai Fairness 0,"Published 14 June 2023 (c) Crown copyright 2023 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visitnationalarchives.gov.uk/doc/open-government-licence/version/3or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:psi@nationalarchives.gov.uk. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned. This publication is available at https://www.gov.uk/government/publications/enabling-responsible-access-to-demographic-data-to-make-ai-systems-fairer/report-enabling-responsible-access-to-demographic-data-to-make-ai-systems-fairer The use of artificial intelligence (AI), and broader data-driven systems, is becoming increasingly commonplace across a variety of public and commercial services.[footnote 1]With this, therisks associated with biasin these systems have become a growing concern. Organisations deploying such technologies have both legal and ethical obligations to consider these risks. TheWhite PaperonAIRegulation, published in March 2023, reinforced the importance of addressing these risks by including fairness as one of five proposed key regulatory principles to guide and inform the responsible development and use ofAI. Many approaches to detecting and mitigating bias require access to demographic data. This includes characteristics that are protected under the Equality Act 2010, such as age, sex, and race, as well as other socioeconomic attributes.[footnote 2] However, many organisations building or deployingAIsystems struggle to access the demographic data they need. Organisations face a number of practical, ethical, and regulatory challenges when seeking to collect demographic data for bias monitoring themselves, and must ensure that collecting or using such data does not create new risks for the individuals that the data refers to. There is growing interest in the potential of novel approaches to overcome some of these challenges. These include techniques to generate synthetic training data that is more representative of the demographics of the overall population, as well as a variety of governance or technical interventions to enable more responsible data access. Access to demographic data to address bias is important for those working across theAIlifecycle, including organisations developing, deploying and regulatingAI. This report primarily explores approaches with the potential to assist service providers, i.e. those who are deploying data-driven systems (includingAI) to offer a service, to responsibly access data on the demographics of their users to assess for potential bias. This has led us to focus on two contrasting sets of promising data access solutions: data intermediaries and proxies. Of course, these approaches may have relevance to other parties. However, we have not considered in detail techniques such as synthetic generation of training data, which are specifically relevant to developers. Data intermediary is a broad term that covers a range of different activities and governance models for organisations that facilitate greater access to or sharing of data.[footnote 3]TheNational Data Strategyidentified data intermediaries as a promising area to enable greater use and sharing of data, andCDEIhas previously published areportexploring the opportunities they present. There is potential for various forms of data intermediary to help service providers collect, manage and/or use demographic data. Intermediaries could help organisations navigate regulatory complexity, better protect user autonomy and privacy, and improve user experience and data governance standards. However, the overall market for data intermediaries remains nascent, and to our knowledge there are currently no intermediaries offering this type of service in the UK. This gap may reflect the difficulties of being a first mover in this complex area, where demand is unclear and the risks around handling such data require careful management. If gathering demographic data is difficult, another option is to attempt to infer it from other proxy data already held. For example, an individual's forename gives some information about their gender, with the accuracy of the inference highly dependent on context, and the name in question. There are already some examples of service providers using proxies to detect bias in theirAIsystems.[footnote 4] Proxies have the potential to offer an approach to understanding bias where direct collection of demographic data is not feasible. In some circumstances, proxies can enable service providers to infer data that is the source of potential bias under investigation, which is particularly useful for bias detection.[footnote 5]Methods that draw inferences at higher levels of aggregation could enable bias analysis without requiring service providers to process individually-identifiable demographic data. However, significant care is needed. Using proxies does not avoid the need for compliance with data protection law. Inferred demographic data (and in some cases proxy data itself) will likely fall under personal or special categories of data under the UKGDPR. Use of proxies without due care can give rise to damaging inaccuracies and pose risks to service users' privacy and autonomy, and there are some cases in which the use of proxies is likely to be entirely inappropriate. Inferring demographic data for bias detection using proxies should therefore only be considered in certain circumstances, such as when bias can be more accurately identified using a proxy than information about an actual demographic characteristic, where inferences are drawn at a level of aggregation that means no individual is identifiable, or where no realistic better alternative exists. In addition, proxies should only be used with robust safeguards and risk mitigations in place. In the short term, direct collection of demographic data is likely to remain the best option for many service providers seeking to understand bias. It is worth emphasising that, in most circumstances, organisations are able to legally collect most types of demographic data for bias detection provided they take relevant steps to comply with data protection law. Where this is not feasible, use of proxies may be an appropriate alternative, but significant care is needed. However, there is an opportunity for an ecosystem to emerge that offers better options for the responsible collection and use of demographic data to improve the fairness ofAIsystems. In a period where algorithmic bias has been a major focus in academia and industry, approaches to data access have received relatively little attention, despite often being highlighted as a major constraint. This report aims to highlight some of the opportunities for responsible innovation in this area. This kind of ecosystem would be characterised by increased development and deployment of a variety of data access solutions that best meet the needs of service providers and service users, such as data intermediaries. This is one area thatCDEIis keen to explore further through the Fairness Innovation Challenge announced in parallel to this report. However, this is only a partial answer to the genuine challenges in this area. Ongoing efforts by others to develop a robust data assurance ecosystem, ensure regulatory clarity, support research and development, and amplify the voices of marginalised groups are also crucial to enable a better landscape for the responsible use of demographic data. Over the last year,CDEIhas been exploring the challenges around access to demographic datafor detecting and mitigating bias inAIsystems, and the potential of novel solutions to address these challenges. Organisations who useAIsystems should be seeking to ensure that the outcomes of these systems are fair. However, many techniques for detecting and mitigating bias inAIsystems rely on access to data about the demographic traits of service users, and many service providers struggle to access the data they need. Despite this, with a few notable exceptions, the topic has received relatively little attention.[footnote 6] In this report: This report has been informed by the work thatCDEIhas conducted over the last year, including: CDEIis grateful to those who contributed to these workshops, or otherwise contributed to this work. This report has been published alongside the announcement ofCDEI's Fairness Innovation Challenge. The challenge will provide an opportunity to test new ideas for addressingAIfairness challenges in collaboration with government and regulators. We hope that it will generate innovative approaches to addressing some of the data access challenges described here. Disclaimer: The information in this report is not intended to constitute legal advice. If you do require legal advice on any of the topics covered by this report, you should seek out independent legal advice. The use of data-driven systems, includingAI, is becoming increasingly commonplace across a variety of public and commercial services.[footnote 7]In the public sector,AIis being used for tasks ranging fromfraud detection to answering customer queries. Companies in the financial services, technology, and retail sectors also make use ofAIto understand customers' preferences and predict consumer behaviour. When service providers make use ofAIsystems in their services or decision-making processes, they can have direct and significant impacts on the lives of those who use these services. As this becomes increasingly commonplace, the risks associated with bias in these systems are becoming a growing concern. Bias inAIsystems can lead to unfair and potentially discriminatory outcomes for individuals. In 2020,CDEIpublished itsReview into Bias in Algorithmic Decision-Making, which explored this topic in detail. In March 2023, the government published theWhite PaperonAIregulation, which included fairness as one of five key proposed principles that might guide and inform the responsible development and use ofAI. The fairness principle states thatAIsystems should not undermine the legal rights of individuals or organisations, discriminate unfairly against individuals or create unfair market outcomes. The fairness principle considers issues of fairness in a wider sense than exclusively in terms of algorithmic bias, but addressing bias would be a key consideration in implementing it. In some circumstances, bias inAIsystems can lead to discriminatory outcomes. TheEquality Act 2010is the key UK legislation related to discrimination. It protects individuals from discrimination, victimisation and harassment and promotes a fair and more equal society. Age, race, disability, sex, gender reassignment, marriage and civil partnership, pregnancy and maternity, religion or belief and sexual orientation are all protected characteristics under the Equality Act 2010. WhereAIsystems produce unfair outcomes for individuals on the basis of these protected characteristics and are used in a context in scope of the act (e.g. the provision of a service), this might result in discrimination. Even when protected characteristics are not present in the training data,AIsystems still have the potential to discriminate indirectly by identifying patterns or combinations of features in the data, which enable them to infer these protected characteristics from other types of data. As noted inCDEI'sReview into Bias in Algorithmic Decision-Making, 'fairness through unawareness' is often not an effective approach. Service providers must address bias inAIsystems to ensure they are not acting unlawfully. Public sector service providers must also have due regard to advance equality of opportunity and eliminate discrimination under thePublic Sector Equality Duty(PSED). The Equality and Human Rights Commission (EHRC) has publishedguidancefor public bodies about how thePSEDapplies when they are usingAI, whichoutlinesthe need to monitor the impact ofAI-related policies and services. When processing personal data in relation toAI, service providers also have obligations relating tofairness under data protection law. The ICO has producedguidanceon how to operationalise the fairness principle in the context of developing and usingAI, as well as moretargeted guidancefor developers. Many approaches to detecting and mitigating bias inAIsystems require access to demographic data about service users. Demographic data refers to information about socioeconomic attributes. This includes characteristics that are protected under theEquality Act 2010, as well as other socioeconomic attributes such as socioeconomic status, geographic location, or other traits that might put people at risk of abuse, discrimination or disadvantage.[footnote 8] In some cases, service user demographic data might be compared to the datasets used to train a model in order to test whether the training data is representative of the population the model is being deployed on. In other cases, service user demographic data could be used to assess performance or make standardisations to identify where a model is treating individuals from different demographic groups differently. Access to good quality demographic data about a service's users is therefore often a prerequisite to detection, mitigation, and monitoring of bias, and an important first step in the fairness lifecycle. However,research byCDEIandothershas found that service providers currently face a range of legal, ethical and practical challenges in accessing the demographic data they need to effectively detect and mitigate bias in theirAIsystems. Routine collection of demographic data to improve the fairness ofAIis not common practice in either the public or private sectors, except in recruitment.[footnote 9]Without the ability to access demographic data about their users, service providers are severely limited in their ability to detect, mitigate, and monitor for bias, and thereby improve the fairness of theirAIsystems. Service providers are faced with a number of barriers when seeking to collect demographic data themselves. Concerns around public trust CDEI'sreview into bias in algorithmic decision-makingfound that some service providers think that the public do not want their data collected for the purpose of bias monitoring, and may be concerned why they are being asked for it. Evidence from public attitudes research thatCDEIhas conducted suggests that the public's willingness to share their data for bias monitoring varies depending on the organisation collecting it. Our2022 Tracker Surveyfound that 65% of the total respondents would be comfortable providing the government with demographic data about themselves in order to check if services are fair to different groups.Further researchwe conducted found that 77% of the public say they are not concerned with sharing their demographic data when applying for a job. The Tracker Surveyalso found that individuals were most reluctant to share their data with big technology and social media companies. Some companies havehighlighted this as a key challenge, suggesting that commercial organisations may need to provide additional safeguards to demonstrate their trustworthiness. Navigating regulatory compliance Most demographic data is also personal, and often special category, data under UK data protection legislation.[footnote 10]This data must be collected, processed and stored in a lawful, fair and transparent manner for specific, explicit and legitimate purposes only. Service providers must have a lawful basis and meet a separate condition for processing in order to process special category data underthe UKGDPR. In some circumstances, data controllers may be required to meet additional terms and safeguards set out inSchedule 1 of the Data Protection Act 2018. Schedule 1 includes a public interest condition around equality of opportunity or treatment, which is satisfied where processing certain kinds of special category data is ""necessary for the purposes of identifying or keeping under review the existence or absence of equality of opportunity or treatment between groups of people specified in relation to that category with a view to enabling such equality to be promoted or maintained"". This might provide a lawful basis for organisations to process special category data for bias detection and mitigation without requiring direct consent from individual data subjects.[footnote 11] Despite this, navigating the existing legal framework to process demographic data for bias detection and mitigation can be complex for service providers. Uncertainty around how equality and data protection law interact in this context can lead to misperceptions about what is or is not permitted under data protection law. TheCDEI'sReview into Bias in Algorithmic Decision-Makingfound that some service providers were concerned that collecting demographic data is not permitted at all under data protection law, or that it is difficult to justify collecting this data, and then storing and using it in an appropriate way. The ICO recently publishedguidanceto support service providers in navigating data protection law to address bias and discrimination inAIsystems. Data quality When used for bias detection and mitigation, inaccurate or misrepresentative data can be ineffective in identifying bias or even exacerbate existing biases, particularly when marginalised groups are poorly represented in the data. However,collecting good quality demographic data can be challenging in practice. Data collected directly from service users is likely to contain at least a degree of inaccuracy due to some users accidentally or intentionally misreporting their demographic traits. In addition, some users may choose to opt out of providing their data, leading to selection bias that results in a dataset that is not representative of service users. This selection bias may particularly impact individuals from groups who have experienced discrimination and marginalisation, who might be less comfortable sharing their data due to concerns about data privacy and misuse. Data collection expertise Collecting demographic data from service users requires establishing data collection procedures, and there is alack of clarity around how service providers should go about doing this. Setting up effective procedures that enable the collection of good quality data may require in-house expertise, which some service providers deployingAIsystems, particularly smaller organisations, may lack. Collecting and using demographic data for bias detection can also pose risks to the individual service users. Privacy Due to the sensitive and personal nature of demographic data, the collection and use of this data exposes individuals to risks of privacy violations. This is particularly problematic given that detecting and mitigating bias requires data on vulnerable and marginalised groups, who may be less comfortable sharing information on their demographic attributes given their disproportionate experiences of discrimination. This has beendescribed by someas a trade-off between 'group invisibility' and privacy. Misrepresentation When collecting demographic data, service providers have to decide which categories of data to collect and how this data will be disaggregated, and this can be challenging. Demographic categories are not static and tend to evolve over time with societal and cultural change. For example, the Race Disparity Unitrecently announcedthat the government would no longer use the demographic category 'BAME' (black, Asian, and minority ethnic), as it obscures meaningful differences in outcomes across ethnic groups. Ensuring that demographic categories remain up-to-date requires that service providers regularly update the data they collect to reflect such changes. In addition, when demographic categories are imposed on individuals, theyrisk misrepresentingthose who do not identify with them, further disempowering groups who are often already vulnerable and marginalised. There are also'unobserved' demographic characteristics, such as sexual orientation and gender identity, which can be fluid and are challenging to measure. Data theft or misuse The collection of demographic data by service providers increases the risk that this data is either stolen or intentionally misused. Cyberattacks by malicious actors could expose individuals to risks of information theft, which could be used for financial gain. Demographic data could also be intentionally misused by ill-intentioned actors for malicious purposes, such as identity theft, discrimination, or reputational damage. Concerns about data misuse may be particularly acute for individuals from demographic groups that have been historically marginalised or discriminated against. Due to the challenges that organisations face when collecting demographic data themselves, there is growing interest in novel approaches that could address these challenges and enable more widespread and responsible access to demographic data for bias detection and mitigation. The term 'access' is broad, and could involve: We have focused particularly on examples where a service provider is offering a service to users, and wants to understand how the outcomes of that service affect different groups. Though our interest in this area is driven by cases where a service or decision-making process is driven by data orAI, similar approaches to gathering data to monitor for potential bias are also relevant in other non data-driven contexts (e.g. monitoring the fairness of interview processes in recruitment). This has led us to a focus on two groups of potential approaches which seem applicable: data intermediaries and proxies. Data intermediaryis a broad term that covers a range of different activities and governance models for organisations that facilitate greater access to or sharing of data.[footnote 12]This can encompass a wide range of different stewardship activities and governance models. Data intermediaries can reduce risks and practical barriers for organisations looking to access data while promoting data subjects' rights and interests. Proxiesare inferences that are associated with and could be used in place of an actual demographic trait. For example, anindividual's postcode could be used as a proxy for their ethnicityor socio-economic status. Though the presence of proxies in algorithmic decision-making systems can be a source of bias, proxy methods could also be applied to infer the demographic characteristics of a service provider's users to enable bias monitoring. These two solutions offer contrasting approaches to the challenges surrounding data access, with differing opportunities and limitations. There has been significant interest in the concept of data intermediaries for some time, and there are a growing number of pilots and real-world examples of their use.[footnote 13]Despite this, data intermediaries have still not been widely adopted, nor used to enable access to demographic data for bias detection and mitigation in the UK. By contrast, proxies offer a relatively simple and implementable alternative to collecting demographic data, but careful consideration of legal and ethical issues is needed if they are to be used. By focusing on these two contrasting approaches, we will explore the range of possibilities in this space, capturing the scope of potential benefits and challenges that novel solutions have to offer. Some service providers, notably technology companies such as Meta andAirbnb, have started to experiment with these solutions in order to access demographic data to make theirAIsystems fairer. This experimentation with data intermediaries and proxies as a means of accessing demographic data demonstrates that they are perceived to be promising solutions to address the challenges surrounding data access. However, this also demonstrates an urgent need to better understand their potential and limitations. Data intermediary is a broad term that covers a range of different activities and governance models for organisations that facilitate greater access to or sharing of data.[footnote 14]Data intermediaries can perform a range of different administrative functions, including providing legal and quality assurances, managing transfer and usage rights, negotiating sharing arrangements between parties looking to share, access or pool data, and empowering individuals to have greater control over their data. Intermediaries can also provide the technical infrastructure and expertise to support interoperability and data portability, or provide independent analytical services, potentially using privacy-enhancing technologies (PETs). This range of administrative and technical functions is explored in detail inCDEI's 2021report exploring the role of data intermediaries. In simple terms, for the purposes of this report, a (demographic) data intermediary can be understood as an entity that facilitates the sharing of demographic data between those who wish to make their own demographic data available and those who are seeking to access and use demographic data they do not have. Some data intermediaries that collect and share sensitive data for research already operate at scale in the UK. One example isGenomics England, a data custodian that collects sensitive data on the human genome, stores it in a trusted research environment, and grants researchers access to anonymised data for specific research projects. Another prominent example is the Office for National Statistics'Secure Research Service, which provides accredited researchers with secure access to de-identified, unpublished data to work on research projects for the public good. As opposed to facilitating data sharing between service users and providers, these intermediaries provide researchers with access to population-level demographic datasets. Outside the UK, there are also limited examples of intermediaries being used to steward demographic data specifically for bias auditing. The USNational Institute of Standards and Technology (NIST)provides a trustworthy infrastructure for sharing demographic information (including photographs and personally identifying metadata on subjects' age, sex, race, and country of birth) for the purposes of testing the performance of facial recognition algorithms. By providing researchers with access to sensitive personal and demographic data that enables them to quality-assure algorithms for fairness, NIST has significantly expanded the evidence base on algorithmic bias and helped developers improve the performance of their facial recognition algorithms. Here we are focused on a different but related set of use cases. Can intermediaries help service providers to access demographic data about the individuals that interact with their service so that they can understand potential biases and differential impacts? Data intermediaries could play a variety of roles. Here, we primarily consider how they could support the collection, storage, and sharing of demographic data with service providers, though a third-party organisation could also take on an auditing role in certain circumstances. Intermediary models have emerged in other domains where large numbers of service providers have a need to provide common functions, and doing so in a consistent way is beneficial to consumer trust, user experience and/or regulatory compliance. Examples include: Some of the challenges that these intermediaries address are similar in nature to those above, so it is natural to ask the question of whether a similar model could emerge to address the challenges of demographic data access. Intermediaries could offer a number of potential benefits over direct collection of data by service providers, including: Various different types of organisations could act as demographic data intermediaries. For example: The type of organisation acting as an intermediary might have some implications for the type of demographic data intermediary service that is offered; given the sensitivity of the data concerned and the variety of different needs, an ecosystem where multiple options are available to service providers and users seems more desirable than a single intermediary service holding large amounts of data and attempting to meet all needs. There are a variety of different models for the role that intermediaries could play in supporting access to demographic data. To describe these potential roles, we have used the following terms: There are then two different roles specifically related to demographic data which often do not exist today: In the different data intermediary models described below, these roles might or might not be played by the same organisation. Potential model 1 One model could be for a data intermediary to collect and steward demographic data on behalf of a service provider, sharing this with them so they can audit their model for bias. This could potentially operate as follows: Diagram depicting indicative relationships and data flows for a data intermediary collecting and managing data on behalf of a service provider. There are examples of somewhat analogous models being used to enable safe research access to population-level datasets including some demographic data, for example theONS Secure Research ServiceorOpenSAFELY. We are not aware of any similar examples targeted at users of individual services, but it is feasible that a suitably trusted organisation could provide a similar kind of service collecting user or customer demographic data, and sharing this with service providers. Potential model 2 Beyond collection and management of demographic data, there is a growingecosystem ofAIassurance service providersseeking to provide independent bias audits using such data. A potential variant of the intermediary model described above is for such a bias audit provider to also act as an intermediary collecting the data. In this model, the intermediary acts as custodian of users' demographic data, collecting and storing it in a secure environment (as in model 1), but then also auditing service providers' models without ever giving them access to the data itself. This provides an additional layer of separation between the service provider and the demographic data about their customers as compared to the previous model, i.e. they receive only results of an audit, not any demographic data itself. For this model to be feasible, a service provider will typically need to provide additional internal service-related data to the bias audit provider, and therefore is likely to require appropriate legal and technical protection for personal data and intellectual property contained within it. Diagram depicting indicative relationships and data flows for a data. We have identified a couple of real world examples of similar approaches to this in the context of fairness: Potential model 3 As an alternative variant of model 1, there are a variety of intermediary models that seek to give users stronger personal control over how their data is used; variously referred to as personal data stores (PDSs), personal information management systems (PIMS) or data wallets. Each of these offers a form of decentralised store of an individual's personal data controlled by that individual. There are several such platforms already in existence, including commercial companies such asWorld Data Exchange, community interest companies likeMyDex, or cooperatives likeMiData, or open specifications such asSOLID. In this context, such platforms could allow individual data subjects to manage and maintain their own demographic data and share it with service providers for bias audit and mitigation on their own terms. Diagram depicting a service user collecting and managing their own demographic data in a personal data store, and sharing it with a service provider. Common features These contrasting models demonstrate the variety and breadth of data intermediaries that could support access to demographic data for bias detection and mitigation. They are not exhaustive or mutually exclusive, and their features could be changed or adapted. It is unlikely that one solution will suit every sector and group of stakeholders, and an ecosystem offering a combination of different demographic data intermediary types could be the most efficient and effective way to support the responsible use of demographic data for bias monitoring. There are additional legal mechanisms and technical interventions that could be integrated into any of these models to provide additional protections for service users who share their data. Novel data governance mechanisms could provide service users with more autonomy over how their demographic data is used. These includedata trusts(mechanisms for individuals to pool their data rights into a trust in which trustees make decisions about data use on their behalf) anddata cooperatives, in which individuals can voluntarily pool their data and repurpose it in their interests. While such mechanisms have been proposed by academics for some time, there have recently been a number of schemes to pilot them in real-world settings. These include the Data Trusts Initiative'spilot projects, the ODI'sdata trusts pilots, and the Liverpool City Region'sCivic Data Cooperative. Pilots like these indicate a shift towards the development and use of novel data governance mechanisms in practice. Data intermediaries could also integrate the use of technical interventions like PETs to provide stronger privacy and security protections. Large technology companies likeAirbnband Meta have experimented with the use of third parties to access demographic data using privacy-preserving techniques, including secure multi-party computation and p-sensitive k-anonymity, to better protect the privacy of their users. Despite offering a range of potential benefits, such an ecosystem of data intermediaries has not yet emerged. To the best of our knowledge, there are currently no intermediaries providing services specifically designed to support service providers to access demographic data from their users to improve the fairness of theirAIsystems in the UK. Our work suggests that the potential of data intermediaries to enable access to demographic data is constrained by a range of barriers and risks. The absence of organisations offering this type of service suggests that there is not sufficient incentive for such data intermediaries to exist. Incentives might be commercial (i.e. confidence that offering such a service would be a viable commercial proposition), but might also be broader, for example an opportunity for a third sector organisation to support fairness. What drives this absence? Demand among service providers and users for third-party organisations sharing demographic data is unclear. Given the relative immaturity of the market for data intermediaries, there may be a lack of awareness about their potential to enable responsible access to demographic data. In addition, the incentives driving data sharing to monitorAIsystems for bias are primarily legal and ethical as opposed to commercial, meaning demand for demographic data intermediation services relies on service providers' motivation to assess their systems for bias, and service users' willingness to provide their data for this purpose. More broadly, the market for many kinds of data intermediary is still relatively nascent. In the EU, the 2022Data Governance Actintroduced new regulations for the 'providers of data intermediation services', requiring them to demonstrate their compliance with conditions placed on their economic activities. The UK government acknowledged in theNational Data Strategy Mission 1 Policy Frameworkthat there is currently no established market framework for the operation of data intermediaries in the UK and has committed to support the development of a thriving data intermediary ecosystem that enables responsible data sharing. The lack of commercial incentives for data intermediaries sharing demographic data, combined with the challenges of operating in this complex area, has created little impetus for first movers. In addition, in order to use their services to share sensitive data, service providers and users must have confidence that data intermediaries are trustworthy. Ourpublic attitudes researchindicates that one of the most common concerns members of the public have around data intermediaries is that third parties are not sufficiently trustworthy. Non-regulatory approaches, such as data assurance, could help to build confidence in data intermediaries and demonstrate their trustworthiness. TheODIdefines data assurance as ""the process, or set of processes, that increase confidence that data will meet a specific need, and that organisations collecting, accessing, using and sharing data are doing so in trustworthy ways"".Research by Frontier Economicssuggests the data assurance sector in the UK is nascent but growing, with approximately 900 firms currently offering a range of different data assurance products and services in the UK. Standards could provide one way to encourage consistent data governance and management across demographic data intermediaries. This could include adoption of mature and commonplace standards such asISO/IEC 27001, as well as other relevant data standards.[footnote 15]In addition, a number of relevant certification and accreditation schemes already exist, such asCoreTrustSeal,FairDataand theMyData Global Operator Award. These could help data intermediaries sharing demographic data to demonstrate their adherence to data protection and ethical standards. Despite the burgeoning ecosystem for data assurance in the UK, work to understand how assurance products and services could demonstrate the trustworthiness and support the uptake of data intermediaries is in its early stages. No one standard-setting body or certification scheme can cover all the areas required to effectively assure data intermediaries and, given their diversity, a one-size-fits all approach is unlikely to be appropriate. For this reason, a greater understanding of how existing assurance products and services can demonstrate the trustworthiness of data intermediaries, how well these can meet the needs of different stakeholders, and where there may be remaining gaps in the data assurance ecosystem is required. This could support third parties sharing demographic data to demonstrate their trustworthiness, and encourage uptake among service providers and service users. Of course, intermediaries gathering sensitive data of this nature must contend with many of the same challenges that a service provider would have managing the same data. Where data intermediaries are collecting and storing sensitive demographic information about service users, they still need to take steps to minimise the risk that personal data is stolen or intentionally misused; ourpublic attitudes researchfound that data theft and misuse was a common concern among respondents in relation to data intermediaries. In addition, any third party collecting demographic data must ensure the data they collect is good quality. Much like service providers seeking to collect this data themselves, third parties must contend with similar challenges around data accuracy and representativeness. Data intermediaries hold real promise as a means to enable responsible access to demographic data for bias detection and mitigation. They could promote the collection and use of demographic data in ways that supports the regulatory compliance of service providers, protects service user privacy and autonomy, and elicits public trust, while providing better user experience and higher standards than service providers collecting this data themselves. Despite this potential, a market for such services is yet to emerge. We have discussed some of the barriers to this above, but for a service provider that could benefit from this approach, the absence of third parties offering such services in the UK prevents this being a straightforward option at present. Longer term, there remains clear potential for intermediaries to play a useful role. There is a need for piloting potential solutions in this area to support the development of the market for data intermediaries, and demonstrate the opportunities to service providers and users that might use them. This is one area thatCDEIhopes to explore further in the Fairness Innovation Challenge announced in parallel to this report. Many organisations hold a range of data about individuals that they provide services to. However, for the reasons discussed above, they often do not hold some or all of the demographic data that they need to audit their own systems and processes for bias. In contexts where collecting this demographic data directly is hard, an alternative is to infer it from data that you already hold, using proxies for the demographic traits that you are interested in. Proxies can be a source of discrimination inAIsystems where algorithms are able to deduce protected characteristics from relevant data points. For example, most insurance pricing models include postcode as a factor for a variety of valid reasons. However, the mix of ethnic groups varies significantly between different postcode areas, and there is a risk that insurance pricing models indirectly treat individuals from certain ethnicities differently via the proxy of their postcode. This is one reason why monitoring for potential bias is increasingly important asAIsystems become more complex and sophisticated. Conversely, there is potential for proxies to be used to detect and address bias inAIsystems. Using data they already hold as proxies, service providers could infer the demographic traits of their service users, and use this data to detect bias in theirAIsystems. Examples of this include: The inferences you might make about demographic traits from such proxy data will inevitably not be fully accurate, and whether this accuracy is enough to be practically useful for bias monitoring will be dependent on both the proxy data that is available, and the use case. Proxies raise a number of challenging ethical and legal concerns. These are discussed in more detail below. There are a wide range of proxy methods and tools in existence, varying from relatively simple methods to more complex machine learning approaches. These can be used to infer different demographic attributes, although ethnicity and gender have been the most common target variables to date. Many proxy methods and tools involve inferring the demographic traits of identifiable individuals (see Example 2 below). However, some approaches avoid this by using personas (see Example 1 below) or by drawing group inferences in such a way that ensures individuals are not identifiable.[footnote 16] Many of the most popular proxy methods and tools have been developed in the US, although some have been trained on large datasets spanning multiple geographies. These methods and tools vary in their accessibility to service providers, with some available open source and other commercial tools requiring payment for access. Some of these methods and tools were developed specifically to assess bias and discrimination, such as RAND'sBayesian Improved Surname Geocoding(BISG). In recent years, a few prominent technology companies including Meta andAirbnbhave begun to pilot more advanced, privacy-preserving proxy methods with the explicit aim of generating demographic data to improve the fairness of theirAIsystems. The examples below provide three contrasting approaches to using proxies, demonstrating the breadth of possibilities for their use to enable bias monitoring. Example 1: Citizens Advice using name and postcode to infer ethnicity In 2022, Citizens Advice conductedexploratory researchto better understand whether people from ethnic minority backgrounds experience worse outcomes in the car insurance market than white consumers. To measure this, they conducted mystery shopping using 649 personas that varied by name and postcode, comparing the prices paid by shoppers with names that are common among people from different ethnic backgrounds and postcodes with different proportions of ethnic minority communities in the population. They found no significant difference in prices charged to people with different names in the same postcode area. However, average quotes were higher in areas where black or South Asian people make up a large proportion of the population, and this could not be explained by common risk factors such as crime rates, road accidents or levels of deprivation in the area. By using personas, Citizens Advice was able to assess a service for bias without requiring access to the personal data of service users. Although their methodology allowed them to test the outcomes of pricing mechanisms, Citizens Advice acknowledge that it cannot explain exactly why the outcomes they identified occurred. Example 2: Airbnb's Project Lighthouse using first name and photos of faces to infer perceived race Airbnb's Anti-Discrimination product team havedeveloped a privacy-by-design approachto infer the perceived race of their service users using their first name and an image of their face. By measuring inequities on the basis of perceived race, they aimed to account for the fact that discrimination often occurs because of people's perceptions of one another's race as opposed to their actual race. The team sent a k-anonymized version of this service user data to a research partner organisation, who was under a confidentiality agreement and had their systems reviewed by Airbnb security. The research partner assigned perceived race to service users and this data was returned to Airbnb, who perturbed the data to achieve a level of p-sensitivity (i.e. ensuring that each equivalence class in the dataset had at least p distinct values for a sensitive attribute) before storing it. Finally, this p-sensitised k-anonymised dataset was used to measure the acceptance rate gap between different perceived racial groups. By including a research partner and making careful use of privacy techniques, Airbnb's approach enables them to analyse whether hosts exhibit bias on the basis of perceived race while protecting the privacy of service users. Example 3: NamSor using first name and surname to infer gender and ethnicity NamSoris a commercial product which uses machine learning to infer ethnicity and gender from first names and surnames. Namsor SAS, the company who owns the product, suggests it can be used to measure gender or ethnic biases inAI-driven processes, and theyoffer a range of toolsto suit different customers, including API documentation, CSV and Excel files analysis, and developer tools. NamSor has processed over 7.5 billion names and is continually maintained with new training data.The company claimsit is the most accurate ethnicity and gender inference service in the world. Oneindependent, comparative studysupports this claim, suggesting the tool achieves an F1 score of 97.9%. There are a range of reasons why a service provider or technology developer might be motivated to use proxies rather than collect data directly. Despite some potential benefits, the use of proxies presents a number of legal and ethical risks, and practical challenges. Legal risk Most demographic data inferred through the use of proxies is likely to be classified as personal or special category data under the UKGDPR, and must be processed in accordance with data protection legislation. The ICO'sguidance around the legal status of inferred datastates that whether an inference counts as personal data or not depends on whether it relates to an identified or identifiable individual. In addition, it may also be possible to infer or guess details about someone which fall withinspecial categories of data.[footnote 17]Whether or not this counts as special category data will depend on the specific circumstances of how the inference is drawn. Given that the use of proxies to generate demographic data for bias detection involves the intentional inference of relevant information about an individual, proxy methods will likely involve the processing of special category data, regardless of whether these inferences are correct or not. Where proxies are used to infer demographic traits at a higher level of aggregation, such that inferences are drawn only about so-called 'affinity groups' and not specific individuals, theICO states thatthese inferences may also count as personal data depending on how easy it is to identify an individual through group membership. When using proxy methods to draw group inferences, service providers should still comply with the data protection principles, including fairness. The use of proxies may pose additional legal risks for service providers where they are unaware of their legal obligations with respect to inferences or find them difficult to interpret and apply in practice. Accuracy Proxies can generate inaccurate inferences which can obscure or even exacerbate bias inAIsystems when used for bias detection and mitigation. Ourpublic attitudes researchsuggests the accuracy of proxy methods is a key concern for members of the public. There are a number of distinct issues related to the accuracy of proxies. Privacy The use of individual-level proxies may interfere with service users' privacy as they reveal personal information about them. Privacy was a key concern about proxies among participants inour public attitudes study. The inference of some demographic traits may not interfere with privacy much, if at all. However, information relating to more sensitive demographic categories, which form part of the individual's private life, could seriously impede on the privacy of service users. This is supported by evidence from thepublic attitudes study, which found that members of the public are more comfortable with organisations inferring their age than they are other demographic traits, such as disability status or sexuality. The sensitivity of demographic traits may also be compounded by other contextual factors, like the individuals' attributes (e.g. if they are a child or otherwise vulnerable) or their circumstances (e.g. if they live in a homophobic environment). Transparency and user autonomy The use of proxies to infer demographic data is inherently less visible to service users than collecting demographic data directly from them. The low visibility of proxy use raises concerns around transparency and service users' autonomy. When processing personal or special category data for bias monitoring, service providers haveobligations related to transparencyunder the UKGDPR. The ICO has providedguidanceon the right to be informed, which is a key transparency requirement under the UKGDPR. Public trust Proxies are a controversial topic, and the public appear to be less comfortable with their use than with providing their data to a third party.Our public attitudes studyindicated that only 36% of respondents were fairly comfortable with the use of proxies, and 23% were uncomfortable. Levels of public comfort varied depending on the type of proxy, the target demographic trait, and the type of organisation using the proxies. Members of the public were particularly concerned about their privacy, the accuracy of the inferences, and the risks of data misuse. Accessibility The use of proxy methods relies on access to relevant proxy data. The type of proxy required will vary depending on the target variable but could include service user postcodes, names, social media data, or facial photographs. Some of this data may already be held by service providers but some may not. The accessibility of proxy data will place limitations on the applicability of different proxy methods. Data quality When used for bias detection, poor quality data can be ineffective in detecting biases or even introduce new ones, particularly when marginalised groups are poorly represented in the data. The ability to draw inferences that are useful for bias detection purposes therefore relies on access to good quality proxy data. Where service providers do hold data that could be used to infer demographic traits of interest, this data may be incomplete or inaccurate. Where poor quality proxy data is used to infer demographic information about service users, it will produce poor quality inferences. This raises related concerns aroundcompliance with the accuracy principleunder data protection law, which applies to input as well as output data. Proxies offer an alternative approach to accessing demographic data for bias detection and mitigation. Proxies can be a practical approach to bias detection for service providers who already hold relevant data, and can prevent the need for service users to provide their demographic data numerous times to different organisations. In some circumstances, proxies may be the best way for service providers to effectively analyse theirAIsystems for bias, particularly where the proxy is more helpful in identifying bias than the demographic trait itself. Methods that rely on personas or group inferences at a level of aggregation such that individuals are not identifiable may pose few privacy risks to individual service users. Despite this, the use of proxies poses a number of legal and ethical risks, as well as practical challenges. There are some cases in which the use of proxies is likely to be entirely inappropriate and should be avoided. Other methods, although not illegal, will likely involve the processing of special category data, which may entail legal risk for service providers. In addition, proxies can give rise to damaging inaccuracies and pose challenges to the privacy and autonomy of service users, and members of the public appear to be less comfortable with their use than other data access solutions. Proxies are therefore likely to be a viable solution to enable access to demographic data for bias detectiononly in certain circumstances, such as when bias can be more accurately identified using a proxy than information about an actual demographic characteristic, or where inferences are drawn at a level of aggregation that means no individual is identifiable. In addition, proxies shouldonly be used with robust safeguards and risk mitigations in place. Here, we set out the key ethical issues for service providers to consider when seeking to use proxies for bias detection and mitigation. Alongside these ethical considerations, service providers using proxies should consider their legal obligations by referring to the ICO's Guidance onAIand Data Protection, includingAnnex A 'Fairness in theAILifecycle'. Step 1: Establish a strong use case for the use of proxies as opposed to other alternatives This is central to ensuring the ethics of using proxy methods, and helps service providers to exclude the use of proxies where a reasonable, less intrusive alternative exists. There are certain demographic traits for which the use of proxies is not advisable. In particular, where service providers wish to test the system for bias relating to demographic traits that are unobservable, such as sexual orientation, they should seek an alternative approach. However, there are a limited number of scenarios in which the use of proxies to address bias may be justifiable. These include: The strength of these justifications should be weighed up in light of the risk that theAIsystem in question is biased, and the severity of the real-world impact of this bias. To make this assessment, knowledge of the context in which theAIsystem is being deployed is critical, and service providers should engage with civil society organisations and affected groups in determining whether using proxies is appropriate in any given use case. Service providers should also refer to the ICO'sGuidance on Data Protection andAIat this stage to establish that their proposed use of proxies is lawful. Step 2: Select an appropriate method and assess associated risks If a strong case for the use of proxies as opposed to other alternatives has been established, service providers need to select an appropriate proxy method and assess the risks and trade-offs associated with its use. There are a number of commercial tools and open source methods available to service providers. A non-exhaustive list of some methods that are applicable in the UK context can be found in thetechnical report by Frazer Nash. When selecting a method, service providers should consider: Testing the performance of proxy methods by conducting an independent review using a representative dataset to determine which may be most appropriate to use. Looking at historic data and current social and cultural trends to make predictions about likely model drift, and consider its implications for the need for model retraining or continuous learning. Alongside these considerations, service providers need to assess the feasibility of using the method or tool, including factors such as the availability and cost of the method or tool, the availability and quality of proxy data, and available resources and expertise within the organisation. Service providers should also consider conducting a risk assessment to assess the risks and trade-offs associated with the use of this method in the specific context they intend to use it in. They should also carefully consider the limitations of the method or approach they have chosen, and whether there are further actions they can take to overcome these limitations. Step 3: Design and develop robust safeguards and risk mitigations If an appropriate method is chosen and the risks and limitations of this method have been identified, service providers should consider the development of risk mitigations and safeguards, including: Measures to ensure model accuracy, such as regular monitoring of model performance and retraining or revalidation of the model at appropriate intervals. No set of safeguards will entirely eliminate the risks associated with the use of sensitive data and there will always be a degree of residual risk. Service providers should consider and document what that residual risk might look like, and whether it is proportionate compared to the established benefits of using the proxy method. This assessment would again benefit from engagement with civil society and affected groups. If residual risks are deemed acceptable given those benefits, the last step is to implement safeguards and proceed with the use of proxies. Otherwise, service providers may need to consider whether further safeguards might be required, or whether the use of proxies is justifiable at all. Residual risk should also be reviewed on an ongoing basis to ensure new risks associated with changes in context are captured and mitigated. The current landscape of options for accessing demographic data is not ideal, and has significant limitations. Organisations are required to navigate significant legal, ethical, and practical challenges to either collect demographic data or infer it via the use of proxies.Evidence suggeststhat members of the public are likely to feel more comfortable sharing their data when governance mechanisms offer them greater privacy and control over their demographic data, particularly in sectors where levels of public trust in data sharing are lower. In this section, we reflect on what needs to happen to improve this ecosystem, and make it easier for organisations to responsibly use demographic data to address bias. This requires the development and scaling up of ambitious data access solutions that best mitigate ethical risks, are most practical for service providers and users, and are trusted by members of the public. Data intermediaries are one promising area for further development, as well as complimentary governance mechanisms like data trusts and technical interventions such as privacy-enhancing technologies. As government, we have a key role to play in spurring responsible innovation in this area, and a variety of work is underway to support this. Firstly, although demographic data can already be legally processed by service providers for bias detection and mitigation, some organisations may find that the existing data protection framework is complex and difficult to navigate in this area. In September 2021, the government launched aconsultationon reforms to the UK's data protection laws, including seeking views on provisions relating to processing personal data for bias detection and mitigation.Respondents agreedthere should be additional legal clarity on how sensitive data can be lawfully processed for bias detection and correction, and some felt that introducing a new processing condition under Schedule 1 of the Data Protection Act 2018 would be beneficial. As outlined in thegovernment response, the government is introducing a statutory instrument to enable the processing of sensitive personal data for the purpose of monitoring and correcting bias inAIsystems, with appropriate safeguards. This measure fits in the wider approach the government is developing around this issue, as proposed in the White Paper onAIregulation, currently out for consultation. Regulators have already published relevantdata protectionandequalitiesguidance related toAI, as well as some data access solutions, includingproxiesandprivacy-enhancing technologies. However, greater clarity around service providers' equality obligations with respect to detecting and mitigating bias in theirAIsystems would be welcome, and could further incentivise service providers to take action to improve the fairness of their systems. Continued regulatory collaboration between the ICO,EHRCand relevant sectoral regulators will also be critical moving forward to ensure the responsible collection and use of demographic data to improve the fairness ofAIsystems, particularly where novel solutions to generate and share this data are being tested. The government also has an important role to play in incentivising innovation and supporting the development and scaling up of promising solutions to enable responsible access to demographic data.As announced alongside this report, theCDEIplans to run a Fairness Innovation Challenge to support the development of novel solutions to address bias and discrimination across theAIlifecycle. The challenge aims to provide greater clarity about which data access solutions andAIassurance tools and techniques can be applied to address and improve fairness inAIsystems, and encourage the development of holistic approaches to bias detection and mitigation, that move beyond purely technical notions of fairness. In theNational Data Strategy, the government also committed to support the development of a thriving data intermediary ecosystem by considering the role of competition, horizontal governance structures, and strategic investment in intermediary markets. The ongoing work in this area could serve to support the emergence of intermediaries that are able to play a useful role in this area. One specific area of focus for this work relevant here is support for the development of a data assurance ecosystem to ensure that new data access solutions, particularly data intermediaries, are trustworthy. There is a burgeoning ecosystem for data assurance in the UK but work to understand how such services could demonstrate the trustworthiness and support the uptake of data intermediaries is in its early stages. The ODI has publishedresearchexploring the data assurance landscapein support of the government's National Data Strategy. Further research could explore the extent to which the existing data assurance market can engender confidence in new data access solutions and meet the needs of different stakeholders, and identify potential gaps in the ecosystem. As discussed above, service providers should already be taking action to identify and address bias inAIsystems that they deploy. Those seeking to collect demographic data themselves should refer to guidance from the ICO around processing ofpersonal data, includingspecial category data, to ensure their collection and use of demographic data is legally compliant. In addition, the ONS has issuedguidancearound some demographic categories that service providers could use when seeking to collect data for the purposes of measuring equality. Service providers should give consideration to the ways in which they can mitigate the risks associated with demographic data collection, for example, by using more participatory and inclusive approaches to data collection. In some cases, proxies may be a more suitable alternative to collecting demographic data themselves. Service providers should refer to the key ethical considerations in the previous section of this report, as well as the ICO'sGuidance onAIand Data Protectionand other sector-specific guidance, to determine whether such approaches are appropriate and, if so, how they could be used responsibly. Given the growing imperative on many service providers to access demographic data, they should demand solutions from the market that better meet their needs, and the needs of their users, by embedding ethical best practice, supporting them to navigate regulation, and providing more practical services. Novel data governance approaches, such as data intermediaries, alongside complementary governance and technical interventions, could help to meet service providers' needs, and demand for these solutions could stimulate innovation in these areas. There is also an important role for the research community in providing continued research into and piloting of solutions to enable responsible demographic data access. More comparative studies of proxy methods using the same test datasets and performance criteria, particularly filtered or weighted accuracy scores, could help service providers to make better informed decisions as to whether such methods are sufficiently accurate for different demographics and acceptable for use to make assessments about fairness.Data quality is also a persistent challenge whether service providers collect demographic data themselves or access it using a generative method or through a third party. Further research into and piloting of novel approaches to improve data quality, such as participatory approaches to data collection, would be beneficial. Finally, civil society groups have a key role to play in informing and mobilising members of the public and ensuring that solutions and services to enable responsible access to demographic data protect their rights and interests. Demographic data is of vital importance in detecting and correcting bias inAIsystems, yet the collection and use of this data poses risks to individuals, particularly those from marginalised groups. Civil society groups can help to raise awareness among individuals, including members of marginalised communities, about the importance of access to demographic data in tackling bias, whilst simultaneously calling for the development of solutions and services that give people greater autonomy, protect their privacy, and are worthy of their trust. Crucially, civil society groups can also help to amplify the voices of marginalised communities in debates around the design and development of new solutions, ensuring they are consulted and their views accounted for. For example, see Bank of England,'Machine Learning in UK Financial Services', Local Government Association (LGA), 'Using predictive analytics in local public services', and NHS England, 'Artificial Intelligence'.- See theEHRC's 'Five components of data collection and analysis' (pg. 54) in'Measurement Framework for Equality and Human Rights'.- Definition fromCDEI's report,'Unlocking the value of data: Exploring the role of data intermediaries'.- Meta, 'How Meta is working to assess fairness in relation to race in the U.S. across its products and systems', Airbnb, 'Measuring discrepancies in Airbnb guest acceptance rates using anonymized demographic data'.- See Airbnb, 'Measuring discrepancies in Airbnb guest acceptance rates using anonymized demographic data', where Airbnb assessed bias on their platform on the basis of 'perceived race'.- Notable exceptions include thePartnership onAI's Workstreamon Demographic Data, as well as some academic scholarship, including Michael Veale and Reuben Binns,'Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data'(2017).- For example, see Bank of England,'Machine Learning in UK Financial Services', Local Government Association (LGA), 'Using predictive analytics in local public services', and NHS England, 'Artificial Intelligence'.- See theEHRC's 'Five components of data collection and analysis' (pg. 54) in'Measurement Framework for Equality and Human Rights'.- SeeCDEI, 'Review in bias in algorithmic decision-making' and Open Data Institute (ODI),'Monitoring Equality in Digital Public Services'.- Some protected characteristics, including race, ethnicity, disability, and sexual orientation, are also special category data under the UK General Data Protection Regulation (GDPR) and the Data Protection Act 1998.- The equality of opportunity condition (Schedule 1, 8.1(b) of the Data Protection Act 1998) does not cover all special category data (e.g. trade union membership is not included).- Definition fromCDEI's report,'Unlocking the value of data: Exploring the role of data intermediaries'.- Pilots include those by theOpen Data Institute (ODI),Data Trusts Initiative, and theLiverpool City Region Civic Data Cooperative.- Definition fromCDEI's report,'Unlocking the value of data: Exploring the role of data intermediaries'.- Including, for example, theISO/IEC CD 5259-1series, which is currently under development.- The ICO providesguidanceabout when group inferences are personal data.- Special category data includes personal data revealing or concerning data about a data subject's racial or ethnic origin, political opinions, religious and philosophical beliefs, trade union membership, genetic data, biometric data for the purpose of uniquely identifying a natural person, data concerning health, sex life and sexual orientation.-",2023
govuk_006,govuk,Ai Fairness 1,"Understand how to use artificial intelligence ethically and safely This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. The Office for Artificial Intelligence (OAI) and the Government Digital Service (GDS) have produced the following chapter's guidance in partnership with The Alan Turing Institute'spublic policy programme. This chapter is a summary ofThe Alan Turing Institute's detailed guidance, and readers should refer to the full guidance when implementing these recommendations. AI has the potential to make a substantial impact for individuals, communities, and society. To make sure the impact of your AI project is positive and does not unintentionally harm those affected by it, you and your team should make considerations of AI ethics and safety a high priority. This section introduces AI ethics and provides a high-level overview of the ethical building blocks needed for the responsible delivery of an AI project. The following guidance is designed to complement and supplement theData Ethics Framework. The Data Ethics Framework is a tool that should be used in any project. This guidance is for everyone involved in the design, production, and deployment of an AI project such as: Ethical considerations will arise at every stage of your AI project. You should use the expertise and active cooperation of all your team members to address them. AI ethics is a set of values, principles, and techniques that employ widely accepted standards to guide moral conduct in the development and use of AI systems. The field of AI ethics emerged from the need to address the individual and societal harms AI systems might cause. These harms rarely arise as a result of a deliberate choice - most AI developers do not want to build biased or discriminatory applications or applications which invade users' privacy. The main ways AI systems can cause involuntary harm are: The field of AI ethics mitigates these harms by providing project teams with the values, principles, and techniques needed to produce ethical, fair, and safe AI applications. The guidance summarised in this chapter and presented at length inThe Alan Turing Institute's further guidance on AI ethics and safetyis as comprehensive as possible. However, not all issues discussed will apply equally to each project using AI. An AI model which filters out spam emails, for example, will present fewer ethical challenges than one which identifies vulnerable children. You and your team should formulate governance procedures and protocols for each project using AI, following a careful evaluation of social and ethical impacts. You should establish ethical building blocks for the responsible delivery of your AI project. This involves building a culture of responsible innovation as well as a governance architecture to bring the values and principles of ethical, fair, and safe AI to life. To build and maintain a culture of responsibility you and your team should prioritise 4 goals as you design, develop, and deploy your AI project. In particular, you should make sure your AI project is: Prioritising these goals will help build a culture of responsible innovation. To make sure they are fully incorporated into your project you should establish a governance architecture consisting of a: You should understand the framework of ethical values which support, underwrite, and motivate the responsible design and use of AI. The Alan Turing Institute calls these 'the SUM Values': These values: You can read further guidance on SUM Values inThe Alan Turing Institute's comprehensive guidance on AI ethics and safety. While the SUM Values can help you consider the ethical permissibility of your AI project, they are not specifically catered to the particularities of designing, developing, and implementing an AI system. AI systems increasingly perform tasks previously done by humans. For example, AI systems can screen CVs as part of a recruitment process. However, unlike human recruiters, you cannot hold an AI system directly responsible or accountable for denying applicants a job. This lack of accountability of the AI system itself creates a need for a set of actionable principles tailored to the design and use of AI systems. The Alan Turing Institute calls these the 'FAST Track Principles': Carefully reviewing the FAST Track Principles helps you: If your AI system processes social or demographic data, you should design it to meet a minimum level of discriminatory non-harm. To do this you should: You should design your AI system to be fully answerable and auditable. To do this you should: The technical sustainability of these systems ultimately depends on their safety, including their accuracy, reliability, security, and robustness. You should make sure designers and users remain aware of: Designers and implementers of AI systems should be able to: To assess these criteria in depth,you should consult The Alan Turing Institute's guidance on AI ethics and safety. The final method to make sure you use AI ethically, fairly, and safely is building a process-based governance framework. The Alan Turing Institute calls it a 'PBG Framework'. Its primary purpose is to integrate the SUM Values and the FAST Track Principles across the implementation of AI within a service. Building a good PBG Framework for your AI project will provide your team with an overview of: You may find it useful toconsider further guidance on allocating responsibility and governance for AI projects.",2023
govuk_003,govuk,Ai Ethics 3,"Published in January 2021 by the British Security Industry Association (BSIA), this guidance promotes ethical and legal use of Automated Facial Recognition (AFR). https://www.bsia.co.uk/ai/ This guidance is available on the BSIA website. Automated Facial Recognition (AFR) is a technology that has been designed to improve the safety and wellbeing of people, as well as providing a tool to assist and speed up operational processes. AFR is one of many data analysis technologies which sit under the overarching umbrella of Artificial Intelligence (AI), a branch of Computer Science. The ethics of AI and its application need to be regularly reviewed to ensure that it is not allowed to act autonomously without human oversight and it should not be used in any way which causes harm to individuals.",2023
govuk_025,govuk,Responsible Ai 0,"Toolkit for practitioners to support the responsible use of AI systems. This toolkit of guidance aims to support organisations and practitioners to safely and responsibly develop and deploy AI systems. Resources and guidance for organisations deploying data-driven tools and technologies including AI. Resources and guidance for practitioners interested in finding out how assurance techniques can support the development of responsible AI. The Algorithmic Transparency Recording Standard helps public sector organisations provide clear information about the algorithmic tools they use, and why they're using them. Research that engages the public, including tracker surveys that measure how public attitudes to data-driven technology and AI vary over time. Additional tools have been added in the AI assurance and Data-Driven Innovation categories. Added the research on parent and pupil attitudes towards the use of AI in education. Added the DfE and DSIT guidance on developing and using data analytics tools in children's social care. First published.",2023
govuk_004,govuk,Ai Ethics 4,"Find guidance for the responsible use and development of data and data technologies developed by and for government and public sector bodies. Use this tool to find data ethics guidance from across government. You can use the filter and search functions to identify the most relevant pieces of information for your organisation's needs. Start now Emaildata.ethics@digital.cabinet-office.gov.ukif you have questions or suggestions about data ethics guidance, or are aware of further relevant guidance that should be included. We encourage anyone working in the government and public sector to refer to the documents below to develop a high-level understanding of key data ethics considerations. TheData Ethics Frameworkexplains how to use data appropriately and responsibly when planning, implementing and evaluating a new policy or service. TheModel for Responsible Innovationis a practical tool to help teams across the public sector and beyond to innovate responsibly with data andAI. When working with public sector data, you have a responsibility to establish whether the data you manage and use is fit for purpose. TheGovernment Data Quality FrameworkandData Sharing Governance Frameworkset out principles and practices to improve the quality and better use of data across government. TheGenerativeAIFramework forHMGexplains how to use generativeAIsafely and responsibly. TheAlgorithmic Transparency Recording Standard (ATRS)provides a standard for public sector organisations to publish information about how and why they are using algorithmic methods in decision-making processes that affect members of the public. TheEthics, Transparency and Accountability Framework for Automated Decision-Makingaims to help government departments with the safe, sustainable and ethical use of automated and algorithmic decision-making systems. AIassurance is vital to ensure the reliability and trustworthiness ofAIsystems. The Responsible Technology Adoption Unit's (RTA)introduction toAIAssuranceidentifies assurance techniques that can support the development of responsibleAI. You can find additional information about tools and processes that support the responsible use ofAIat theRTA'sResponsibleAIToolkitpage. You must use the criteria inThe Technology Code of Practiceto design, build and buy technology in government. When procuringAIsolutions from third parties, refer to theGuidelines forAIprocurement. This includes principles for buyingAItechnology and insights on tackling challenges that may arise during procurement.",2023
govuk_024,govuk,Automated Decision-Making 4,"Published 27 November 2020 (c) Crown copyright 2020 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visitnationalarchives.gov.uk/doc/open-government-licence/version/3or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:psi@nationalarchives.gov.uk. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned. This publication is available at https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making Fairness is a highly prized human value.Societies in which individuals can flourish need to be held together by practices and institutions that are regarded as fair. What it means to be fair has been much debated throughout history, rarely more so than in recent months. Issues such as the global Black Lives Matter movement, the ""levelling up"" of regional inequalities within the UK, and the many complex questions of fairness raised by the COVID-19 pandemic have kept fairness and equality at the centre of public debate. Inequality and unfairness have complex causes, but bias in the decisions that organisations make about individuals is often a key aspect. The impact of efforts to address unfair bias in decision-making have often either gone unmeasured or have been painfully slow to take effect. However, decision-making is currently going through a period of change. Use of data and automation has existed in some sectors for many years, but it is currently expanding rapidly due to an explosion in the volumes of available data, and the increasing sophistication and accessibility of machine learning algorithms. Data gives us a powerful weapon to see where bias is occurring and measure whether our efforts to combat it are effective; if an organisation has hard data about differences in how it treats people, it can build insight into what is driving those differences, and seek to address them. However, data can also make things worse. New forms of decision-making have surfaced numerous examples where algorithms have entrenched or amplified historic biases; or even created new forms of bias or unfairness. Active steps to anticipate risks and measure outcomes are required to avoid this. Concern about algorithmic bias was the starting point for this policy review. When we began the work this was an issue of concern to a growing, but relatively small, number of people. As we publish this report, the issue has exploded into mainstream attention in the context of exam results, with a strong narrative that algorithms are inherently problematic. This highlightsthe urgent need for the world to do better in using algorithms in the right way: to promote fairness, not undermine it.Algorithms, like all technology, should work for people, and not against them. This is true in all sectors, but especially key in the public sector. When the state is making life-affecting decisions about individuals, that individual often can't go elsewhere. Society may reasonably conclude that justice requires decision-making processes to be designed so that human judgement can intervene where needed to achieve fair and reasonable outcomes for each person, informed by individual evidence. As our work has progressed it has become clear that we cannot separate the question of algorithmic bias from the question of biased decision-making more broadly.The approach we take to tackling biased algorithms in recruitment, for example, must form part of, and be consistent with, the way we understand and tackle discrimination in recruitment more generally. A core theme of this report is thatwe now have the opportunity to adopt a more rigorous and proactive approach to identifying and mitigating bias in key areas of life, such as policing, social services, finance and recruitment. Good use of data can enable organisations to shine a light on existing practices and identify what is driving bias. There is an ethical obligation to act wherever there is a risk that bias is causing harm and instead make fairer, better choices. The risk is growing as algorithms, and the datasets that feed them, become increasingly complex. Organisations often find it challenging to build the skills and capacity to understand bias, or to determine the most appropriate means of addressing it in a data-driven world. A cohort of people is needed with the skills to navigate between the analytical techniques that expose bias and the ethical and legal considerations that inform best responses. Some organisations may be able to create this internally, others will want to be able to call on external experts to advise them.Senior decision-makers in organisations need to engage with understanding the trade-offs inherent in introducing an algorithm.They should expect and demand sufficient explainability of how an algorithm works so that they can make informed decisions on how to balance risks and opportunities as they deploy it into a decision-making process. Regulators and industry bodies need to work together with wider society to agree best practice within their industry and establish appropriate regulatory standards.Bias and discrimination are harmful in any context. But the specific forms they take, and the precise mechanisms needed to root them out, vary greatly between contexts. We recommend that there should be clear standards for anticipating and monitoring bias, for auditing algorithms and for addressing problems. There are some overarching principles, but the details of these standards need to be determined within each sector and use case. We hope that CDEI can play a key role in supporting organisations, regulators and government in getting this right. Lastly,society as a whole will need to be engaged in this process.In the world before AI there were many different concepts of fairness. Once we introduce complex algorithms to decision-making systems, that range of definitions multiplies rapidly. These definitions are often contradictory with no formula for deciding which is correct. Technical expertise is needed to navigate these choices, but the fundamental decisions about what is fair cannot be left to data scientists alone. They are decisions that can only be truly legitimate if society agrees and accepts them. Our report sets out how organisations might tackle this challenge. Transparency is key to helping organisations build and maintain public trust.There is a clear, and understandable, nervousness about the use and consequences of algorithms, exacerbated by the events of this summer. Being open about how and why algorithms are being used, and the checks and balances in place, is the best way to deal with this.Organisational leaders need to be clear that they retain accountability for decisions made by their organisations, regardless of whether an algorithm or a team of humans is making those decisions on a day-to-day basis. In this report we set out some key next steps for the government and regulators to support organisations to get their use of algorithms right, whilst ensuring that the UK ecosystem is set up to support good ethical innovation. Our recommendations are designed to produce a step change in the behaviour of all organisations making life changing decisions on the basis of data, however limited, and regardless of whether they used complex algorithms or more traditional methods. Enabling data to be used to drive better, fairer, more trusted decision-making is a challenge that countries face around the world. By taking a lead in this area, the UK, with its strong legal traditions and its centres of expertise in AI, can help to address bias and inequalities not only within our own borders but also across the globe. The Board of the Centre for Data Ethics and Innovation Unfair biases, whether conscious or unconscious, can be a problem in many decision-making processes. This review considers the impact that an increasing use of algorithmic tools is having on bias in decision-making, the steps that are required to manage risks, and the opportunities that better use of data offers to enhance fairness. We have focused on the use of algorithms in significant decisions about individuals, looking across four sectors (recruitment, financial services, policing and local government), and makingcross-cutting recommendations that aim to help build the right systems so that algorithms improve, rather than worsen, decision-making. It is well established that there is a risk that algorithmic systems can lead to biased decisions, with perhaps the largest underlying cause being the encoding of existing human biases into algorithmic systems. But the evidence is far less clear on whether algorithmic decision-making tools carry more or less risk of bias than previous human decision-making processes.Indeed, there are reasons to think that better use of data can have a role in making decisions fairer, if done with appropriate care. When changing processes that make life-affecting decisions about individuals we should always proceed with caution.It is important to recognise that algorithms cannot do everything.There are some aspects of decision-making where human judgement, including the ability to be sensitive and flexible to the unique circumstances of an individual, will remain crucial. Using data and algorithms in innovative ways can enable organisations to understand inequalities and to reduce bias in some aspects of decision-making. But there are also circumstances where using algorithms to make life-affecting decisions can be seen as unfair by failing to consider an individual's circumstances, or depriving them of personal agency. We do not directly focus on this kind of unfairness in this report, but note that this argument can also apply to human decision-making, if the individual who is subject to the decision does not have a role in contributing to the decision. History to date in the design and deployment of algorithmic tools has not been good enough. There are numerous examples worldwide of the introduction of algorithms persisting or amplifying historical biases, or introducing new ones. We must and can do better. Making fair and unbiased decisions is not only good for the individuals involved, but it is good for business and society.Successful and sustainable innovation is dependent on building and maintaining public trust.Polling undertaken for this review suggested that, prior to August's controversy over exam results, 57% of people were aware of algorithmic systems being used to support decisions about them, with only 19% of those disagreeing in principle with the suggestion of a ""fair and accurate"" algorithm helping to make decisions about them. By October, we found that awareness had risen slightly (to 62%), as had disagreement in principle (to 23%). This doesn't suggest a step change in public attitudes, but there is clearly still a long way to go to buildtrustin algorithmic systems. The obvious starting point for this is to ensure that algorithms aretrustworthy. The use of algorithms in decision-making is a complex area, with widely varying approaches and levels of maturity across different organisations and sectors. Ultimately, many of the steps needed to challenge bias will be context specific. But from our work, we have identified a number of concrete steps for industry, regulators and government to take that can support ethical innovation across a wide range of use cases.This report is not a guidance manual, but considers what guidance, support, regulation and incentives are needed to create the right conditions for fair innovation to flourish. It is crucial to take a broad view of the whole decision-making process when considering the different ways bias can enter a system and how this might impact on fairness.The issue is not simply whether an algorithm is biased, but whether the overall decision-making processes are biased.Looking at algorithms in isolation cannot fully address this. It is important to consider bias in algorithmic decision-making in the context of all decision-making systems. Even in human decision-making, there are differing views about what is and isn't fair. But society has developed a range of standards and common practices for how to manage these issues, and legal frameworks to support this. Organisations have a level of understanding on what constitutes an appropriate level of due care for fairness. The challenge is to make sure that we can translate this understanding across to the algorithmic world, and apply a consistent bar of fairness whether decisions are made by humans, algorithms or a combination of the two.We must ensure decisions can be scrutinised, explained and challenged so that our current laws and frameworks do not lose effectiveness, and indeed can be made more effective over time. Significant growth is happening both in data availability and use of algorithmic decision-making across many sectors;we have a window of opportunity to get this right and ensure that these changes serve to promote equality, not to entrench existing biases. The four sectors studied inPart IIof this report are at different maturity levels in their use of algorithmic decision-making. Some of the issues they face are sector-specific, but we found common challenges that span these sectors and beyond. In recruitment we saw a sector that is experiencing rapid growth in the use of algorithmic tools at all stages of the recruitment process, but also one that is relatively mature in collecting data to monitor outcomes. Human bias in traditional recruitment is well evidenced and therefore there is potential for data-driven tools to improve matters by standardising processes and using data to inform areas of discretion where human biases can creep in. However, we also found that a clear and consistent understanding of how to do this well is lacking, leading to a risk that algorithmic technologies will entrench inequalities. More guidance is needed on how to ensure that these tools do not unintentionally discriminate against groups of people, particularly when trained on historic or current employment data. Organisations must be particularly mindful to ensure they are meeting the appropriate legislative responsibilities around automated decision-making and reasonable adjustments for candidates with disabilities. The innovation in this space has real potential for making recruitment fairer. However, given the potential risks, further scrutiny of how these tools work, how they are used and the impact they have on different groups, is required, along with higher and clearer standards of good governance to ensure that ethical and legal risks are anticipated and managed. In financial services, we saw a much more mature sector that has long used data to support decision-making. Finance relies on making accurate predictions about peoples' behaviours, for example how likely they are to repay debts. However, specific groups are historically underrepresented in the financial system, and there is a risk that these historic biases could be entrenched further through algorithmic systems. We found financial service organisations ranged from being highly innovative to more risk averse in their use of new algorithmic approaches. They are keen to test their systems for bias, but there are mixed views and approaches regarding how this should be done. This was particularly evident around the collection and use of protected characteristic data, and therefore organisations' ability to monitor outcomes. Our main focus within financial services was on credit scoring decisions made about individuals by traditional banks. Our work found the key obstacles to further innovation in the sector included data availability, quality and how to source data ethically, available techniques with sufficient explainability, risk averse culture, in some parts, given the impacts of the financial crisis and difficulty in gauging consumer and wider public acceptance. The regulatory picture is clearer in financial services than in the other sectors we have looked at. The Financial Conduct Authority (FCA) is the main regulator and is showing leadership in prioritising work to understand the impact and opportunities of innovative uses of data and AI in the sector. The use of data from non-traditional sources could enable population groups who have historically found it difficult to access credit, due to lower availability of data about them from traditional sources, to gain better access in future. At the same time, more data and more complex algorithms could increase the potential for the introduction of indirect bias via proxy as well as the ability to detect and mitigate it. Adoption of algorithmic decision-making in the public sector is generally at an early stage. In policing, we found very few tools currently in operation in the UK, with a varied picture across different police forces, both on usage and approaches to managing ethical risks. There have been notable government reviews into the issue of bias in policing, which is important context when considering the risks and opportunities around the use of technology in this sector. Again, we found potential for algorithms to support decision-making, but this introduces new issues around the balance between security, privacy and fairness, and there is a clear requirement for strong democratic oversight. Police forces have access to more digital material than ever before, and are expected to use this data to identify connections and manage future risks. The PS63.7 million funding for police technology programmes announced in January 2020 demonstrates the government's drive for innovation. But clearer national leadership is needed. Though there is strong momentum in data ethics in policing at a national level, the picture is fragmented with multiple governance and regulatory actors, and no single body fully empowered or resourced to take ownership. The use of data analytics tools in policing carries significant risk. Without sufficient care, processes can lead to outcomes that are biased against particular groups, or systematically unfair. In many scenarios where these tools are helpful, there is still an important balance to be struck between automated decision-making and the application of professional judgement and discretion. Given the sensitivities in this area it is not sufficient for care to be taken internally to consider these issues; it is also critical that police forces are transparent in how such tools are being used to maintain public trust. In local government, we found an increased use of data to inform decision-making across a wide range of services. Whilst most tools are still in the early phase of deployment, there is an increasing demand for sophisticated predictive technologies to support more efficient and targeted services. By bringing together multiple data sources, or representing existing data in new forms, data-driven technologies can guide decision-makers by providing a more contextualised picture of an individual's needs. Beyond decisions about individuals, these tools can help predict and map future service demands to ensure there is sufficient and sustainable resourcing for delivering important services. However, these technologies also come with significant risks. Evidence has shown that certain people are more likely to be overrepresented in data held by local authorities and this can then lead to biases in predictions and interventions. A related problem occurs when the number of people within a subgroup is small. Data used to make generalisations can result in disproportionately high error rates amongst minority groups. Data-driven tools present genuine opportunities for local government. However, tools should not be considered a silver bullet for funding challenges and in some cases additional investment will be required to realise their potential. Moreover, we found that data infrastructure and data quality were significant barriers to developing and deploying data-driven tools effectively and responsibly. Investment in this area is needed before developing more advanced systems. Sector-specific recommendations to regulators and government Most of the recommendations in this report are cross-cutting, but we identified the following recommendations specific to individual sectors. More details are given in sector chapters below. Recruitment Recommendation 1:TheEquality and Human Rights Commissionshould update its guidance on the application of the Equality Act 2010 to recruitment, to reflect issues associated with the use of algorithms, in collaboration with consumer and industry bodies. Recommendation 2:TheInformation Commissioner's Officeshould work with industry to understand why current guidance is not being consistently applied, and consider updates to guidance (e.g. in the Employment Practices Code), greater promotion of existing guidance, or other action as appropriate. Policing Recommendation 3:TheHome Officeshould define clear roles and responsibilities for national policing bodies with regards to data analytics and ensure they have access to appropriate expertise and are empowered to set guidance and standards. As a first step, the Home Office should ensure that work underway by the National Police Chiefs' Council and other policing stakeholders to develop guidance and ensure ethical oversight of data analytics tools is appropriately supported. Local government Recommendation 4: Governmentshould develop national guidance to support local authorities to legally and ethically procure or develop algorithmic decision-making tools in areas where significant decisions are made about individuals, and consider how compliance with this guidance should be monitored. We found underlying challenges across the four sectors, and indeed other sectors where algorithmic decision-making is happening. InPart IIIof this report, we focus on understanding these challenges, where the ecosystem has got to on addressing them, and the key next steps for organisations, regulators and government. The main areas considered are: Theenablersneeded by organisations building and deploying algorithmic decision-making tools to help them do this in a fair way, see Chapter 7. Theregulatory levers, both formal and informal, needed to incentivise organisations to do this, and create a level playing field for ethical innovation see Chapter 8. How thepublic sector, as a major developer and user of data-driven technology, can show leadership in this area throughtransparencysee Chapter 9. There are inherent links between these areas. Creating the right incentives can only succeed if the right enablers are in place to help organisations act fairly, but conversely, there is little incentive for organisations to invest in tools and approaches for fair decision-making if there is insufficient clarity on expected norms. We want a system that is fair and accountable; one that preserves, protects or improves fairness in decisions being made with the use of algorithms.We want to address the obstacles that organisations may face to innovate ethically, to ensure the same or increased levels of accountability for these decisions and how society can identify and respond to bias in algorithmic decision-making processes.We have considered the existing landscape of standards and laws in this area, and whether they are sufficient for our increasingly data-driven society. To realise this vision we need clear mechanisms for safe access to data to test for bias; organisations that are able to make judgements based on data about bias; a skilled industry of third parties who can provide support and assurance, and regulators equipped to oversee and support their sectors and remits through this change. We found that many organisations are aware of the risks of algorithmic bias, but are unsure how to address bias in practice. There is no universal formulation or rule that can tell you an algorithm is fair. Organisations need to identify what fairness objectives they want to achieve and how they plan to do this. Sector bodies, regulators, standards bodies and the government have a key role in setting out clear guidelines on what is appropriate in different contexts;getting this right is essential not only for avoiding bad practice, but for giving the clarity that enables good innovation.However, all organisations need to be clear about their own accountability for getting it right. Whether an algorithm or a structured human process is being used to make a decision doesn't change an organisation's accountability. Improving diversity across a range of roles involved in the development and deployment of algorithmic decision-making tools is an important part of protecting against bias.Government and industry efforts to improve this must continue, and need to show results. Data is needed to monitor outcomes and identify bias, but data on protected characteristics is not available often enough.One reason for this is an incorrect belief that data protection law prevents collection or usage of this data. Indeed, there are a number of lawful bases in data protection legislation for using protected or special characteristic data when monitoring or addressing discrimination. But there are some other genuine challenges in collecting this data, and more innovative thinking is needed in this area; for example around the potential for trusted third party intermediaries. The machine learning community has developed multiple techniques to measure and mitigate algorithmic bias. Organisations should be encouraged to deploy methods that address bias and discrimination. However, there is little guidance on how to choose the right methods, or how to embed them into development and operational processes.Bias mitigation cannot be treated as a purely technical issue; it requires careful consideration of the wider policy, operational and legal contexts.There is insufficient legal clarity concerning novel techniques in this area. Many can be used legitimately, but care is needed to ensure that the application of some techniques does not cross into unlawful positive discrimination. Recommendations to government Recommendation 5: Governmentshould continue to support and invest in programmes that facilitate greater diversity within the technology sector, building on its current programmes and developing new initiatives where there are gaps. Recommendation 6: Governmentshould work withrelevant regulatorsto provide clear guidance on the collection and use of protected characteristic data in outcome monitoring and decision-making processes. They should then encourage the use of that guidance and data to address current and historic bias in key sectors. Recommendation 7: Governmentand theOffice for National Statistics (ONS)should open the Secure Research Service more broadly, to a wider variety of organisations, for use in evaluation of bias and inequality across a greater range of activities. Recommendation 8: Governmentshould support the creation and development of data-focused public and private partnerships, especially those focused on the identification and reduction of biases and issues specific to under-represented groups. TheOffice for National Statistics (ONS)andGovernment Statistical Serviceshould work with these partnerships and regulators to promote harmonised principles of data collection and use into the private sector, via shared data and standards development. Recommendations to regulators Recommendation 9: Sector regulators and industry bodiesshould help create oversight and technical guidance for responsible bias detection and mitigation in their individual secin individual sectors, adding context-specific detail to the existing cross-cutting guidance on data protection, and any new cross-cutting guidance on the Equality Act. Good, anticipatory governance is crucial here.Many of the high profile cases of algorithmic bias could have been anticipated with careful evaluation and mitigation of the potential risks. Organisations need to make sure that the right capabilities and structures are in place to ensure that this happens both before algorithms are introduced into decision-making processes, and through their life. Doing this well requires understanding of, and empathy for, the expectations of those who are affected by decisions, which can often only be achieved through the right engagement with groups. Given the complexity of this area,we expect to see a growing role for expert professional servicessupporting organisations. Although the ecosystem needs to develop further, there is already plenty that organisations can and should be doing to get this right. Data Protection Impact Assessments and Equality Impact Assessments can help with structuring thinking and documenting the steps taken. Guidance to organisation leaders and boards Those responsible for governance of organisations deploying or using algorithmic decision-making tools to support significant decisions about individuals should ensure that leaders are in place with accountability for: This especially applies in the public sector when citizens often do not have a choice about whether to use a service, and decisions made about individuals can often be life-affecting. Clear industry norms, and good, proportionate regulation, are key both for addressing risks of algorithmic bias, and for promoting a level playing field for ethical innovation to thrive. The increased use of algorithmic decision-making presents genuinely new challenges for regulation, and brings into question whether existing legislation and regulatory approaches can address these challenges sufficiently well. There is currently limited case law or statutory guidance directly addressing discrimination in algorithmic decision-making, and the ecosystems of guidance and support are at different maturity levels in different sectors. Though there is only a limited amount of case law, the recent judgement of the Court of Appeal in relation to the usage of live facial recognition technology by South Wales Police seems likely to be significant. One of the grounds for successful appeal was that South Wales Police failed to adequately consider whether their trial could have a discriminatory impact, and specifically that they did not take reasonable steps to establish whether their facial recognition software contained biases related to race or sex. In doing so, the court found that they did not meet their obligations under the Public Sector Equality Duty, even though there was no evidence that this specific algorithm was biased.This suggests a general duty for public sector organisations to take reasonable steps to consider any potential impact on equality upfront and to detect algorithmic bias on an ongoing basis.The current regulatory landscape for algorithmic decision-making consists of the Equality and Human Rights Commission (EHRC), the Information Commissioner's Office (ICO) and sector regulators.At this stage, we do not believe that there is a need for a new specialised regulator or primary legislation to address algorithmic bias. However, algorithmic bias means the overlap between discrimination law, data protection law and sector regulations is becoming increasingly important. We see this overlap playing out in a number of contexts, including discussions around the use of protected characteristics data to measure and mitigate algorithmic bias, the lawful use of bias mitigation techniques, identifying new forms of bias beyond existing protected characteristics.The first step in resolving these challenges should be to clarify the interpretation of the law as it stands, particularly the Equality Act 2010, both to give certainty to organisations deploying algorithms and to ensure that existing individual rights are not eroded, and wider equality duties are met. However, as use of algorithmic decision-making grows further,we do foresee a future need to look again at the legislation itself, which should be kept under consideration as guidance is developed and case law evolves. Existing regulators need to adapt their enforcement to algorithmic decision-making, and provide guidance on how regulated bodies can maintain and demonstrate compliance in an algorithmic age. Some regulators require new capabilities to enable them to respond effectively to the challenges of algorithmic decision-making. While larger regulators with a greater digital remit may be able to grow these capabilities in-house, others will need external support. Many regulators are working hard to do this, and the ICO has shown leadership in this area both by starting to build a skills base to address these new challenges, and in convening other regulators to consider issues arising from AI. Deeper collaboration across the regulatory ecosystem is likely to be needed in future. Outside of the formal regulatory environment, there is increasing awareness within the private sector of the demand for abroader ecosystem of industry standards and professional services to help organisations address algorithmic bias.There are a number of reasons for this: it is a highly specialised skill that not all organisations will be able to support, it will be important to have consistency in how the problem is addressed, and because regulatory standards in some sectors may require independent audit of systems. Elements of such an ecosystem might be licenced auditors or qualification standards for individuals with the necessary skills. Audit of bias is likely to form part of a broader approach to audit that might also cover issues such as robustness and explainability. Government, regulators, industry bodies and private industry will all play important roles in growing this ecosystem so that organisations are better equipped to make fair decisions. Recommendations to government Recommendation 10: Governmentshould issue guidance that clarifies the Equality Act responsibilities of organisations using algorithmic decision-making. This should include guidance on the collection of protected characteristics data to measure bias and the lawfulness of bias mitigation techniques. Recommendation 11:Though the development of this guidance and its implementation,governmentshould assess whether it provides both sufficient clarity for organisations on meeting their obligations, and leaves sufficient scope for organisations to take actions to mitigate algorithmic bias. If not,governmentshould consider new regulations or amendments to the Equality Act to address this. Recommendations to regulators Recommendation 12:TheEHRCshould ensure that it has the capacity and capability to investigate algorithmic discrimination. This may include EHRC reprioritising resources to this area, EHRC supporting other regulators to address algorithmic discrimination in their sector, and additional technical support to the EHRC. Recommendation 13: Regulatorsshould consider algorithmic discrimination in their supervision and enforcement activities, as part of their responsibilities under the Public Sector Equality Duty. Recommendation 14: Regulatorsshould develop compliance and enforcement tools to address algorithmic bias, such as impact assessments, audit standards, certification and/or regulatory sandboxes. Recommendation 15: Regulatorsshould coordinate their compliance and enforcement efforts to address algorithmic bias, aligning standards and tools where possible. This could include jointly issued guidance, collaboration in regulatory sandboxes, and joint investigations. Making decisions about individuals is a core responsibility of many parts of the public sector, and there is increasing recognition of the opportunities offered through the use of data and algorithms in decision-making.The use of technology should never reduce real or perceived accountability of public institutions to citizens.In fact, it offers opportunities to improve accountability and transparency, especially where algorithms have significant effects on significant decisions about individuals. A range of transparency measures already exist around current public sector decision-making processes; both proactive sharing of information about how decisions are made, and reactive rights for citizens to request information on how decisions were made about them.The UK government has shown leadership in setting out guidance on AI usage in the public sector, including a focus on techniques for explainability and transparency.However, more is needed to make transparency about public sector use of algorithmic decision-making the norm. There is a window of opportunity to ensure that we get this right as adoption starts to increase, but it is sometimes hard for individual government departments or other public sector organisations to be first in being transparent; a strong central drive for this is needed. The development and delivery of an algorithmic decision-making tool will often include one or more suppliers, whether acting as technology suppliers or business process outsourcing providers. While the ultimate accountability for fair decision-making always sits with the public body, there is limited maturity or consistency in contractual mechanisms to place responsibilities in the right place in the supply chain. Procurement processes should be updated in line with wider transparency commitments to ensure standards are not lost along the supply chain. Recommendations to government Recommendation 16: Governmentshould place a mandatory transparency obligation on all public sector organisations using algorithms that have a significant influence on significant decisions affecting individuals. Government should conduct a project to scope this obligation more precisely, and to pilot an approach to implement it, but it should require the proactive publication of information on how the decision to use an algorithm was made, the type of algorithm, how it is used in the overall decision-making process, and steps taken to ensure fair treatment of individuals. Recommendation 17: Cabinet Officeand theCrown Commercial Serviceshould update model contracts and framework agreements for public sector procurement to incorporate a set of minimum standards around ethical use of AI, with particular focus on expected levels of transparency and explainability, and ongoing testing for fairness. This review has considered a complex and rapidly evolving field. There is plenty to do across industry, regulators and government to manage the risks and maximise the benefits of algorithmic decision-making. Some of the next steps fall within CDEI's remit, andwe are happy to support industry, regulators and government in taking forward the practical delivery work to address the issues we have identified and future challenges which may arise.Outside of specific activities, and noting the complexity and range of the work needed across multiple sectors, we see a key need for national leadership and coordination to ensure continued focus and pace in addressing these challenges across sectors. This is a rapidly moving area. A level of coordination and monitoring will be needed to assess how organisations building and using algorithmic decision-making tools are responding to the challenges highlighted in this report, and to the proposed new guidance from regulators and government. Government should be clear on where it wants this coordination to sit; for example in central government directly, in a specific regulator or in CDEI. In this review we have concluded that there is significant scope to address the risks posed by bias in algorithmic decision-making within the law as it stands, but if this does not succeed then there is a clear possibility that future legislation may be required. We encourage organisations to respond to this challenge; to innovate responsibly and think through the implications for individuals and society at large as they do so. The adoption of data-driven technology affects every aspect of our society and its use is creating opportunities as well as new ethical challenges. The Centre for Data Ethics and Innovation (CDEI) is an independent expert committee, led by a board of specialists, set up and tasked by the UK government to investigate and advise on how we maximise the benefits of these technologies. Our goal is to create the conditions in which ethical innovation can thrive: an environment in which the public are confident their values are reflected in the way data-driven technology is developed and deployed; where we can trust that decisions informed by algorithms are fair; and where risks posed by innovation are identified and addressed. More information about CDEI can be found atwww.gov.uk/cdei. In the October 2018 Budget, the Chancellor announced that we would investigate the potential bias in decisions made by algorithms. This review formed a key part of our 2019/2020 work programme, though completion was delayed by the onset of COVID-19. This is the final report of CDEI's review and includes a set of formal recommendations to the government. Government tasked us to draw on expertise and perspectives from stakeholders across society to provide recommendations on how they should address this issue.We also provide advice for regulators and industry, aiming to support responsible innovation and help build a strong, trustworthy system of governance. The government has committed to consider and respond publicly to our recommendations. The use of algorithms in decision-making is increasing across multiple sectors of our society. Bias in algorithmic decision-making is a broad topic, so in this review, we have prioritised the types of decisions where potential bias seems to represent a significant and imminent ethical risk. This has led us to focus on: This scope is broad, but it doesn't cover all possible areas where algorithmic bias can be an issue. For example, theCDEI Review of online targeting, published earlier this year, highlighted the risk of harm through bias in targeting within online platforms. These are decisions which are individually very small, for example on targeting an advert or recommending content to a user, but the overall impact of bias across many small decisions can still be problematic. This review did touch on these issues, but they fell outside of our core focus on significant decisions about individuals. It is worth highlighting that the main work of this review was carried out before a number of highly relevant events in mid 2020; the COVID-19 pandemic, Black Lives Matter, the awarding of exam results without exams, and (with less widespread attention, but very specific relevance) the judgement of the Court of Appeal in Bridges v South Wales Police. We have considered links to these issues in our review, but have not been able to treat them in full depth.[footnote 1] The ethical questions in relation to bias in algorithmic decision-making vary depending on the context and sector. We chose four initial areas of focus to illustrate the range of issues. These were recruitment, financial services, policing and local government. Our rationale for choosing these sectors is set out in the introduction toPart II. From the work we carried out on the four sectors, as well as our engagement across government, civil society, academia and interested parties in other sectors, we were able to identify themes, issues and opportunities that went beyond the individual sectors. We set out three key cross-cutting questions in ourinterim report, which we have sought to address on a cross-sector basis: 1. Data:Do organisations and regulators have access to the data they require to adequately identify and mitigate bias? 2. Tools and techniques:What statistical and technical solutions are available now or will be required in future to identify and mitigate bias and which represent best practice? 3. Governance:Who should be responsible for governing, auditing and assuring these algorithmic decision-making systems? These questions have guided the review. While we have made sector-specific recommendations where appropriate, our recommendations focus more heavily on opportunities to address these questions (and others) across multiple sectors. Our evidence base for this final report is informed by a variety of work including: Alandscape summaryled by Professor Michael Rovatsos of the University of Edinburgh, which assessed the current academic and policy literature. Anopen call for evidencewhich received responses from a wide cross section of academic institutions and individuals, civil society, industry and the public sector. A series of semi-structured interviews with companies in the financial services and recruitment sectors developing and using algorithmic tools. Work with the Behavioural Insights Team on attitudes to the use of algorithms in personal banking.[footnote 2] Representative polling on public attitudes to a number of the issues raised in this report, conducted by Deltapoll as part of CDEI's ongoing public engagement work. Meetings with a variety of stakeholders including regulators, industry groups, civil society organisations, academics and government departments, as well as desk-based research to understand the existing technical and policy landscape. Summary Algorithms are structured processes, which have long been used to aid human decision-making. Recent developments in machine learning techniques and exponential growth in data has allowed for more sophisticated and complex algorithmic decisions, and there has been corresponding growth in usage of algorithm supported decision-making across many areas of society. This growth has been accompanied by significant concerns aboutbias; that the use of algorithms can cause a systematic skew in decision-making that results in unfair outcomes. There is clear evidence that algorithmic bias can occur, whether through entrenching previous human biases or introducing new ones. Some forms of bias constitutediscriminationunder the Equality Act 2010, namely when bias leads to unfair treatment based on certain protected characteristics. There are also other kinds of algorithmic bias that are non-discriminatory, but still lead to unfair outcomes. There are multiple concepts of fairness, some of which are incompatible and many of which are ambiguous. In human decisions we can often accept this ambiguity and allow for human judgement to consider complex reasons for a decision. In contrast, algorithms are unambiguous. Fairness is about much more than the absence of bias: fair decisions need to also be non-arbitrary, reasonable, consider equality implications and respect the circumstances and personal agency of the individuals concerned. Despite concerns about 'black box' algorithms, in some ways algorithms can be more transparent than human decisions; unlike a human it is possible to reliably test how an algorithm responds to changes in parts of the input. There are opportunities to deploy algorithmic decision-making transparently, and enable the identification and mitigation of systematic bias in ways that are challenging with humans.Human developers and users of algorithms must decide the concepts of fairness that apply to their context, and ensure that algorithms deliver fair outcomes. Fairness through unawareness is often not enough to prevent bias: ignoring protected characteristics is insufficient to prevent algorithmic bias and it can prevent organisations from identifying and addressing bias. The need to address algorithmic bias goes beyond regulatory requirements under equality and data protection law.It is also critical for innovation that algorithms are used in a way that is both fair, and seen by the public to be fair. Human decision-making has always been flawed, shaped by individual or societal biases that are often unconscious. Over the years, society has identified ways of improving it, often by building processes and structures that encourage us to make decisions in a fairer and more objective way, from agreed social norms to equality legislation. However, new technology is introducing new complexities. The growing use of algorithms in decision-making has raised concerns around bias and fairness. Even in this data-driven context, the challenges are not new. In 1988, the UK Commission for Racial Equality found a British medical school guilty of algorithmic discrimination when inviting applicants to interview.[footnote 5]The computer program they had used was determined to be biased against both women and applicants with non-European names. The growth in this area has been driven by the availability and volume of (often personal) data that can be used to train machine learning models, or as inputs into decisions, as well as cheaper and easier availability of computing power, and innovations in tools and techniques. As usage of algorithmic tools grows, so does their complexity. Understanding the risks is therefore crucial to ensure that these tools have a positive impact and improve decision-making. Algorithms have different but related vulnerabilities to human decision-making processes. They can be more able to explain themselves statistically, but less able to explain themselves in human terms.They are more consistent than humans but are less able to take nuanced contextual factors into account. They can be highly scalable and efficient, but consequently capable of consistently applying errors to very large populations. They can also act to obscure the accountabilities and liabilities that individual people or organisations have for making fair decisions. In simple terms, an algorithm is a structured process. Using structured processes to aid human decision-making is much older than computation. Over time, the tools and approaches available to deploy such decision-making have become more sophisticated. Many organisations responsible for making large numbers of structured decisions (for example, whether an individual qualifies for a welfare benefits payment, or whether a bank should offer a customer a loan), make these processes scalable and consistent by giving their staff well-structured processes and rules to follow. Initial computerisation of such decisions took a similar path, with humans designing structured processes (or algorithms) to be followed by a computer handling an application. However, technology has reached a point where the specifics of those decision-making processes are not always explicitly manually designed. Machine learning tools often seek to find patterns in data without requiring the developer to specify which factors to use or how exactly to link them, before formalising relationships or extracting information that could be useful to make decisions. The results of these tools can be simple and intuitive for humans to understand and interpret, but they can also be highly complex. Some sectors, such as credit scoring and insurance, have a long history of using statistical techniques to inform the design of automated processes based on historical data. An ecosystem has evolved that helps to manage some of the potential risks, for example credit reference agencies offer customers the ability to see their own credit history, and offer guidance on the factors that can affect credit scoring. In these cases, there are a range of UK regulations that govern the factors that can and cannot be used. We are now seeing the application of data-driven decision-making in a much wider range of scenarios. There are a number of drivers for this increase, including: The usage is usually clear from context. In this review we are focused mainly on decision-making processes involving machine learning algorithms, although some of the content is also relevant to other structured decision-making processes. Note that there is no hard definition of exactly which statistical techniques and algorithms constitute novel machine learning. We have observed that many recent developments are associated with applying existing statistical techniques more widely in new sectors, not about novel techniques. We interpret algorithmic decision-making to include any decision-making process where an algorithm makes, or meaningfully assists, the decision.This includes what is sometimes referred to as algorithmically-assisted decision-making. In this review we are focused mainly on decisions about individual people. Figure 1 below shows an example of how a machine learning algorithm can be used within a decision-making process, such as a bank making a decision on whether to offer a loan to an individual. Machine-learning algorithms can be used within decision-making processes. First, a set of data is gathered, for example a collection of input data from historical applications for a service (e.g. a loan) along with the decisions reached, and any data on whether those outcomes were the right ones (e.g. was the loan repaid). A human decides what data to make available to the model. Second, A machine learning algorithm is chosen, and uses historical data (e.g. a set of past input data (e.g. a set of past input data, the decisions reached) to build a model, optimising against a set of criteria specified by a human. The model can take a number of different forms in different machine learning techniques, but might be a weighted average of a number of a number of input data fields, or a complex structured decision tree. Third, the resulting model is then used repeatedly as part of the decision-making process, either to make an automated decision, or to offer guidance to a human making the final decision. A human can be involved at this stage to vet the machine-learning model's outputs and make judgements about how to incorporate this information into a final decision. Fourth, new input data and associated decisions can be fed back into the data set to enable the model to be updated (either periodically or continuously). Figure 1:How data and algorithms come together to support decision-making It is important to emphasise that algorithms often do not represent the complete decision-making process. There may be elements of human judgement, exceptions treated outside of the usual process and opportunities for appeal or reconsideration. In fact, for significant decisions, an appropriate provision for human review will usually be required to comply with data protection law. Even before an algorithm is deployed into a decision-making process, it is humans that decide on the objectives it is trying to meet, the data available to it, and how the output is used. It is therefore critical to consider not only the algorithmic aspect, but the whole decision-making process that sits around it.Human intervention in these processes will vary, and in some cases may be absent entirely in fully automated systems. Ultimately the aim is not just to avoid bias in algorithmic aspects of a process, but that the process as a whole achieves fair decision-making. As algorithmic decision-making grows in scale, increasing concerns are being raised around the risks of bias. Bias has a precise meaning in statistics, referring to a systematic skew in results, that is an output that is not correct on average with respect to the overall population being sampled. However in general usage, and in this review,bias is used to refer to an output that is not only skewed, but skewed in a way that is unfair(see below for a discussion on what unfair might mean in this context). Bias can enter algorithmic decision-making systems in a number of ways, including: Historical bias:The data that the model is built, tested and operated on could introduce bias. This may be because of previously biased human decision-making or due to societal or historical inequalities. For example, if a company's current workforce is predominantly male then the algorithm may reinforce this, whether the imbalance was originally caused by biased recruitment processes or other historical factors. If your criminal record is in part a result of how likely you are to be arrested (as compared to someone else with the same history of behaviour, but not arrests), an algorithm constructed to assess risk of reoffending is at risk of not reflecting the true likelihood of reoffending, but instead reflects the more biased likelihood of being caught reoffending. Data selection bias:How the data is collected and selected could mean it is not representative. For example, over or under recording of particular groups could mean the algorithm was less accurate for some people, or gave a skewed picture of particular groups. This has been the main cause of some of the widely reported problems with accuracy of some facial recognition algorithms across different ethnic groups, with attempts to address this focusing on ensuring a better balance in training data.[footnote 6] Algorithmic design bias:It may also be that the design of the algorithm leads to introduction of bias. For example, CDEI'sReview of online targetingnoted examples of algorithms placing job advertisements online designed to optimise for engagement at a given cost, leading to such adverts being more frequently targeted at men because women are more costly to advertise to. Human oversightis widely considered to be a good thing when algorithms are making decisions, and mitigates the risk that purely algorithmic processes cannot apply human judgement to deal with unfamiliar situations. However, depending on how humans interpret or use the outputs of an algorithm, there is also a risk that bias re-enters the process as the human applies their own conscious or unconscious biases to the final decision. There is also risk that bias can be amplified over time by feedback loops, as models are incrementally re-trained on new data generated, either fully or partly, via use of earlier versions of the model in decision-making.For example, if a model predicting crime rates based on historical arrest data is used to prioritise police resources, then arrests in high risk areas could increase further, reinforcing the imbalance. CDEI'sLandscape summarydiscusses this issue in more detail. In this report we use the worddiscriminationin the sense defined in the Equality Act 2010, meaning unfavourable treatment on the basis of a protected characteristic.[footnote 7] The Equality Act 2010[footnote 8]makes it unlawful to discriminate against someone on the basis of certainprotected characteristics(for example age, race, sex, disability) in public functions, employment and the provision of goods and services. The choice of these characteristics is a recognition that they have been used to treat people unfairly in the past and that, as a society, we have deemed this unfairness unacceptable. Many, albeit not all, of the concerns about algorithmic bias relate to situations where that bias may lead to discrimination in the sense set out in the Equality Act 2010. The Equality Act 2010[footnote 9]defines two main categories of discrimination:[footnote 10] Direct Discrimination:When a person is treated less favourably than another because of a protected characteristic. Indirect Discrimination:When a wider policy or practice, even if it applies to everyone, disadvantages a group of people who share a protected characteristic (and there is not a legitimate reason for doing so). Where this discrimination is direct, the interpretation of the law in an algorithmic decision-making process seems relatively clear. If an algorithmic model explicitly leads to someone being treated less favourably on the basis of a protected characteristic that would be unlawful. There are some very specific exceptions to this in the case of direct discrimination on the basis of age (where such discrimination could be lawful if a proportionate means to a proportionate aim, e.g. services targeted at a particular age range) or limited positive actions in favour of those with disabilities. However, the increased use of data-driven technology has created new possibilities for indirect discrimination. For example, a model might consider an individual's postcode. This is not a protected characteristic, but there is some correlation between postcode and race. Such a model, used in a decision-making process (perhaps in financial services or policing) could in principle cause indirect racial discrimination. Whether that is the case or not depends on a judgement about the extent to which such selection methods are a proportionate means of achieving a legitimate aim.[footnote 11]For example, an insurer might be able to provide good reasons why postcode is a relevant risk factor in a type of insurance. The level of clarity about what is and is not acceptable practice varies by sector, reflecting in part the maturity in using data in complex ways. As algorithmic decision-making spreads into more use cases and sectors, clear context-specific norms will need to be established. Indeed as the ability of algorithms to deduce protected characteristics with certainty from proxies continues to improve, it could even be argued that some examples could potentially cross into direct discrimination. Discrimination is a narrower concept than bias. Protected characteristics have been included in law due to historical evidence of systematic unfair treatment, but individuals can also experience unfair treatment on the basis of other characteristics that are not protected. There will always be grey areas where individuals experience systematic and unfair bias on the basis of characteristics that are not protected, for example accent, hairstyle, education or socio-economic status.[footnote 12]In some cases, these may be considered as indirect discrimination if they are connected with protected characteristics, but in other cases they may reflect unfair biases that are not protected by discrimination law. However the increased use of algorithms may exacerbate this difficulty. The introduction of algorithms can encode existing biases into algorithms, if they are trained from existing decisions. This can reinforce and amplify existing unfair bias, whether on the basis of protected characteristics or not. Algorithmic decision-making can also go beyond amplifying existing biases, to creating new biases that may be unfair, though difficult to address through discrimination law. This is because machine learning algorithms find new statistical relationships, without necessarily considering whether the basis for those relationships is fair, and then apply this systematically in large numbers of individual decisions. We defined bias as including an element of unfairness. This highlights challenges in defining what we mean by fairness, which is a complex and long debated topic. Notions of fairness are neither universal nor unambiguous, and they are often inconsistent with one another. In human decision-making systems, it is possible to leave a degree of ambiguity about how fairness is defined. Humans may make decisions for complex reasons, and are not always able to articulate their full reasoning for making a decision, even to themselves. There are pros and cons to this. It allows for good fair-minded decision-makers to consider the specific individual circumstances, and human understanding of the reasons for why these circumstances might not conform to typical patterns. This is especially important in some of the most critical life-affecting decisions, such as those in policing or social services, where decisions often need to be made on the basis of limited or uncertain information; or where wider circumstances, beyond the scope of the specific decision, need to be taken into account. It is hard to imagine that automated decisions could ever fully replace human judgement in such cases. But human decisions are also open to the conscious or unconscious biases of the decision-makers, as well as variations in their competence, concentration levels or mood when specific decisions are made. Algorithms, by contrast, are unambiguous. If we want a model to comply with a definition of fairness, we must tell it explicitly what that definition is. How significant a challenge that is depends on context. Sometimes the meaning of fairness is very clearly defined; to take an extreme example, a chess playing AI achieves fairness by following the rules of the game. Often though, existing rules or processes require a human decision-maker to exercise discretion or judgement, or to account for data that is difficult to include in a model (e.g. context around the decision that cannot be readily quantified).Existing decision-making processes must be fully understood in context in order to decide whether algorithmic decision-making is likely to be appropriate.For example, police officers are charged with enforcing the criminal law, but it is often necessary for officers to apply discretion on whether a breach of the letter of the law warrants action. This is broadly a good thing, but such discretion also allows an individual's personal biases, whether conscious or unconscious, to affect decisions. Even in cases where fairness can be more precisely defined, it can still be challenging to capture all relevant aspects of fairness in a mathematical definition.In fact, the trade-offs between mathematical definitions demonstrate that a model cannot conform to all possible fairness definitions at the same time. Humans must choose which notions of fairness are appropriate for a particular algorithm, and they need to be willing to do so upfront when a model is built and a process is designed. The General Data Protection Regulation (GDPR) and Data Protection Act 2018 contain a requirement that organisations should use personal data in a way that is fair. The legislation does not elaborate further on the meaning of fairness, but the ICO guides organisations that ""In general, fairness means that you should only handle personal data in ways that people would reasonably expect and not use it in ways that have unjustified adverse effects on them.""[footnote 13]Note that the discussion in this section is wider than the notion in GDPR, and does not attempt to define how the word fair should be interpreted in that context. Notions of fair decision-making (whether human or algorithmic) are typically gathered into two broad categories: procedural fairnessis concerned with 'fair treatment' of people, i.e. equal treatment within the process ofhowa decision is made. It might include, for example, defining an objective set of criteria for decisions, and enabling individuals to understand and challenge decisions about them. outcome fairnessis concerned withwhatdecisions are made i.e. measuring average outcomes of a decision-making process and assessing how they compare to an expected baseline. The concept of what a fair outcome means is of course highly subjective; there are multiple different definitions of outcome fairness. Some of these definitions are complementary to each other, and none alone can capture all notions of fairness. A 'fair' process may still produce 'unfair' results, and vice versa, depending on your perspective. Even within outcome fairness there are many mutually incompatible definitions for a fair outcome. Consider for example a bank making a decision on whether an applicant should be eligible for a given loan, and the role of an applicant's sex in this decision. Two possible definitions of outcome fairness in this example are: A. The probability of getting a loan should be the same for men and women. B. The probability of getting a loan should be the same for men and women who earn the same income. Taken individually, either of these might seem like an acceptable definition of fair. But they are incompatible. In the real world sex and income are not independent of each other; the UK has a gender pay gap meaning that, on average, men earn more than women.[footnote 14]Given that gap, it is mathematically impossible to achieve both A and B simultaneously. This example is by no means exhaustive in highlighting the possible conflicting definitions that can be made, with a large collection of possible definitions identified in the machine learning literature.[footnote 15] In human decision-making we can often accept ambiguity around this type of issue, but when determining if an algorithmic decision-making process is fair, we have to be able to explicitly determine what notion of fairness we are trying to optimise for. It is a human judgement call whether the variable (in this case salary) acting as a proxy for a protected characteristic (in this case sex) is seen as reasonable and proportionate in the context. We investigated public reactions to a similar example to this in work with the Behavioural Insights Team (see further detail in Chapter 4. Even when we can agree what constitutes fairness, it is not always clear how to respond. Conflicting views about the value of fairness definitions arise when the application of a process intended to be fair produces outcomes regarded as unfair. This can be explained in several ways, for example: The first argument implies greater outcome fairness is consistent with more accurate and fair decision-making. The second argues that different groups ought to be treated differently to correct for historical wrongs and is the argument associated with quota regimes. It is not possible to reach a general opinion on which argument is correct, this is highly dependent on the context (and there are also other possible explanations). In decision-making processes based on human judgement it is rarely possible to fully separate the causes of differences in outcomes. Human recruiters may believe they are accurately assessing capabilities, but if the outcomes seem skewed it is not always possible to determine the extent to which this in fact reflects bias in methods of assessing capabilities. How do we handle this in the human world? There are a variety of techniques, for example steps to ensure fairness in an interview-based recruitment process might include: The increased use of more complex algorithmic approaches in decision-making introduces a number of new challenges and opportunities. The need for conscious decisions about fairness:In data-driven systems, organisations need to address more of these issues at the point a model is built, rather than relying on human decision-makers to interpret guidance appropriately (an algorithm can't apply ""common sense"" on a case-by-case basis). Humans are able to balance things implicitly, machines will optimise without any balance if asked to do so. Explainability:Data-driven systems allow for a degree of explainability about the factors causing variation in the outcomes of decision-making systems between different groups and to assess whether or not this is regarded as fair. For example, it is possible to examine more directly the degree to which relevant characteristics are acting as a proxy for other characteristics, and causing differences in outcomes between different groups. If a recruitment process included requirements for length of service and qualification, it would be possible to see whether, for example, length of service was generally lower for women due to career breaks and that this was causing an imbalance. The extent to which this is possible depends on the complexity of the algorithm used. Dynamic algorithms drawing on large datasets may not allow for a precise attribution of the extent to which the outcome of the process for an individual woman was attributable to a particular characteristic and its association with gender. However, it is possible to assess the degree to which over a time period, different characteristics are influencing recruitment decisions and how they correlate with characteristics during that time. The term 'black box' is often used to describe situations where, for a variety of different reasons, an explanation for a decision is unobtainable. This can include commercial issues (e.g. the decision-making organisation does not understand the details of the algorithm which their supplier considers their own intellectual property) or technical reasons (e.g. machine learning techniques that are less accessible for easy human explanation of individual decisions). The Information Commissioner's Office and the Alan Turing Institute have recently published detailed joint advice on how organisations can overcome some of these challenges and provide a level of explanation of decisions.[footnote 16] Scale of impact:The potential breadth of impact of an algorithm links to the market dynamics. Many algorithmic software tools are developed as platforms and sold across many companies. It is therefore possible, for example, that individuals applying to multiple jobs could be rejected at sift by the same algorithm (perhaps sold to a large number of companies recruiting for the same skill sets in the same industry). If the algorithm does this for reasons irrelevant to their actual performance, but on the basis of a set of characteristics that are not protected, then this feels very much like systematic discrimination against a group of individuals, but the Equality Act provides no obvious protection against this. Algorithmic decision-making will inevitably increase over time; the aim should be to ensure that this happens in a way that acts to challenge bias, increase fairness and promote equality, rather than entrenching existing problems. The recommendations of this review are targeted at making this happen. Case study: Exam results in August 2020 Due to COVID-19, governments across the UK decided to cancel school examinations in summer 2020, and find an alternative approach to awarding grades. All four nations of the UK attempted to implement similar processes to deliver this; combining teacher assessments with a statistical moderation process that attempted to achieve a similar distribution of grades to previous years. The approaches were changed in response to public concerns, and significant criticism about both individual fairness and concerns that grades were biased. How should fairness have been interpreted in this case? There were a number ofdifferent notionsof fairness to consider,including: Fairness between year groups: Achieve a similar distribution of grades to previous and future year groups. Group fairness between different schools: Attempt to standardise teacher assessed grades, given the different levels of strictness/optimism in grading between different schools to be fair to individual students from different schools. Group fairness and discrimination: Avoid exacerbating differences in outcomes correlated with protected characteristics; particularly sex and race. This did not include addressing any systematic bias in results based on inequality of opportunity; this was seen as outside the mandate of an exam body. Avoid any bias based on socio-economic status. A fair process for allocating grades to individual students, i.e. allocating them a grade that was seen to be a fair representation of their own individual capabilities and efforts. The main work of this review was complete prior to the release of summer 2020 exam results, but there are some clear links between the issues raised and the contents of this review, including issues of public trust, transparency and governance. The way decisions are made, the potential biases which they are subject to, and the impact these decisions have on individuals, are highly context dependent. It is unlikely that all forms of bias can be entirely eliminated. This is also true in human decision-making; it is important to understand the status quo prior to the introduction of data-driven technology in any given scenario. Decisions may need to be made about what kinds and degrees of bias are tolerable in certain contexts and the ethical questions will vary depending on the sector. We want to help create the conditions where ethical innovation using data-driven technology can thrive. It is therefore essential to ensure our approach is grounded in robust ethical principles. The UK government, along with 41 other countries, has signed up to the OECD Principles on Artificial Intelligence[footnote 18]. They provide a good starting point for considering our approach to dealing with bias, as follows:[footnote 19] 1: AI should benefit people and the planet by driving inclusive growth, sustainable development and well-being. There are many potential advantages of algorithmic decision-making tools when used appropriately, such as the potential efficiency and accuracy of predictions. There is also the opportunity for these tools to support good decision-making by reducing human error and combating existing bias. When designed correctly, they can offer a more objective alternative (or supplement) to human subjective interpretation. It is core to this review, and the wider purpose of CDEI, to identify how we can collectively ensure that these opportunities outweigh the risks. 2: AI systems should be designed in a way that respects the rule of law, human rights, democratic values and diversity, and they should include appropriate safeguards - for example, enabling human intervention where necessary - to ensure a fair and just society. This principle sets out some core terms for what we mean by fairness in an algorithmic decision-making process. We cover a number of aspects of it throughout the review. Our focus in this review on significant decisions means that we have been largely considering decisions where the algorithm forms only part of an overall decision-making process, and hence there is a level of direct human oversight of individual decisions. However, consideration is always needed on whether the role of the human remains meaningful; does the human understand the algorithm (and its limitations) sufficiently well to exercise that oversight effectively? Does the organisational environment that they are working within empower them to do so? Is there a risk that human biases could be reintroduced through this oversight? In Chapter 8 below we consider the ability of existing UK legal and regulatory structures to ensure fairness in this area, especially data protection and equality legislation, and how they will need to evolve to adapt to an algorithmic world. 3: There should be transparency and responsible disclosure around AI systems to ensure that people understand AI-based outcomes and can challenge them. Our sector-led work has identified variable levels of transparency on the usage of algorithms. A variety of other recent reviews have called for increased levels of transparency across the public sector. It is clear that more work is needed to achieve this level of transparency in a consistent way across the economy, and especially in the public sector where many of the highest stakes decisions are made. We discuss how this can be achieved in Chapter 9. 4: AI systems must function in a robust, secure and safe way throughout their life cycles and potential risks should be continually assessed and managed. In Chapter 7 we identify approaches taken to mitigate the risk of bias through the development lifecycle of an algorithmic decision-making system, and suggest action that the government can take to support development teams in taking a fair approach. 5: Organisations and individuals developing, deploying or operating AI systems should be held accountable for their proper functioning in line with the above principles. The use of algorithmic decision-making tools within decisions can have a significant impact on individuals or society, raising a requirement for clear lines of accountability in their use and impact. When decisions are made by humans in large organisations, we don't generally consider it possible to get it right every time. Instead, we expect organisations to have appropriate structures, policies and procedures to anticipate and address potential bias, offer redress when it occurs, and set clear governance processes and lines of accountability for decisions. Organisations that are introducing algorithms into decisions that were previously purely made by humans should be looking to achieve at least equivalent standards of fairness, accountability and transparency, and in many cases should look to do better. Defining equivalence is not always easy of course, there may be occasions where these standards have to be achieved in a different way in an algorithmic world. We discuss this issue in more detail inPart IIIof the report. For all of these issues, it is important to remember that we are not just interested in the output of an algorithm, but the overall decision-making process that sits around it.Organisations have existing accountability processes and standards, and the use of algorithms in decision-making needs to sit within existing accountability processes to ensure that they are used intentionally and effectively, and therefore that the organisation is as accountable for the outcome as they are for traditional human decision-making. We must decide how far to mitigate bias and how we should govern our approach to doing so. These decisions require value judgements and trade-offs between competing values. Humans are often trusted to make these trade-offs without having to explicitly state how much weight they have put on different considerations. Algorithms are different. They are programmed to make trade-offs according to rules and their decisions can be interrogated and made explicit. The OECD principles are clearly high level, and only take us so far when making difficult ethical balances for individual decision-making systems. The work in this review suggests that as algorithmic decision-making continues to grow in scale, we should be ambitious in aiming not only to avoid new bias, but to use this as an opportunity to address historical unfairness. Organisations responsible for using algorithms require more specific guidance on how principles apply in their circumstances. The principles are often context specific and are discussed in more detail in the sector sections below. However, we can start to outline some rules of thumb that can guide all organisations using algorithms to support significant decision-making processes: The ethical questions in relation to bias in algorithmic decision-making vary depending on the context and sector. We therefore chose four initial areas of focus to illustrate the range of issues. These were recruitment, financial services, policing and local government. All of these sectors have the following in common: There are of course other sectors that we could have considered; these were chosen as a representative sample across the public and private sector, not because we have judged that the risk of bias is most acute in these specific cases. In this part of the review, we focus on the sector-specific issues, and reach a number of recommendations specific to individual sectors. The sector studies then inform the cross-cutting findings and recommendations inPart IIIbelow. Summary Overview of findings: The use of algorithms in recruitment has increased in recent years, in all stages of the recruitment process. Trends suggest these tools will become more widespread, meaning that clear guidance and a robust regulatory framework are essential. When developed responsibly, data-driven tools have the potential to improve recruitment by standardising processes and removing discretion where human biases can creep in, however if using historical data, these human biases are highly likely to be replicated. Rigorous testing of new technologies is necessary to ensure platforms do not unintentionally discriminate against groups of people, and the only way to do this is to collect demographic data on applicants and use this data to monitor how the model performs. Currently, there is little standardised guidance for how to do this testing, meaning companies are largely self-regulated. Algorithmic decision-making in recruitment is currently governed primarily by the Equality Act 2010 and the Data Protection Act 2018, however we found in both cases there is confusion regarding how organisations should enact their legislative responsibilities. Recommendations to regulators: Recommendation 1:TheEquality and Human Rights Commissionshould update its guidance on the application of the Equality Act 2010 to recruitment, to reflect issues associated with the use of algorithms, in collaboration with consumer and industry bodies. Recommendation 2:TheInformation Commissioner's Officeshould work with industry to understand why current guidance is not being consistently applied, and consider updates to guidance (e.g. in the Employment Practices Code), greater promotion of existing guidance, or other action as appropriate. Advice to industry Future CDEI work Decisions about who to shortlist, interview and employ have significant effects on the lives of individuals and society. When certain groups are disadvantaged either directly or indirectly from the recruitment process, social inequalities are broadened and embedded. The existence of human bias in traditional recruitment is well-evidenced.[footnote 20]A famous study found that when orchestral players were kept behind a screen for their audition, there was a significant increase in the number of women who were successful.[footnote 21]Research in the UK found that candidates with ethnic minority backgrounds have to send as many of 60% more applications than white candidates to receive a positive response.[footnote 22]Even more concerning is the fact that there has been very little historical improvement in these figures over the last few decades.[footnote 23]Recruitment is also considered a barrier to employment for people with disabilities.[footnote 24]A range of factors from affinity biases, where recruiters tend to prefer people similar to them, to informal processes that recruit candidates already known to the organisation all amplify these biases, and some people believe technology could play a role in helping to standardise processes and make them fairer.[footnote 25] The internet has also meant that candidates are able to apply for a much larger number of jobs, thus creating a new problem for organisations needing to review hundreds, sometimes thousands, of applications. These factors have led to an increase in new data-driven tools, promising greater efficiency, standardisation and objectivity. There is a consistent upwards trend in adoption, with around 40% of HR functions in international companies now using AI.[footnote 26]It is however important to distinguish between new technologies and algorithmic decision-making. Whilst new technology is increasingly being applied across the board in recruitment, our research was focused on tools that utilise algorithmic decision-making systems, training on data to predict a candidate's future success. There areconcernsabout thepotential negative impactsof algorithmic decision-making inrecruitment. There are also concerns about the effectiveness of technologies to be able to predict good job performance given the relative inflexibility of systems and the challenge of conducting a thorough assessment using automated processes at scale. For the purpose of this report, our focus is on bias rather than effectiveness. How we approached our work Our work on recruitment as a sector began with a call for evidence and the landscape summary. This evidence gathering provided a broad overview of the challenges and opportunities presented by using algorithmic tools in hiring. In addition to desk-based research, we conducted a series of semi-structured interviews with a broad range of software providers and recruiters. In these conversations we focused on how providers currently test and mitigate bias in their tools. We also spoke with a range of other relevant organisations and individuals including think tanks, academics, government departments, regulators and civil society groups. Tools are being created and used for every stage of the recruitment process There are many stages in a recruitment process and algorithms are increasingly being used throughout.[footnote 27]Starting with the sourcing of applicants via targeting online advertisements[footnote 28]through to CV screening, then interview and selection phases. Data-driven tools are sold as a more efficient, accurate and objective way of assisting with recruiting decisions. Figure 2:Examples of algorithmic tools used through the sourcing, screening, interview and selection stages of the recruitment process In recruitment, AI tools are available to support all stages of the hiring process: sourcing, screening, interview and selection. At the sourcing stage, tools are available to review job descriptions, target job advertising to potential candidates, power recruiting chatbots and 'headhunt' for high-performing candidates. At the screening stage, AI tools can be used to screen qualifications, match CVs to specific job roles, run psychometric and game-based tests to assess cognitive skills, and rank applications. At the interview stage, voice and face recognition technology can be used in video interviewing. At the selection stage, AI software can be used to perform background checks and predict offers. Organisations may use different providers for the stages of the recruitment process and there are increasing options to integrate different types of tools. Algorithms trained on historic data carry significant risks for bias There are many ways bias can be introduced into the recruiting process when using data-driven technology. Decisions such as how data is collected, which variables to collect, how the variables are weighted, and the data the algorithm is trained on all have an impact and will vary depending on the context. However one theme that arises consistently is the risk of training algorithms on biased historical data. High profile cases of biased recruiting algorithms include those trained using historical data on current and past employees within an organisation, which is then used to try and predict the performance of future candidates.[footnote 29]Similar systems are used for video interviewing software where existing employees or prospective applicants undertake the assessment and this is assessed and correlated in line with a performance benchmark.[footnote 30]The model is then trained on this data to understand the traits of people who are considered high performers. Without rigorous testing, these kinds of predictive systems can pull out characteristics that have no relevance to job performance but are rather descriptive features that correlate with current employees. For example, one company developed a predictive model trained on their company data that found having the name ""Jared"" was a key indicator of a successful applicant.[footnote 31]This is an example where a machine learning process has picked up a very explicit bias, others are often more subtle but can be still as damaging. In the high profile case of Amazon, an application system trained on existing employees never made it past the development phase when testing showed that women's CVs were consistently rated worse.[footnote 32]Pattern detection of this type is likely to identify various factors that correspond with protected characteristics if development goes unchecked, so it is essential that organisations interrogate their models to identify proxies or risk indirectly discriminating against protected groups. Another way bias can arise is through having a dataset that is limited in respect to candidates with certain characteristics. For example, if the training set was from a company that had never hired a woman, the algorithm would be far less accurate in respect to female candidates. This type of bias arises from imbalance, and can easily be replicated across other demographic groups. Industry should therefore be careful about the datasets used to develop these systems both with respect to biases arising through historical prejudice, but also from unbalanced data. Whilst most companies we spoke to evaluated their models to check that the patterns being detected did not correlate with protected characteristics, there is very little guidance or standards companies have to meet so it is difficult to evaluate the robustness of these processes.[footnote 33]Further detail can be found in Section 7.4 on the challenges and limitations of bias mitigation approaches. Recruiting tool providers are largely self-regulated but tend to follow international standards Currently guidance on discrimination within recruitment sits with the Equality and Human Rights Commission who oversee compliance with the Equality Act (2010) through theEmployment Statutory Code of Practicesetting out what fair recruitment looks like under the Equality Act. They also provide detailed guidance to employers on how to interpret and apply the Equality Act.[footnote 34]However there is not currently any guidance on how the Equality Act extends to algorithmic recruitment. This means providers of recruiting tools are largely self-regulating, and often base their systems on equality law in other jurisdictions, especially the US (where there have been some high profile legal cases in this area).[footnote 35] Our research found that most companies test their tools internally and only some independently validate results. This has led to researchers and civil society groups calling for greater transparency around bias testing in recruiting algorithms as a way of assuring the public that appropriate steps have been taken to minimise the risk of bias.[footnote 36]We are now seeing some companies publish information on how their tools are validated and tested for bias.[footnote 37]However, researchers and civil society groups believe this has not gone far enough, calling for recruiting algorithms to be independently audited.[footnote 38]Further discussion of the regulatory landscape and auditing can be found in Chapter 8. Recommendations to regulators Recommendation 1:TheEquality and Human Rights Commissionshould update its guidance on the application of the Equality Act 2010 to recruitment, to reflect issues associated with the use of algorithms, in collaboration with relevant industry and consumer bodies. CDEI is happy to support this work if this would be helpful. Collecting demographic data for monitoring purposes is increasingly widespread and helps to test models for biases and proxies The only way to be sure a model is not directly or indirectly discriminating against a protected group is to check, and doing so requires having the necessary data. The practice of collecting data on protected characteristics is becoming increasingly common in recruitment as part of the wider drive to monitor and improve recruiting for underrepresented groups. This then allows vendors or recruiting organisations to test their models for proxies and monitor the drop-out rate of groups across the recruitment process. Compared to the other sectors we studied, recruitment is more advanced in the practice of collecting equality data for monitoring purposes. We found in our interviews that it is now standard practice to collect this data and provide applicants with disclaimers highlighting that the data will not be used as part of the process. One challenge that was raised in our interviews was that some applicants may not want to provide this data as part of a job application, which is within their rights to withhold. We consider this issue in detail in Section 7.3 below, and conclude that clearer national guidance is needed to support organisations in doing this. Organisations should also be encouraged to monitor the overlap for people with multiple protected characteristics, as this may not be picked up through monitoring that only reviews data through a one-dimensional lens. This form of intersectional analysis is essential for ensuring people are not missed as a result of having multiple protected characteristics.[footnote 39] Advice to employers and industry:Organisations should carry out equality impact assessments to understand how their models perform for candidates with different protected characteristics, including intersectional analysis for those with multiple protected characteristics. In the US there is specific guidance setting the minimum level of drop-off allowed for applicants from protected groups before a recruitment process could be considered discriminatory. This is known as the ""four-fifths rule"" and was introduced as a mechanism to adjudicate on whether a recruitment process was considered to have had a disparate impact on certain groups of people.[footnote 40]We found in our research that many third-party software providers use these standards and some tools offer this feature as part of their platforms to assess the proportion of applicants that are moving through the process. However, the four-fifths rule is not part of UK law, and not a meaningful test of whether a system might lead to discrimination under UK law. It is therefore important for the EHRC to provide guidance on how the Equality Act 2010 applies (see Chapter 7 and Chapter 8 below for further discussion in this area). Many tools are developed and used with fairness in mind Although the most frequently cited reason for adopting data-driven technologies is efficiency, we found a genuine desire to use tools to make processes fairer. Where historically decisions about who to hire were made through referrals or unconscious biases. Recruiters also often do not have the relevant demographic data on applicants to know whether they are being fair in the criteria they are applying when looking for candidates. Many companies developing these tools want to provide less biased assessments of candidates by standardising processes and using more accurate assessment for candidates' potential to succeed in a job. For example, one provider offers machine learning software that redacts parts of CVs associated with protected characteristics so those assessing the application can make a fairer judgement. Others try to equalise the playing field by developing games that assess core skills rather than relying on CVs which place weight on socio-demographic markers like educational institutions. The innovation in this space has real potential for making recruitment less biased if developed and deployed responsibly.[footnote 41]However, the risks if they go wrong are significant because the tools are incorporating and replicating biases on a larger scale. Given the potential risks, there is a need for scrutiny in how these tools work, how they are used and the impact they have on different groups. More needs to be done to ensure that data-driven tools can support reasonable adjustments for those who need them, or that alternative routes are available One area where there is particular concern is how certain tools may work for those with disabilities. AI often identifies patterns related to a defined norm, however those with disabilities often require more bespoke arrangements because their requirements will likely differ from the majority, which may lead to indirect discrimination.[footnote 42]For example, someone with a speech impediment may be at a disadvantage in an AI assessed video interview, or someone with a particular cognitive disability may not perform as well in a gamified recruitment exercise. In the same way that reasonable adjustments are made for in-person interviews, companies should consider how any algorithmic recruitment process takes these factors into account, meeting their obligations under the Equality Act 2010. Organisations should start by building inclusive design into their processes and include explicit steps for considering how certain tools may impact those with disabilities. This may include increasing the number of people with disabilities hired in development and design teams or offering candidates with disabilities the option of a human-assessed alternative route where appropriate. It is worth noting that some reports have found that AI recruitment could improve the experience for disabled applicants by reducing biases, however this will likely vary depending on the tools and the wide spectrum of barriers to progression faced by candidates with disabilities. A one size fits all approach is unlikely to be successful. Automated rejections are governed by data protection legislation but compliance with guidance seems mixed Most algorithmic tools in recruitment are designed to assist people with decision-making, however some fully automate elements of the process. This appears particularly common around automated rejections for candidates at application stage that do not meet certain requirements. Fully automated decision-making is regulated under Article 22 of the General Data Protection Regulation (GDPR), overseen by the Information Commissioner's Office (ICO). The ICO have set out how this requirement should be operationalised for automated decision-making, with guidance that states organisations should be: It is not clear how organisations screening many thousands of candidates should make provisions for the second of these suggestions, and indeed this is often not common practice for large scale sifts carried out by either algorithmic or non-algorithmic methods. Our research suggested the guidance was rarely applied in the way outlined above, particularly on introducing ways to request human intervention or review. We therefore think there would be value in the ICO working with employers to understand how this guidance (and the more detailed guidance set out in theEmployment Practices Code) is being interpreted and applied, and consider how to ensure greater consistency in the application of the law so individuals are sufficiently able to exercise their rights under data protection. Recommendations to regulators: Recommendation 2:TheInformation Commissioner's Officeshould work with industry to understand why current guidance is not being consistently applied, and consider updates to guidance (e.g. in the Employment Practices Code), greater promotion of existing guidance, or other action as appropriate. Clearly it would be most helpful for the EHRC and ICO to coordinate their work to ensure that organisations have clarity on how data protection and equality law requirements interact; they may even wish to consider joint guidance addressing recommendations 1 and 2. Topics where there may be relevant overlaps include levels of transparency, auditing and recording recommended to improve standards of practice and ensure legal compliance. CDEI is happy to support this collaboration. Summary Overview of findings: Financial services organisations have long used data to support their decision-making. They range from being highly innovative to more risk averse in their use of new algorithmic approaches. For example, when it comes to credit scoring decisions, most banks are using logistic regression models rather than more advanced machine learning algorithms. There are mixed views and approaches amongst financial organisations on the collection and use of protected characteristics data and this affects the ability of organisations to check for bias. Explainability of models used in financial services, in particular in customer-facing decisions, is key for organisations and regulators to identify and mitigate discriminatory outcomes and for fostering customer trust in the use of algorithms. The regulatory picture is clearer in financial services than in the other sectors we have looked at. The Financial Conduct Authority (FCA) is the lead regulator and is conducting work to understand the impact and opportunities of innovative uses of data and AI in the sector. Future CDEI work: Financial services organisations make decisions which have a significant impact on our lives, such as the amount of credit we can be offered or the price our insurance premium is set at. Algorithms have long been used in this sector but more recent technological advances have seen the application of machine learning techniques to inform these decisions.[footnote 4]Given the historical use of algorithms, the financial services industry is well-placed to embrace the most advanced data-driven technology to make better decisions about which products to offer to which customers. However, these decisions are being made in the context of a socio-economic environment where financial resources are not spread evenly between different groups. For example, there is established evidence documenting the inequalities experienced by ethnic minorities and women in accessing credit, either as business owners or individuals, though these are generally attributed to wider societal and structural inequalities, rather than to the direct actions of lenders.[footnote 45]If financial organisations rely on making accurate predictions about peoples' behaviours, for example how likely they are to repay debts like mortgages, and specific individuals or groups are historically underrepresented in the financial system, there is a risk that these historic biases could be entrenched further through algorithmic systems.[footnote 46] In theory, using more data and better algorithms could help make these predictions more accurate. For example, incorporating non-traditional data sources could enable groups who have historically found it difficult to access credit, because of a paucity of data about them from traditional sources, to gain better access in future. At the same time, more complex algorithms could increase the potential of indirect bias via proxy as we become less able to understand how an algorithm is reaching its output and what assumptions it is making about an individual in doing so. Difficulty inassessingcredit discrimination by gender New York's Department of Financial Services' investigated Goldman Sachs for potential credit discrimination by gender. This came from the web entrepreneur David Heinemeier Hansson who tweeted that the Apple Card, which Goldman manages, had given him a credit limit 20 times that extended to his wife, though the two filed joint tax returns and she had a better credit score. Goldman Sachs' response was that it did not consider gender when determining creditworthiness, as this would be illegal in the US, and therefore there was no way they could discriminate on the basis of gender. The full facts around this case are not yet public, as the regulatory investigation is ongoing. Nonetheless, there is evidence that considering gender could help mitigate gender bias or at least test the algorithm to better understand whether it is biased. This example brings up a key challenge for financial organisations in terms of testing for bias which we will explore later in this chapter. Despite plenty of anecdotal evidence, there has previously been a lack of structured evidence about the adoption of machine learning (ML) in UK financial services. In 2018, a Financial Times global survey of banks provided evidence of a cautious approach being taken by firms.[footnote 48]However, in 2019, the Bank of England and FCA conducted a joint survey into the use of ML in financial services, which was the first systematic survey of its kind. The survey found that ML algorithms were increasingly being used in UK financial services, with two thirds of respondents[footnote 49]reporting its use in some form and the average firm using ML applications in two business areas.[footnote 50]It also found that development is entering the more mature stages of deployment, in particular in the banking and insurance sectors. The survey focused on ML algorithms in financial services, rather than rules-based algorithms. The key difference being that a human does not explicitly programme an ML algorithm, but instead computer programmes fit a model or recognise patterns from data. Many ML algorithms constitute an incremental, rather than fundamental, change in statistical methods used in financial services. They also provide more flexibility as they are not constrained by the linear relationships often imposed in traditional economic and financial analysis and can often make better predictions than traditional models or find patterns in large amounts of data from increasingly diverse sources. The key uses of ML algorithms in financial services are to inform back-office functions, such as risk management and compliance. This can include identifying third parties who are trying to damage customers, or the bank itself, through fraud, identity theft and money laundering. This is the area where ML algorithms find the highest extent of application due to their ability to connect large datasets and pattern detection. However, ML algorithms are also increasingly being applied to front-office areas, such as credit scoring, where ML applications are used in granting access to credit products such as credit cards, loans and mortgages. The Financial Conduct Authority and the Bank of England conducted a survey about the maturity of machine learning tools, by business area within financial services. The survey results are organised by the following levels of deployment: ""initial experiments"", ""development phase"", ""small-scale deployment"", ""medium-scale deployment"" and ""full deployment."" Roughly 30% to 50% of respondents reported ""risk management"", ""customer engagement"" and ""other"" use cases were in ""initial experiments"", ""small-scale deployment"" and ""full deployment"". Approximately 20% to 30% of respondents reported that ""credit"" and ""sales and trading"" were in ""small-scale deployment"". Around 10% to 20% of respondents reported that ""customer engagement"", ""other"", ""credit"", ""sales and trading"", ""investment banking (M&A, ECM, DCM)"" and ""miscellaneous"" were being deployed at all levels. Between 0% and 10% of respondents stated that ""asset management', ""general insurance"", ""life insurance"" and ""treasury"" uses were present at all levels of deployment. Figure 3:Machine learning maturity of different business areas in financial services, as surveyed by the FCA and Bank of England[footnote 51] The underlying methodology behind these different applications varies from more traditional methods such as logistic regression and random forest models, to more advanced machine learning and natural language processing. There are varying reports on how widely the most advanced tools are being used. For example, the FCA and Bank of England report highlights how many cases of ML development have entered into the more advanced stages of deployment, in particular in the banking and insurance sectors.[footnote 52] We have seen a real acceleration in the last five years with machine learning and deep learning being more widely adopted in financial services. We do not see this slowing down. - CDEI interview with leading credit reference agency The Bank of England has identified the following potential benefits of increased use of algorithmic decision-making in financial services: improved customer choice, services and more accurate pricing; increased access to credit for households and SMEs; substantially lower cross border transaction costs; and improved diversity and resilience of the system.[^53] However, there are obstacles to the adoption of algorithmic decision-making in financial services. Organisations report these to be mainly internal to firms themselves, rather than stemming from regulation, and range from lack of data accessibility, legacy systems, and challenges integrating ML into existing business processes.[footnote 54] The FCA is the lead sector regulator for financial services and regulates 59,000 financial services firms and financial markets in the UK and is the prudential regulator for over 18,000 of those firms. The FCA and Bank of England recently jointly announced that they would be establishing the Financial Services Artificial Intelligence Public-Private Forum (AIPPF).[footnote 55]The Forum was set up in recognition that work is needed to better understand how the pursuit of algorithmic decision-making and increasing data availability are driving change in financial markets and consumer engagement, and a wide range of views need to be gathered on the potential areas where principles, guidance or good practice examples could support the safe adoption of these technologies. Our main focus within financial services has been on credit scoring decisions made about individuals by traditional banks. We did not look in detail at how algorithms are being used by fintech companies and in the insurance industry, but do incorporate key trends and findings from these areas in our Review. We also separately conducted a short piece of research on AI in personal insurance.[footnote 56]In order to understand the key opportunities and risks with regards to the use of algorithms in the financial sector we conducted semi-structured interviews with financial services organisations, predominantly traditional banks and credit reference agencies. We also ran an online experiment with the Behavioural Insights Team to understand people's perceptions of the use of algorithms in credit scoring and how fair they view the use of data which could act as a proxy for sex or ethnicity, particularly newer forms of data, such as social media, in informing these algorithms. On the whole, the financial organisations we interviewed range from being very innovative to more risk averse with regards to the models they are building and the data sources they are drawing on. However, they agreed that the key obstacles to further innovation in the sector were as follows: Algorithms are mainly trained using historical data, with financial organisations being hesitant to incorporate newer, non-traditional, data-sets In our interviews, organisations argued that financial data would be biased due to the fact that, in the past, mainly men have participated in the financial system. One could also see another data-related risk in the fact that there are fewer training datasets for minority communities might result in the reduced performance of investment advice algorithms for these communities. A key challenge is posed by data... the output of a model is only as good as the quality of data fed into it - the so-called ""rubbish in, rubbish out""[footnote 57]problem... AI/ML is underpinned by the huge expansion in the availability and sources of data: as the amount of data used grows, so the scale of managing this problem will increase.[footnote 58]- James Proudman, Bank of England On the whole, financial organisations train their algorithms on historical data. The amount of data that a bank or credit reference agency has at its disposal varies. We know from our interview with one of the major banks that they use data on the location of transactions made, along with data they share with other companies to identify existing credit relationships between banks and consumers. In the case of a credit reference agency we spoke to, models are built on historical data, but are trained on a variety of public sources including applications made on the credit market, the electoral registry, public data, such as filing for bankruptcy, data provided by the clients themselves, and behavioural information such as: turnover, returned items, rental data. In terms of using non-traditional forms of data, the phenomenon of credit-worthiness by association[footnote 59]describes the move from credit scoring algorithms just using data from an individual's credit history, to drawing on additional data about an individual for example their rent repayment history or their wider social network. Of the companies we spoke to, most were not using social media data and were sceptical of its value. For example, a credit reference agency and major bank we interviewed had explored using social media data a few years ago, but decided against it as they did not believe it would sufficiently improve the accuracy of the algorithm to justify its use. The use of more data from non-traditional sources could enable population groups who have historically found it difficult to access credit, due to there being less data about them from traditional sources, to gain better access in future. For example in our interview with a credit reference agency they spoke of customers who are referred to as ""thin files"", as there is little data available about them, which could be a source of financial exclusion. Their approach with these customers is to ensure decisions about them are subjected to manual review. In order to address the problem of ""thin files"", Experian added rent repayments to the credit reports of more than 1.2 million tenants in the UK with the intention of making it easier for renters to access finance deals.[footnote 60] While having more data could improve inclusiveness and improve the representativeness of datasets, more data and more complex algorithms could also increase the potential for the introduction of indirect bias via proxy as well as the ability to detect and mitigate it. Although there is a general standard not to collect protected characteristics data, financial organisations are developing approaches to testing their algorithms for bias It is common practice to avoid using data on protected characteristics, or proxies for those characteristics, as inputs into decision-making algorithms, as to do so is likely to be unlawful or discriminatory. However, understanding the distribution of protected characteristics among the individuals affected by a decision is necessary to identify biased outcomes. For example, it is difficult to establish the existence of a gender pay gap at a company without knowing whether each employee is a man or woman. This tension between the need to create algorithms which are blind to protected characteristics while also checking for bias against those same characteristics creates a challenge for organisations seeking to use data responsibly. This means that whilst organisations will go to lengths to ensure they are not breaking the law or being discriminatory, their ability to test how the outcomes of their decisions affect different population groups is limited by the lack of demographic data. Instead organisations test their model's accuracy through validation techniques[footnote 61]and ensuring sufficient human oversight of the process as a way of managing bias in the development of the model. Case study - London fintech company We spoke to a London fintech company which uses supervised ML in order to predict whether people are able to repay personal loans and to detect fraud. In line with legislation, they do not include protected characteristics in their models, but to check for bias they adopt a 'fairness through unawareness' approach involving ongoing monitoring and human judgement. The ongoing monitoring includes checking for sufficiency across the model performance, business optimisation and building test models to counteract the model. The human judgement involves interpreting the direction in which their models are going and if a variable does not fit the pattern rejecting or transforming it. This approach requires significant oversight to ensure fair operation and to effectively mitigate bias. Some organisations do hold some protected characteristic data, which they do not use in their models. For example, a major bank we interviewed has access to sex, age and postcode data on their customers, and can test for bias on the basis of sex and age. Moreover, banks advise that parameters that they consider to strongly correlate with protected characteristics are usually removed from the models. Given there is no defined threshold for bias imposed by the FCA or any standards body, organisations manage risks around algorithmic bias using their own judgement and by managing data quality. A small proportion of companies analyse model predictions on test data, such as representative synthetic data or anonymised public data. The extent to which a problem of algorithmic bias exists in financial services is still relatively unclear given that decisions around finance and credit are often highly opaque for reasons of commercial sensitivity and competitiveness. Even where it is apparent that there are differences in outcomes for different demographic groups, without extensive access to the models used by companies in their assessments of individuals, and access to protected characteristic data, it is very difficult to determine whether these differences are due to biased algorithms or to underlying societal, economic or structural causes. Insights from our work in financial services have fed into our wider recommendation around collecting protected characteristics data which is set out in Chapter 7. Case study: Bias in insurance algorithms A Propublicainvestigationin the US found that people in minority neighbourhoods on average paid higher car insurance premiums than residents of majority-white neighbourhoods, despite having similar accident costs. While the journalists could not confirm the cause of these differences, they suggest biased algorithms may be to blame. Like any organisation using algorithms to make significant decisions, insurers must be mindful of the risks of bias in their AI systems and take steps to mitigate unwarranted discrimination. However, there may be some instances where using proxy data may be justified. For example, while car engine size may be a proxy for sex, it is also a material factor in determining damage costs, giving insurers more cause to collect and process information related to it. Another complication is that insurers often lack the data to identify where proxies exist. Proxies can in theory be located by checking for correlations between different data points and the protected characteristic in question (e.g. between the colour of a car and ethnicity). Yet insurers are reluctant to collect this sensitive information for fear of customers believing the data will be used to directly discriminate against them. The Financial Conduct Authority conductedresearchin 2018 on the pricing practices of household insurance firms. One of the key findings was the risk that firms could discriminate against consumers by using rating factors in pricing based either directly or indirectly on data relating to or derived from protected characteristics. The FCA has since done further work, including a market study and initiating a public debate, on fair pricing and related possible harms in the insurance industry. Ensuring explainability of all models used in financial services is key Explainability refers to the ability to understand and summarise the inner workings of a model, including the factors that have gone into the model. As set out in Section 2.5, explainability is key to understanding the factors causing variation in outcomes of decision-making systems between different groups and to assess whether or not this is regarded as fair. In polling undertaken for this review, of the possible safeguards which could be put in place to ensure an algorithmic decision-making process was as fair as possible, an easy to understand explanation came in second in a list of six options, after only human oversight. In the context of financial services, the explainability of an algorithm is important for regulators, banks and customers. For banks, when developing their own algorithms, explainability should be a key requirement in order to have better oversight of what their systems do and why, so they can identify and mitigate discriminatory outcomes. For example when giving loans, using an explainable algorithm makes it possible to examine more directly the degree to which relevant characteristics are acting as a proxy for other characteristics, and causing differences in outcomes between different groups. This means that where there may be valid reasons for loans to be disproportionately given to one group over another this can be properly understood and explained. For customers, explainability is crucial so that they can understand the role the model has played in the decision being made about them. For regulators, understanding how an algorithmically-assisted decision was reached is vital to knowing whether an organisation has met legal requirements and treated people fairly in the process. Indeed, the expert panel we convened for our AI Barometer discussions, viewed a lack of explainability for regulators as a significantly greater risk than a lack of transparency for consumers of algorithmic decision-making in financial services.[footnote 65] The lack of explainability of machine learning models was highlighted as one of the top risks by respondents to the Bank of England and FCA's survey. The survey highlighted that in use cases such as credit scoring where explainability was a priority, banks were opting for logistic regression techniques, with ML elements, to ensure decisions could be explained to customers where required. However, research by the ICO[footnote 66]has shown that while some organisations in banking and insurance are continuing to select interpretable models in their customer-facing AI decision-support applications, they are increasingly using more opaque 'challenger' models alongside these, for the purposes of feature engineering or selection, comparison, and insight. In our interviews with banks they reported using tree-based models, such as 'random forests', as they claim they generate the most accurate predictions. However, they acknowledged that the size and complexity of the models made it difficult to explain exactly how they work and the key variables that drive predictions. As a result, logistic regression techniques with ML elements continue to be popular in this type of use case, and provide a higher degree of apparent explainability. There are approaches to breaking down the procedures of neural networks in order to justify why a decision is made about a particular customer or transaction. In our interview with a credit reference agency they described an example in which their US team had calculated the impact that every input parameter had on the final score and then used this information to return the factors that had the biggest impact, in a format that was customer-specific but still general enough to work across the entire population. This then means a low credit score could be explained with a simple statement such as ""rent not paid on time"". Nonetheless, even if there are approaches to explain models at a system level and understand why credit has been denied, these are not always directly available as individual level explanations to customers and it may be difficult to assign it to one factor, rather than a combination. In other cases, firms are required to provide information about individual decisions. This includes under the GDPR (Articles 13, 14, 15 and 22 in particular), under FCA rules for lenders and under industry standards such as the Standards of Lending Practice. The risks to explainability may not always come from the type of model being used, but from other considerations for example commercial sensitivities or concerns that people may game or exploit a model if they know too much about how it works. Interestingly, public attitudesresearchconducted by the RSA suggested that customers could consider some circumstances in which commercial interests could supersede individuals' rights, for example when making financial decisions, in recognition that providing a detailed explanation could backfire by helping fraudsters outwit the system, and where such interests should be overruled. The firms we interviewed reported mostly designing and developing tools in-house, apart from sometimes procuring from third-parties for the underlying platforms and infrastructure such as cloud computing, which should mean that intellectual property considerations do not impinge on explainability standards. Nonetheless, where there may be commercial sensitivities, concerns around gaming, or other risks these should be clearly documented from the outset and justified in the necessary documentation. There are clear benefits to organisations, individuals and society in explaining algorithmic decision-making in financial services. Providing explanations to individuals affected by a decision can help organisations ensure more fairness in the outcomes for different groups across society. Moreover, for organisations it makes business sense as a way of building trust with your customers by empowering them to understand the process and providing them the opportunity to challenge where needed. ICO and the Alan Turing Institute's Explainability Guidance In May 2020, the ICO and the Alan Turing Institute published theirguidanceto organisations on how to explain decisions made with AI, to the individuals affected by them. The guidance sets out key concepts and different types of explanations, along with more tailored support to senior management teams on policies and procedures organisations should put in place to ensure they provide meaningful explanations to affected individuals. The FCA has fed into this guidance to ensure it takes into account the opportunities and challenges facing banks in explaining AI-assisted decisions to customers. Public acceptability of the use of algorithms in financial services is higher than in other sectors, but can be difficult to gauge In polling undertaken for this review, when asked how aware they were of algorithms being used to support decisions in the context of the four sectors we have looked at in this report, financial services was the only option selected by a majority of people (around 54-57%). This was in contrast to only 29-30% of people being aware of their use in local government. In our interviews with financial companies, it was evident they were making efforts to understand public acceptability, mainly in the context of their customers. For example, financial organisations we interviewed had conducted consumer polling and focus groups to understand how the public felt about the use of payment data. In another interview, we learnt that a bank gauged public acceptability with a focus on customer vulnerability, by conducting surveys and interviews, but also by considering the impact of a new product on customers through their risk management framework. Moreover, each product goes through an annual review, which takes into account if there have been any problems, for example customer complaints. In order to better understand public attitudes we conducted a public engagement exercise with the Behavioural Insights Team (BIT)[footnote 69]through their online platform, Predictiv. We measured participants' perceptions of fairness of banks' use of algorithms in loan decisions. In particular we wanted to understand how people's fairness perceptions of banking practices varied depending on the type of information an algorithm used in a loan decision, for example the use of a variable which could serve as a proxy for a protected characteristic such as sex or ethnicity. We found that, on average, people moved twice as much money away from banks that use algorithms in loan application decisions, when told that the algorithms draw on proxy data for protected characteristics or social media data. Not surprisingly, those historically most at risk of being discriminated against in society feel most strongly that it is unfair for a bank to use proxy information for protected characteristics. For example, directionally, women punish the bank that used information which could act as a proxy for sex more strongly than men. However, some people thought it was fair to use the proxy variable if it produced a more accurate result. This brings into question whether there are legitimate proxies, for example salary, which although could function as proxies for sex and ethnicity, may also accurately assist a bank in making decisions around loan eligibility. The experiment also found that people are less concerned about the use of social media data than about data that relates to sex and ethnicity. However, the frequency with which an individual uses social media does not have an impact on how concerned they are about its use in informing loan decisions. This experiment highlighted the challenges in framing questions about a bank's use of algorithms in an unbiased and nuanced way. More research is needed into the use of proxies and non-traditional forms of data in financial services to give financial organisations the confidence that they are innovating in a way that is deemed acceptable by the public. Regulation on bias and fairness in financial services is currently not seen as an unjustified barrier to innovation, but additional guidance and support would be beneficial The majority (75%) of respondents to the FCA and the Bank of England's survey, said that they did not consider Prudential Regulation Authority[footnote 70]/ FCA regulations an unjustified barrier to deploying ML algorithms. This view was supported by organisations we interviewed. This may be because the FCA has responded constructively to the increased use of ML algorithms and is proactively finding ways to support ethical innovation. However, there were respondents in the survey who noted challenges of meeting regulatory requirements to explain decision-making when using more advanced, complex algorithms. Moreover, firms also highlighted that they would benefit from additional guidance from regulators on how to apply existing regulations to ML. Whilst it is positive that the FCA is seen as a constructive, innovation-enabling regulator, future regulation may need to be adapted or adjusted to account for developments in ML algorithms in order to protect consumers. The AIPPF will be well-placed to identify where this may be the case whilst also identifying good practice examples. Future CDEI work:CDEI will be an observer on the Financial Conduct Authority and Bank of England's AI Public Private Forum which will explore means to support the safe adoption of machine learning and artificial intelligence within financial services. Summary Overview of findings: Adoption of algorithmic decision-making is at an early stage, with very few tools currently in operation in the UK. There is a varied picture across different police forces, both on levels of usage and approaches to managing ethical risks. Police forces have identified opportunities to use data analytics and AI at scale to better allocate resources, but there is a significant risk that without sufficient care systematically unfair outcomes could occur. The use of algorithms to support decision-making introduces new issues around the balance between security, privacy and fairness. There is a clear requirement for strong democratic oversight of this and meaningful engagement with the public is needed on which uses of police technology are acceptable. Clearer national leadership is needed around the ethical use of data analytics in policing. Though there is strong momentum in data ethics in policing at a national level, the picture is fragmented with multiple governance and regulatory actors and no one body fully empowered or resourced to take ownership. A clearer steer is required from the Home Office. Recommendation to government: Advice to police forces/ suppliers: Police forces should conduct an integrated impact assessment before investing in new data analytics software as a full operational capability, to establish a clear legal basis and operational guidelines for use of the tool. Further details of what the integrated impact assessment should include are set out in the report we commissioned from RUSI. Police forces should classify the output of statistical algorithms as a form of police intelligence, alongside a confidence rating indicating the level of uncertainty associated with the prediction. Police forces should ensure that they have appropriate rights of access to algorithmic software and national regulators should be able to audit the underlying statistical models if needed (for instance, to assess risk of bias and error rates). Intellectual property rights must not be a restriction on this scrutiny. Future CDEI work: There have been notable government reviews into the issue of bias in policing which are important when considering the risks and opportunities around the use of technology in policing. For example, the 2017 Lammy Review[footnote 71]found that BAME individuals faced bias, including overt discrimination, in parts of the justice system. And prior to the Lammy Review, the 1999 public inquiry into the fatal stabbing of Black teenager Stephen Lawrence branded the Metropolitan Police force ""institutionally racist""[footnote 72].More recently, the 2017 Race Disparity Audit[footnote 73]highlighted important disparities in treatment and outcomes for BAME communities along with lower levels of confidence in the police among younger Black adults. With these findings in mind, it is vital to consider historic and current disparities and inequalities when looking at how algorithms are incorporated into decision-making in policing. Whilst there is no current evidence of police algorithms in the UK being racially biased, one can certainly see the risks of algorithms entrenching and amplifying widely documented human biases and prejudices, in particular against BAME individuals, in the criminal justice system. The police have long been under significant pressure and scrutiny to predict, prevent and reduce crime. But as Martin Hewitt QPM, Chair of the National Police Chiefs' Council (NPCC) and other senior police leaders, have highlighted that ""the policing environment has changed profoundly in many ways and the policing mission has expanded in both volume and complexity. This has taken place against a backdrop of diminishing resources"".[footnote 74] Prime Minister Boris Johnson's announcement to recruit 20,000 new police officers, as one of his headline policy pledges[footnote 75], signals a government commitment to respond to mounting public unease about local visibility of police officers. But the decentralised nature of policing in England and Wales means that each force is developing their own plan for how to respond to these new pressures and challenges. Police forces have access to more digital material than ever before[footnote 76], and are expected to use this data to identify connections and manage future risks. Indeed, the PS63.7 million ministerial funding announcement[footnote 77]in January 2020 for police technology programmes, amongst other infrastructure and national priorities, demonstrates the government's commitment to police innovation. In response to these incentives to innovate, some police forces are looking to data analytics tools to derive insight, inform resource allocation and generate predictions. But drawing insights and predictions from data requires careful consideration, independent oversight and the right expertise to ensure it is done legally, ethically and in line with existing policing codes[footnote 78]. Despite multiple legal frameworks and codes setting out clear duties, the police are facing new challenges in adhering to the law and following these codes in their development and use of data analytics. Case study: Innovation in Avon and Somerset Constabulary Avon and Somerset Constabulary have been successful in building in-house data science expertise through their Office for Data Analytics. One of their tools is Qlik Sense, a software product that connects the force's own internal databases and other local authority datasets. It applies predictive modelling to produce individual risk-assessment and intelligence profiles, to assist the force in triaging offenders according to their perceived level of risk. Although Avon and Somerset Constabulary do not operate a data ethics committee model, like West Midlands Police, they do have governance and oversight processes in place. Moreover, their predictive models are subject to ongoing empirical validation, which involves revisiting models on a quarterly basis to ensure they are accurate and adding value. In theory, tools which help spot patterns of activity and potential crime, should lead to more effective prioritisation and allocation of scarce police resources. A range of data driven tools are being developed and deployed by police forces including tools which help police better integrate and visualise their data, tools which help guide resource allocation decisions and those that inform decisions about individuals such as someone's likelihood to reoffend. However, there is a limited evidence base regarding the claimed benefits, scientific validity or cost effectiveness of police use of algorithms[footnote 79]. For example, there is empirical evidence around the effectiveness of actuarial tools to predict reoffending. However, experts disagree over the statistical and theoretical validity of individual risk-assessment tools. More needs to be done to establish benefits of this technology. In order to do this the technology must be tested in a controlled, proportionate manner, following national guidelines. The use of data-driven tools in policing also carries significant risk. The Met Police's Gangs Matrix[footnote 80]is an example of a highly controversial intelligence and prioritisation tool in use since 2011. The tool intends to identify those at risk of committing, or being a victim of, gang-related violence in London. Amnesty International raised serious concerns with the Gangs Matrix in 2018, in particular that it featured a disproportionate number of Black boys and young men and people were being kept on the database despite a lack of evidence and a reliance on out-of-date information[footnote 81]. In addition, the Gangs Matrix was found by the Information Commissioner's Office to have breached data protection laws and an enforcement notice was issued to the Met Police[footnote 82]. Since, the Mayor of London, Sadiq Khan announced an overhaul[footnote 83]of the Gangs Matrix highlighting that the number of people of a Black African Caribbean background added to the database had dropped from 82.8 per cent in 2018 to 66 per cent in 2019. The Gangs Matrix is likely to continue to be closely scrutinised by civil society, regulators and policymakers. It is evident that without sufficient care, the use of intelligence and prioritisation tools in policing can lead to outcomes that are biased against particular groups, or systematically unfair in other regards. In many scenarios where these tools are helpful, there is still an important balance to be struck between automated decision-making and the application of professional judgement and discretion. Where appropriate care has been taken internally to consider these issues fully, it is critical for public trust in policing that police forces are transparent in how such tools are being used. Given the breadth of applications and areas where technology is being used in law enforcement we chose to focus on the use of data analytics in policing to derive insights, inform operational decision-making or make predictions. This does not include biometric identification, automated facial recognition[footnote 84], digital forensics or intrusive surveillance. However, some of the opportunities, risks and potential approaches that we discuss remain relevant to other data-driven technology issues in policing. To build on and strengthen existing research and publications on these issues[footnote 85]we commissioned new, independent research from the Royal United Services Institute (RUSI).[footnote 86]The aim of this research was to identify the key ethical concerns, in particular on the issue of bias, and propose future policy to address these issues. We incorporated the findings in RUSI's Report[footnote 87]into this chapter and, where relevant, throughout this report. We also issued a call for evidence on the use of algorithmic tools, efforts to mitigate bias, engagement with the public on these issues, and governance and regulation gaps across the four sectors addressed in this report, including policing, receiving a diverse range of responses. We have conducted extensive stakeholder engagement over the last year to understand the key challenges and concerns about the development and use of data analytics tools in this sector. For example, we have spoken to local police forces, including Avon and Somerset, Durham, Essex, Hampshire, Police Scotland and South Wales. Working with West Midlands Police West Midlands Police are one of the leading forces in England and Wales in the development of data analytics. They have an in-house data analytics lab and are the lead force on the National Data Analytics Solution. Their PCC has also set up anEthics Committeeto review data science projects developed by the lab and advise the PCC and Chief Constable on whether the proposed project has sufficiently addressed legal and ethical considerations. We have met with representatives at West Midlands Police and the PCC's Office multiple times throughout this project and we were invited to observe a meeting of their ethics committee. They were also interviewed for and contributed to the RUSI report and development of our policing framework. We are interested in seeing, going forward, to what extent other forces follow the West Midlands PCC Ethics Committee model and hope to continue working closely with West Midlands on future policing work. We established a partnership with the Cabinet Office's Race Disparity Unit (RDU), a UK Government unit which collates, analyses and publishes government data on the experiences of people from different ethnic backgrounds in order to drive policy change where disparities are found. We have drawn on their expertise to better understand how algorithmic decision-making could disproportionately impact ethnic minorities. Our partnership has included jointly meeting with police forces and local authorities, along with the RDU and their Advisory Group contributing to our roundtables with RUSI and reviewing our report and recommendations. We have met with senior representatives from policing bodies including the National Police Chiefs' Council (NPCC), Her Majesty's Inspectorate of Constabulary and Fire Rescue Services (HMICFRS), the Police ICT Company, the College of Policing, the Association of Police and Crime Commissioners (APCC), and regulators with an interest in this sector, including the Information Commissioner's Office. We have also engaged with teams across the Home Office, with an interest in police technology. A draft framework to support police to develop data analytics ethically CDEI has been developing a Draft Framework to support police in innovating ethically with data. It is intended for police project teams developing or planning to develop data analytics tools. It should also help senior decision-makers in the police identify the problems best addressed using data analytics along with those not suited to a technological solution. The Framework is structured around the agile delivery cycle and sets out the key questions that should be asked at each stage. We have tested the Framework with a small group of stakeholders from police forces, academics and civil society and plan to release it more widely following the publication of this review. The feedback we have received to date has also highlighted that a well-informed public debate around AI in policing is missing. These are complex issues where current public commentary is polarised. But without building a common consensus on where and how it is acceptable for police to use AI, the police risk moving ahead without public buy-in. CDEI will be exploring options for facilitating that public conversation going forward and testing the Framework with police forces. Future CDEI work:CDEI will be applying and testing its draft ethics framework for police use of data analytics with police partners on real-life projects and developing underlying governance structures to make the framework operational. Algorithms are in development and use across some police forces in England and Wales but the picture is varied From the responses we received to ourcall for evidenceand wider research, we know there are challenges in defining what is meant by an algorithmic tool and consequently understanding the extent and scale of adoption. In line with this, it is difficult to say with certainty how many police forces in England and Wales are currently developing, trialling or using algorithms due in part to different definitions and also a lack of information sharing between forces. The RUSI research surfaced different terms being used to refer to data analytics tools used by police forces. For example, several interviewees considered the term 'predictive policing' problematic. Given that many advanced analytics tools are used to 'classify' and 'categorise' entities into different groups, it would be more accurate to describe them as tools for 'prioritisation' rather than 'prediction'. For instance, 'risk scoring' offenders according to their perceived likelihood of reoffending by comparing selected characteristics within a specified group does not necessarily imply that an individual is expected to commit a crime. Rather, it suggests that a higher level of risk management is required than the level assigned to other individuals within the same cohort. RUSI sorted the application of data analytics tools being developed by the police into the following categories: Data scoring tools: the use of advanced machine learning algorithms applied to police data to generate 'risk' scores of known offenders. Other: complex algorithms used to forecast demand in control centres, or triage crimes for investigation according to their predicted 'solvability'. Examples of the data scoring tools include: A Harm Assessment Risk Tool (HART), developed and being deployed by Durham police. It uses supervised machine learning to classify individuals in terms of their likelihood of committing a violent or non-violent offence within the next two years. Use of Qlik Sense (a COTS analytics platform) by Avon and Somerset to link data from separate police databases to generate new insights into crime patterns. The Integrated Offender Management Model, in development but not currently deployed by West Midlands Police. It makes predictions as to the probability that an individual will move from committing low / middling levels of harm, via criminal activity, to perpetrating the most harmful offending. There have also been reports of individual forces buying similar technology, for example the Origins software which is reportedly currently being used by the Metropolitan Police Service and has previously been used by several forces including Norfolk, Suffolk, West Midlands and Bedfordshire police forces[footnote 91]. The software intends to help identify whether different ethnic groups ""specialise"" in particular types of crime and has come under strong critique from equality and race relations campaigners who argue that it is a clear example of police forces racial profiling at a particularly fraught time between police and the Black community. In England and Wales, police forces are currently taking a variety of different approaches to their development of algorithmic systems, ethical safeguards, community engagement and data science expertise. Mitigating bias and ensuring fairness requires looking at the entire decision-making process As set out earlier in the report, we think it is crucial to take a broad view of the whole decision-making process when considering the different ways bias can enter a system and how this might impact on fairness. In the context of policing, this means not only looking at the development of an algorithm, but also the context in which it is deployed operationally. At the design and testing stage, there is a significant risk of bias entering the system due to the nature of the police data which the algorithms are trained on. Police data can be biased due to it either being unrepresentative of how crime is distributed or in more serious cases reflecting unlawful policing practices. It is well-documented[footnote 92]that certain communities are over or under-policed and certain crimes are over or under-reported. For example, a police officer interviewed in our RUSI research, highlighted that 'young Black men are more likely to be stopped and searched than young white men, and that's purely down to human bias'. Indeed this is backed by Home Office data released last year stating that those who identify as Black or Black British are 9.7 times more likely to be stopped and searched by an officer than a white person[footnote 93]. Another way police data can provide a misrepresentative picture is that individuals from disadvantaged socio demographic backgrounds are likely to engage with public services more frequently, which means that more data is held on them. Algorithms could then risk calculating groups with more data held on them by the police as posing a greater risk. Further empirical research is needed to assess the level of bias in police data and the impact of that potential bias. A further challenge to be considered at this stage is the use of sensitive personal data to develop data analytics tools. Whilst models may not include a variable for race, in some areas postcode can function as a proxy variable for race or community deprivation, thereby having an indirect and undue influence on the outcome prediction. If these biases in the data are not understood and managed early on this could lead to the creation of a feedback loop whereby future policing, not crime, is predicted. It could also influence how high or low risk certain crimes or areas are deemed by a data analytics tool and potentially perpetuate or exacerbate biased criminal justice outcomes for certain groups or individuals. At the deployment stage, bias may occur in the way the human decision-maker receiving the output of the algorithm responds. One possibility is that the decision-maker over-relies on the automated output, without applying their professional judgement to the information. The opposite is also possible, where the human decision-maker feels inherently uncomfortable with taking insights from an algorithm to the point where they are nervous to use it at all[footnote 94], or simply ignores it in cases where their own human bias suggests a different risk level. A balance is important to ensure due regard is paid to the insights derived, whilst ensuring the professional applies their expertise and understanding of the wider context and relevant factors. It has been argued, for example by Dame Cressida Dick in her keynote address at the launch event of the CDEI/RUSI report on data analytics in policing, that police officers may be better equipped than many professionals to apply a suitable level of scepticism to the outcome of an algorithm, given that weighing the reliability of evidence is so fundamental to their general professional practice. Without sufficient care of the multiple ways bias can enter the system, outcomes can be systematically unfair and lead to bias and discrimination against individuals or those within particular groups. There is a need for strong national leadership on the ethical use of data analytics tools The key finding from the RUSI research was a ""widespread concern across the UK law enforcement community regarding the lack of any official national guidance for the use of algorithms in policing, with respondents suggesting that this gap should be addressed as a matter of urgency.""[footnote 95]Without any national guidance, initiatives are being developed to different standards and to varying levels of oversight and scrutiny. For example, while all police forces in England and Wales have established local ethics committees, these are not currently resourced to look at digital projects. Instead, some Police and Crime Commissioners, like West Midlands, have established data ethics committees to provide independent ethical oversight to police data analytics projects. However, given the absence of guidance it is unclear whether each force should be setting up data ethics committees, upskilling existing ones, or whether regional or a centralised national structure should be set up to provide digital ethical oversight to all police forces. A review of existing police ethics committees would be useful in order to develop proposals for ethical oversight of data analytics projects. Similarly, the lack of national coordination and oversight means that data initiatives are developed at a local level. This can lead to pockets of innovation and experimentation. However, it also risks meaning that efforts are duplicated, knowledge and lessons are not transferred across forces and systems are not made interoperable. As described by a senior officer interviewed as part of the RUSI project, ""it's a patchwork quilt, uncoordinated, and delivered to different standards in different settings and for different outcomes"".[footnote 96] There is work underway at a national level, led by the National Police Chiefs' Council, in order to develop a coordinated approach to data analytics in policing. This is reflected in the National Digital Policing Strategy[footnote 97], which sets out an intention to develop a National Data Ethics Governance model, and to provide clear lines of accountability on data and algorithm use at the top of all policing organisations. This should continue to be supported to ensure a more consistent approach across police forces. Moreover, HMICFRS should be included in national work in this area for example by establishing an External Reference Group for police use of data analytics, with a view to incorporating use of data analytics and its effectiveness into future crime data integrity inspections. The RUSI report sets out in detail what a policy framework for data analytics in policing should involve and at CDEI we have been developing a draft Framework to support police project teams in addressing the legal and ethical considerations when developing data analytics tools. Without clear, consistent national guidance, coordination and oversight, we strongly believe that the potential benefits of these tools may not be fully realised, and the risks will materialise. Recommendations to government: Recommendation 3: TheHome Officeshould define clear roles and responsibilities for national policing bodies with regards to data analytics and ensure they have access to appropriate expertise and are empowered to set guidance and standards. As a first step, the Home Office should ensure that work underway by the National Police Chiefs' Council and other policing stakeholders to develop guidance and ensure ethical oversight of data analytics tools is appropriately supported. Significant investment is needed in police project teams to address new challenges Whilst it is crucial that a national policy framework is developed, without significant investment in skills and expertise in police forces, no framework will be implemented effectively. If police forces are expected to be accountable for these systems,engage with developers and make ethical decisions including trade-off considerations, significant investment is needed. The announcement to recruit 20,000 new police officers provides an opportunity to bring in a diverse set of skills, however work is needed to ensure existing police officers are equipped to use data analytics tools. We also welcome the announcement in January 2020 of a Police Chief Scientific Advisor and dedicated funding for investment in Science, Technology and Research[footnote 98]as first steps in addressing this skills gap. Based on the RUSI research and our engagement with police stakeholders, we know that a wide range of skills are required, ranging from, and not limited to legal, data science, and evaluation. In particular, our research with RUSI highlighted insufficient expert legal consultation at the development phase of data analytics projects, leading to a problematic delay in adhering to legal requirements. Developing a mechanism by which specialist expertise, such as legal, can be accessed by forces would help ensure this expertise is incorporated from the outset of developing a tool. Moreover, there have been examples where the police force's Data Protection Officer was not involved in discussions at the beginning of the project and has not been able to highlight where the project may interact with GDPR and support with the completion of a Data Protection Impact Assessment. Similarly, upskilling is needed of police ethics committees if they are to provide ethical oversight of data analytics projects. Public deliberation on police use of data-driven technology is urgently needed The decisions police make on a daily basis about which neighbourhoods or individuals to prioritise monitoring affect us all. The data and techniques used to inform these decisions are of great interest and significance to society at large. Moreover, due to wider public sector funding cuts, police are increasingly required to respond to non-crime problems[footnote 99]. For example, evidence suggests that police are spending less time dealing with theft and burglary and more time investigating sexual crime and responding to mental health incidents.[footnote 100] The longstanding Peelian Principles, which define the British approach of policing byconsent, are central to how a police force should behave and their legitimacy in the eyes of the public. And the values at the core of the Peelian Principles, integrity, transparency and accountability, continue to be as relevant today, in particular in light of the ethical considerations brought up by new technologies. Research by the RSA and DeepMind[footnote 101]highlights that people feel more strongly against the use of automated decision systems in the criminal justice system (60 percent of people oppose or strongly oppose its use in this domain) than other sectors such as financial services. Moreover, people are least familiar with the use of automated decision-making systems in the criminal justice system; 83 percent were either not very familiar or not at all familiar with its use. These findings suggest a risk that if police forces move too quickly in developing these tools, without engaging meaningfully with the public, there could be significant public backlash and a loss of trust in the police's use of data. A failure to engage effectively with the public is therefore not only an ethical risk, but a risk to the speed of innovation. Police have many existing ways of engaging with the public through relationships with community groups and through Police and Crime Commissioners (PCC). West Midlands PCC have introduced a community representative role to their Ethics Committee to increase accountability for their use of data analytics tools. However, a civil society representative interviewed by RUSI highlighted that ethics committees could ""act as a fig leaf over wider discussions"" which the police should be having with the public. We should take the steady increase in public trust in police to tell the truth since the 1980s[footnote 102]as a promising overarching trend. This signals an opportunity for police, policymakers, technologists, and regulators, to ensure data analytics tools in policing are designed and used in a way that builds legitimacy and is trustworthy in the eyes of the public. Summary Overview of findings: Local authorities are increasingly using data to inform decision-making across a wide range of services. Whilst most tools are still in the early phase of deployment, there is an increasing demand for sophisticated predictive technologies to support more efficient and targeted services. Data-driven tools present genuine opportunities for local government when used to support decisions. However, tools should not be considered a silver bullet for funding challenges and in some cases will require significant additional investment to fulfil their potential and possible increase in demand for services. Data infrastructure and data quality are significant barriers to developing and deploying data-driven tools; investing in these is necessary before developing more advanced systems. National guidance is needed as a priority to support local authorities in developing and using data-driven tools ethically, with specific guidance addressing how to identify and mitigate biases. There is also a need for wider sharing of best practice between local authorities. Recommendation to government: Future CDEI work: Local authorities are responsible for making significant decisions about individuals on a daily basis. The individuals making these decisions are required to draw on complex sources of evidence, as well as their professional judgement. There is also increasing pressure to target resources and services effectively following a reduction of PS16 billion in local authority funding over the last decade.[footnote 103]These competing pressures have created an environment where local authorities are looking to digital transformation as a way to improve efficiency and service quality.[footnote 104] Whilst most research has found machine learning approaches and predictive technologies in local government to be in a nascent stage, there is growing interest in AI as a way to maximise service delivery and target early intervention as a way of saving resources further down the line when a citizen's needs are more complex.[footnote 105]By bringing together multiple data sources, or representing existing data in new forms, data-driven technologies can guide decision-makers by providing a more contextualised picture of an individual's needs. Beyond decisions about individuals, these tools can help predict and map future service demands to ensure there is sufficient and sustainable resourcing for delivering important services. However, these technologies also come with significant risks. Evidence has shown that certain people are more likely to be overrepresented in data held by local authorities and this can then lead to biases in predictions and interventions.[footnote 106]A related problem is when the number of people within a subgroup is small, data used to make generalisations can result in disproportionately high error rates amongst minority groups. In many applications of predictive technologies, false positives may have limited impact on the individual. However in particularly sensitive areas, such as when deciding if and how to intervene in a case where a child may be at risk, false negatives and positives both carry significant consequences, and biases may mean certain people are more likely to experience these negative effects. Because the risks are more acute when using these technologies to support individual decision-making in areas such as adult and children's services, it is for this reason that we have focused predominantly on these use cases.[footnote 107] Where is data science in local government most being used? Welfare and social care Healthcare Transportation Housing and planning Environment and sustainability Waste management Education Policing and public safety Our work on local government as a sector began with desk based research facilitated through our call for evidence and the landscape summary we commissioned. This evidence gathering provided a broad overview of the challenges and opportunities presented by predictive tools in local government. We wanted to ensure our research was informed by those with first-hand accounts of the challenges of implementing and using these technologies, so we met and spoke with a broad range of people and organisations. This included researchers based in academic and policy organisations, third-party tool providers, local authorities, industry bodies and associations, and relevant government departments. It is difficult to map how widespread algorithmic decision-making is in local government There have been multiple attempts to map the usage of algorithmic decision-making tools across local authorities but many researchers have found this challenging.[footnote 109]An investigation by The Guardian found that, at a minimum, 140 councils out of 408 have invested in software contracts that cover identifying benefit fraud, identifying children at risk and allocating school places.[footnote 110]However this did not include additional use cases found in a report by the Data Justice Lab, a research group based in Cardiff University. The Data Justice Lab used Freedom of Information requests to learn which tools are being used and how frequently. However, there were many challenges with this approach with one fifth of requests being delayed or never receiving a response.[footnote 111]On the part of the local authorities, we have heard that there is often a significant challenge presented by the inconsistent terminology being used to describe algorithmic decision-making systems leading to varied reporting across local authorities using similar technologies. It is also sometimes difficult to coordinate activities across the whole authority because service delivery areas can operate relatively independently. Given the rising interest in the use of predictive tools in local government, local authorities are keen to emphasise that their algorithms support rather than replace decision-makers, particularly in sensitive areas such as children's social services. Our interviews found that local authorities were concerned that the current narrative focused heavily on automation rather than their focus which is more towards using data to make more evidence based decisions. There is a risk that concerns around public reaction or media reporting on this topic will disincentivize transparency in the short-term. However, this is likely to cause further suspicion if data-driven technologies in local authorities appear opaque to the public. This may go on to harm trust if citizens do not have a way to understand how their data is being used to deliver public services. We believe that introducing requirements to promote transparency across the public sector will help standardise reporting, support researchers and build public trust (see Chapter 9 below for further discussion). Comparison: Bias in local government and policing There are many overlaps in the risks and challenges of data-driven technologies in both policing and local government. In both cases, public sector organisations are developing tools with data that may not be high quality and where certain populations are more likely to be represented, which could lead to unintentional discrimination. Both sectors often rely on procuring third-party software and may not have the necessary capacity and capability to question providers over risks around bias and discrimination. There is scope for greater sharing and learning between these two sectors, and the wider public sector, around how to tackle these challenges, as well as considering adopting practices that have worked well in other places. For example, local authorities may want to look to certain police forces that have set up ethics committees as a way of providing external oversight of their data projects. Similarly, initiatives to develop integrated impact assessments, taking into account both data protection and equality legislation, may be applicable in both contexts. Some local authorities have developed algorithmic decision-making tools in-house, others have tools procured from third-parties. A. In-house approaches Some local authorities have developed their own tools in-house, such as the Integrated Analytical Hub used by Bristol City Council. Bristol developed the hub in response to the Government's Troubled Families programme, which provided financial incentives to local authorities who could successfully identify and support families at risk.[footnote 112]The hub brings together 35 datasets covering a wide range of topics including school attendance, crime statistics, children's care data, domestic abuse records and health problem data such as adult involvement in alcohol and drug programmes. The datasets are then used to develop predictive modelling with targeted interventions then offered to families who are identified as most at risk.[footnote 113] One of the benefits of using in-house approaches is that they offer local authorities greater control over the data being used. They also require a fuller understanding of the organisation's data quality and infrastructure, which is useful when monitoring the system. However, building tools in-house often require significant investment in internal expertise, which may not be feasible for many local authorities. They also carry significant risks if an in-house project ultimately does not work. B. Third-party software There is an increasing number of third-party providers offering predictive analytics and data analysis software to support decision-making. Software to support detecting fraudulent benefit claims which is reportedly used by around 70 local councils.[footnote 114] Other third-party providers offer predictive software that brings together different data sources and uses them to develop models to identify and target services. The use cases are varied and include identifying children at risk, adults requiring social care, or those at risk of homelessness. Software that helps with earlier interventions has the potential to bring costs down in the longer-term, however this relies on the tools being accurate and precise and so far there has been limited evaluation on the efficacy of these interventions. Third-party providers offer specialist data science expertise that is likely not available to most local authorities and are likely to have valuable experience from previous work with other local authorities. However there are also risks around the costs of procuring the technologies. Transparency and accountability are also particularly important when procuring third-party tools, because commercial sensitivities may prevent providers wanting to share information to explain how a model is developed. Local authorities have a responsibility to understand how decisions are made regardless of whether they are using a third-party or developing an in-house approach, and third-parties should not be seen as a way to outsource these complex decisions. Local authorities should also consider how they will manage risks around bias, that may be outside the scope of the provider's service (see Section 9.3 for further discussion of public sector procurement and transparency). Local authorities struggle with data quality and data sharing when implementing data-driven tools There are multiple challenges local authorities face when introducing new data-driven technologies. Due to legacy systems, local authorities often struggle with maintaining their data infrastructure and developing standardised processes for data sharing. Many of the conversations we had with companies who partnered with local authorities found that the set-up phase took a lot longer than expected due to these challenges which led to costly delays and a need to reprioritise resources. Local authorities should be wary of introducing data-driven tools as a quick-fix, particularly in cases where data infrastructure requires significant investment. For many local authorities, investing in more basic data requirements is likely to reap higher rewards than introducing more advanced technologies at this stage. There is also an associated risk that legacy systems will have poor data quality. Poor data quality creates significant challenges because without a good quality, representative dataset, the algorithm will face the challenge of ""rubbish in, rubbish out"", where poor quality training data results in a poor quality algorithm. One of the challenges identified by data scientists is that because data was being pulled from different sources, data scientists did not always have the necessary access to correct data errors.[footnote 115]As algorithms are only as good as their training data, interrogating the data quality of all data sources being used to develop a new predictive tool should be a top priority prior to procuring any new software. CDEI's work on data sharing One of the challenges most frequently mentioned by local authorities wanting to explore the opportunities presented by data-driven technologies are concerns around data sharing. Often decision-support systems require bringing together different datasets, but physical barriers, such as poor infrastructure, and cultural barriers, such as insufficient knowledge of how and when to share data in line with data protection legislation, often mean that innovation is slow, even in cases where there are clear benefits. For example, we often hear that in children's social services, social workers do not always have access to the data they need to assess whether a child is at risk. Whilst the data may be held within the local authority and there is a clear legal basis for social workers to have access, local authorities experience various challenges in facilitating sharing. For data sharing to be effective, there also needs to be consideration of how to share data whilst retaining trust between individuals and organisations. Our recentreport on data sharingexplores these challenges and potential solutions in more detail. National guidance is needed to govern the use of algorithms in the delivery of public services There is currently little guidance for local authorities wanting to use algorithms to assist decision-making. We found that whilst many local authorities are confident in understanding the data protection risks, they are less clear on how legislation such as the Equality Act 2010 and Human Rights Act 1998 should be applied. There is a risk that without understanding and applying these frameworks, some tools may be in breach of the law. The What Works Centre for Children's Social Care recently commissioned a review of the ethics of machine learning approaches to children's social care, conducted by the Alan Turing Institute and University of Oxford's Rees Centre. They also found that national guidance should be a priority to ensure the ethical development and deployment of new data-driven approaches.[footnote 116]The review concludes that a ""cautious, thoughtful and inclusive approach to using machine learning in children's social care"" is needed, but that this will only be facilitated through a series of recommendations, including nationally mandated standards. The research echoed what we have found in our work, that stakeholders felt strongly that national guidance was needed to protect vulnerable groups against the misuse of their data, including reducing the risk of unintentional biases. Whilst most research has looked at the need for guidance in children's social care, similar challenges are likely to arise across a range of services within local government that make important decisions about individuals, such as housing, adult social care, education, public health. We therefore think that guidance should be applicable across a range of areas, recognising there are likely to be places where supplementary detailed guidance is necessary, particularly where regulatory frameworks differ. Taken together, there is a strong case for national guidelines setting out how to responsibly develop and introduce decision-supporting algorithms in local government. Government departments such as the Department for Education, the Ministry of Housing, Communities, and Local Government (MHCLG) and Department of Health and Social Care are best placed to support and coordinate the development of national guidance. The Local Government Association has also started a project bringing local authorities together to understand the challenges and opportunities with the aim of bringing this expertise together to develop guidance. National guidelines should look to build upon this work. Recommendations to government Recommendation 4:Governmentshould develop national guidance to support local authorities to legally and ethically procure or develop algorithmic decision-making tools in areas where significant decisions are made about individuals, and consider how compliance with this guidance should be monitored. Introducing data-driven technologies to save money may result in significant challenges Local authorities have a variety of motivations for introducing algorithmic tools, with many focused on wanting to improve decision-making. However, given the significant reduction in income over the last decade, there is a drive towards using technology to improve efficiencies in service delivery within local government. In their research exploring the uptake of AI across local government, the Oxford Internet Institute found that deploying tools with cost-saving as a primary motivation was unlikely to yield the results as expected. They state: ""The case for many such projects is often built around the idea that they will save money. In the current climate of intense financial difficulty this is understandable. But we also believe this is fundamentally the wrong way to conceive data science in a government context: many useful projects will not, in the short term at least, save money.""[footnote 117]The focus of predictive tools is often grounded in the idea of early intervention. If there is a way to identify someone who is at risk and put assistive measures in place early, then the situation is managed prior to escalation, thus reducing overall resources. This longer-term way of thinking may result in less demand overall, however in the short-term it is likely to lead to increased workload and investment in preventative services. There is a challenging ethical issue around the follow-up required once someone is identified. We heard examples of local authorities who held off adopting new tools because it would cost too much to follow up on the intelligence provided. Due to the duty of care placed on local authorities, there is also a concern that staff may be blamed for not following up leads if a case later develops. Therefore councils need to carefully plan how they will deploy resources in response to a potential increase in demands for services and should be wary of viewing these tools as a silver bullet for solving resourcing needs. There should be greater support for sharing lessons, best practice and joint working between local authorities Local authorities often experience similar challenges, but the networks to share lessons learned are often ad hoc and informal and rely on local authorities knowing which authorities have used similar tools. The Local Government Association's work has started bringing this knowledge and experience together which is an important first step. There should also be opportunities for central government to learn from the work undertaken within local government, so as not to miss out on the innovation taking place and the lessons learned from challenges that are similar in both sectors. The Local Digital Collaboration Unit within the Ministry of Housing, Communities and Local Government has also been set up to provide support and training to local authorities undertaking digital innovation projects. The Local Digital Collaboration Unit oversees the Local Digital Fund that provides financial support for digital innovation projects in local government. Greater support for this fund, particularly for projects looking at case studies for identifying and mitigating bias in local government algorithms, evaluating the effectiveness of algorithmic tools, public engagement and sharing best practice, would all add significant value. Our research found local authorities thought this fund was a very helpful initiative, however felt that greater investment would improve access to the benefits and be more cost effective over the long term. In Part I we surveyed the issue of bias in algorithmic decision-making, and in Part II we studied the current state in more detail across four sectors. Here, we move on to identify how some of the challenges we identified can be addressed, the progress made so far, and what needs to happen next. There are three main areas to consider: Theenablersneeded by organisations building and deploying algorithmic decision-making tools to help them do so in a fair way (see Chapter 7). Theregulatorylevers, both formal and informal, needed to incentivise organisations to do this, and create a level playing field for ethical innovation (see Chapter 8). How thepublic sector, as a major developer and user of data-driven technology, can show leadership through transparency (see Chapter 9). There are inherent links between these areas. Creating the right incentives can only succeed if the right enablers are in place to help organisations act fairly, but conversely there is little incentive for organisations to invest in tools and approaches for fair decision-making if there is insufficient clarity on the expected norms. Lots of good work is happening to try to make decision-making fair, but there remains a long way to go. We see the status quo as follows: Governance is a key theme throughout this part of the review; how should organisations and regulators ensure that risks of bias are being anticipated and managed effectively? This is not trivial to get right, but there is clear scope for organisations to do better in considering potential impacts of algorithmic decision-making tools, and anticipating risks in advance. The terms anticipatory governance and anticipatory regulation are sometimes used to describe this approach; though arguably anticipatory governance or regulation is simply part of any good governance or regulation. In Chapter 7 we consider how organisations should approach this, in Chapter 8 the role of regulators and the law in doing so, and in Chapter 9 how a habit of increased transparency in the public sector's use of such tools could encourage this. Summary Overview of findings: Many organisations are unsure how to address bias in practice.Support is needed to help them consider, measure, and mitigate unfairness. Improving diversityacross a range of roles involved in technology development is an important part of protecting against certain forms of bias. Government and industry efforts to improve this must continue, and need to show results. Data is needed to monitor outcomes and identify bias, butdata on protected characteristics is not available often enough.One cause is an incorrect belief that data protection law prevents collection or usage; but there are a number of lawful bases in data protection legislation for using protected or special characteristic data for monitoring or addressing discrimination. There are some other genuine challenges in collecting this data, and more innovative thinking is needed in this area; for example, the potential for trusted third party intermediaries. The machine learning community has developed multiple techniques to measure and mitigate algorithmic bias. Organisations should be encouraged to deploy methods that address bias and discrimination. However, there is little guidance on how to choose the right methods, or how to embed them into development and operational processes.Bias mitigation cannot be treated as a purely technical issue; it requires careful consideration of the wider policy, operational and legal context. There is insufficient legal clarity concerning novel techniques in this area; some may not be compatible with equality law. Recommendations to government: Recommendation 5:Governmentshould continue to support and invest in programmes that facilitate greater diversity within the technology sector, building on its current programmes and developing new initiatives where there are gaps. Recommendation 6:Governmentshould work withrelevant regulatorsto provide clear guidance on the collection and use of protected characteristic data in outcome monitoring and decision-making processes. They should then encourage the use of that guidance and data to address current and historic bias in key sectors. Recommendation 7:Governmentand theONSshould open the Secure Research Service more broadly, to a wider variety of organisations, for use in evaluation of bias and inequality across a greater range of activities. Recommendation 8:Governmentshould support the creation and development of data-focused public and private partnerships, especially those focused on the identification and reduction of biases and issues specific to under-represented groups. TheONSandGovernment Statistical Serviceshould work with these partnerships andregulatorsto promote harmonised principles of data collection and use into the private sector, via shared data and standards development. Recommendations to regulators: Advice to industry: Organisations building and deploying algorithmic decision-making tools should make increased diversity in their workforce a priority.This applies not just to data science roles, but also to wider operational, management and oversight roles. Proactive gathering and use of data in the industry is required to identify and challenge barriers for increased diversity in recruitment and progression, including into senior leadership roles. Whereorganisationsoperating within the UK deploy bias detection or mitigation tools developed in the US, they must be mindful that relevant equality law (along with that across much of Europe) is different. Whereorganisationsface historical issues, attract significant societal concern, or otherwise believe bias is a risk, they will need to measure outcomes by relevant protected characteristics to detect biases in their decision-making, algorithmic or otherwise. They must then address any uncovered direct discrimination, indirect discrimination, or outcome differences by protected characteristics that lack objective justification. In doing so,organisationsshould ensure that their mitigation efforts do not produce new forms of bias or discrimination. Many bias mitigation techniques, especially those focused on representation and inclusion, can legitimately and lawfully address algorithmic bias when used responsibly. However, some risk introducing positive discrimination, which is illegal under the Equality Act. Organisations should consider the legal implications of their mitigation tools, drawing on industry guidance and legal advice. Guidance to organisation leaders and boards: Those responsible for governance of organisations deploying or using algorithmic decision-making tools to support significant decisions about individuals should ensure that leaders are in place with accountability for: Understanding the capabilities and limits of those tools. Considering carefully whether individuals will be fairly treated by the decision-making process that the tool forms part of. Making a conscious decision on appropriate levels of human involvement in the decision-making process. Putting structures in place to gather data and monitor outcomes for fairness. Understanding their legal obligations and having carried out appropriate impact assessments. This especially applies in the public sector when citizens often do not have a choice about whether to use a service, and decisions made about individuals can often be life-affecting. There is clear evidence, both from wider public commentary and our research, that many organisations are aware of potential bias issues and are keen to take steps to address them. However, the picture is variable across different sectors and organisations, and many do not feel that they have the right enablers in place to take action. Some organisations are uncertain of how they should approach issues of fairness, including associated reputational, legal and commercial issues. To improve fairness in decision-making, it needs to be as easy as possible for organisations to identify and address bias. A number of factors are required to help build algorithmic decision-making tools and machine learning models with fairness in mind: Sufficient diversity in the workforceto understand potential issues of bias and the problems they cause. Availability of the right datato understand bias in data and models. Access to the righttools and approachesto help identify and mitigate bias. An ecosystem ofexpert individuals and organisations able to supportthem. Governance structuresthat anticipate risks, and build in opportunities to consider the wider impact of an algorithmic tool with those affected. Confidence that efforts to behave ethically (by challenging bias) and lawfully (by eliminating discrimination) will attract thesupport of organisational leadership and the relevant regulatory bodies. Some of these strategies can only be achieved by individual organisations, but the wider ecosystem needs to enable them to act in a way that is both effective and commercially viable. It is always better to acknowledge biases, understand underlying causes, and address them as far as possible, but the 'correct' approach for ensuring fairness in an algorithmic decision-making tool will depend strongly on use case and context. The real-world notion of what is considered 'fair' is as much a legal, ethical or philosophical idea as a mathematical one, which can never be as holistic, or as applicable across cases. What good practice should a team then follow when seeking to ensure fairness in an algorithmic decision-making tool? We investigate the issue further in this chapter. There is increasing recognition that it is not algorithms that cause bias alone, but rather that technology may encode and amplify human biases. One of the strongest themes in responses to ourCall for Evidence, and our wider research and engagement, was the need to have a diverse technology workforce; better able to interrogate biases that may arise throughout the process of developing, deploying and operating an algorithmic decision-making tool. By having more diverse teams, biases are more likely to be identified and less likely to be replicated in these systems. There is a lot to do to make the technology sector more diverse.A report from Tech Nation found that only 19% of tech workers are women.[footnote 117]What is perhaps more worrying is how little this has changed over the last 10 years, compared with sectors such as engineering, which have seen a significant increase in the proportion of women becoming engineers. This gender gap is similarly represented at senior levels of tech companies. Although the representation of people with BAME backgrounds is proportionate to the UK population (15%), when this is broken down by ethnicity we see that Black people are underrepresented by some margin.It should be a priority to improve this representation. Organisations should also undertake research to understand how ethnicity intersects with other characteristics, as well as whether this representation is mirrored at more senior levels.[footnote 118] There is less data on other forms of diversity, which has spurred calls for greater focus on disability inclusion within the tech sector.[footnote 119]Similarly, more work needs to be done in terms of age, socio-economic background, and geographic spread across the UK. It is important to note that the technology sector is doing well in some areas. For example, the tech workforce is much more international than many others.[footnote 120]The workforce relevant to algorithmic decision-making is, of course, not limited to technology professionals; a diverse range of skills is necessary within teams and organisations to properly experience the benefits of diversity and equality.Beyond training and recruitment, technology companies need to support workers by building inclusive workplaces, which are key to retaining, as well as attracting, talented staff from different backgrounds. There are a lot of activities aimed at improving the current landscape. The Government is providing financial support to a variety of initiatives including the Tech Talent Charter[footnote 121], founded by a group of organisations wanting to work together to create meaningful change for diversity in tech. Currently, the charter has around 500 signatories ranging from small start-ups to big businesses and is intending to grow to 600 by the end of 2020. In 2018 the Government also launched a PS1 million ""Digital Skills Innovation Fund"", specifically for helping underrepresented groups develop skills to move into digital jobs. The Government's Office for AI and AI Council is conducting a wide range of work in this area, including helping to drive diversity in the tech workforce, as well as recently securing PS10 million in funding for students from underrepresented backgrounds to study AI related courses.[footnote 122] Recommendations to government Recommendation 5:Governmentshould continue to support and invest in programmes that facilitate greater diversity within the technology sector, building on its current programmes and developing new initiatives where there are gaps. There are also a huge number of industry initiatives and nonprofits aimed at encouraging and supporting underrepresented groups in the technology sector.[footnote 123]They are wide-ranging in both their approaches and the people they are supporting. These efforts are already helping to raise the profile of tech's diversity problem, as well as supporting people who want to either move into the tech sector or develop further within it. The more government and industry can do to support this work the better. Advice to industry Organisations building and deploying algorithmic decision-making tools should make increased diversity in their workforce a priority.This applies not just to data science roles, but also to wider operational, management and oversight roles. Proactive gathering and use of data in the industry is required to identify and challenge barriers for increased diversity in recruitment and progression, including into senior leadership roles. Given the increasing momentum around a wide-range of initiatives springing up both within government and from grassroots campaigns, we hope to soon see a measurable improvement in data on diversity in tech. A key part of understanding whether a decision-making process is achieving fair outcomes is measurement. Organisations may need to compare outcomes across different demographic groups to assess whether they match expectations. To do this, organisations must have some data on the demographic characteristics of groups they are making decisions about. In recruitment, especially in the public sector, the collection of some 'protected characteristic' data (defined under the Equality Act, 2010) for monitoring purposes has become common-place, but this is less common in other sectors. Removing or not collecting protected characteristic data does not by itself ensure fair data-driven outcomes. Although this removes the possibility of direct discrimination, it may make it impossible to evaluate whether indirect discrimination is taking place. This highlights an important tension: to avoid direct discrimination as part of the decision-making process, protected characteristic attributes should not be considered by an algorithm. But, in order to assess the overall outcome (and hence assess the risk of indirect discrimination), data on protected characteristics is required.[footnote 124] There have been calls for wider data collection, reflecting an acceptance that doing so helps promote fairness and equality in areas where bias could occur.[footnote 125]CDEI supports these calls; we think thatgreater collection of protected characteristic data would allow for fairer algorithmic decision-making in many circumstances. In this section we explore why that is, and the issues that need to be overcome to make this happen more often. The need to monitor outcomes is important even when no algorithm is involved in a decision, but the introduction of algorithms makes this more pressing. Machine learning detects patterns and can find relationships in data that humans may not see or be able to fully understand. Although machine learning models optimise against objectives they have been given by a human, if data being analysed reflects historical or subconscious bias, then imposed blindness will not prevent models from finding other, perhaps more obscure, relationships. These could then lead to similarly biased outcomes, encoding them into future decisions in a repeatable way. This is therefore the right time to investigate organisational biases and take the actions required to address them. There are a number of reasons why organisations are not currently collecting protected characteristic data, including concerns or perceptions that: Collecting protected characteristic data is not permitted by data protection law. This is incorrect in the UK, but seems to be a common perception (see below for further discussion). It may be difficult to justify collection in data protection law, and then store and use that data in an appropriate way (i.e. separate to the main decision-making process). Service users and customers will not want to share the data, and may be concerned about why they are being asked for it. Our own survey work suggests that this is not necessarily true for recruitment[footnote 126], although it may be elsewhere. In this section, we consider what is needed to overcome these barriers, so that organisations in the public and private sector can collect and use data more often, in a responsible way. Not all organisations will need to collect or use protected characteristic data. Services may not require it, or an assessment may find that its inclusion does more harm than good. However, many more should engage in collection than do so at present. Our research suggests a degree of confusion about how data protection law affects the collection, retention and use of protected characteristic data. Data protection law sets additional conditions for processing special category data. This includes many of the protected characteristics in the Equality Act 2010 (discussed in this chapter), as well as other forms of sensitive data that are not currently protected characteristics, such as biometric data. There are overlaps between the protected characteristics covered by the Equality Act, and the special categories of personal data under data protection law. Protected characteristics solely covered under the Equality Act are ""sex"", ""marriage & civil partnership"" and ""age"". Special categories of personal data under Data Protection law include ""genetic data"", ""biometric data (where used for identification"", ""health data"", ""sex life"", ""political opinion"", ""trade union membership"" and ""criminal convictions"". The overlapping characteristics are: ""race, colour, ethnic origin & nationality"", ""religion or philosophical belief"", ""sexual orientation"", ""disability"", ""pregnancy & maternity"" and ""gender reassignment."" Figure 4:Overlap between the Protected Characteristics of equality law and Special Categories of personal data under data protection law Several organisations we spoke to believed that data protection requirements prevent the collection, processing or use of special category data to test for algorithmic bias and discrimination. This is not the case:data protection law sets out specific conditions and safeguards for the processing of special category data, but explicitly includes use for monitoring equality. The collection, processing and use of special category data is allowed if it is for ""substantial public interest"", among other specific purposes set out in data protection law. In Schedule 1, the Data Protection Act sets out specific public interest conditions that meet this requirement, including 'equality of opportunity or treatment' where the Act allows processing of special category data where it ""is necessary for the purposes of identifying or keeping under review the existence or absence of equality of opportunity or treatment between groups of people specified in relation to that category with a view to enabling such equality to be promoted or maintained,"" (Schedule 1, 8.1(b)). Notably, this provision also specifically mentions equality rather than discrimination, which allows for this data to address broader fairness and equality considerations rather than just discrimination as defined by equality or human rights law. However, this provision of the Data Protection Act clarifies that it does not allow for using special category data for individual decisions (Schedule 1, 8.3), or if it causes substantial damage or distress to an individual (Schedule 1, 8.4). In addition to collection, data retention also needs some thought. Organisations may want to monitor outcomes for historically disadvantaged groups over time, which would require longer data retention periods than needed for individual use cases. This may lead to a tension between monitoring equality and data protection in practice, but these restrictions are much less onerous than sometimes described by organisations. The recently published ICO guidance on AI and data protection[footnote 127]sets out some approaches to assessing these issues, including guidance on special category data. Section 8.3 of this report sets out further details of how equality and data protection law apply to algorithmic decision-making. As with diversity monitoring in recruitment, pockets of the public sector are increasingly viewing the collection of data on protected characteristics as essential to the monitoring of unintentional discrimination in their services. The Open Data Institute recently explored how public sector organisations should consider collecting protected characteristic data to help fulfil their responsibilities under the Public Sector Equality Duty[footnote 128]. They recognise that: there is no accepted practice for collecting and publishing data about who uses digital services, which makes it hard to tell whether they discriminate or not.[footnote 129] The EHRC provides guidance[footnote 130]on how to deal with data protection issues when collecting data in support of obligations in the Public Sector Equality Duty, but is yet to update it for the significant changes to data protection law through GDPR and the Data Protection Act 2018, or to consider the implications of algorithmic decision-making on data collection. This needs to be addressed. There should also be consistent guidance for public sector organisations wanting to collect protected characteristic data specifically for equality monitoring purposes, which should become standard practice. Such practice is essential for testing algorithmic discrimination against protected groups. Organisations need to be assured that by following guidance, they are not just making their systems more fair and reducing their legal risk, but also minimising any unintended consequences of personal data collection and use, and thus helping to maintain public trust. The picture is more complicated in the private sector because organisations do not have the same legal responsibility under the Public Sector Equality Duty[footnote 131]. Equalities law requires that all organisations avoid discrimination, but there is little guidance on how they should practically identify it in algorithmic contexts. Without guidance or the PSED, private sector organisations have to manage different expectations from customers, employees, investors and the public about how to measure and manage the risks of algorithmic bias. There are also concerns about balancing the trade-off between fairness and privacy. In our interviews with financial institutions, many focused on principles such as data minimisation within data protection legislation. In some cases it was felt that collecting this data at all may be inappropriate, even if the data does not touch upon decision-making tools and models. In insurance, for example, there are concerns around public trust in whether providing this information could affect an individual's insurance premium. Organisations should think carefully about how to introduce processes that secure trust, such as being as transparent as possible about the data being collected, why and how it is used and stored, and how people can access and control their data. Building public trust is difficult, especially when looking to assess historic practices which may hide potential liabilities. Organisations may fear that by collecting data, they identify and expose patterns of historic bad practice. However, data provides a key means of addressing issues of bias and discrimination, and therefore reducing risk in the long term. Although public services normally sit within a single national jurisdiction, private organisations may be international. Different jurisdictions have different requirements for the collection of protected characteristic data, which may even be prohibited. The French Constitutional Council, for example, prohibits data collection or processing regarding race or religion. International organisations may need help to satisfy UK specific or nationally devolved regulation. There will be exceptions to the general principle that collection of protected or special characteristic data is a good thing. In cases where action is not needed, or urgently required, due to context or entirely obvious pre-existing biases, collecting protected characteristic data will be unnecessary. In others it may be seen as disproportionately difficult to gather the relevant data to identify bias. In others still, it may be impossible to provide privacy for very small groups, where only a very small number of service users or customers have a particular characteristic. Overcoming and navigating such barriers and concerns will require a combination of effective guidance, strong promotion of new norms from a centralised authority, or even regulatory compulsion. Recommendations to government Recommendation 6:Governmentshould work withrelevant regulatorsto provide clear guidance on the collection and use of protected characteristic data in outcome monitoring and decision-making processes. They should then encourage the use of that guidance and data to address current and historic bias in key sectors. Guidance is a first step, but more innovative thinking may be needed on new models for collecting, protecting or inferring protected characteristic data. Such models include a safe public third-party, collecting protected characteristic data on behalf of organisations, and securely testing their algorithms and decision-making processes without ever providing data to companies themselves.[footnote 132]This could be a responsibility of the relevant sector regulator or a government organisation such as the Office for National Statistics. There are also models where a private sector company could collect and store data securely, offering individuals guarantees on privacy and purpose, but then carrying out testing on behalf of other companies as a third party service. Where organisations do not collect protected characteristic data explicitly, they can sometimes infer it from other data; for example by extracting the likely ethnicity of an individual from their name and postcode. If used within an actual decision-making process, such proxies present some of the key bias risks, and using this information in relation to any individual presents substantial issues for transparency, accuracy, appropriateness and agency. In cases where collecting protected characteristic data is unfeasible, identifying proxies for protected characteristics purely for monitoring purposes may be a better option than keeping processes blind. However, there are clear risks around the potential for this type of monitoring to undermine trust, so organisations need to think carefully about how to proceed ethically, legally and responsibly. Inferred personal data (under data protection law) is still, legally, personal data, and thus subject to the relevant laws and issues described above.[footnote 133]A right to reasonable inference is under current academic discussion.[footnote 134] Further development is needed around all of these concepts, and few models exist of how they would work in practice.As above, legal clarity is a necessary first step, followed by economic viability, technical capability, security, and public trust. However, there are some models of success to work from, such as the ONS Secure Research Service, described below, and the NHS Data Safe Havens[footnote 135], as well as ongoing research projects in the field[footnote 136]. If a successful model could be developed, private sector companies would be able to audit their algorithms for bias without individuals being required to hand over their sensitive data to multiple organisations.We believe further research is needed to develop a longer-term proposal for the role of third-parties in such auditing, and will consider future CDEI work in this area. Where organisations determine that collecting protected characteristics is appropriate for assessing bias, they will often need to collect information about their service users or customers, and compare it with relevant wider (often national) demographic data. It is hard to tell if a decision is having a negative effect on a group without some sense of what should be considered normal. A lack of relevant and representative wider data can make it difficult for both public and private organisations to tell if their processes are biased, and then to develop responsible algorithmic tools in response. Relevant data is already made available publicly, including UK Census and survey data published by the ONS. The devolved administrations have also made significant volumes of data widely accessible (throughStatsWales,Statistics.gov.scot, andNISRA), as have a number ofGovernment departments and programmes. Sector specific datasets and portals add to this landscape inpolicing,finance, and others. More detailed population data can be accessed through the ONS' Secure Research Service which provides a wide variety of national scale information, including pre-existing survey and administrative data resources. Usage of this service is managed through its ""5 Safes"" (safe projects, people, settings, data and outputs) framework, and restricted to the purposes of research, evaluation and analysis. This often restricts access to academic research groups, but there may be opportunities to widen the service to support evaluation of diversity outcomes by regulators and delivery organisations. Regulators can help by promoting key public datasets of specific value to their sector, along with guidance material accessible for their industry. Wider availability of aggregate demographic information for business use would also allow for better data gathering, or better synthetic data generation. Publicly available, synthetically augmented, and plausible versions of more surveys (beyond the Labour Force Survey[footnote 137]) would help more users find and develop use cases. Government announcements in 2020 included PS6.8 million (over three years) to help the ONS share more, higher-quality data across government, and to link and combine datasets in new ways (for example, to inform policy or evaluate interventions). Recommendations to government Recommendation 7:Governmentand theONSshould open the Secure Research Service more broadly, to a wider variety of organisations, for use in evaluation of bias and inequality across a greater range of activities. In the short term, organisations who find publicly held data insufficient will need to engage in partnership with their peers, or bodies that hold additional representative or demographic information, to create new resources. In the private sphere these approaches include industry specific data sharing initiatives (Open Banking in finance, Presumed Open in energy, and more under discussion by the Better Regulation Executive), and trusted sector-specific data intermediaries. Recommendations to government Recommendation 8:Governmentshould support the creation and development of data-focused public and private partnerships, especially those focused on the identification and reduction of biases and issues specific to under-represented groups. TheOffice for National StatisticsandGovernment Statistical Serviceshould work with these partnerships andregulatorsto promote harmonised principles of data collection and use into the private sector, via shared data and standards development. Case Study: Open Banking In 2016 the Competition and Markets Authority (CMA) intervened in the UK ecosystem to require that nine of the largest UK banks grant direct, transaction level, data access to licensed startups. Although compliance and enforcement sits with the CMA, Open Banking represents a regulatory partnership as much as a data partnership, with the FCA providing financial oversight, and the ICO providing data protection. Open Banking has led to over 200 regulated companies providing new services, including financial management and credit scoring. As a result access to credit, debt advice and financial advice is likely to widen, which in turn is expected to allow for better service provision for under-represented groups. This provides an opportunity to address unfairness and systemic biases, but new forms of (digital) exclusion and biasmay yet appear. Examples of wider partnerships include projects within the Administrative Data Research UK programme (bringing together government, academia and public bodies) and the increasing number of developmental sandboxes aimed at industry or government support (see Section 8.5). Where new data ecosystems are created around service-user data, organisations like the new Global Open Finance Centre of Excellence can then provide coordination and research support. Empowering organisations to share their own data with trusted bodies will enable industry wide implementation of simple but specific common data regimes. Relatively quick wins are achievable in sectors that have open data standards in active development such as Open Banking and Open Energy. Case Study: Monitoring for bias in digital transformation of the courts Accessing protected characteristic data to monitor outcomes is not only necessary when introducing algorithmic decision-making, but also when making other major changes to significant decision-making processes. Her Majesty's Courts and Tribunal Service (HMCTS) iscurrently undergoinga large-scale digital transformation process aimed largely at making the court system more affordable and fair,includingonline dispute resolution and opt-in automated fixed penalties for minor offences where there is a guilty plea. As part of this transformation, they haverecogniseda need for more information about people entering and exiting the judicial process. More protected characteristic data would allow HMCTS to assess the effectiveness of different interventions and the level of dependency on, and uptake of, different parts of the judicial system within different groups. Senior justices would largely prefer to see a general reduction in the number of people going through criminal courts and greater diversity in use of civil courts. It is hard to objectively measure these outcomes, or whether courts are acting fairly and without bias, without data. In order to achieve these goals, HMCTS have focused on access to protected characteristic data, predominantly through data linkage and inference from wider administrative data. They haveworkedwith the Government Statistical Service's Harmonisation Team and academic researchers to rebuild their data architecture to support this. The resulting information is intended to both be valuable to the Ministry of Justice for designing fair interventions in the functioning of the courts, but also eventually to be made available for independent academic research (via Administrative Data Research UK and the Office for National Statistics). This is just one example of a drive toward new forms of data collection, designed to test and assure fair processes and services within public bodies. It is also illustrative of a project navigating the Data Protection Act 2018 and the 'substantial public interest' provision of the GDPR to assess risks around legal exposure. It is essential for public bodies to establish whether or not their digital services involve personal data, are classed as statistical research, or sit within other legislative 'carve-outs'. This is especially true when dealing with data that is not necessarily accompanied by end-user consent. In the previous section we argue that it is preferable to seek to identify bias and to address it, rather than hope to avoid it by unawareness. There is a high level of focus on this area in the academic literature, and an increasing number of practical algorithmic fairness tools have appeared in the last three years. Approaches for detecting bias include: Comparing training with population datasets to see if they are representative. Analysing the drivers of differences in outcomes that are likely to cause bias. For example, if it could be shown that certain recruiters in an organisation held measurable biases compared to other recruiters (after controlling for other characteristics), it would be possible to train an algorithm with a less biased subset of the data (e.g. by excluding the biased group). Analysing how and where relevant model variables correlate with different groups. For example, if qualifications are a factor in a model for recommending recruitment, analysis can show the extent to which this results in more job offers being made to particular groups. Different approaches are necessary in different contexts. For most organisations, bias monitoring and analysis are a necessary part of their decision-making (whether algorithmic or not). Where that monitoring suggests a biased process, the question is then how to address it. Ensuring that the data being collected (Section 7.3) is both necessary and sufficient is an important first step. Further methods (detailed below) will need to be proportionate to organisational needs. Case study: Bias detection in 1988 The 1988 medical school case mentioned in Section 2.1 is an interesting example of bias detection. Their program was developed to match human admissions decisions, doing so with 90-95% accuracy. Despite bias against them, the school still had a higher proportion of non-European students admitted than most other London medical schools. The human admissions officers' biases would probably never have been demonstrated, but for the use of their program. Had that medical school been equipped with a current understanding of how to assess an algorithm for bias, and been motivated to do so, perhaps they would have been able to use their algorithm to reduce bias rather than propagate it. Organisations that need to directly mitigate bias in their models now have a number of interventions at their disposal. This is a generally positive development but the ecosystem is complex. Organisations see a need for clarity on which mitigation tools and techniques are appropriate and legal in which circumstances. Crucially, what is missing is practical guidance about how to create, deploy, monitor, audit, and adjust fairer algorithms, using the most effective tools and techniques available. It is important to recognise that the growing literature and toolsets on algorithmic fairness often only address part of the issue (that which can be quantified), and wider interventions to promote fairness and equality remain key to success. As part of this review, we contracted[footnote 143]Faculty to analyse, assess and compare the various technical approaches to bias mitigation. This section is informed by their technical work. The outputs from that work are being made availableelsewhere. If we want model development to include a definition of fairness, we must tell the relevant model what that definition is, and then measure it. There is, however, no single mathematical definition of fairness that can apply to all contexts[footnote 144]. As a result, the academic literature has seen dozens of competing notions of fairness introduced, each with their own merits and drawbacks, and many different terminologies for categorising these notions, none of which are complete. Ultimately,humans must choose which notions of fairness an algorithm will work to, taking wider notions and considerations into account, and recognising that there will always be aspects of fairness outside of any statistical definition. Fairness definitions can be grouped by notion of fairness sought and stage of development involved. In the first instance, these fall into the broad categories ofprocedural and outcome fairnessdiscussed in Section 2.5. Within the technical aspects of machine learning, procedural fairness approaches often concern the information used by a system, and thus include ""Fairness Through Unawareness"", which is rarely an effective strategy. The statistical concept of fairness as applied to algorithms is then focused on achieving unbiased outcomes, rather than other concepts of fairness. Explicit measurement of equality across results for different groups is necessary for most of these approaches. Within Outcome Fairness we can make additional distinctions, between Causal and Observational notions of fairness, as well as Individual and Group notions. Individual notionscompare outcomes for individuals to see if they are treated differently. However, circumstances are generally highly specific to individuals, making them difficult to compare without common features. Group notionsaggregate individual outcomes by a common feature into a group, then compare aggregated outcomes to each other. Group and individual notions are not mutually exclusive: an idealised 'fair' algorithm could achieve both simultaneously. Observational approachesthen deal entirely with the measurable facts of a system, whether outcomes, decisions, data, mathematical definitions, or types of model. Causal approachescan consider 'what if?' effects of different choices or interventions. This typically requires a deeper understanding of the real-world system that the algorithm interacts with. In their technical review of this area, Faculty describe a way to categorise different bias mitigation strategies within these notions of fairness (see table below). They also identified that the four Group Observational notions (highlighted) are currently the most practical approaches to implement for developers: being relatively easy to compute and providing meaningful measures for simple differences between groups (this does not necessarily mean they are an appropriate choice of fairness definition in all contexts). The majority of existing bias mitigation tools available to developers address (Conditional) Demographic Parity, or Equalised Odds, or are focused on removing sensitive attributes from data. The table below shows how the most commonly used approaches (and other examples) sit within wider definitions as described above. Visual demonstrations of these notions are shown in a web app that accompanies this review.[footnote 145] An example of how these different definitions play out in practice can be seen in the US criminal justice system, as per the box below. Case study: COMPAS For a given risk score in the US COMPAScriminal recidivism model, the proportion of defendants who reoffend is roughly the same independent of a protected attribute, including ethnicity (Calibration). Otherwise, a risk score of 8 for a white person would mean something different for a Black person. Propublica's criticism of this model highlighted that Black defendants who didn't reoffend were roughly twice as likely to be given scores indicating a medium/high risk of recidivism as white defendants. However, ensuring equal risk scores among defendants who didn't offend or re-offend (Equalised Odds) would result in losing Calibration at least to some degree. Fully satisfying both measuresprovesimpossible. If attributes of individuals (protected or otherwise) are apparently linked, such asrecidivism and race, then generally equality of opportunity (the generalised form of Equalised Odds) and Calibrationcannot be reconciled. If a model satisfies Calibration, then in each risk category, the proportion of defendants who reoffend is the same, regardless of race. The only way of achieving this if the recidivism rate is higher for one group, is if more individuals from that group are predicted to be high-risk. Consequently, this means that the model will make more false positives for that group than others, meaning Equalised Odds cannot be satisfied. Similarly, if a recidivism model satisfies Demographic Parity, then the chance a defendant ends up in any particular risk category is the same, regardless of their race. If one group has a higher recidivism rate than the others, that means models must make more false negatives for that group to maintain Demographic Parity, which (again) means Equalised Oddscannot be satisfied. Similar arguments apply for other notions of fairness. It is worth noting that none of these fairness metrics take into account whether or not a given group is more likely to be arrested than another, or treated differently by a given prosecution service. This example serves to illustrate both the mutual incompatibility of many metrics, and their distinct limitations in context. Once an organisation has understood how different statistical definitions of fairness are relevant to their context, and relate to institutional goals, they can then be used to detect, and potentially mitigate, bias in their statistical approaches. Detection protocols and interventions can take place before, during, or after an algorithm is deployed. Pre-processingprotocols and interventions generally concern training data, aiming to detect and remove sources of unfairness before a model is built. Modified data can then be used for any algorithmic approach. Fairness-focused changes in decision-making then exist at the most fundamental level. However, the nature of a given application should inform data and definitions of fairness used. If an organisation seeks to equalise the odds of particular outcomes for different groups (an Equalised Odds approach), pre-processing needs to be informed by those outcomes, and a prior round of model output. Most of the pre-processing interventions present in the machine learning literature do not incorporate model outcomes, only inputs and the use of protected attributes. Some pre-processing methods only require access to the protected attributes in the training data, and not in the test data.[footnote 151]In finance it is clear that companies place more emphasis on detecting and mitigating bias in the pre-processing stages (by carefully selecting variables and involving human judgement in the loop) than in- or post-processing. In-processingmethods are applied during model training and analyse or affect the way a model operates. This typically involves a model's architecture or training objectives, including potential fairness metrics. Modification (and often retraining) of a model can be an intensive process but the resulting high level of specification to a particular problem can allow models to retain a higher level of performance against their (sometimes new) goals. Methods such as constrained optimisation have been used to address both Demographic Parity and Equalised Odds requirements[footnote 152]. In-processing is a rapidly evolving field and often highly model dependent. It is also the biggest opportunity in terms of systemic fairness, but many approaches need to be formalised, incorporated into commonly used toolsets and, most importantly, be accompanied with legal certainty (see below). Post-processingapproaches concern a model's outputs, seeking to detect and correct unfairness in its decisions. This approach only requires scores or decisions from the original model and corresponding protected attributes or labels that otherwise describe the data used. Post-processing approaches are usually model-agnostic, without model modification or retraining. However, they effectively flag and treat symptoms of bias, not original causes. They are also often disconnected from model development, and are relatively easy to distort, making them risky if not deployed as part of a broader oversight process. Different interventions at different stages can sometimes be combined. Striving to achieve a baseline level of fairness in a model via pre-processing, but then looking for bias in particularly sensitive or important decisions during post-processing is an attractive approach. Care must be taken, however, that combinations of interventions do not hinder each other. Bias mitigation methods by stage of intervention and notion of fairness are shown in Appendix A. Detailed references can then be found in Faculty's ""Bias identification and mitigation in decision-making algorithms"", published separately. There are a number of challenges facing organisations attempting to apply some of these statistical notions of fairness in practical situations: Using statistical notions of fairness appropriately Statistical definitions of fairness deliver specific results to specific constraints. They struggle to encompass wider characteristics that do not lend themselves to mathematical formulation. There is no clear decision-making framework (logistical or legal) for selecting between definitions. Decisions over which measure of fairness to impose need extensive contextual understanding and domain knowledge beyond issues of data science. In the first instance, organisations should strive to understand stakeholder and end-user expectations around fairness, scale of impact and retention of agency, and consider these when setting desired outcomes. Any given practitioner is then forced, to some extent, to choose or mathematically trade off between different definitions. Techniques to inform this trade off are currently limited. Although exceptions exist[footnote 153]there seems to be a gap in the literature regarding trade-offs between different notions of fairness, and in the general maturity of the industry in making such decisions in a holistic way (i.e. not relying on data science teams to make them in isolation). Compatibility with accuracy A large part of the machine learning literature on fairness is concerned with the trade-off between fairness and accuracy, i.e. ensuring that the introduction of fairness metrics has minimal impacts on model accuracy. In a pure statistical sense there is often a genuine trade-off here; imposing a constraint on fairness may lower the statistical accuracy rate. But this is often a false trade-off when thinking more holistically. Applying a fairness constraint to a recruitment algorithm sifting CVs might lower accuracy measured by a loss function over a large dataset, but doesn't necessarily mean that company recruiting is sifting in worse candidates, or that the company's sense of accuracy is free from historic bias. The effects of accuracy are relevant even when models attempt to satisfy fairness metrics in ways that run counter to wider notions of fairness. Random allocation of positions in a company would likely satisfy Demographic Parity, but would not generally be considered fair. Implementing any specific fairness measure also fundamentally changes the nature of what a model is trying to achieve. Doing so may make models 'less accurate' when compared to prior versions. This apparent incompatibility can lead to models being seen as less desirable because they are less effective at making choices that replicate those of the past. Debiasing credit models might require accepting 'higher risk' loans, and thus greater capital reserves, but (as mentioned below) these choices do not exist in isolation. Accuracy can in itself be a fairness issue. Notions of accuracy that are based on average outcomes, or swayed by outcomes for specific (usually large) demographic groups, may miss or conceal substantial biases in unexpected or less evident parts of a model's output. Accuracy for one individual does not always mean accuracy for another. Organisations need to consider these trade-offs in the round, and understand the limitations of purely statistical notions of both fairness and accuracy when doing so. Understanding causality and reasons for unfairness Causes of unfairness are not part of these definitions and must be assessed on an organisational level. Most techniques look at outcomes, and can't understand how they come to be, or why biases may exist (except in specific circumstances). Defining fairness based on causal inference[footnote 154]has only been developed to a limited extent[footnote 155]due to the difficulty of validating underlying (apparent) causal factors. Real-world definition of these factors can introduce further bias, especially for less well understood groups with less data. Static measurement and unintended consequences Definitions of fairness are ""static"", in the sense that we measure them on a snapshot of the population at a particular moment in time. However, a static view of fairness neglects that most decisions in the real world are taken in sequence. Making any intervention into model predictions, their results, or the way decisions are implemented will cause that population to change over time. Failing to account for this risks leading to interventions that are actively counter-productive, and there are cases where a supposedly fair intervention could lead to greater unfairness.[footnote 156]There is the scope for unintended consequence here, and strategic manipulation on the part of individuals. However, the cost of manipulation will typically be higher for any disadvantaged group. Differing costs of manipulation can result in disparities between protected groups being exaggerated.[footnote 157]In implementing a process to tackle unfairness, organisations must deploy sufficient context-aware oversight, and development teams must ask themselves if they have inadvertently created the potential for new kinds of bias. Checking back against reference data is especially useful over longer time periods. Although these bias mitigation techniques can seem complex and mathematical,they are encoding fundamental policy choices concerning organisational aims around fairness and equality, and there are legal risks. Organisations must bring a wide range of expertise into making these decisions. A better set of common language and understanding between the machine learning and equality law communities would assist this. Seeking to detect bias in decision-making processes, and to address it, is a good thing. However, there is a need for care in how some of the bias mitigation techniques listed above are applied. Interventions can affect the outcomes of decisions about individuals, and even if the intent is to improve fairness, this must be done in a way that is compatible with data protection and equality law. Many of the algorithmic fairness tools currently in use have been developed under the US regulatory regime, which is based on a different set of principles to those in the UK and includes different ideas of fairness that rely on threshold levels (most notably the ""4/5ths"" rule), and enable affirmative action to address imbalances. Tools developed in the US may not be fit for purpose in other legal jurisdictions. Advice to industry Whereorganisationsoperating within the UK deploy tools developed in the US, they must be mindful that relevant equality law (along with that across much of Europe) is different. This uncertainty presents a challenge to organisations seeking to ensure their use of algorithms is fair and legally compliant. Further guidance is needed in this area (an example of the general need for clarity on interpretation discussed in Chapter 9); our understanding of the current position is as follows. Data Protection law The bias mitigation interventions discussed involve the processing of personal data, and therefore must have a lawful basis under data protection law. Broadly speaking the same considerations apply as for any other use of data; it must be collected, processed and stored in a lawful, fair and transparent manner, for specific, explicit and legitimate purposes. Its terms of use must be adequately communicated to the people it describes. The ICO has provided guidance on how to ensure that processing of this type complies with data protection law, along with some examples, in their recently published guidance on AI and data protection.[footnote 158]Processing data to support the development of fair algorithms is a legitimate objective (provided it is lawful under the Equality Act, see below), and broadly speaking if the right controls are put in place, data protection law does not seem to present a barrier to these techniques. There are some nuances to consider, especially for pre-processing interventions involving modification of labels on training data. In usual circumstances modifying personal data to be inaccurate would be inappropriate. However, where alterations made to training data are anonymised and not used outside of model development contexts, this can be justified under data protection legislation if care is taken. Required care might include ensuring that model features cannot be related back to an individual, the information that is stored is never used directly to make decisions about an individual, and that there is a lawful basis for processing the data in this way to support training a model (whether consent or another basis). Particular care is needed when dealing with Special Category data, which requires additional protections under data protection law.[footnote 159]While special category data is allowed to be used for measuring bias, this explicitly excludes decisions about individuals: which would include many mitigation techniques, particularly in post-processing. Instead, automated processing of special category data would need to rely on explicit consent from its subjects, or one of a small number of explicit exceptions. It is not enough to rest on the proportionate means to legitimate ends provision (in this case, fairer models) that otherwise applies. Equality law The position regarding the Equality Act 2010 is less clear. All of the mitigation approaches discussed in this section are intended to reduce bias, including indirect discrimination. However, there is a risk that some of the techniques used to do this could themselves be a cause of new direct discrimination. Even if ""positive"", i.e. discrimination to promote equality for a disadvantaged group, this is generally illegal under the Equality Act. It is not yet possible to give definitive general guidance on exactly which techniques would or would not be legal in a given situation; organisations will need to think this through on a case-by-case basis. Issues to consider might include: Explicit use of a protected characteristic (or relevant proxies) to reweight models to achieve a fairness metric (e.g. in some applications of Feature Modification, or Decision Threshold Modification) carries risk. Organisations need to think through whether the consequence of using such a technique could disadvantage an individual explicitly on the basis of a protected characteristic (which is direct discrimination) or otherwise place those individuals at a disadvantage (which can lead to indirect discrimination). Resampling data to ensure a representative set of inputs is likely to be acceptable; even if it did have a disparate impact across different groups any potential discrimination would be indirect, and likely justifiable as a proportionate means to a legitimate end. Though there is a need for caution here, the legal risk of attempting to mitigate bias should not be overplayed. If an organisation's aim is legitimate, and decisions on how to address this are taken carefully with due regard to the requirements of the Equality Act, then the law will generally be supportive. Involving a broad team in these decisions, and documenting them (e.g. in an Equality Impact Assessment) is good practice. If bias exists, and an organisation can identify a non-discriminatory approach to mitigate that, then there seems to be an ethical responsibility to do so. If this can't be done at the level of a machine learning model itself, then wider action may be required. Organisations developing and deploying algorithmic decision-making should ensure that their mitigation efforts do not lead to direct discrimination, or outcome differences without objective justification. Despite the complexity here, algorithmic fairness approaches will be essential to facilitate widespread adoption of algorithmic decision-making. Advice to industry Whereorganisationsface historical issues, attract significant societal concern, or otherwise believe bias is a risk, they will need to measure outcomes by relevant protected characteristics to detect biases in their decision-making, algorithmic or otherwise. They must then address any uncovered direct discrimination, indirect discrimination, or outcome differences by protected characteristics that lack objective justification. In doing so,organisationsshould ensure that their mitigation efforts do not produce new forms of bias or discrimination. Many bias mitigation techniques, especially those focused on representation and inclusion, can legitimately and lawfully address algorithmic bias when used responsibly. However, some risk introducing positive discrimination, which is illegal under the Equality Act. Organisations should consider the legal implications of their mitigation tools, drawing on industry guidance and legal advice. The best approach depends strongly on the use case and context. Interviews with organisations in the finance sector did not reveal a commonly used approach; companies use a mix of in-house and external tools. There is a general appetite for adapting open-source tools to internal uses, and among the companies consulted, none had developed in-house tools from scratch. In recruitment, we found that vendors of machine learning tools had established processes for examining their models, both off-the-shelf and bespoke tools. The most elaborate processes had three stages: pre-deployment checks with dummy data or sampled real-world data on models prior to deployment; post deployment checks where anonymised data from customers was used for further adjustments and correction of over-fitting; and third-party audits conducted by academic institutions particularly focused on identifying sources of bias. Firms used a mixture of proprietary techniques and open-source software to test their models. In terms of mitigation, there is a lot that can be done within the current legislative framework, but regulators will need to keep an eye on the way the law is applied, what guidance is needed to guide ethical innovation and whether the law might need to change in the future. Engagement with the public and industry will be required in many sectors to identify which notions of fairness and bias mitigation approaches are acceptable and desirable. Recommendations to regulators Recommendation 9:Sector regulators and industry bodiesshould help create oversight and technical guidance for responsible bias detection and mitigation in their individual sectors, adding context-specific detail to the existing cross-cutting guidance on data protection, and any new cross-cutting guidance on the Equality Act. We think it is likely that a significant industry and ecosystem will need to develop with the skills to audit systems for bias, in part because this is a highly specialised skill that not all organisations will be able to support; in part because it will be important to have consistency in how the problem is addressed; and in part because regulatory standards in some sectors may require independent audit of systems. Elements of such an ecosystem might be licenced auditors or qualification standards for individuals with the necessary skills. Audit of bias is likely to form part of a broader approach to audit that might also cover issues such as robustness and explainability. Within an organisation, especially a large one, good intentions in individual teams are often insufficient to ensure that the organisation as a whole achieves the desired outcome. A proportionate level of governance is usually required to enable this. What does this look like in this context? There is no one-size-fits-all approach, and unlike in some other areas (e.g. health and safety or security management), not yet an agreed standard on what such an approach should include. However, there is an increasing range of tools and approaches available. What is clear is that, given the pace of change, and the wide range of potential impacts, governance in this space must be anticipatory. Anticipatory Governance aims to foresee potential issues with new technology, and intervene before they occur, minimising the need for advisory or adaptive approaches, responding to new technologies after their deployment. Tools, ways of working, and organisations already exist to help proactively and iteratively test approaches to emerging challenges while they are still in active development. The goal is to reduce the amount of individual regulatory or corrective action and replace it with more collaborative solutions to reduce costs, and develop best practice, good standards, policy and practice. In practical terms, assessment of impacts and risks, and consultation with affected parties, are core to doing this within individual organisations. However, it is critical that they aren't simply followed as tick box procedures. Organisations need to show genuine curiosity about the short, medium and long term impacts of increasingly automated decision-making, and ensure that they have considered the views of a wide range of impacted parties both within their organisation and in wider society. Assessments must not only consider the detail of how an algorithm is implemented, but whether it is appropriate at all in the circumstances, and how and where it interacts with human decision-makers. There are many published frameworks and sets of guidance offering approaches to structuring governance processes[footnote 160], including guidance from GDS and the Alan Turing Institute targeted primarily at the UK public sector.[footnote 161]Different approaches will be appropriate to different organisations, but some key questions that should be covered include the following. Guidance to organisation leaders and boards Those responsible for governance of organisations deploying or using algorithmic decision-making tools to support significant decisions about individuals should ensure that leaders are in place with accountability for: Understanding the capabilities and limits of those tools. Considering carefully whether individuals will be fairly treated by the decision-making process that the tool forms part of. Making a conscious decision on appropriate levels of human involvement in the decision-making process. Putting structures in place to gather data and monitor outcomes for fairness. Understanding their legal obligations and having carried out appropriate impact assessments. This especially applies in the public sector when citizens often do not have a choice about whether to use a service, and decisions made about individuals can often be life-affecting. The list above is far from exhaustive, but organisations that consider these factors early on, and as part of their governance process, will be better placed to form a robust strategy for fair algorithmic deployment. In Chapters 8 and 9 below we discuss some of the more specific assessment processes (e.g. Data Protection Impact Assessments, Equality Impact Assessments, Human Rights Impact Assessments) which can provide useful structures for doing this. Summary Overview of findings: Regulation can help to address algorithmic bias by setting minimum standards, providing clear guidance that supports organisations to meet their obligations, and enforcement to ensure minimum standards are met. AI presents genuinely new challenges for regulation, and brings into question whether existing legislation and regulatory approaches can address these challenges sufficiently well. There is currently little case law or statutory guidance directly addressing discrimination in algorithmic decision-making. The current regulatory landscape for algorithmic decision-making consists of the Equality and Human Rights Commission (EHRC), the Information Commissioner's Office (ICO) , and sector regulators and non-government industry bodies.At this stage, we do not believe that there is a need for a new specialised regulator or primary legislation to address algorithmic bias. However, algorithmic bias means that the overlap between discrimination law, data protection law and sector regulations is becoming increasingly important. This is particularly relevant for the use of protected characteristics data to measure and mitigate algorithmic bias, the lawful use of bias mitigation techniques, identifying new forms of bias beyond existing protected characteristics, and for sector-specific measures of algorithmic fairness beyond discrimination. Existing regulators need to adapt their enforcement to algorithmic decision-making, and provide guidance on how regulated bodies can maintain and demonstrate compliance in an algorithmic age.Some regulators require new capabilities to enable them to respond effectively to the challenges of algorithmic decision-making. While larger regulators with a greater digital remit may be able to grow these capabilities in-house, others will need external support. Recommendations to government: Recommendation 10: Governmentshould issue guidance that clarifies the Equality Act responsibilities of organisations using algorithmic decision-making. This should include guidance on the collection of protected characteristics data to measure bias and the lawfulness of bias mitigation techniques. Recommendation 11:Through the development of this guidance and its implementation,governmentshould assess whether it provides both sufficient clarity for organisations on their obligations, and leaves sufficient scope for organisations to take actions to mitigate algorithmic bias. If not,governmentshould consider new regulations or amendments to the Equality Act to address this. Recommendations to regulators: Recommendation 12:TheEHRCshould ensure that it has the capacity and capability to investigate algorithmic discrimination. This may include EHRC reprioritising resources to this area, EHRC supporting other regulators to address algorithmic discrimination in their sector, and additional technical support to the EHRC. Recommendation 13: Regulatorsshould consider algorithmic discrimination in their supervision and enforcement activities, as part of their responsibilities under the Public Sector Equality Duty. Recommendation 14: Regulatorsshould develop compliance and enforcement tools to address algorithmic bias, such as impact assessments, audit standards, certification and/or regulatory sandboxes. Recommendation 15: Regulatorsshould coordinate their compliance and enforcement efforts to address algorithmic bias, aligning standards and tools where possible. This could include jointly issued guidance, collaboration in regulatory sandboxes, and joint investigations. Advice to industry: Future CDEI work: CDEI plans to grow its ability to provide expert advice and support to regulators, in line with our existing terms of reference. This will include supporting regulators to coordinate efforts to address algorithmic bias and to share best practice. CDEI will monitor the development of algorithmic decision-making and the extent to which new forms of discrimination or bias emerge. This will include referring issues to relevant regulators, and working with government if issues are not covered by existing regulations. This report has shown the problem of algorithmic bias, and ways that organisations can try to address the problem. There are good reasons for organisations to address algorithmic bias, ranging from ethical responsibility through to pressure from customers and employees. These are useful incentives for companies to try to do the right thing, and can extend beyond minimum standards to creating a competitive advantage for firms that earn public trust. However, the regulatory environment can help organisations to address algorithmic bias in three ways. Government can set clear minimum standards through legislation that prohibits unacceptable behaviour. Government, regulators and industry bodies can provide guidance and assurance services to help organisations correctly interpret the law and meet their obligations. Finally, regulators can enforce these minimum standards to create meaningful disincentives for organisations who fail to meet these obligations. Alternatively, a regulatory environment with unclear requirements and weak enforcement creates the risk that organisations inadvertently break the law, or alternatively that this risk prevents organisations from adopting beneficial technologies. Both of these situations are barriers to ethical innovation, which can be addressed through clear and supportive regulation. Data-driven technologies and AI present a range of new challenges for regulators. The rapid development of new algorithmic systems means they now interact with many aspects of our daily lives. These technologies have the power to transform the relationship between people and services across most industries by introducing the ability to segment populations using algorithms trained on larger and richer datasets. However, as we have seen in our sector-focused work, there are risks of these approaches reinforcing old biases, or introducing new ones, by treating citizens differently due to features beyond their control, and in ways they may not be aware of. The regulatory approach of every sector where decision-making takes place about individuals will need to adapt and respond to these new practices that algorithmic decision-making brings. Given this widespread shift, it is necessary to reflect both on whether the existing regulatory and legislative frameworks are sufficient to deal with these novel challenges, as well as how compliance and enforcement may operate in an increasingly data-driven world. For example, regulatory approaches that rely on individual complaints may not be sufficient in a time where people are not always aware of how an algorithm has impacted their life. Similarly, the pace of change in the development of decision-making technologies may mean that certain approaches are too slow to respond to the new ways algorithms are already impacting people's lives. Regulators will need to be ambitious in their thinking, considering the ways algorithms are already transforming their sectors, and what the future may require. The government and some regulators have already recognised the need for anticipatory regulation to respond to these challenges. Regulation For The Fourth Industrial Revolution[footnote 162]lays out the challenge as a need for proactive, flexible, outcome-focused regulation, enabling greater experimentation under appropriate supervision, and supporting innovators to actively seek compliance. It also details the need for regulators to build dialogue across society and industry, and to engage in global partnerships. NESTA adds[footnote 163]that such regulation should be inclusive and collaborative, future-facing, iterative, and experimental, with methods including ""sandboxes: experimental testbeds; use of open data; interaction between regulators and innovators; and, in some cases, active engagement of the public"". In this section we look at both the current landscape, and the steps required to go further. The UK's regulatory environment is made up of multiple regulators, enforcement agencies, inspectorates and ombudsmen (which this report will call 'regulators' for simplicity) with a range of responsibilities, powers and accountabilities. These regulators are typically granted powers by the primary legislation that established them, although some 'private regulators' may be set up through industry self-regulation. Some regulators have an explicit remit to address bias and discrimination in their enabling legislation, while others may need to consider bias and discrimination in decision-making when regulating their sectors. In practice, however, there is a mixed picture of responsibility and prioritisation of the issue. Data-driven algorithms do not necessarily replace other decision-making mechanisms wholesale, but instead fit into existing decision-making processes.Therefore, rather than a new algorithmic regulatory system, the existing regulatory environment needs to evolve in order to address bias and discrimination in an increasingly data-driven world. The key piece of legislation that governs discrimination is theEquality Act 2010. The Act provides a legal framework to protect the rights of individuals and provides discrimination law to protect individuals from unfair treatment, including through algorithmic discrimination. Underlying anti-discrimination rights are also set out in theHuman Rights Act 1998(which establishes the European Convention on Human Rights in UK law). When a decision is made by an organisation on the basis of recorded information (which is the case for most significant decisions), theData Protection Act 2018and theGeneral Data Protection Regulation (GDPR)are also relevant. This legislation controls how personal information is used by organisations, businesses or the government and sets out data protection principles which includes ensuring that personal information is used lawfully, fairly and transparently. Data protection law takes on a higher level of relevance in the case ofalgorithmicdecision-making, where decisions are inherently data-driven, and specific clauses related to automated processing and profiling apply (see below for more details). In support of this legislation, there are two primary cross-cutting regulators: theEquality and Human Rights Commission(EHRC, for the Equality Act and Human Rights Act) and theInformation Commissioner's Office(ICO, for the Data Protection Act and GDPR). However, given the range of types of decisions that are being made with the use of algorithmic tools, there is clearly a limit in how far cross-cutting regulators can define and oversee what is acceptable practice. Many sectors where significant decisions are made about individuals have their own specific regulatory framework with oversight on how those decisions are made. These sector regulators have a clear role to play:algorithmic bias is ultimately an issue of how decisions are made by organisations, and decision-making is inherently sector-specific.In sectors where algorithmic decision-making is already significant, the relevant enforcement bodies are already considering the issues raised by algorithmic decision-making tools, carrying out dedicated sector-specific research and increasing their internal skills and capability to respond. Overall, the picture is complex, reflecting the overlapping regulatory environment of different types of decisions. Some have called for a new cross-cutting algorithms regulator, for example Lord Sales of the UK Supreme Court.[footnote 164]We do not believe that this is the best response to the issue of bias, given that many of the regulatory challenges raised are inevitably sector-specific, and typically algorithms only form part of an overall decision-making process regulated at sector level. However, more coordinated support for and alignment between regulators may be required (see below) to address the challenge across the regulatory landscape. The Equality Act 2010 (the Act) legally protects people from discrimination and sets out nine'protected characteristics'which it is unlawful to discriminate on the basis of: The Act prohibits direct discrimination, indirect discrimination, victimisation and harassment based on these characteristics.[footnote 165]It also establishes a requirement to make reasonable adjustments for people with disabilities, and allows for, but does not require, 'positive action' to enable or encourage the participation of disadvantaged groups. The Act also establishes the Public Sector Equality Duty[footnote 166]which requires all public sector bodies to address inequality through their day-to-day activities. The Equality Act has effect in England, Wales and Scotland. Although Northern Ireland has similar anti-discrimination principles, they are covered in different legislation. There are some legal differences in the scope of protected characteristics (e.g. political opinions are protected in Northern Ireland), thresholds for indirect discrimination, and some practical differences in the Public Sector Equality Duty. However, for the purpose of this report, we will use the language of the Equality Act. Section 1 of the Equality Act requires public bodies to actively consider the socio-economic outcomes of any given policy. It is currently in effect in Scotland, and will commence in Wales next year. Increasingly large parts of the public sector (and those contracted by it) must show that they have given due diligence to such issues ahead of time, as part of their development and oversight chain. Recent controversies over exam results have highlighted broad public concern about socio-economic disparities. Each of these provisions apply to any area where individuals are treated differently, regardless of whether an algorithm was involved in the decision. The UK also protects against discrimination in the Human Rights Act 1998, which establishes the European Convention on Human Rights in UK domestic law. This Act explicitly prohibits discrimination in Article 14: ""The enjoyment of the rights and freedoms set forth in this Convention shall be secured without discrimination on any ground such as sex, race, colour, language, religion, political or other opinion, national or social origin, association with a national minority, property, birth or other status."" This is a broader set of characteristics, notably preventing discrimination based on language, political opinion and property. However, this also provides narrower protection than the Equality Act, as it applies specifically to realising the other human rights in the Act. This means that government bodies cannot discriminate based on these characteristics when granting or protecting rights such as the right to a fair trial (Article 6), freedom of expression (Article 10), or freedom of assembly (Article 11). The Council of Europe has recently established an Ad-hoc Committee on AI (CAHAI)[footnote 167]to consider a potential legal framework to support the application of AI based on human rights, democracy and the rule of law. The Data Protection Act 2018 alongside the EU General Data Protection Regulation (GDPR) regulates how personal information is processed[footnote 168]by organisations, businesses or the government. The Data Protection Act supplements and tailors the GDPR in UK domestic law. Under data protection law, organisations processing personal data must follow data protection principles, which includes ensuring that information is used lawfully, fairly and transparently. Data protection law gives individuals (""data subjects"" in GDPR language) a number of rights that are relevant to algorithmic decision-making, for example the right to find out what information organisations store about them, including how their data is being used. There are additional specific rights when an organisation is using personal data for fully automated decision-making processes and profiling which have legal or other significant effects on individuals. The introduction of the Data Protection Act and the GDPR, which make organisations liable for significant financial penalties for serious breaches, has led to a strong focus on data protection issues at the top level of organisations, and a significant supporting ecosystem of guidance and consultancy helping organisations to comply. A wide range of data protection provisions are highly relevant to AI generally, and automated decision-making, and there has been widespread public commentary (both positive and negative) on approaches to training and deploying AI tools compliant with them.[footnote 169]The GDPR sets out other provisions relating to algorithmic bias and discrimination, including: Provisions around the illegality of discriminatory profiling. In Recital 71, the GDPR advises that organisations should avoid any form of profiling that results in ""discriminatory effects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation, or processing that results in measures having such an effect."" Data subjects have a right to not be subject to a solely automated decision-making process with significant effects. Article 22(1) states that ""The data subject shall have the right not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her or similarly significantly affects him or her."" The ICO specifies[footnote 171]that organisations have proactive obligations to bring details of these rights to the attention of individuals. Data protection legislation provides several strong levers to ensure procedural fairness. However, there are some inherent limitations in thinking about fair decisions purely through the lens of data protection; processing of personal data processing is a significant contributor to algorithmic decisions, but is not the decision itself, and other considerations less directly relevant to data may apply. Data protection should therefore not be seen as the entirety of regulation applying to algorithmic decisions.Efforts to comply with data protection law must not distract organisations from considering other ethical and legal obligations, for example those defined in the Equality Act. Beyond the three cross-cutting Acts above, additional laws establish fair or unfair conduct in a specific area of decision-making. These laws also apply in principle where this conduct is made or supported by an algorithm, although this is often untested in case law. Consumer Protection law such as the Consumer Rights Act 2015 sets out consumer rights around misleading sales practices, unfair contract terms, and defective products and services. This law sets out cross-sector standards for commercial behaviour, but is typically enforced through sector-specific Ombudsmen. Some regulated sectors, particularly those that are consumer facing, set out additional requirements for fair treatment, notably the Financial Conduct Authority's principles[footnote 172]for fair treatment of customers, or Ofcom's framework for assessing fairness in telecommunications services.[footnote 173]Again, algorithmic decisions would still remain subject to these rules, though it is not always clear how algorithmic decision-making could meet them in practice. The requirement for consumers to be ""provided with clear information"" and to be ""kept appropriately informed before, during and after the point of sale"" is straightforward to apply to algorithmic processes, but 'consumers can be confident they are dealing with firms where the fair treatment of customers is central to the corporate culture' is less clear. As previously discussed, the Equality Act defines a list of protected characteristics which it is unlawful to use as the basis for less favourable treatment. These characteristics reflect the evidence of systematic discrimination at a point in time, and can (and should) evolve as new forms of discrimination emerge and are recognised by society and the legal system. There are also multiple situations where algorithms could potentially lead to unfair bias that does not amount to discrimination, such as bias based on non-protected characteristics.[footnote 174]In some cases we may expect the emergence of new protected characteristics to cover these issues, but this will reflect society recognising new forms of discrimination that have been amplified by algorithms, rather than the use of algorithms themselves creating a new type of discrimination. While these situations challenge current equality legislation, they do not imply that an entirely new framework is required for algorithmic decision-making. In these examples, data protection legislation would offer affected people some levers to understand and challenge the process by which these decisions had been reached. Furthermore, the requirement for 'fair' data processing under GDPR could mean that this kind of bias is non-compliant with data protection law, but this is legally untested. In the public sector, bias based on arbitrary characteristics could also be challenged under the Human Rights Act where Article 14 prohibits discrimination based on 'other status', although any specific type of arbitrary bias would also need to be tested by the courts. Therefore, we do not believe there is evidence to justify an entirely new legislative or regulatory regime for algorithmic bias. Furthermore, a specific regulatory regime for algorithmic bias would risk inconsistent standards for bias and discrimination across algorithmic and non-algorithmic decisions, which we believe would be unworkable. Instead, thecurrent focus should be on clarifying how existing legislation applies to algorithmic decision-making, ensuring that organisations know how to comply in an algorithmic context, alongside effective enforcement of these laws to algorithmic decision-making. This is a matter of some urgency; as we have set out in this report, there are clearly risks that algorithmic decision-making can lead to discrimination.This is unlawful and the application of current legislation must be clear and enforced accordingly to ensure bad practice is reduced as much as possible. While legislation sets out the principles and minimum requirements for behaviour, these principles need to be interpreted in order to be applied in practice. This interpretation can occur by individual decision makers and/or regulators, but this interpretation is only definitive when tested by the courts. While there is a growing body of case law that addresses algorithms in data protection law, there have been very few examples of litigation in which algorithmic or algorithm supported decisions have been challenged under the Equality Act. In the absence of such case law, such interpretations are inherently somewhat speculative. One of the few examples was on the use of facial recognition technology by South Wales Police, which was recently challenged via a judicial review, both on data protection and Equality Act grounds: Case study: Facial recognition technology One of the few legal cases to test the regulatory environment of algorithmic bias was on the use of live facial recognition technology by police forces, following concerns around violations of privacy and potential biases within the system. Facial recognition technology has beenfrequently criticisedfor performing differently against people with different skin tones, meaning accuracy of many systems is often higher for white men compared to people with other ethnicities. South Wales Police have trialled the use of live facial recognition in public spaces on several occasions since 2017. These trials were challenged through judicial review, and werefound unlawfulin the Court of Appeal on 11 August 2020. One of the grounds for successful appeal was that South Wales Police failed to adequately consider whether their trial could have a discriminatory impact, and specifically that they did not take reasonable steps to establish whether their facial recognition software contained biases related to race or sex. In doing so, the court found that they did not meet their obligations under the Public Sector Equality Duty. Note that in this case there was no evidence that this specific algorithm was biased in this way, but that South West Police failed to take reasonable steps to consider this. This judgement is very new as this report goes to press, but it seems likely that this could have significant legal implications for public sector use of algorithmic decision-making, suggesting thatthe Public Sector Equality Duty requires public sector organisations to take reasonable steps to consider potential biaswhen deploying algorithmic systems, and to detect algorithmic bias on an ongoing basis. Beyond this, we are not aware of any other litigation in which the use of AI in decision-making has been challenged under the Equality Act. This means there is little understanding of what the Equality Act requires in relation to data-driven technology and AI. Whilst it is clear that if an algorithm was using a protected characteristic as input into a model and was making decisions on this basis, that would likely constitute discrimination, it is less clear in what circumstances use of variables that correlate with protected characteristics would be considered (indirectly) discriminatory. The Equality Act states that in some cases, apparent bias may not constitute indirect discrimination if it involves proportionate means of achieving a legitimate aim. There is guidance and case law to help organisations understand how to interpret this in a non-algorithmic context. However in algorithmic decision-making this is perhaps less clear. For example, the ruling by the European Court of Justice in the Test-Achats case made it unlawful for insurers to charge different rates based on sex or gender.[footnote 177]UK car insurance providers had routinely charged higher premiums for men, based on their higher expected claims profile. These insurers responded by pricing insurance with more opaque algorithms based on other observable characteristics such as occupation, car model and size of engine, or even telematics that tracked individual driver behaviour. This change eliminated direct discrimination by sex and arguably shifted pricing towards more 'objective' measures of insurance risk. However, auto insurance prices remain significantly higher for men, and it is unclear and legally untested where these algorithms cross from legitimate pricing based on risk, to indirect discrimination based on proxies for sex, such as occupation. The lack of case law has meantorganisations are often left to figure out the appropriate balance for themselves or look to international standards that do not necessarily reflect the equality framework in the UK. The uncertainty in this area is both a risk to fairness and a constraint on innovation.Guidance on appropriate good practice would help organisations navigate some of these challenges, as well as help understand the parameters of what is considered acceptable within the law. Government and regulators have several ways to provide clearer guidance on how to interpret the law. These types of guidance and regulations differ in their legal status and their audience. Statutory Codes of Practice are provided by regulators to clarify how existing law applies to a particular context. These are typically prepared by a regulator but presented by a minister in parliament. These codes and guidelines are legal in nature, and are targeted at courts, lawyers and other specialists such as HR professionals. Technical guidelines are similar to statutory codes, but are prepared by a regulator without statutory backing. Courts are not required to follow them, but will generally consider them (and whether an organisation followed them) as evidence. They must draw from existing statute and case law, and focus on how to apply the existing law to particular situations. Regulators can also issue guidance as information and advice for particular audiences, e.g. for employers or service providers. This could extend beyond current statute and case law, but must be compatible with the existing law. EHRC guidance is harmonised with statutory codes, and is focused on making the existing legal rights and obligations accessible to different audiences such as employers or affected individuals. ICO guidance often takes a similar approach, though some ICO guidance (such as that on AI) offers additional best practice recommendations which organisations are not required to follow if they can find another way to meet their legal obligations. The issues of algorithmic bias raised in this report require both clarification of the existing law, and more practical guidance that supports different stakeholders to understand and meet their obligations.In particular, organisations need clarity on the lawfulness of bias mitigation techniques, so that they can understand what they can do to address bias This clarification of existing law requires detailed knowledge of both employment law and how bias mitigation techniques work. This cross-functional effort should be led by government in order to provide official sanction as government policy, but draw on relevant expertise across the broader public sector, including from EHRC and CDEI. Recommendations to government Recommendation 10: Governmentshould issue guidance that clarifies the Equality Act responsibilities of organisations using algorithmic decision-making. This should include guidance on the collection of protected characteristics data to measure bias and the lawfulness of bias mitigation techniques. It is possible that the work to clarify existing legal obligations could still leave specific areas of uncertainty on how organisations can lawfully mitigate algorithmic bias while avoiding direct positive discrimination, or highlight undesirable constraints in what is possible. We believe this situation would be unacceptable, as it could leave organisations with an ethical, and often a legal, obligation to monitor algorithmic bias risks, but make them unable to deploy proportionate methods to address the bias they find. In this case, further clarity or amendments to equality law could be required, for example to help to clarify what lawful positive action means in the context of mitigating algorithmic bias, and where this might cross a line into unlawful (positive) discrimination. Government can clarify or amend existing law by issuing supplementary regulations or statutory instruments. These regulations are usually implemented by a minister presenting a statutory instrument in parliament. In some areas, a regulator is specifically authorised to issue rules or regulations that are also legally enforceable, such as the Financial Conduct Authority Handbook. However, under the Equality Act, the EHRC and other regulators do not have this power, and any regulations would need to be issued by a minister. If current law is unable to provide enough clarity to allow organisations to address algorithmic bias, government should issue regulations to help clarify the law. Recommendations to government Recommendation 11:Through the development of this guidance and its implementation,governmentshould assess whether it provides both sufficient clarity for organisations on their obligations, and leaves sufficient scope for organisations to take actions to mitigate algorithmic bias. If not,governmentshould consider new regulations or amendments to the Equality Act to address this. Beyond clarifying existing obligations, organisations need practical guidance that helps them meet their obligations. This should include their obligations under equality law, but also includes sector-specific concepts of fairness, and best practices and advice that go beyond minimum standards. As described in Recommendation 9 above, we believe that many of the specific issues and methods are likely to be sector-specific. Private sector industry bodies can also play a leadership role to facilitate best practice sharing and guidance within their industry. The use of algorithms to make decisions will develop and be deployed differently depending on the context and sector. Algorithmic decision-making is taking place increasingly across sectors and industries, and in novel ways. For algorithmic bias, both the EHRC and ICO have explicit responsibilities to regulate, while there are also responsibilities within the mandate of each sector regulator. The Equality and Human Rights Commission (EHRC) is a statutory body responsible for enforcing the Equality Act 2010, as well as responsibilities as a National Human Rights Institution. Their duties include reducing inequality, eliminating discrimination and promoting and protecting human rights. The EHRC carries out its functions through a variety of means, including providing advice and issuing guidance to ensure compliance with the law. They also take on investigations where substantial breaches of the law are suspected, however these resource intensive investigations are limited to a few high priority areas. In addition to investigations, the EHRC uses an approach of strategic litigation where they pursue legal test cases in areas where the law is unclear.[footnote 178]The EHRC is less likely to be involved in individual cases, and rather directs people to the Equality Advisory Support Service. Given its broad mandate, the EHRC leverages its limited resources by working collaboratively with other regulators to promote compliance with the Equality Act 2010, for example by incorporating equality and human rights in sector-specific standards, compliance and enforcement. They also produce joint guidance in collaboration with sector regulators. Within their 2019-22 strategic plan, the EHRC highlights that technology affects many equality and human rights concerns but does not currently have a strand of work specifically addressing the risks of data-driven technologies. Instead, the implications of new technologies for the justice system, transport provision and decision-making in the workplace are captured within those specific programmes. In March 2020, the EHRC called for the suspension of the use of automated facial recognition and predictive algorithms in policing in England and Wales, until their impact has been independently scrutinised and laws are improved. However this was a specific response to a UN report and does not yet appear to be part of a wider strand of work.[footnote 179]The EHRC continues to monitor the development and implementation of such tools across policy areas to identify opportunities for strategic litigation to clarify privacy and equality implications. It also recently completed an inquiry into the experiences of people with disabilities in the criminal justice system, including the challenges arising from a move towards digital justice, and has undertaken research into the potential for discrimination in using AI in recruitment. Due to the importance of the Equality Act in governing bias and discrimination, the EHRC has a key role to play in supporting the application and enforcement of the Equality Act to algorithmic decision-making. While the EHRC has shown some interest in these issues, we believe they should further prioritise the enforcement of the Equality Act in relation to algorithmic decision-making. This will partly involve a re-prioritisation of the EHRC's own enforcement, but there is also room to leverage the reach of sector regulators, by ensuring they have the necessary capability to carry out investigations and provide guidance for specific contexts. Data-driven technologies present a genuine shift in how discrimination operates in the 21st Century, so the EHRC will also need to consider whether they have sufficient technical skills in this area to carry out investigations and enforcement work, and how they might build up that expertise. Recommendations to regulators Recommendation 12:TheEHRCshould ensure that it has the capacity and capability to investigate algorithmic discrimination. This may include EHRC reprioritising resources to this area, EHRC supporting other regulators to address algorithmic discrimination in their sector, and additional technical support to the EHRC. Equalities bodies across Europe are facing similar challenges in addressing these new issues, and others have previously identified the need for additional resourcing.[footnote 180] The Information Commissioner's Office (ICO) is the UK's independent regulator for information rights. It is responsible for the implementation and enforcement of a number of pieces of legislation, including the Data Protection Act 2018 and GDPR. The ICO has a range of powers to carry out its work: It can require organisations to provide information. It can issue assessment notices that enable it to assess whether an organisation is complying with data protection regulation. Where it finds a breach of data protection regulation, it can issue an enforcement notice telling the organisation what it needs to do to bring itself into compliance (including the power to instruct an organisation to stop processing). It can impose significant financial penalties for breaches: up to EUR20m or 4% of annual total worldwide turnover. The ICO has a broad, cross-sectoral remit. It is focused on the challenge of overseeing new legislation: the interpretation and application of the GDPR is still evolving; case law under this legislation remains limited; and organisations and the public are still adapting to the new regime. The ICO has played a prominent role both in the UK and internationally in thinking about regulatory approaches to AI. Relevant activities have included: Leading a Regulators and AI Working Group providing a forum for regulators, and other relevant organisations (including CDEI) to share best practice and collaborate effectively. Developing, at the request of the government, detailed guidance on explainability, in partnership with the Alan Turing Institute.[footnote 181] This activity is, in part, a reflection of the increased scope of responsibilities placed on organisations within the Data Protection Act 2018, but also reflects gradual growth in the importance of data-driven technologies over several decades. These efforts have been useful in pushing forward activity in this space. The ICO has recently stated that bias in algorithms may fall under data protection law via the Equality Act: ""The DPA 2018 requires that any processing is lawful, so compliance with the Equality Act 2010 is also a requirement of data protection law.""[footnote 182]The ICO also makes clear in its guidance that data protection also includes broader fairness requirements, for example: ""Fairness, in a data protection context, generally means that you should handle personal data in ways that people would reasonably expect and not use it in ways that have unjustified adverse effects on them.""[footnote 183] In the sectors we studied in this review, relevant bodies include the Financial Conduct Authority (FCA) for financial services, Ofsted for children's social care and HM Inspectorate of Constabulary and Fire and Rescue Services in policing. Recruitment does not fall under the remit of a specific sector regulator, although it is an area that has been a focus for the EHRC. There are other sector regulators in areas not studied in detail in this review, e.g. Ofgem for energy services. For all consumer-facing services, the remit of the Competition and Markets Authority (CMA) is also relevant, with obligations within consumer protection legislation for consumers to be treated fairly. Public Sector Equality Duty Whilst the Equality Act applies to both the public and private sector, there are further provisions for the public sector under the Public Sector Equality Duty (PSED). This duty sets out a legal mandate for public authorities to undertake activity to promote equality. A public authority must, in the exercise of its functions, have due regard to the need to: eliminate discrimination, harassment, victimisation and any other conduct that is prohibited by or under the Act; advance equality of opportunity between persons who share a relevant protected characteristic * and persons who do not share it; foster good relations between persons who share a relevant protected characteristic and persons who do not share it. Public authorities include sector regulators who should therefore deliver the commitments set out above. These obligations under the Equality Act provide the necessary mandate for regulators to work towards eliminating risks of discrimination from algorithmic decision-making within their sectors. There is a mixed picture of how well enforcement bodies are equipped to respond to bias in algorithmic decision-making. There are regulators such as the FCA who have explored specific research and have been proactive in understanding and addressing these concerns through regulatory guidance such as the Draft Guidance on Fair Treatment of Vulnerable Customers.[footnote 184]The FCA has also deployed innovations such as the regulatory sandbox, which temporarily reduces regulatory requirements for selected products and services, in exchange for more direct supervision and guidance from the FCA.[footnote 185]Some other regulators, for example the CMA, are taking action to build their expertise and activities in this area. However, many others are not as well resourced, do not have the relevant expertise to develop guidance in these areas, or are not treating this issue as a priority. There are particular challenges for enforcement bodies in sectors where these tools are particularly novel. Case study: The Financial Conduct Authority As we set out in Chapter 4, the financial services sector is one where the use of algorithmic decision-making tools are growing in development and deployment. One of the key enforcement bodies in this sector is the FCA, who have a responsibility for consumer protection. The FCA has focused a lot of attention on the sector's use of technology, big data and AI, and identified this as a key research priority. They have spoken publicly about how the use of big data and algorithmic approaches could raise ethical issues, including concerns of algorithmic bias, and committed to further work to investigate issues in financial markets and present strategies for reducing potential harm. The FCA's joint survey with the Bank of England of the use of ML by financial institutions demonstrates their focus on this area. Following this study, they have established a public-private working group on AI to further address some of the issues. The FCA sees its role to support the safe, beneficial, ethical and resilient deployment of these technologies across the UK financial sector. It acknowledges that firms are best placed to make decisions on which technologies to use and how to integrate them into their business, but that regulators will seek to ensure that firms identify, understand and manage the risks surrounding the use of new technologies, and apply the existing regulatory framework in a way that supports good outcomes for consumers. As algorithmic decision-making grows, we expect to see similar responses from sector bodies in areas where high stakes decisions are being made about people's lives. This might involve developing technical standards on how these tools can be assessed for fairness and appropriate routes for challenge and redress for individuals. We believe there is a role for support from both the EHRC, within their regulatory remit, to work with other regulators, as well as CDEI for advice and coordination. This demonstrates the need for regulators to be sufficiently resourced to deal with equality issues related to the use of AI and data-driven technology in their sectors. It also raises the question of how the equality legislation is applied, regardless of the use of algorithms. This concern was also raised by the Women and Equalities Committee in their report ""Enforcing the Equality Act: the law and the role of the Equality and Human Rights"", which stated: As public bodies all enforcement bodies should be using their powers to secure compliance with the Equality Act 2010 in the areas for which they are responsible. Such bodies are far better placed than the Equality and Human Rights Commission could ever be to combat the kind of routine, systemic, discrimination matters where the legal requirements are clear and employers, service providers and public authorities are simply ignoring them because there is no realistic expectation of sanction.[footnote 186] Consumer facing regulators such as the FCA, Ofgem and CMA also need to ensure fair treatment for vulnerable customers within their remit. While not an issue of discrimination, regulators set out guidelines for unfair treatment and monitor outcomes for this group. This regulatory activity is conducted separately for each sector, and there is scope for greater collaboration between enforcement bodies to share best practice and develop guidance, as well as being sufficiently skilled and resourced to carry out this work. CDEI can play a key role in providing advice to regulators as well as coordinating activities. Recommendation to regulators Recommendation 13: Regulatorsshould consider algorithmic discrimination in their supervision and enforcement activities, as part of their responsibilities under the Public Sector Equality Duty. Beyond enforcement and guidance, there are a range of tools that can help organisations to meet their regulatory requirements. These range from more proactive supervision models to methods that assure whether organisations have compliant processes and suitably skilled staff. All of these complementary tools should be considered by regulators and industry as they attempt to address algorithmic bias. A regulatory sandbox is a differentiated regulatory approach where a regulator provides more direct supervision for new products and services in a controlled environment. This supervision can range from advice whether new practices are compliant, through to limited exemptions from existing regulatory requirements. A number of regulators currently offer sandbox-based support for their sector, such as the FCA, Ofgem and the ICO. The main focus of these initiatives is to help organisations understand how they can operate effectively within regulatory frameworks, and help regulators understand how innovative products and services interact with existing regulations. However, this service is most useful to those organisations adopting new business models or innovative approaches to persistent problems that may not fit existing regulations. Examples include new applications of blockchain technology in the FCA sandbox, peer-to-peer energy trading in the Ofgem sandbox, and the use of health and social care data to reduce violence in London in the ICO sandbox. Addressing algorithmic bias is an important area of regulatory complexity where closer regulatory supervision may be helpful, particularly when new innovations are being adopted that do not easily fit the existing regulatory model. Regulators with existing sandboxes should consider applications where algorithmic bias is a serious risk, potentially with additional engagement from the EHRC. Regulators in sectors that are seeing accelerated deployment of algorithmic decision-making could consider the regulatory sandbox approach to provide greater support and supervision for innovations that may need new ways of addressing algorithmic bias. In the UK, organisations are already required to produce Data Protection Impact Assessments (DPIAs) when processing personal data that is high risk to individual rights and freedoms. These assessments must consider 'risks to the rights and freedoms of natural persons' more generally including the 'impact on society as a whole'.[footnote 187]As a consequence, issues like discrimination may be considered within the remit of data protection impact assessments. However our sector work suggests that in practice, bias and discrimination are not often considered within DPIAs. Public sector organisations are also required to have due regard to a number of equality considerations when exercising their functions, which are focused on addressing the obligations organisations have under the Equality Act 2010.[footnote 188]Equality Impact Assessments are often carried out by public sector organisations prior to implementing a policy, ascertaining its potential impact on equality. Though not required by law, they are considered good practice as a way of facilitating and evidencing compliance with the Public Sector Equality Duty. There have been efforts to extend the role of Equality Impact Assessments more broadly to assess the risks to fairness raised by AI,[footnote 189]particularly in areas like recruiting.[footnote 190] Algorithmic bias and discrimination should be incorporated into existing Equality and Data Protection Impact Assessments as part of their internal governance and quality assurance processes. However, our research has indicated that there are a variety of challenges with using impact assessments for addressing algorithmic bias as a regulatory approach. There is limited evidence regarding the effectiveness of impact assessments for providing useful course correction in the development and implementation of new technologies. While the impact assessment process can usefully uncover and resolve compliance issues throughout the development and use of algorithms, we found that in practice[footnote 191]impact assessments are usually treated as a static document, completed either at the very beginning or very end of a development process and therefore do not capture the dynamic nature of machine learning algorithms, which is where algorithmic bias issues are likely to occur. It is therefore hard to regulate only against an impact assessment as it only shows one point in time; they should be seen as one tool complemented by others. There have also been efforts to combine equality and data protection concerns into a combined Algorithmic Impact Assessment[footnote 192]or Integrated Impact Assessment.[footnote 193]This could be an effective way to remove duplication and support a more consistent way of managing the regulatory and ethical risks raised by these technologies, including fairness. It may also help to highlight to regulators and organisations any tensions between different aspects of current law or guidance. One of the frequently cited challenges with the governance of algorithmic decision-making is around how organisations demonstrate compliance with equality legislation. For individuals who are the subject of algorithmic decision-making, the systems can appear opaque and commentators often refer to fears around the risk of ""black-boxes"" that hide the variables making the decisions. These concerns have led to calls for ways to assure that algorithmic systems have met a particular standard of fairness. These calls are often framed in terms of auditing, certification or impact assessments, which could also be used to assess other measures of algorithmic appropriateness, such as privacy or safety. In algorithmic bias, this lack of explainability also raises challenges for the burden of proof. In discrimination cases, the Equality Act (Section 136) reverses the burden of proof, meaning that if outcomes data suggest algorithmic discrimination has occurred, courts will assume this has occurred, unless the accused discriminating organisation can prove otherwise. That is, it is not enough for an organisation to say that it does not believe the discrimination has occurred, it needs to explicitly demonstrate that it doesn't. It is therefore essential for organisations to know what would constitute a proportionate level of proof that their AI systems are not unintentionally discriminating against protected groups.[footnote 194] There are many contexts where organisations are required to meet standards or regulations, including health and safety, cyber security and financial standards. Each of these systems have evolved into ecosystems of services that allow organisations to prove to themselves, their customers and regulators, that they have met the standard. These ecosystems include auditing, professional accreditation, and product certification. There are some parts of the 'AI assurance' ecosystem that are starting to emerge, such as firms offering 'AI ethics' consultancy and calls for 'AI auditing' or 'AI certification'. However, these efforts tend to be focused on data protection and accuracy, rather than fairness and discrimination. The ICO has recently published ""Guidance on AI and data protection"", which sets out a set of key considerations for development of an AI system. It is focused largely on compliance with data protection principles, but it also touches on the areas of data protection that relate to discrimination, including discussion on the legal basis upon which to collect sensitive data for testing for bias. However, this guidance does not directly address compliance with equality law, including the lawfulness of mitigation. The ICO has also announced a process for assessing GDPR certification schemes[footnote 195]which could be used to show that algorithmic decision-making is GDPR compliant. These steps reflect real progress in the governance of algorithms, but algorithmic bias and discrimination would inevitably be a secondary concern in a data protection centred framework. ICO's Guidance on AI and Data Protection The ICO published itsguidanceon AI and data protection in July 2020. This guidance is aimed at two audiences: those with a compliance focus, such as data protection officers (DPOs), general counsel, risk managers, senior management and the ICO's own auditors; and technology specialists, including machine learning experts, data scientists, software developers and engineers, and cybersecurity and IT risk managers. This guidance does not provide ethical or design principles for the use of AI, but corresponds to application of data protection principles. There is currently no equivalent assurance ecosystem for bias and discrimination in algorithmic decision-making. We see this as a gap that will need to be filled over time, but will require increasing standardisation and guidance in the steps to prevent, measure and mitigate algorithmic bias. In the US, the National Institute of Standards and Technology (NIST), a non-regulatory agency of the United States Department of Commerce, provides a model for how external auditing of algorithms could emerge. The NIST developed the Facial Recognition Vendor Tests which requested access to commonly used facial recognition algorithms and to then test them under 'black box' conditions, by subjecting them all to the same set of validated test images. It initially started these efforts by benchmarking false positive and false negative rates of these algorithms, allowing them to be compared based on their accuracy. In 2019 this test was extended to examine racial bias, and found that many of these algorithms had much higher error rates, particularly false positives for women and minority ethnic groups. It also found that some algorithms had much lower demographic bias, and were often the algorithms that were the most accurate in general. This analysis has allowed benchmarking and standards based on accuracy to evolve into performance comparisons of algorithmic bias. Importantly for this role, NIST is seen as a trusted, independent, third party standards body by algorithm developers. However, this function does not necessarily need to be conducted by the government or regulators. Given sufficient expertise and commonly agreed standards, testing and certification against these standards could just as easily be provided by industry bodies or trusted intermediaries. As well as testing and certification of algorithmic systems themselves, there is a need for good practice standards for organisations and individuals developing these systems, and a relevant ecosystem of training and certification. This ecosystem of private or third sector services to support organisations to address algorithmic bias should be encouraged and is a growth opportunity for the UK. Professional services are a strong and growing area of the UK economy, including those providing audit and related professional services in a number of areas. Many companies are already looking at services that they can provide to help others build fair algorithms. By showing leadership in this area the UK can both ensure fairness for UK citizens, but also unlock an opportunity for growth. Recommendations to regulators Recommendation 14: Regulatorsshould develop compliance and enforcement tools to address algorithmic bias, such as impact assessments, audit standards, certification and/or regulatory sandboxes. Advice to Industry Industry bodies and standards organisations should develop the ecosystem of tools and services to enable organisations to address algorithmic bias, including sector-specific standards, auditing and certification services for both algorithmic systems and the organisations and developers who create them. Algorithmic bias is likely to grow in importance, and this report shows that regulators will need to update regulatory guidance and enforcement to respond to this challenge. Given the overlapping nature of equality, data protection and sector-specific regulations, there is a risk that this could lead to a more fragmented and complex environment. Regulators will need to coordinate their efforts to support regulated organisations through guidance and enforcement tools. This will need to go further than cross-regulator forums, through to practical collaboration in their supervision and enforcement activities. Ideally, regulators should avoid duplicative compliance efforts by aligning regulatory requirements, or jointly issue guidance. Regulators should also pursue joint enforcement activities, where sector regulators pursue non-compliant organisations in their sector, with the support of cross-cutting regulators like the EHRC[footnote 197]and ICO. This will require additional dedicated work to coordinate efforts between regulators, who have traditionally focused on their regulatory responsibility. However, there has been an increasing effort for regulatory collaboration in other areas such as the UK Regulators Network which has more formally brought together economic sector regulators for collaboration and joint projects. Similar efforts to collaborate should be explored by sector regulators when addressing algorithmic bias. Recommendations to regulators Recommendation 15: Regulatorsshould coordinate their compliance and enforcement efforts to address algorithmic bias, aligning standards and tools where possible. This could include jointly issued guidance, collaboration in regulatory sandboxes, and joint investigations. Future CDEI work CDEI plans to grow its ability to provide expert advice and support to regulators, in line with our existing terms of reference. This will include supporting regulators to coordinate efforts to address algorithmic bias and to share best practice. CDEI will monitor the development of algorithmic decision-making and the extent to which new forms of discrimination or bias emerge. This will include referring issues to relevant regulators, and working with government if issues are not covered by existing laws and regulations. Summary Overview of findings: Making decisions about individuals is a core responsibility of many parts of the public sector, and there is increasing recognition of the opportunities offered through the use of data and algorithms in decision-making. The use of technology should never reduce real or perceived accountability of public institutions to citizens. In fact, it offers opportunities to improve accountability and transparency, especially where algorithms have significant effects on significant decisions about individuals. A range of transparency measures already exist around current public sector decision-making processes. There is a window of opportunity to ensure that we get transparency right for algorithmic decision-making as adoption starts to increase. The supply chain that delivers an algorithmic decision-making tool will often include one or more suppliers external to the public body ultimately responsible for the decision-making itself. While the ultimate accountability for fair decision-making always sits with the public body, there is limited maturity or consistency in contractual mechanisms to place responsibilities in the right place in the supply chain. Recommendations to government: Recommendation 16:Governmentshould place a mandatory transparency obligation on all public sector organisations using algorithms that have a significant influence on significant decisions affecting individuals. Government should conduct a project to scope this obligation more precisely, and to pilot an approach to implement it, but it should require the proactive publication of information on how the decision to use an algorithm was made, the type of algorithm, how it is used in the overall decision-making process, and steps taken to ensure fair treatment of individuals. Recommendation 17:Cabinet Officeand theCrown Commercial Serviceshould update model contracts and framework agreements for public sector procurement to incorporate a set of minimum standards around ethical use of AI, with particular focus on expected levels transparency and explainability, and ongoing testing for fairness. Advice to industry: Ensuring fairness in how the public sector uses algorithms in decision-making is crucial. The public sector makes many of the highest impact decisions affecting individuals, for example related to individual liberty or entitlement to essential public services. There is also precedent of failures in large scale, but not necessarily algorithmic, decision-making processes causing impacts on a large number of individuals, for example fitness-to-work assessments for disability benefits[footnote 198]or immigration case-working.[footnote 199]These examples demonstrate the significant impact that decisions made at scale by public sector organisations can have if they go wrong and why we should expect the highest standards of transparency and accountability. The lines of accountability are different between the public and private sectors. Democratically-elected governments bear special duties of accountability to citizens.[footnote 200]We expect the public sector to be able to justify and evidence its decisions. Moreover, an individual has the option to opt-out of using a commercial service whose approach to data they do not agree with, but they do not have the same option with essential services provided by the state. There are already specific transparency obligations and measures relevant to fair decision-making in the public sector in the UK, for example: TheFreedom of Information Actoffers citizens the ability to access a wide range of information about the internal workings of public sector organisations. Subject Access Requests under the Data Protection Act enable individuals to request and challenge information held about them (also applicable to the private sector). Some organisations publish Personal Information Charters describing how they manage personal information in line with the Data Protection Act.[footnote 204] Publication of Equality Impact Assessments for decision-making practices (which is not strictly required by the Equality Act 2010, but is often conducted as part of organisations demonstrating compliance with the Public Sector Equality Duty). Various other existing public sector transparency policies enable an understanding of some of the wider structures around decision-making, for example the publication of spending[footnote 205]and workforce data.[footnote 206] Parliamentary questions and other representation by MPs. Disclosure related to legal challenges to decision-making, e.g. judicial review. Inquiries and investigations by some statutory bodies and commissioners on behalf of individuals, e.g. the EHRC. There is also an opportunity for the government to set an example for the highest levels of transparency. Government can do this through the strong levers it has at its disposal to affect behaviour, either through direct management control over the use of algorithmic decision-making, or strategic oversight of arms-length delivery bodies, for example in policing or the NHS. Setting high ethical standards in how it manages private sector service delivery also offers a potential lever for strong standards of transparency in the public sector to raise standards in the private sector. For example, in a different context, mandation in 2016 of Cyber Essentials certification for all new public sector contracts not only improved public sector cyber security, but also cyber security in a marketplace of service providers who supply both public and private sector organisations.[footnote 207] The public is right to expect services to be delivered responsibly and ethically, regardless of how they are being delivered, or who is providing those services. - The Committee on Standards in Public life (2018)[footnote 208] Public bodies have a duty to use public money responsibly[footnote 209]and in a way that is ""conducive to efficiency"". Given that a potential benefit of the use of algorithms to support decision-making, if done well, is optimising the deployment of scarce resources,[footnote 210]it could be argued that the public sector has a responsibility to trial new technological approaches. Nonetheless, this must be done in a way that manages potential risks, builds clear evidence of impact, and upholds the highest standards of transparency and accountability. Currently, it is difficult to find out what algorithmic systems the UK public sector is using and where.[footnote 211]This is a problem because it makes it impossible to get a true sense of the scale of algorithmic adoption in the UK public sector and therefore to understand the potential harms, risks and opportunities with regard to public sector innovation. The recent report by the Committee on Standards in Public Life on 'AI and Public Standards' noted that adoption of AI in the UK public sector remains limited, with most examples being under development or at a proof-of-concept stage.[footnote 212]This is consistent with what CDEI has observed in the sectors we have looked at in this Review. Nonetheless, these varying accounts could lead to a perception of intended opacity from government by citizens. Government is increasingly automating itself with the use of data and new technology tools, including AI. Evidence shows that the human rights of the poorest and most vulnerable are especially at risk in such contexts. A major issue with the development of new technologies by the UK government is a lack of transparency. - Philip Alston, The UN Special Rapporteur on Extreme Poverty and Human Rights[footnote 213] The case for transparency has been made in multiple contexts, including for government policy[footnote 214]and algorithms.[footnote 215]Yet the term 'transparency' can be ambiguous, mean different things in different contexts, and should not in itself be considered a universal good.[footnote 216]For example, publishing all details of an algorithm could lead to the gaming of rules through people understanding how the algorithm works or disincentivise the development of relevant intellectual property. Another risk is that actors with misaligned interests could abuse transparency as a way of sharing selective pieces of information to serve communication objectives or purposefully manipulating an audience. However, we should be able to mitigate these risks if we consider transparency within the context of decisions being made by the public sector and if it is not seen as an end in itself, but alongside other principles of good governance[footnote 217]including accountability. We should also not assume that greater transparency from public sector organisations will inevitably lead to greater trust in the public sector. In fact, just providing information, if not intelligible to the public could fail to inform the public and even foster concern. Baroness Onora O'Neill established the principle of ""intelligent accountability""[footnote 218]in her 2002 Reith Lecture and has since spoken of the need for ""intelligent transparency"" summarised below. According to Onora O'Neill's principle of ""intelligent transparency"" information should be: Accessible: interested people should be able to find it easily. Intelligible: they should be able to understand it. Useable: it should address their concerns. Assessable: if requested, the basis for any claims should beavailable. These are useful requirements to bear in mind when considering what type of transparency is desirable given that simply providing more information just for the sake of it will not automatically build trust. Trust requires an intelligent judgement of trustworthiness. So those who want others' trust have to do two things. First, they have to be trustworthy, which requires competence, honesty and reliability. Second, they have to provide intelligible evidence that they are trustworthy, enabling others to judge intelligently where they should place or refuse their trust. - Onora O'Neill, 'How to trust intelligently'[footnote 220] Sir David Spiegelhalter has built on Onora O'Neill's work by articulating the need to be able to interrogate the trustworthiness of claims made about an algorithm, and those made by an algorithm. This led him to produce the following set of questions that we should expect to be able to answer about an algorithm:[footnote 221] Is it any good (when tried in new parts of the real world)? Would something simpler, and more transparent and robust, be just as good? Could I explain how it works (in general) to anyone who is interested? Could I explain to an individual how it reached its conclusion in their particular case? Does it know when it is on shaky ground, and can it acknowledge uncertainty? Do people use it appropriately, with the right level of scepticism? Does it actually help in practice? These questions are a helpful starting point for public sector organisations when evaluating an algorithm they are developing or using and considering the sort of information they need to know and share in order to ensure it is meaningful in the public's eyes. Based on the discussion above, we believe that more concrete action is needed to ensure a consistent standard of transparency across the public sector related to the use of algorithmic decision-making. In this section, we assess in some detail how this could work, the key conclusion of which is the following: Recommendations to government Recommendation 16:Governmentshould place a mandatory transparency obligation on all public sector organisations using algorithms that have a significant influence on significant decisions affecting individuals. Government should conduct a project to scope this obligation more precisely, and to pilot an approach to implement it, but it should require the proactive publication of information on how the decision to use an algorithm was made, the type of algorithm, how it is used in the overall decision-making process, and steps taken to ensure fair treatment of individuals. Below we discuss in more detail where this recommendation comes from. Further work is needed to precisely scope this, and define what is meant by transparency. But rooting this thinking in O'Neill's principle of ""intelligent transparency"" and Spiegelhalter's questions of what we should expect from a trustworthy algorithm provide a solid basis to ensure there is careful thinking about the algorithm itself and the information that is published. The use of the word significant clearly requires more careful definition: Significant influencemeans that the output of the machine learning model is likely to meaningful affect the overall decision made about an individual, i.e. not just providing automation of a routine process but informing decision-making in a more meaningful way e.g. by assessing risk or categorising applications in a way that influences the outcome. Significant decisionmeans that the decision has a direct impact on the life of an individual or group of individuals. In the Data Protection Act 2018, a decision is a ""significant decision"" if it produces an adverse legal effect concerning an individual or otherwise significantly affects them. Although according to the Data Protection Act this applies specifically to fully automated significant decisions, we would suggest a similar interpretation here which includes decisions made with human input. Some potential examples of algorithmic decision-making that would be in or out of scope are shown in Figure 5. Our scope is defined by the significance of the ""effect"" of the algorithm on the overall outcome of a decision-making process, and the significance of the ""impact"" of the overall decision on the life of the person(s) affected. A low impact decision can involve an algorithm which has a significant effect on the overall outcome of the decision-making process. An example of this would be a chatbot suggesting a webpage to a user on a government website. A decision which is only marginally affected by an algorithm can have a high impact on a person's life. For example, assigning applications to a given queue, pure process automation (generating a standard form letter after a decision has been made), or moving data between two systems. Finally, a decision can have a high impact on a person's life and be significantly affected by an algorithm during the decision-making process. An example of this would be risk allocation - high, medium or low. Decisions that fall into this category are in scope for the purposes of this review. Figure 5:Decisions can be differentiated by the influence of algorithms over the decision, and the significance of the overall decision When defining impactful or significant decisions, due consideration should be paid to where decisions relate to potentially sensitive areas of government policy, or where there may be low levels of trust in public sector institutions. These could include social care, criminal justice or benefits allocation. The definition of public sector in this context could be sensibly aligned with that used in the Equality Act 2010 or Freedom of Information Act 2000. Some exemptions to this general scoping statement will clearly be needed, which will require careful consideration. Potential reasons for exemption are: Transparency risks compromising outcomes:e.g. Where publication of too many details could undermine the use of the algorithm by enabling malicious outsiders to game it, such as in a fraud detection use case. Intellectual Property:In some cases the full details of an algorithm or model will be proprietary to an organisation that is selling it. We believe that it is possible to achieve a balance, and achieve a level of transparency that is compatible with intellectual property concerns of suppliers to the public sector. This is already achieved in other areas where suppliers accept standard terms around public sector spending data etc. There is some detailed thinking around this area that needs to be worked through as part of government's detailed design of these transparency processes. National Security and Defence:e.g. there may be occasional cases where the existence of work in this area cannot be placed in the public domain. In general, our view is that risks in areas 1 and 2 should be managed by being careful about the actual information that is being published (i.e. keeping details at a sufficiently high level), while area 3 is likely to require a more general exemption scoped under the same principles as those under Freedom of Information legislation. Defining the precise details of what should be published is a complex task, and will require extensive further consultation across government and elsewhere. This section sets out a proposed draft scope, which will need to be refined as the government considers its response to this recommendation. A number of views on this have been expressed previously. For example, the Committee on Standards in Public Life report defines openness, which they use as interchangeable with transparency in their report, as: ""fundamental information about the purpose of the technology, how it is being used, and how it affects the lives of citizens must be disclosed to the public.""[footnote 222] As a starting point, we would anticipate a mandatory transparency publication to include: A. Overall details of the decision-making process in which an algorithm/model is used. B. A description of how the algorithm/model is used within this process (including how humans provide oversight of decisions and the overall operation of the decision-making process). C. An overview of the algorithm/model itself and how it was developed, covering for example: The type of machine learning technique used to generate the model. A description of the data on which it was trained, an assessment of the known limitations of the data and any steps taken to address or mitigate these. The steps taken to consider and monitor fairness. D. An explanation of the rationale for why the overall decision-making process was designed in this way, including impact assessments covering data protection, equalities, human rights, carried out in line with relevant legislation. It is important to emphasise that this cannot be limited to the detailed design of the algorithm itself, but also needs to consider the impact of automation within the overall process, circumstances where the algorithm isn't applicable, and indeed whether the use of an algorithm is appropriate at all in the context. Much of this is already common practice for public sector decision-making. However, identifying the right level of information on the algorithm is the most novel aspect. There are examples elsewhere that can help guide this. For example: The Office for AI, Turing Institute and Government Digital Service's Understanding AI Ethics and Safety guidance[footnote 227]have set out a process-based governance framework for responsible AI innovation projects in the UK public sector. Within this guidance document they provide a definition of transparency within AI ethics as including both the interpretability of an AI system and the justifiability of its processes and outcome. This Guidance should be the starting point, along with the ideas and other examples set out in this report, for the UK government when considering precisely what set of information makes sense in the UK public sector. CDEI is happy to provide independent input into this work if required. We listed above a variety of existing public sector transparency measures related to decision-making. A theme of public commentary on the use of algorithms is that they can potentially undermine this transparency and accountability. Government should seek to demonstrate that this is not the case. In fact, existing FOI and DPA obligations arguably already give individuals the right to request access to all of the information listed in the scope above. Moreover, initiatives like the local government transparency code[footnote 228]which sets out the minimum data that local authorities should be publishing, the frequency it should be published and how it should be published are good examples to build on. In some regards, we are not proposing more transparency but more effective transparency. Whilst there are obligations for proactive disclosure under FOI and the DPA, these are not always effective as a transparency tool in practice and are often more reactive. By making publication of information a truly proactive process it can help government: Build in expectations of what will eventually have to be published at the early stages of projects. Structure releases in a consistent way which hopefully helps external groups (e.g. journalists, academia and civil society) engage with the data being published in an effective way, i.e. over time fewer genuine misunderstandings in the communication. Manage the overhead of responding to large numbers of similar reactive requests. The House of Lords Science and Technology Select Committee and the Law Society have both recently recommended that parts of the public sector should maintain a register of algorithms in development or use. ...the Government should produce, publish, and maintain a list of where algorithms with significant impacts are being used within Central Government, along with projects underway or planned for public service algorithms, to aid not just private sector involvement but also transparency. - House of Lords Science and Technology Select Committee[footnote 229] A National Register of Algorithmic Systems should be created as a crucial initial scaffold for further openness, cross-sector learning and scrutiny. - The Law Society, 'Algorithms in the Criminal Justice System'[footnote 230] CDEI agrees that there are some significant advantages both to government and citizens in some central coordination around this transparency. For example it would enable easier comparisons across different organisations, e.g. by promoting consistent style of transparency. Moreover, there are delivery and innovation benefits in allowing public sector organisations themselves to see what their peers are doing. However, implementing this transparency process in a coordinated way across the entire public sector is a challenging task, much greater in extent than either of the proposals quoted above (e.g. the use by local government in social care settings that we discussed in Chapter 6 would not be included in either of those examples). There are a number of comparators to consider in levels of coordination: Centralised for central government only:GDS Spend Controls Devolved to individual organisations:Publication of transparency data Central publication across public and private sector:Gender pay gap reporting[footnote 231] We suspect that there is a sensible middle ground in this case. The complexities of coordinating such a register across the entire public sector would be high, and subtle differences in what is published in transparency data might well apply in different sectors. We therefore conclude that the starting point here is to set an overall transparency obligation, and for the government to decide on the best way to coordinate this as it considers implementation. The natural approach to such an implementation is to pilot in a specific part of the public sector. For example, it could be done for services run directly by central government departments (or some subset of them), making use of existing coordination mechanisms managed by the Government Digital Service. It is likely that a collection of sector-specific registers might be the best approach, with any public sector organisations out of scope of any sector register remaining responsible for publishing equivalent transparency data themselves. To uphold accountability, public sector organisations should be able to provide some kind of explanation of how an algorithm operates and reaches its conclusion. As David Spiegelhalter says ""a trustworthy algorithm should be able to 'show its working' to those who want to understand how it came to its conclusions"".[footnote 232]Crucially, the working needs to be intelligible to a non-expert audience and therefore focusing on publishing the algorithm's source code or technical details as a demonstration of transparency can be a red herring. An area of explainability which previous reports and research have focused on is the black box. Indeed, the House of Lords Select Committee on AI expressed that it was unacceptable to deploy any AI system that could have a substantial impact on an individuals' life, unless it can generate ""a full and satisfactory explanation"" for the decisions it will take and that this was extremely difficult to do with a black box algorithm.[footnote 233]In the case of many key administrative decisions, often based on well structured data, there may not be a need to develop highly sophisticated, black box algorithms to inform decisions; often simpler statistical techniques may perform as well. Where an algorithm is proposed that does have limitations in its explainability (i.e. a black box) the organisation should be able to satisfactorily answer Spiegelhalter's questions in particular around whether something simpler would be just as good and whether you can explain how it works and how it reaches its conclusion. As mentioned in Chapter 4 the ICO and ATI have jointly developed guidance for organisations on how to explain decisions made with AI. The guidance offers several types of examples of explanations for different contexts and decisions, along with advice on the practicalities of explaining these decisions to internal teams and individuals. Whilst the guidance is not directed exclusively at the public sector, it contains valuable information for public sector organisations who are using AI to make decisions. There is also the potential for public sector organisations to publish case studies and examples of where they are applying the guidance to explain decisions made with AI. Ultimately, the algorithmic element of the decision-making process should not be so unexplainable and untransparent that it undermines the extent to which the public sector organisation is able to publish intelligent and intelligible information about the whole decision-making process. The development and delivery of an algorithmic decision-making tool will often include one or more suppliers, whether acting as technology suppliers or business process outsourcing providers. Even where development and delivery of an algorithmic decision-making tool is purely internal, there is always reliance on externally developed tools and libraries, e.g. open source machine learning libraries in Python or R. In such supply chain models, the ultimate accountability for good decision-making always sits with the public body. Ministers are still held to account by Parliament and the public for the overall quality and fairness of decisions made (along with locally elected councillors or Police and Crime Commissioners where relevant). The Committee on Standards in Public Life noted in 2018 that the public is right to expect services to be delivered responsibly and ethically, regardless of how they are being delivered, or who is providing those services.[footnote 234] The transparency mechanisms discussed in the section above form part of this overall accountability, and therefore need to be practical in all of these different potential supply chain models. Some examples of possible models for outsourcing a decision-making process are as follows. Many of the issues around defining and managing such a supply chain in a sensible way are common with any government procurement of services dependent on technology. But the source and ownership of the data on which a machine learning model is trained can make the interdependency between customer and supplier more complex in this context than in many others. Where a model is trained on data provided by the customer, it's not straightforward to flow down requirements on fairness in a supplier contract, as the ability to meet those requirements will be dependent in part on the customer's data. This is not just a public sector issue. In the wider marketplace, the ecosystem around contracting for AI is not fully developed. There is a natural desire from those at the top of the tree to push some of the responsibilities for ethics and legal compliance of AI systems down their supply chain. This is common practice in a number of other areas, e.g. TUPE regulations create obligations on organisations involved in the transfer of services between suppliers, related to the employees providing those services. There are commonly understood standard clauses included in contracts that make it clear where those any financial liabilities associated with this sit. A similar notion of commonly understood contractual wording does not exist in this case. There are pros and cons of this position. On the positive side, it ensures that organisations with responsibility for the overall decision-making process cannot attempt to pass this off onto their suppliers without properly considering the end-to-end picture. But conversely, it means that there may be limited commercial incentive for suppliers further down the supply chain to really focus on how their products and services can support ethical and legally compliant practices. The Office for AI, working in partnership with the World Economic Forum, has developed detailed draft guidance[footnote 235]on effective procurement of AI in the public sector, which includes useful consideration of how ethics issues can be handled in procurement. This is a helpful step forward, and it is encouraging that the UK government is taking a leading role in getting this right globally. The recent Committee on Standards in Public Life report on AI and Public Standards[footnote 236]noted that ""...firms did not feel that the public sector often had the capability to make their products and services more explainable, but that they were rarely asked to do so by those procuring technology for the public sector."" This guidance aims to help address this, but there is clearly more to do to implement this effectively across the UK public sector. The guidance as drafted is focused on projects that are primarily focused on buying AI solutions. This is a relevant situation, but as AI increasingly becomes a generic technology present in a whole variety of use cases, much public sector procurement of AI will be implicitly within wider contracts. It is unlikely (and not necessarily desirable) that procurement teams across all areas will focus specifically on AI procurement amongst a range of other guidance and best practice. Similar issues occur for other common underlying requirements, such as those around data protection, cyber security and open book accounting. Part of the approach taken for these is to include standard terms with model contracts and framework agreements used across the public sector that capture a minimum set of core principles. These can never achieve as much as careful thought about how to contract for the right outcome in a specific context, but help establish a minimum common standard. A similar approach should be taken for AI ethics. For procurement activity where AI is a specific focus then procurement teams need to be designing specific requirements applicable to the use case, drawing on the Office for AI and World Economic Forum guidelines. But where use of algorithmic decision-making is not specifically expected, but could form part of possible supplier solutions to an output based requirement, a common baseline requirement is needed to give the contracting authority the ability to manage that risk in life. Given the range of different possible use cases it is difficult to place highly specific requirements in a model contract. The focus should be on enabling the contracting authority to have an appropriate level of oversight on the development and deployment of an algorithmic decision-making tool to oversee whether fairness considerations have been taken into account, along with rights to reject or request changes if they are not. Helpfully, in central government, and to some extent in the wider public sector, there is a centrally managed set of procurement policies, model contracts and framework agreements which underpin the majority of procurement processes. These are mainly managed by Cabinet Office's Government Commercial Function (policy and model contracts), and the Crown Commercial Service (framework agreements). Work is already underway by these bodies to incorporate findings from the Office for AI/WEF procurement guidelines into AI-specific procurement activities, and the new AI framework RM6200.[footnote 237]However, there is scope to go further than this to cover all procurement activity which could potentially result in purchasing an AI-reliant service: Recommendations to government Recommendation 17: Cabinet Officeand theCrown Commercial Serviceshould update model contracts and framework agreements for public sector procurement to incorporate a set of minimum standards around ethical use of AI, with particular focus on expected levels of transparency and explainability, and ongoing testing for fairness. In developing the details of such terms, the government will need to consult with the marketplace to ensure that eventual terms are commercially palatable. The intention of this recommendation is to find a balance that gives commercial mechanisms for public bodies to manage concerns about bias in algorithmic decision-making (and indeed other ethical concerns around AI), but does not impose a burden on the market that is disproportionate to the risk or to other common terms within public sector procurement. In developing such standard terms, the government may want to draw on support from the Office for AI and CDEI. This review has considered a complex and rapidly evolving field. Recognising the breadth of the challenge, we have focused heavily on surveying the maturity of the landscape, identifying the gaps, and setting out some concrete next steps. There is plenty to do across industry, regulators and government to manage the risks and maximise the benefits of algorithmic decision-making. Some of the next steps fall within CDEI's remit, and we are keen to help industry, regulators and government in taking forward the practical delivery work to address the issues we have identified and future challenges which may arise. Government, industry bodies and regulators need to give more help to organisations building and deploying algorithmic decision-making tools on how to interpret the Equality Act in this context. Drawing on the understanding built up through this review, CDEI is happy to support several aspects of the work in this space by, for example: Supporting the development of any guidance on the application of the Equality Act to algorithmic decision-making. Supporting government on developing guidance on collection and use of protected characteristics to meet responsibilities under the Equality Act, and in identifying any potential future need for a change in the law, with an intent to reduce barriers to innovation. Drawing on the draft technical standards work produced in the course of this review and other inputs tohelp industry bodies, sector regulators and government departments in defining norms for bias detection and mitigation. Supporting the Government Digital Service as they seek to scope and pilot an approach to transparency. Growing our ability to provide expert advice and support to regulators, in line with our terms of reference, including supporting regulators to coordinate efforts to address algorithmic bias and to share best practice. As an example, we have been invited to take an observer role on the Financial Conduct Authority and Bank of England's AI Public Private Forum which will explore means to support the safe adoption of machine learning and artificial intelligence within financial services, with an intent to both support that work, and draw lessons from a relatively mature sector to share with others. We have noted the need for an ecosystem of skilled professionals and expert supporting services to help organisations in getting fairness right, and provide assurance. Some of the development needs to happen organically, but we believe that action may be needed to catalyse this.CDEI plans to bring together a diverse range of organisations with interest in this area, and identifying what would be needed to foster and develop a strong AI accountability ecosystem in the UK.This is both an opportunity to manage ethical risks for AI in the UK, but also to support innovation in an area where there is potential for UK companies to offer audit services worldwide. Through the course of the review, a number of public sector organisations have expressed interest in working further with us to apply the general lessons learnt in specific projects. For example, we will be supporting a police force and a local authority as they develop practical governance structures to support responsible and trustworthy data innovation. Looking across the work listed above, and the future challenges that will undoubtedly arise,we see a key need for national leadership and coordination to ensure continued focus and pace in addressing these challenges across sectors. Government should be clear on where it wants this coordination to sit. There are a number of possible locations; for example in central government directly, in a regulator or in CDEI.Government should be clear on where responsibilities sit for tracking progress across sectors in this area, and driving the pace of change.As CDEI agrees our future priorities with government, we hope to be able to support them in this area. This review has been, by necessity, a partial look at a very wide field. Indeed, some of the most prominent concerns around algorithmic bias to have emerged in recent months have unfortunately been outside of our core scope, including facial recognition and the impact of bias within how platforms target content (considered in CDEI's Review of online targeting). Our AI Monitoring function will continue to monitor the development of algorithmic decision-making and the extent to which new forms of discrimination or bias emerge. This will include referring issues to relevant regulators, and working with government if issues are not covered by existing regulations. Experience from this review suggests that many of the steps needed to address the risk of bias overlap with those for tackling other ethical challenges, for example structures for good governance, appropriate data sharing, and explainability of models. We anticipate that we will return to issues of bias, fairness and equality through much of our future work, though likely as one cross-cutting ethical issue in wider projects. If you are interested in knowing more about the projects listed above, or CDEI's future work, please get in touch viabias@cdei.gov.uk. Bias mitigation methods by stage of intervention and notion of fairness. Detailed references for each of these techniques can be found in Faculty's ""Bias identification and mitigation in decision-making algorithms"", publishedseparately. Note that Roger Taylor, the chair of the CDEI Board, is also the chair of Ofqual, the English exams regulator. Following the controversy around August 2020 exam results, Roger has stepped away from involvement in any changes made to the final version of the review. CDEI has not had any direct role in assessing Ofqual's approach, at the time of writing we understand a number of regulators are looking into the issues raised in detail.- The Behavioural Insights Team,'The perceptions of fairness of algorithms and proxy information in financial services', 2019- Royal United Services Institute, Briefing Paper:'Data Analytics and Algorithmic Bias in Policing', 2019; and Royal United Services Institute, Occasional Paper:'Data Analytics and Algorithms in Policing in England and Wales', 2019- Main report, produced under contract--2 Lowry, Stella; Macpherson, Gordon;'A Blot on the Profession', British Medical Journal 1988, pp.657-8- See, for example, IBM's initiative around thishere- For avoidance of confusion, in place of the more neutral meaning often used in machine learning or other scientific literature (e.g. ""to discriminate between"") we use ""distinguish"".- Equality Act 2010- Equality Act 2010- Note that in addition to discrimination the Equality Act also forbids victimisation and harassment, and places a requirement on organisations to make reasonable adjustments for people with disabilities, see Section 8.3 for more details.- Equality and Human Rights Commission,'Words and terms used in the Equality Act'- Note that public sector bodies in Scotland must address socio-economic inequalities in their decision-making under the Fairer Scotland Duty.- ICO,'Guide to the General Data Protection Regulation (GDPR) - Principles- ONS,'Gender pay gap in the UK: 2019'- A comprehensive review of different possibilities is given in, for example, Mehrabi, Ninareh; Morstatter, Fred; Saxena, Nripsuta; Lerman, Kristina; Galstyan, Aram;'A Survey on Bias and Fairness in Machine Learning', 2019; or Chouldechova, Alexandra; Roth, Aaron;'The Frontiers of Fairness in Machine Learning', 2018- ICO,'Explaining decisions made with AI'- OECD,'Principles on Artificial Intelligence'- There are of course many other sets of ethical principles and frameworks for AI from a variety of organisations, including various non-profit organisations, consultancies and theCouncil of Europe.- Correll, Shelley J., and Stephen Benard.'Gender and racial bias in hiring. Memorandum report for University of Pennsylvania', 2006- Gender Action Portal,'Orchestrating Impartiality: The Impact of ""Blind"" Auditions on Female Musicians'- Nuffield College Oxford, Centre for Social Investigation,'New CSI Research reveals high levels of job discrimination faced by ethnic minorities in Britain', 2019- Applied,' ""It's a pandemic of racism"": the failure of data, implicit bias and systemic discrimiation', 2020- Trades Union Congress,'Disability employment and pay gaps 2018'- Chartered Institute of Personnel and Development,'A head for hiring: the behavioural science of recruitment', 2015- PwC,'Artificial Intelligence in HR: a No-brainer', 2017- For a comprehensive analysis of different tools and the associated risks see Bogen, Miranda and Aaron Rieke,'Help wanted: an examination of hiring algorithms, equity, and bias.', Upturn, 2018- CDEI's recentreview of online targetingcovers this in more detail- BBC News,'Amazon scrapped 'sexist AI' tool', 2018- HireVue,'Train, Validate, Re-Train: How We Build HireVue Assessments Models', 2018- Quartz,'Companies are on the hook if their hiring algorithms are biased', 2018- BBC News,'Amazon scrapped 'sexist AI' tool', 2018- For an detailed report on the challenges and gaps related to auditing AI in recruitment, see the report from the Institute for the Future of Work,'Artificial intelligence in hiring: Assessing impacts on equality', 2020- See: Equality and Human Rights Commission,'What equality law means for you as an employer: when you recruit someone to work for you'- Sanchez-Monedero, Javier; Dencik, Lina; Edwards, Lilian;'What Does It Mean to 'Solve' the Problem of Discrimination in Hiring?', 2019- Ibid.- HireVue,'Train, Validate, Re-Train: How We Build HireVue Assessments Models', 2018 and ScienceDaily,'Are hiring algorithms fair? They're too opaque to tell, study finds', 2019- Sanchez-Monedero, Javier; Dencik, Lina; Edwards, Lilian.'What Does It Mean to 'Solve' the Problem of Discrimination in Hiring?', 2019- Crenshaw, Kimberle. 'Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics.' in University of Chicago Legal Forum, Volume 1989, Issue 1, pp.139-167- U.S. Equal Employment Opportunity Commission,Questions and Answers on EEOC Final Rule on Disparate Impactand""Reasonable Factors Other Than Age"" Under the Age Discrimination in Employment Act of 1967- Due to the focus of our review being bias, we were less concerned in our research with the accuracy of the tools involved. This is clearly an important question because if tools are ineffective, they are also arguably unethical however this sits outside the scope of this report.- Financial Times,'How to design AI that eliminates disability bias'and Wired,'An AI to stop hiring bias could be bad news for disabled people', 2019, 2020- ICO,Guide to General Data Protection Regulation (GDPR) - Rights related to automated decision making including profiling- Carter, S., Mwaura, S., Ram, M., Trehan, K., and Jones, T., 'Barriers to ethnic minority and women's enterprise: Existing evidence, policy tensions and unsettled questions', in International Small Business Journal, no.33, 2015, pp.49-69- In the case of credit scoring, credit reference agency data tends to only go back six years, and lenders generally only look at the last few years, which should provide some mitigation against discriminatory lending practices from decades ago.- Financial Times,'AI in banking reality behind the hype', 2018- The survey was sent to almost 300 firms and a total of 106 responses were received.- Financial Conduct Authority and Bank of England,'Machine learning in UK financial services', 2019- Ibid.- Ibid.- McKinsey and Company,'Adoption of AI advances, but foundational barriers remain', 2018- Financial Conduct Authority,'Financial Services AI Public Private Forum', 2020- Ibid.- The term was coined in the early days of computing to describe the concept that nonsense input data produces nonsense output.- Bank of England, Speech by James Proudman,'Managing machines: the governance of artificial intelligence', 2019- Hurley, M., and Adebayo, J.; 'Credit scoring in the era of big data.', in Yale Journal of Law and Technology, Volume 18, Issue 1, 2016; pp.148-216- Which?,'Experian credit reports to include rent payments for the first time', 2018- Validation techniques including detecting errors and risks in the data.- CDEI,'AI Barometer', 2020- ICO and The Alan Turing Institute,'Guidance: Part 2 Explaining decisions made with AI'- The Behavioural Insights Team,'The perceptions of fairness of algorithms and proxy information in financial services', 2019- The Bank of England prudentially regulates and supervises financial services firms through thePrudential Regulation Authority- The Lammy Review, 2017- Institutional Racism was defined in The Stephen Lawrence Inquiry as: ""the collective failure of an organisation to provide an appropriate and professional service to people because of their colour, culture or ethnic origin"" -The Stephen Lawrence Inquiry, 1999- GOV.UK,'Ethnicity facts and figures'- Strategic Review of Policing in England and Wales,'Sir Michael Barber to head major review of the police service', 2019- GOV.UK,'Prime Minister launches police recruitment drive', 2019- Police Foundation,Data-Driven Policing and Public Value, 2019- Kit Malthouse MP,'Police Funding 2020/21: Written statement HCWS51', 2020;- Policing codes, such as theCollege of Policing's National Decision ModelandCode of Ethicsare key reference points for decision-making in policing.- Babuta, Alexander and Oswald, Marion;'Analytics and Algorithms in Policing in England and Wales Towards A New Policy Framework', Royal United Services Institute, 2020- Now referred to as the Gangs Violence Matrix.- Amnesty International,'Trapped in the Matrix', 2020- ICO,'ICO finds Metropolitan Police Service's Gangs Matrix breached data protection laws', 2018- Mayor of London,'Mayor's intervention results in overhaul of Met's Gangs Matrix', 2020- CDEI have published aresearch paperon facial recognition technology, which covers police use of live facial recognition technology, along with other uses.- See for example: Richardson, Rashida and Schultz, Jason and Crawford, Kate,'Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice', AI Now Institute; Kearns, Ian and Rick Muir, 'Data Driven Policing and Public Value',The Police Foundation, 2019;'Policing By Machine', Liberty, 2019- RUSI sent Freedom of Information requests to all police forces in England and Wales, interviewed over 60 people from police forces, technology providers, academia, civil society, government, and regulation, and ran roundtables, jointly with CDEI and TechUK.- Babuta, Alexander and Oswald, Marion;'Analytics and Algorithms in Policing in England and Wales Towards A New Policy Framework', RUSI, 2020- Couchman, Hannah;'Policing by machine', Liberty, 2019- Robin Moore,'A Compendium of Research and Analysis on the Offender Assessment System (OASys) 2009-2013', National Offender Management Service, Ministry of Justice Analytical Series, 2015- The Guardian,'Met uses software that can be deployed to see if ethnic groups 'specialise' in areas of crime', 2020- See for example: Richardson, Rashida and Schultz, Jason and Crawford, Kate,'Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice', AI Now Institute- Home Office,'Police powers and procedures, England and Wales, year ending 31 March 2019'- Nesta,'Decision-making in the Age of the Algorithm', 2019- Babuta, Alexander and Oswald, Marion;'Analytics and Algorithms in Policing in England and Wales Towards A New Policy Framework', RUSI, 2020- Ibid.- NPCC and APCC,'National Policing Digital Strategy, Digital, Data and Technology Strategy 2020 - 2030'- TechUK,'Police funding settlement and tech', 2020- Police Foundation,'Data-Driven Policing and Public Value', 2019- Quote from Rick Muir, Director of Police Foundation, in, Strategic View of Policing,'Sir Michael Barber to head major review of the police service', 2019- Royal Society for the encouragement of Arts, Manufactures and Commerce,'Artificial Intelligence: Real Public Engagement', 2018- 'Ipsos MORI Veracity Index 2019'; 76% survey respondents trust the police to tell the truth - an increase of 15ppt since 1983.- Local Government Association,'Local government funding - Moving the conversation on', 2018- Open Access Government,'Changing the face of local government with digital transformation', 2019- Oxford Internet Institute, University of Oxford,'Data Science in Local Government', 2019- Eubanks, Virginia. Automating inequality: How high-tech tools profile, police, and punish the poor. St. Martin's Press, 2018.- As outlined in Chapter 5 on the policing sector, we are considering the use of tools as part of the full decision-making process.- The Guardian,'One in three councils using algorithms to make welfare decisions', 2019; and, Dencik, L. et al.,'Data Scores as Governance: Investigating uses of citizen scoring in public services', Data Justice Lab, Cardiff University, 2018- The Guardian,'One in three councils using algorithms to make welfare decisions', 2019- Ibid.- House of Commons Library,'The Troubled Families programme (England)', 2020- Dencik, L. et al.,'Data Scores as Governance: Investigating uses of citizen scoring in public services', Data Justice Lab, Cardiff University, 2018- The Guardian,'One in three councils using algorithms to make welfare decisions', 2019; and, Dencik, L. et al.,'Data Scores as Governance: Investigating uses of citizen scoring in public services', Data Justice Lab, Cardiff University, 2018- Denick, L. et al.,'Data Scores as Governance: Investigating uses of citizen scoring in public services', Data Justice Lab, Cardiff University, 2018, p.34- What Works for Children's Social Care,'Ethics Review of Machine Learning in Children's Social Care', 2020- Tech Nation,'Diversity and inclusion in UK tech companies'--2 Ibid.- New Statesman Tech,'London Tech Week's missing voice', 2019- Tech Nation,'Diversity and inclusion in UK tech companies'- SeeTech Talent Charter- GOV.UK,'PS18.5 million to boost diversity in AI tech roles and innovation in online training for adults', 2019- For a comprehensive list, seehere- Kilbertus, N.; Gascon, A.; Kusner, M.; Veale, M.; Gummadi, K. P.; and Weller, A.;'Blind Justice: Fairness with Encrypted Sensitive Attributes'. In the International Conference on Machine Learning (ICML), 2018- See, for example,hereandhere- With over 75% of respondents comfortable sharing information on age and ethnicity, and over 65% sharing disability, religious belief or sex information with new employers in order to test for and prevent unintentional bias in their algorithms.- ICO,'Guidance on AI and data protection'- Open Data Institute,'Monitoring Equality in Digital Public Services', 2020- Open Data Institute,'Protected Characteristics in Practice', 2019- Equality and Human Rights Commission,'The Public Sector Equality Duty and Data Protection', 2015- Equality and Human Rights Commission,'Public Sector Equality Duty'- Kilbertus, N., Gascon, A., Kusner, M., Veale, M., Gummadi, K. P., and Weller, A.,'Blind Justice: Fairness with Encrypted Sensitive Attributes'. In the International Conference on Machine Learning (ICML), 2018- ICO,'What do we need to do to ensure lawfulness, fairness, and transparency in AI systems'- Wachter, Sandra, and Mittelstadt, Brent;'A right to reasonable inferences: Re-thinking data protection law in the age of big data and AI', Columbia Business Law Review, 2019, p.494- NHS Research Scotland,'Data Safe Haven'- The Alan Turing Institute,'Enabling trust models for differential privacy', ongoing- Office for National Statistics,'ONS methodology working paper series number 16 - Synthetic data pilot', 2019- Under contract ref101579- Note that in the machine learning literature on fairness, some terms used throughout this report take on specific, often narrower, definitions. Discrimination is sometimes used to refer to both different outcomes for different groups, and the statistical ability to distinguish between them. Bias is both favourable or unfavourable treatment of a group, and the statistical over or under-estimation of their quantitative properties. The field of study of how to create a mathematical system that is unbiased, is called ""algorithmic fairness"". In this report we use ""discrimination"" and ""bias"" in the common language sense as defined in Chapter 2 (rather than their statistical meanings), and note that the concept of ""fairness"" discussed in this section is narrower than that described above.- CDEI,'Training a biased model', 2020- Calders, Toon, Kamiran, Faisal; and Pechenizkiy, Mykola;'Building Classifiers with Independency Constraints ', ICDMW '09: Proceedings of the 2009 IEEE International Conference on Data Mining Workshops, 2009; pp.13-18; and Zemel, Rich; Wu, Yu; Swersky, Kevin; Pitassi, Toni; and Dwork, Cynthia,'Learning Fair Representations, in International Conference on Machine Learning/', 2013, pp.325-33- Larson, Jeff; Mattu, Surya; Kirchner, Lauren; and Angwin, Julia;'How We Analyzed the COMPAS Recidivism Algorithm'2016; and Corbett-Davies, Sam; Pierson, Emma; Feller, Avi; Goel, Sharad; and Huq, Aziz;'Algorithmic decision-making and the Cost of Fairness', 2017- Kleinberg, Jon; Mullainathan, Sendhil; and Raghavan, Manis,'Inherent Trade-Offs in the Fair Determination of Risk Scores', 2016- Kusner, Matt J.; Joshua, Loftus R.; , Russell, Chris and Silva, Ricardo;'Counterfactual Fairness', in Advances in Neural Information Processing Systems, 2017; and Kilbertus, Niki; Rojas-Carulla, Mateo; Parascandolo, Giambattista; Hardt, Moritz; Janzing, Dominik; and Scholkopf, Bernhard,'Avoiding Discrimination through Causal Reasoning', in Advances in Neural Information Processing Systems, 2018- Garg, Sahaj; Perot, Vincent; Limtiaco, Nicole; Taly, Ankur; Chi, Ed., and Beutel, Alex;'Counterfactual Fairness in Text Classification through Robustness', in AAAI/ACM Conference on AI, Ethics, and Society, 2019; and Chiappa, Silvia and Gillam, Thomas P. S.;'Path-Specific Counterfactual Fairness', in AAAI Conference on Artificial Intelligence, 2018; and Russell, Chris; Kusner, Matt J.; Loftus, Joshua and Ricardo Silva,'When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness', In Advances in Neural Information Processing Systems, edited by Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S. and Garnett, R.; Curran Associates, Inc., 2017- Kusner, Matt J.; Joshua, Loftus R.; , Russell, Chris and Silva, Ricardo;'Counterfactual Fairness', in Advances in Neural Information Processing Systems, 2017; and Liu, Lydia T.; Dean, Sarah; Rolf, Esther; Simchowitz, Max and Hardt, Moritz;'Delayed Impact of Fair Machine Learning', in International Conference on Machine Learning, 2018- Hu, Lily; Immorlica, Nicole; Wortman Vaughan, Jennifer,'The Disparate Effects of Strategic Manipulation', in ACM Conference on Fairness, Accountability, and Transparency, 2018- ICO,'Guidance on AI and data protection'- ICO,'Guide to the General Data Protection Regulation (GDPR) - Special category data'- See, for example, many of those highlighted in the open source list curated by the Institute for Ethical AI and Machine Learninghere- Leslie, David;'Understanding artificial intelligence ethics and safety', The Alan Turing Institute, (2019)- Regulation for the Fourth Industrial Revolution, Department for Business, Energy and Industrial Strategy, 2019- Renewing regulation: Anticipatory regulation in an age of disruption, Nesta, 2019- Lord Sales, Justice of the UK Supreme Court,'Algorithms, Artificial Intelligence and the Law', The Sir Henry Brooke Lecture for BAILI, 2019- For a more detailed discussion on direct and indirect discrimination, see Section 2.4- Equality and Human Rights Commission,'Public Sector Equality Duty'- Seehttps://www.coe.int/en/web/artificial-intelligence/cahai, CDEI is providing expert input into this work.- GDPR defines data processing broadly in Article 4(2): collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction- See, for example,https://techgdpr.com/blog/develop-artificial-intelligence-ai-gdpr-friendly/ https://iapp.org/news/a/want-europe-to-have-the-best-ai-reform-the-gdpr/- ICO,'Guide to the General Data Protection Regulation (GDPR) - Principle (a): Lawfulness, fairness and transparency'- ICO,'Guide to the General Data Protection Regulation (GDPR) - Rights related to automated decision making including profiling'- Financial Conduct Authority,'Fair treatment of customers'- Ofcom,'Statement: Making communications markets work for customers - a framework for assessing fairness in broadband, , mobile, home phone and pay TV', 2019- See Section 2.4 for detailed discussion on these types of unfair bias. In other cases, algorithms could lead to bias based on arbitrary characteristics. It would not be practical to address these issues through discrimination law, as these biases are based on characteristics that differ by algorithm, and may not be identified in advance.- Case C 236/09 Test-Achats ECLI:EU:C:2011:100, see summaryhere- Equality and Human Rights Commission,'Our powers'- Equality and Human Rights Commission,Civil and political rights in Great Britain: Submission to the UN', 2020- Equinet (European Network of Equalities Bodies):Meeting the new challenges to equality and non-discrimination from increased digitisation and the use of Artificial Intelligence(by Robin Allen QC and Dee Masters, June 2020)- ICO,'ICO and the Turing consultation on Explaining AI decisions guidance', 2020- ICO,'ICO investigation into how the police use facial recognition technology in public places', 2019- ICO,'What do we need to do to ensure lawfulness, fairness and transparency in AI systems', 2020- Financial Conduct Authority,'GC20/3: Guidance from firms on the fair treatment of vulnerable customers', 2020- Financial Conduct Authority,'Regulatory Sandbox'- https://old.parliament.uk/business/committees/committees-a-z/commons-select/women-and-equalities-committee/inquiries/parliament-2017/enforcing-the-equality-act-17-19/- ICO,'Guide to the General Data Protection Regulation (GDPR) - What is a DPIA?', Retrieved March 30, 2019- Equality and Human Rights Commission,'Equality impact assessments'- Allen and Masters, 2020- https://www.ifow.org/publications/artificial-intelligence-in-hiring-assessing-impacts-on-equality- It is of course good practice to update impact assessments over time, and indeed GDPR requires DPIAs to be revisited when there is a change in the risk profile (see GDPR Article 35(11)), but there is not always a clear trigger point for an organisation to invest the time to do this.- https://www.ifow.org/publications/mind-the-gap-the-final-report-of-the-equality-task-force- https://rusi.org/sites/default/files/rusi_pub_165_2020_01_algorithmic_policing_babuta_final_web_copy.pdf- For further discussion of this issue, see Allen, R and Masters, D, 2020. Cloisters, September 2019, The Legal Education Foundation, In the matter of Automated Data Processing in Government Decision Making, availablehere- ICO,'Guide to the General Data Protection Regulator (GDPR) - Certification'- See also recommendation A12here- The Guardian,'Government to review 1.6m disability benefit claims after U-turn', 2018; and BBC,'Personal independence payments: All 1.6m claims to be reviewed', 2018; andGovernment update on progress of this review- National Audit Office,'Reforming the UK border and immigration system', 2014- Brauneis, Robert and Goodman, Ellen P.,'Algorithmic Transparency for the Smart City '(August 2, 2017). 20 Yale J. of Law and Tech. p.103, 2018- Home Office Visas and Immigration operational guidance- DWP Decision-Makers guide- HMRC Internal Guidance manuals- See, for example, GOV.UK, Department of Health and Social Care,'Personal information charter'; and GOV.UK, Department for Work and Pensions,'Personal information charter; and GOV.UK, Home Office,'Personal information charter'- FOI release,'Publication of spending data by local authorities'- Transparency data,'HMRC's headcount and payroll data for January 2020'- See Procurement Policy Note 09/14 (updated 25 May 2016)- The Committee on Standards in Public Life,'The Continuing Importance of Ethical Standards for Public Service Providers', 2018- Guidance,'Managing public money- Oxford Internet Institute, University of Oxford,'Data Science in Local Government', 2019- There are differing accounts. For example, an investigation byThe Guardianlast year showed some 140 of 408 councils in the UK are using privately-developed algorithmic 'risk assessment' tools, particularly to determine eligibility for benefits and to calculate entitlements; theNew Statesmanrevealed that Experian secured PS2m from British councils in 2018; andData Justice Labresearch in late 2018 showed 53 out of 96 local authorities and about a quarter of police authorities are now using algorithms for prediction, risk assessment and assistance in decision-making.- Committee on Standards in Public Life,'Artificial Intelligence and Public Standards', 2020- United Nations Human Rights, Office of the High Commissioner, 2019- Vishwanath, T., Kaufmann, D.: Toward transparency: New approaches and their application to financial markets. The World Bank Research Observer 16(1), 2001; 41-57- Mortier, R.; Haddadi, H.; Henderson, T.; McAuley, D.; Crowcroft, J.;'Human-data interaction: the human face of the data-driven society', 2014- Weller, A.;'Transparency: Motivations and Challenges', 2019- The Centre for Data Ethics and Innovation'sapproach to the governance of data-driven technology- O'Neill, Onora, 'Reith 2002:A Question of Trust', Open University, 2002- Ted Blog,'How to trust intelligently', 2013- Spiegelhalter, David -'Should we trust algorithms?', Harvard Data Science Review, 2020- Committee on Standards in Public Life,'Artificial Intelligence and Public Standards', 2020- Mitchell, Margaret et al.,'Model Cards for Model Reporting', 2018- Government of Canada's'Algorithmic Impact Assessment'- ProPublica,'New York City Moves to Create Accountability for Algorithms', 2017- GOV.UK, HM Treasury,'Review of quality assurance of government of government models', 2013- Leslie, David;'Understanding artificial intelligence ethics and safety', The Alan Turing Institute, 2019- Local Government Transparency Code, 2015- House of Commons Science and Technology Committee,'Algorithms in decision-making, Fourth Report of Session 2017-19'- The Law Society,'Algorithms in the criminal justice system', 201- Guidance: Gender pay gap reporting: overview- See reference from Spiegelhalter, D.;'Should We Trust Algorithms?' Harvard Data Science Review, 2(1). (2020)- House of Lords Select Committee on Artificial Intelligence,'AI in the UK:, ready, willing and able? Report of session 2017-19'- The Committee on Standards in Public Life,'The Continuing Importance of Ethical Standards for Public Service Providers', 2018- GOV.UK,'Guidelines for AI procurement', 2020; and World Economic Forum,'UK Government First to Pilot AI Procurement Guidelines Co-Designed with World Economic Forum', 2019- Committee on Standards in Public Life,'Artificial Intelligence and Public Standards', 2020- Crown Commercial Service,'Artificial Intelligence (AI)'-",2023
govuk_026,govuk,Responsible Ai 1,"Understand how to use artificial intelligence ethically and safely This guidance is part of a wider collection aboutusing artificial intelligence (AI) in the public sector. The Office for Artificial Intelligence (OAI) and the Government Digital Service (GDS) have produced the following chapter's guidance in partnership with The Alan Turing Institute'spublic policy programme. This chapter is a summary ofThe Alan Turing Institute's detailed guidance, and readers should refer to the full guidance when implementing these recommendations. AI has the potential to make a substantial impact for individuals, communities, and society. To make sure the impact of your AI project is positive and does not unintentionally harm those affected by it, you and your team should make considerations of AI ethics and safety a high priority. This section introduces AI ethics and provides a high-level overview of the ethical building blocks needed for the responsible delivery of an AI project. The following guidance is designed to complement and supplement theData Ethics Framework. The Data Ethics Framework is a tool that should be used in any project. This guidance is for everyone involved in the design, production, and deployment of an AI project such as: Ethical considerations will arise at every stage of your AI project. You should use the expertise and active cooperation of all your team members to address them. AI ethics is a set of values, principles, and techniques that employ widely accepted standards to guide moral conduct in the development and use of AI systems. The field of AI ethics emerged from the need to address the individual and societal harms AI systems might cause. These harms rarely arise as a result of a deliberate choice - most AI developers do not want to build biased or discriminatory applications or applications which invade users' privacy. The main ways AI systems can cause involuntary harm are: The field of AI ethics mitigates these harms by providing project teams with the values, principles, and techniques needed to produce ethical, fair, and safe AI applications. The guidance summarised in this chapter and presented at length inThe Alan Turing Institute's further guidance on AI ethics and safetyis as comprehensive as possible. However, not all issues discussed will apply equally to each project using AI. An AI model which filters out spam emails, for example, will present fewer ethical challenges than one which identifies vulnerable children. You and your team should formulate governance procedures and protocols for each project using AI, following a careful evaluation of social and ethical impacts. You should establish ethical building blocks for the responsible delivery of your AI project. This involves building a culture of responsible innovation as well as a governance architecture to bring the values and principles of ethical, fair, and safe AI to life. To build and maintain a culture of responsibility you and your team should prioritise 4 goals as you design, develop, and deploy your AI project. In particular, you should make sure your AI project is: Prioritising these goals will help build a culture of responsible innovation. To make sure they are fully incorporated into your project you should establish a governance architecture consisting of a: You should understand the framework of ethical values which support, underwrite, and motivate the responsible design and use of AI. The Alan Turing Institute calls these 'the SUM Values': These values: You can read further guidance on SUM Values inThe Alan Turing Institute's comprehensive guidance on AI ethics and safety. While the SUM Values can help you consider the ethical permissibility of your AI project, they are not specifically catered to the particularities of designing, developing, and implementing an AI system. AI systems increasingly perform tasks previously done by humans. For example, AI systems can screen CVs as part of a recruitment process. However, unlike human recruiters, you cannot hold an AI system directly responsible or accountable for denying applicants a job. This lack of accountability of the AI system itself creates a need for a set of actionable principles tailored to the design and use of AI systems. The Alan Turing Institute calls these the 'FAST Track Principles': Carefully reviewing the FAST Track Principles helps you: If your AI system processes social or demographic data, you should design it to meet a minimum level of discriminatory non-harm. To do this you should: You should design your AI system to be fully answerable and auditable. To do this you should: The technical sustainability of these systems ultimately depends on their safety, including their accuracy, reliability, security, and robustness. You should make sure designers and users remain aware of: Designers and implementers of AI systems should be able to: To assess these criteria in depth,you should consult The Alan Turing Institute's guidance on AI ethics and safety. The final method to make sure you use AI ethically, fairly, and safely is building a process-based governance framework. The Alan Turing Institute calls it a 'PBG Framework'. Its primary purpose is to integrate the SUM Values and the FAST Track Principles across the implementation of AI within a service. Building a good PBG Framework for your AI project will provide your team with an overview of: You may find it useful toconsider further guidance on allocating responsibility and governance for AI projects.",2023
govuk_019,govuk,Algorithm Transparency 4,"A template for public sector organisations to share information about how and why they are using algorithmic tools. MS Excel Spreadsheet,270 KB This file may not be suitable for users of assistive technology. https://docs.google.com/spreadsheets/u/0/d/15YzxQiGXT2E4d4AiWEbqkumMVIh4UF6Dg5PjjeDUxEU/edit?gid=2100191493&pli=1&authuser=0 The ATRS template is available in Microsoft Excel and Google Sheets formats. It is divided into 2 tiers. Tier 1: Tier 2, whilst still accessible to the general public, is aimed at specialist audiences such as: Tier 2 is also split across eight further sheets. You should complete all sections of the template. For further assistance read theguidance to using the ATRS. Email the completed template to the ATRS team onalgorithmic-transparency@dsit.gov.ukfor publication to theATRS repository. Added the updated version of the template in two formats, Excel and Google Sheets. Added the updated version of the template in two formats, Excel and Google Sheets. Added an HTML version of the form content. First published.",2023
govuk_029,govuk,Responsible Ai 4,"Case study from Credo AI. The Credo AI Responsible AI Governance Platform (the ""Platform"") is designed to help organisations ensure responsible development and use throughout the entire AI value chain. The Platform enables organisations to assess their AI systems for risks related to fairness, performance, transparency, security, and privacy, and to produce standardised AI/ML transparency artefacts for internal AI governance reviews, external compliance requirements, and independent audits. More information on the AI White Paper Regulatory Principles. The UK's principles-based approach to regulating AI (outlined in its ""pro-innovation approach to AI regulation"") helps to guide businesses by setting out the key elements of responsible AI design, development and use. The Platform helps organisations assess their AI systems for risks related to these same cross-sectoral principles. Within the Platform, Policy Packs provide modular technical, process, and documentation requirements to guide developers and deployers in adequately documenting important information about their AI use cases, while Credo AI's AI Registry provides a centralised database that allows organisations to gain comprehensive oversight of multiple AI initiatives. Based on provided use case context, the Platform recommends risk scenarios related to safety, security, and robustness, and mitigating controls which developers and deployers can use to evaluate, address and monitor the AI system. The Platform also provides guidance on the implementation of these controls for generally available ML libraries. Developers and deployers can catalogue and document details about models and datasets, architectural considerations, and risk assessments in the Platform. These details enable users to access, interpret, and understand the AI system's decision-making processes and potential impacts. The Platform also provides means to assemble and manage stakeholders of an AI system to establish accountability mechanisms and generate customised reports on different aspects of the AI use case. These reports shed light on the risks, compliance aspects, and potential impact of the system. These outputs can be adapted to align with different audiences' needs (e.g. impact assessments or model cards), promoting transparency to multiple stakeholders. In order to address fairness in AI systems across their life cycle, users are recommended risk scenarios (based on provided use case context) related to potential unwanted bias, discrimination, and fairness issues, and mitigating controls to evaluate and govern their use cases. This includes guides for evaluating AI systems for potential unwanted bias and discrimination, adhering to relevant laws, appropriate fairness definitions, and statistical bias metrics. Accountability and governance in AI systems throughout their entire life cycle is critical. The Credo AI Registry enables developers, deployers, and buyers of AI systems to track use cases that need to be governed, including third-party AI tools, effectively monitoring the supply and use of these systems. Governance tasks are also assigned to individual stakeholders, further promoting accountability. Policy packs and controls include guidance that instructs developers and deployers of AI systems to include mechanisms for contestability and redress (such as the possibility for the end users and impacted communities to appeal system outcomes). The Platform maintains an audit log of all governance actions relevant to each use case, providing transparency and traceability in decision-making processes and outcomes, helping users and affected parties understand decisions and outcomes of the systems. Experience working with enterprise has demonstrated that adopting mature governance procedures, like rigorous process controls and technical mitigations, can be difficult to control given a lack of visibility into the list of AI systems that an enterprise is already developing, buying, using, or selling. Standardising the information collected about AI systems, and an interface that reduces the burden (psychological, time, and effort) of collecting, viewing, and analysing AI use case information, allows governance owners to both decide what information to track, and effectively manage complicated information structures. By maintaining an AI registry, companies can manage multiple AI projects effectively, identify project ownership, and ascertain individuals responsible for reporting on their outcomes (success or failure). A clear and concise dashboard of use case metadata, relevant geographic markets, and information about known risks enables critical triaging of which AI initiative to focus on as an organisation employs governance. This systematic approach helps enhance transparency, accountability, and visibility, making it easier for businesses to navigate the rapidly expanding AI landscape with confidence and compliance. Once an AI use case is tracked effectively, stakeholders can seamlessly move to governing the use case. Improving the efficiency, standardisation, and quality of tracking AI use case information cannot solve all human challenges associated with standing up a governance framework at an organisation. Building support for responsible AI practices and governance remains a critical prerequisite to deriving value from a centralised AI Registry and downstream governance practices. After tracking AI initiatives and their associated risks, stakeholders will have to triage high-risk and high-impact use cases to govern and then implement governance practices including data controls, performance evaluations, evaluations of fairness and bias, steps to ensure regulatory compliance, and reporting to relevant decision-makers. Full governance of the entire AI lifecycle - as enabled by tools like the Credo AI Responsible AI Governance Platform - is critical.",2023
govuk_021,govuk,Automated Decision-Making 1,"Guidance for public sector organisations on how to use automated or algorithmic decision-making systems in a safe, sustainable and ethical way. HTML ODT,23.1 KB This file is in anOpenDocumentformat This file may not be suitable for users of assistive technology. The 'Ethics, Transparency and Accountability Framework for Automated Decision-Making' is a 7 point framework to help government departments use automated or algorithmic decision-making systems safely, sustainably and ethically. It is aimed at civil servants, particularly: The 'Ethics, Transparency and Accountability Risk Potential Assessment Form' document will help teams assess the possible risk of an automated or algorithmic decision. Updated with the introduction of Government Digital and Data, the first official name for the function and profession which was sometimes previously referred to as DDaT. First published.",2023
govuk_008,govuk,Ai Fairness 3,"FairNow's synthetic bias evaluation technique creates synthetic job resumes that reflect a wide range of jobs, specialisations and job levels so that organisations can conduct a bias assessment with data that reflects their candidate pool. New York City's Local Law 144 has been in effect since July 2023 and was the first law in the US to require bias audits of employers and employment agencies who use AI in hiring or promotion. Under the law, in-scope employers and employment agencies are required to enlist an independent auditor to conduct a disparate impact analysis by race, gender, and intersectional categories thereof. This type of analysis typically requires historical data, but when sufficient historical data is not available (for example: because an AI tool hasn't launched yet or because data is otherwise unavailable), the NYC law allows for test data to be used. FairNow's synthetic bias evaluation technique creates synthetic job resumes that reflect a wide range of jobs, specialisations and job levels so that organisations can conduct a bias assessment with data that reflects their candidate pool. The synthetic resumes are constructed using templates where various attributes are added to connect the resume to a given race and gender. The resumes are otherwise identical in attributes related to the candidate's capability to do the job successfully. Because of this construction, differences in model scores can be attributed to the candidate's demographic attributes. This approach can also be extended to bias testing beyond the NYC LL144 audit requirements. FairNow has leveraged this method to evaluate a leading HR recruitment software provider's AI for bias by disability status and gender identity. More information on the AI White Paper Regulatory Principles With FairNow's synthetic data audit capabilities, organisations that use AI tools to support employment decisions can detect potential bias, even when sufficient real-world data is unavailable or a company does not wish to share certain data with an external third-party auditor. By identifying potential issues early, users can prevent the deployment of biased AI before it can cause harm to job candidates and workers. FairNow's bias testing - including evaluations using synthetic data - provides transparency into areas of potential bias in AI tools used for hiring, promotion, and worker management. Supplemental explainability testing available on FairNow's platform can help organisations quickly pinpoint potential drivers of any differences. FairNow's synthetic bias audit capabilities allow users to detect bias in their AI across dimensions such as gender, race, gender identity, disability status, and more. Leveraging FairNow's platform, users can conduct regular testing and monitoring to proactively address any issues. Assessments for bias are a critical component of AI risk management and mitigation and are referenced in multiple laws globally. On the FairNow platform, bias testing and monitoring can be automated, with key stakeholders in AI governance alerted automatically if findings fall outside threshold ranges. This approach addresses several of the most significant pain points companies face when conducting bias audits on their data. First, companies often lack historical data to conduct a bias audit. This could be because they haven't launched the AI yet to collect data, they have some data but not enough for a statistically significant sample size, or because their demographic data collection is sparse. Second, companies may have thin data on a particular segment or subtype of customers that they'd like to understand better. Our approach can enable organisations to test for potential bias even where actual data is not available. Our approach solves many of the data-related problems that companies face when they look to test for bias. Another benefit is privacy - the organisation does not need to share confidential applicant data with a third party because the data used for this bias audit is synthetic. This saves significant time and effort in procurement and privacy workflows and reduces privacy risks. Because the data used for this audit is synthetically constructed, it may lack some of the nuance and variability seen in real-world job application data. Additionally, while the synthetic data can be customised to the organisation's applicant pool, it may lag real-world shifts in applicant distributions or types.",2023
govuk_011,govuk,Ai Procurement 1,"Published 8 June 2020 (c) Crown copyright 2020 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visitnationalarchives.gov.uk/doc/open-government-licence/version/3or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:psi@nationalarchives.gov.uk. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned. This publication is available at https://www.gov.uk/government/publications/guidelines-for-ai-procurement/guidelines-for-ai-procurement Artificial Intelligence (AI) comprises a set of technologies that have the potential to greatly improve public services by reducing costs, enhancing quality, and freeing up valuable time for frontline staff. A guide to using artificial intelligence in the public sectorprovides the following definition and detailed explanation of AI: 'AI can be defined as the use of digital technology to create systems capable of performing tasks commonly thought to require intelligence.' The development of AI is constantly evolving, but generally it involves machines using statistics to find patterns in large amounts of data and using that data to perform repetitive tasks without the need for constant human guidance. We are in the early days of deploying AI systems in Government and are continuously discovering new benefits for using AI systems to drive decision making, as well as challenges and risks that need to be addressed. This guidance mostly refers to the use of machine learning. Machine learning is a subset of AI, and refers to the development of digital systems that improve their performance on a given task over time through experience. Machine learning is the most widely-used form of AI, and has contributed to innovations like self-driving cars, speech recognition and machine translation. There are many new concepts used in the field of AI and you may find it useful to refer to a glossary of AI terms in the Annex. For more examples on how AI has been used in the UK public sector, you can explore the case studies inA guide to using artificial intelligence in the public sector. To learn more about AI technologies, theBiscuit Bookwas developed by data science experts at the Defence Science and Technology Laboratory AI Lab to help Ministry of Defence customers understand AI, data science and machine learning Public procurement can be an enabler for the adoption of AI and could be used to improve public service delivery. Government's purchasing power can drive this innovation and spur growth in AI technologies development in the UK. As AI is an emerging technology, it can be more difficult to establish the best route to market for your requirements, to engage effectively with innovative suppliers or to develop the right AI-specific criteria and terms and conditions that allow effective and ethical deployment of AI technologies. These guidelines provide a set of guiding principles on how to buy AI technology, as well as insights on tackling challenges that may arise during procurement. It is the first of such guidance, and is not exhaustive. These guidelines have been developed by theOffice for AIin collaboration with theWorld Economic Forum Centre for the Fourth Industrial Revolution, Government Digital Service, Government Commercial Function and Crown Commercial Service. A wide range of stakeholders from industry, academia and Government Departments have helped to shape this update. The guidelines were initiated through the World Economic Forum's 'Unlocking public sector AI' project. Also, as part of the project, the Office for AI co-created theAI Procurement in a box, a toolkit to help public sector procurement professionals across the globe rethink their approaches to AI procurement. The guidelines will be updated as government use of AI technologies evolves. If you are in the process of, or considering procuring, an AI system or interested in providing feedback on the guidelines please contact: ai-procurement-guidelines@officeforai.gov.uk These guidelines are aimed at central government departments who are considering the suitability of AI technology to improve existing services or as part of future service transformation. Other public sector bodies may also follow these guidelines. These guidelines will be useful to: Public procurement is governed by a framework of procurement rules and regulations, this guidance assumes the reader has a sound working knowledge of those rules and of the end-to-end procurement process. Apply commercial judgement when using this guide, and seek legal advice where appropriate. Some research contracts may be out of scope of procurement legislation. These guidelines provide an overview of what themes to consider when assessing the viability of an AI system, and what procurement teams should consider when implementing AI technology projects. As a guiding principle, be transparent about your AI project and the tools, data and algorithms you will be using, working in the open where possible. Not all forms of AI systems will be the same and increasingly, forms of AI technologies will be built-in to many types of technology products. AI systems can be developed from scratch, bought off the shelf or added to systems that are already in use. Where suppliers are proposing to utilise AI technologies as part of the delivery of a non-AI specific requirement or system - there may be a need for some additional considerations to be taken into account. In such instances, discuss with your Chief Digital Information Officer/Technical Architect to assess the impact AI models may have on the solution, and use your commercial judgement to establish appropriate provisions within the contract to accommodate the use of AI technologies as part of the delivery. This guidance should be considered alongside existing policy and guidance in relation to the use of technology and digital services such as: The Digital Service Standard The Technology Code of Practice Data Ethics Framework Guide to using Artificial Intelligence in the public sector Open Data Standards Other Technology standards and guidance Refer to theOutsourcing Playbook, anddefine your purchasing strategyin the same way as you would for any other technology requirement. A guide to using artificial intelligence in the public sectorcan help teams in government understand how AI systems can be used to solve problems and help to guide decisions as to whether AI technologies could be part of the solution that needs to be developed. The'AI Procurement in a box' toolkitprovides detailed guidance on AI procurement, and greater detail on the main issues to consider during the process. Also consult sector specific guidance such asA Buyer's Checklist for AI in Health and Carepublished by NHSx. Ensure your Technology and Data strategies are updated to incorporate AI technology adoption. Use procurement strategically to support AI adoption across government, take advantage of economies of scale of AI technology deployment through collaboration and share your knowledge with interested teams across government. Consider aligning your work with other teams across central government departments and organisations leading on relevant AI initiatives. Establish networks within your organisation and across the civil service to share insights and learn from best practice. Developing, evaluating and delivering AI projects will be more effective with diverse teams that understand the interdependent disciplines that AI technologies incorporate. This could include: Require the successful supplier(s) to assemble a team with the right skill sets, and to address the need for diversity to mitigate bias in the AI system. Data is currently the basis of the majority of AI-powered solutions. Availability of relevant data is often a prerequisite for any AI system, so time should not be spent discussing AI procurement if no data will be available. Defining the public benefit goal provides an anchor for the overall project and procurement process that the AI system is intended to achieve. AI technology also brings specific risks which must be identified and managed early in the procurement phase. Explain in your procurement documentation that the public benefit is a main driver of your decision-making process when assessing proposals. Consider the human and socio-economic impact and benefits of your AI system in line with Social Value guidance. Public benefit goals must be relevant to what you are procuring (and not generic in nature) and must comply with the principles of non-discrimination, equal treatment and proportionality. Set out clearly in your procurement documentation why you consider AI to be relevant to the problem, and be open to alternative solutions. Conduct initial AI impact assessments at the start of the procurement process, and ensure that your interim findings inform the procurement. Be sure to revisit the assessments at key decision points. Government spending can be used to create a fair, competitive market, which leads to better AI systems. Early engagement with AI vendors can result in more relevant responses, increasing the likelihood of a successful procurement and better project delivery. Focus on proportionality in your approach and do not impose unnecessary burdens that would deter suppliers, including start-ups, small and medium-sized enterprises (SMEs), Voluntary, Community and Social Enterprise (VCSE) suppliers and those owned by under-represented groups from competing. The AI systems being procured must address the challenges you want to solve and promote a responsible, innovative response from the market. Carefully written requirements can help a supplier understand what you need and propose their best solution. Tell suppliers about the situation or challenge, and let them propose a solution that meets your needs. You must establish appropriate oversight mechanisms to allow scrutiny of AI systems throughout their lifecycle. You will need to apply different considerations depending on the AI use case and the risk profile of the project, and ensure that your approach can withstand scrutiny. Highlight the need to comply with existing laws and regulations and support the standardisation of norms through your procurement documentation. Be sure to refer to existing codes of practice, guidance and regulations when drafting your requirement and ensure these are carried over to the terms of the contract where suitable. Adhere to theTech Code of PracticeandGovernment Design Principles, theData Ethics Frameworkand other relevant standards. Maximise transparency in AI decision-making to give users confidence that an AI system functions well. Encourage explainability and interpretability of algorithms and make this one of your design criteria. This means using methods and techniques that allow the results to be understood by your team. Highly 'explainable' outputs from your AI system will be able to be interpreted by your team, and by other suppliers. This will also make it more likely for you to be able to engage with other suppliers to continue or build upon your AI system in the future, limiting the risk of vendor lock-in. Make use of the experience within your multidisciplinary team to support the evaluation process and ensure there is broad expertise conducting the tender evaluation. For AI-powered solutions in the public sector, implementation plans, sustainable and ongoing evaluation methods, and mechanisms to feed back into the data model are crucial to ensure ethical use. Further, the functionality and consequences of AI systems may not be apparent in the procurement process and often only become evident during deployment, requiring extended communication and information sharing between the buyer and supplier. This section raises specific considerations to be addressed throughout the procurement process: As a general principle any AI procurement should be investigated with the mindset of ""how could AI technologies potentially benefit us?"" rather than ""how can we make our problem fit an AI system solution?"". Treat AI technology like you would any other technological solution, and use it in appropriate situations. Every service is different, and many of the decisions you make about technology will be unique to your service. Preparation is the key to achieving flexible and efficient procurement processes that encourage broad participation which are open and accessible to all. Work in the open where possible and you need to comply with pre-procurement legal requirements. This includes making due considerations required by the Public Services (Social Value) Act 2012 (as amended), as applicable, and assessing application of the Public Sector Equality Duty under the Equality Act 2010. Before you commence any procurement project likely to include deployment of AI systems, ensure you have considered the following: Bring people in your team who have the knowledge and experience to consider whether AI is a viable and appropriate solution. Seek to establish a multidisciplinary team with a diverse combination of roles and skills to support the procurement and implementation of your AI system. A team with a diverse skill set will help you to conduct data and impact assessments and ensure that your business case and procurement process reflects their key findings. Some specialist roles to consider for your AI project team: You may not need all of these roles from the very beginning, but consider your needs before you start. It is useful to consult experts to ensure that you have the right foundations in place to go to market, and have verified that you can integrate an AI system with your existing processes, technologies and services. It is important to ensurerobust practices are in place, and that work is carried out within the team's skillset. If your team lacks expertise, you could reach out to professional networks within your organisation, or across government to gather important insight into your desired use case. You may also consider completing a discovery exercise as part of the decision making process to explore your requirements. Examples for teams and organisations that might be useful to consult are the Office for AI, the Government Digital Service, theCentre for Data Ethics and Innovationor teams and organisations with specific domain knowledge. You can also find examples of AI use cases in theguide to using artificial intelligence in the public sector. Also, learn about best practices and share knowledge and feedback via expert communities such asKnowledge Hub, theDigital Buying Community, theData Science Community of Interestor other similar networks. Ensure that a discovery into your data is conducted before you go to market. You may need to seek out specific expertise to support this; data architects and data scientists should lead this process. This will help you and your team to understand the complexities, completeness and limitations of the data you have available. If a thorough assessment of the data proves difficult or has not been made, make it a requirement in the invitation-to-tender to conduct a comprehensive check of the data the AI system will use to base its decisions upon. Your data governance needs to cover all data activities related to your project: Depending on the sensitivity of your project and data, it is worth considering releasing data to suppliers during the procurement process. This can allow suppliers to gain insight into the available data, and improve responses to the invitation to tender. Ensure you provide the same data to all suppliers at the same time and that you are acting in accordance withData Protection legislationand GDPR, and consider the use of Non-Disclosure Agreements (NDAs) or supplier engagement events to support this. TheData Ethics Framework - Principle 3provides further information on data governance and proportionality. You can also consider the use of anonymisation techniques to help safeguard data privacy, including data aggregation, masking, and synthetic data. Invitations-to-tender should encourage innovative technological approaches that make less intrusive use of data or that achieve the same or similar outcomes with less sensitive datasets. Your AI impact assessment should be initiated at the project design stage. Ensure that the solution design and procurement process seeks to mitigate any risks that you identify in the assessment. Your AI impact assessment should be an iterative process, as without knowing the specification of the AI system you will acquire, it is not possible to conduct a complete assessment. Your AI impact assessment should outline: Associated risks and their respective mitigation strategies must be provided and agreed upon within the impact assessment, and should include 'go/no go' key decision points where applicable. Review your impact assessment at these decision points, or every time a substantial change to the design of an AI system is made. Data protection impact assessmentsandequality impact assessmentscan provide a useful starting point for assessing potential unintended consequences. For examples of risk assessment questionnaires for automated decision making, refer to the Government of Canada'sDirective on Automated Decision Making, and the framework onAlgorithmic Impact Assessmentsfrom AI Now. Preliminary market engagement will help to understand if, whether and how AI can be part of the solution. Learning taken from your preliminary market engagement will help you to better define your problem statement and can also help you to determine the scope and feasibility of your requirements. Your preliminary market engagement should actively seek out suppliers that can help to improve service delivery, includingSMEsand anyVCSEswho are experts in AI design and delivery across the country. All preliminary market engagement must observe the principles of public procurement and be handled in such a way that no supplier gains a preferential advantage. In practice, this means not setting the technical specification to suit a particular solution or supplier and making sure any information shared is also available during the procurement process. Completing preliminary market engagement or a discovery phase will help you to understand your problem before you commit to buying or building a service. Find more information on how to organise yourdiscovery phaseinA guide to using artificial intelligence in the public sectorsection onplanning and preparing for artificial intelligence implementation. There are currently a number of routes to market to purchase AI systems. Depending upon which kind of challenges you need to address. Framework agreements includingG-Cloud,Digital Outcomes and Specialists, and theSpark Dynamic Purchasing System (DPS)are useful starting points to consider. Innovation-oriented procurement procedures provide opportunities to accelerate the adoption of new technologies within government, and promote innovation and ethical development of AI. These can include: Agile procurement processes that allow you to go to market at different stages and can include proof-of-concepts to test the technologies before the solution goes live. Discovery phases or proofs-of-concepts can demonstrate if the AI system is likely to meet your wider requirements. Technology contests, demonstrators and challenge-based procurement processes have vendors compete against each other based on their AI skills and include an evaluation of the technologies applied to the challenges they mean to address. These processes focus on innovation and allow you to explore different approaches. Examples include theGovTech Catalystor the Scottish Government-run CivTech(r) accelerator programme. Innovation Partnerships enable the procurement of technologies that cannot be delivered by the current options available to the market. Theupdate of the Public Contracts Regulationshighlights the opportunities that this route to markets can unlock. Thisresearch reportprovides analysis on how these partnerships can work on a local level. Specialist AI Procurement Frameworks or Dynamic Purchasing Systems that prescribe the terms and conditions applying to any subsequent contract and allow you to assess suppliers against a set of predefined criteria that can include ethical requirements. TheDynamic Purchasing System for AIfrom Crown Commercial Service is the first example of this kind of novel approach. Whichever route to market you choose, be mindful that AI technologies are rapidly developing, with new technologies and products constantly coming to market. Use output-based requirements in your invitation-to-tender that focus on describing the challenges and opportunities you are facing. This will allow suppliers to determine which technologies are most appropriate for your requirements. The drafting of your requirement can drive innovation and set the foundations for the effective, responsible and ethical deployment of AI technologies. Use output-based requirements, which allow the supplier to propose how they will respond to your requirement. You will have to draft sufficiently detailed problem statements backed by user needs and required performance. Key considerations when drafting your AI requirement: Start with your problem statement: Set out clearly what challenge you are aiming to address, including any limitations and additional functional requirements. Describe why you consider AI to be relevant to the challenge, and remain open to alternative solutions. Highlight your data strategy and requirements: Describe how the AI system will fit within your current data strategy and practices based on your data discovery. Reference yourdata protection impact assessmentwhere possible, and add the findings of your data assessment, your data requirements and details on your data governance approach. Focus on data quality, bias and limitations: Use the insights from your data discovery to highlight known limitations of the data in your invitation to tender and ask suppliers to describe their strategies on how to address these shortcomings. Have a plan for addressing relevant limitations you may have missed and ask suppliers for their strategies to mitigate them. Underline the need for you to understand the supplier's AI approach: Draft evaluation questions that give you information about the algorithms and models, including how the supplier selects variables and what AI techniques the model is based on (e.g. supervised, unsupervised, or reinforcement learning), and try to establish any limitations of the model. Seek clarity on the origin and nature of any data the supplier trained the algorithms with and/or plans on bringing to the project. You can also consider the requirement for independent audits of the algorithms. Ensure your evaluation criteria appropriately assess these points. Consider strategies to avoid vendor lock-in and 'Black Box' AI systems: Avoid relying on 'black-box' algorithms. Underline the need for an 'explainable approach' to AI development (the extent to which an AI system's decision making process can be understood) in your invitation to tender. Highly 'explainable' outputs from your AI system will be able to be interpreted by your team, and by other suppliers. This will increase the likelihood for you to be able to engage with other suppliers in the future to continue or build upon the initial AI system, limiting the risk of vendor lock-in. Consider addressing these issues in your procurement documentation. Good practice could involve adopting open standards, royalty-free licensing agreements, and public domain publication terms. Apply the Data Ethics Framework -Principle 6: Make your work transparent and be accountableand consider the use of other tools such as independent audits. For more information on this topic read thepolicy briefing of the Royal Society on Explainable AI. Indicate the importance of intellectual property ownership: Suppliers may not wish to disclose details of the inner workings of their solution in order to protect their intellectual property (IP) and commercial advantage. During the design and deployment of the AI system, it is likely that either a new algorithm will be developed, or an existing one will be tailored (for example, re-trained through your data). Consider whether you or the supplier should own any new intellectual property and who can best exploit the IP generated. Ensure that arrangements are mutually beneficial and fair. Consider the10th Government Design Principleand ensure your work is open and available to others for reuse. Mention any integration with associated technologies or services: Work with your Data and Technical Architects to define any associated technologies or services that the AI system will have to integrate with. There may be specific design criteria required that potential suppliers need to be aware of when responding to your invitation-to-tender. Consider your ongoing support and maintenance requirements: Operational or service staff must have enough knowledge of or training on the AI system to understand how to use it and act on its outputs. You may wish to highlight the importance of training and knowledge transfer to ensure your team is upskilled during the life of the contract and has a deep understanding of the solution the supplier has put in place, and consider how non-specialist users can be supported. Consider ongoing support requirements, hosting or additional development that may be required beyond the term of your initial contract. If you have an in-house team taking over support and maintenance of the service, ensure they have been consulted. Add considerations on liability and risk: Allocate risk to the parties best able to manage it. Correct risk allocation is crucial to the long term viability of the service, but also to achieving best value. Liability for certain areas will reside with the department, particularly around the use and application of the AI-powered solution, and in relation to data access and transfer. Liability may also need to sit with the supplier, including areas focused around technical, security and quality assurance. Underline these considerations in your invitation-to-tender. See chapter 8 of theOutsourcing Playbookfor more information on Risk Allocation. The application must also have an easy way to report any suspected unauthorised behaviour to relevant authorities within or outside the organisation. Use the experience of the teamto support the drafting process, and engage with users and stakeholders to establish the core areas of assessment which should be included. All requirements should be transparent and should not discriminate against particular types of suppliers, such asSMEsandVCSEs, or those from countries with which the UK has trade agreements with procurement obligations. Follow the commercial best practice outlined within theOutsourcing Playbook, and adhere to theTech Code of PracticeandGovernment Design Principles. During the selection and evaluation stages, consider the suppliers' responses to your requirement. Make use of the experience within your multidisciplinary team to support the evaluation process and to ensure there is a broad base of expertise conducting the evaluation. There are keyrobust practicesthat you can ask for suppliers to demonstrate when providing AI-powered solutions and then look out for when evaluating tenders. Robust practices may include, but are not limited to: As part of the evaluation process, also review the specialist skills, qualifications and diversity of the team that will develop and deploy the AI system. This can also help to anticipate or detect unfair bias in the system. Be aware that suppliers' responses will give you an indication of a supplier's general approach. Do not expect a fully detailed and definite plan, as AI development is an iterative process and the system will invariably change and evolve as the project progresses. The resources developed by the World Economic Forum in partnership with the Office for AI alsoprovide examples for evaluating responsesin AI tenders, and will provide you with some ideas on further considerations during the evaluation stage. TheMagenta Bookcan provide general guidance on monitoring and evaluation strategies in government. As with any contract, time and care should be taken to onboard the new supplier in accordance with theOutsourcing playbookand best practice forContract Management. An AI system may need continued support throughout its lifecycle. Accepting the potential impact of any support gaps, or employing outside expertise, both come at a cost. This should be factored in when purchasing an AI-powered solution. Consider implementing process-based governance frameworks as suggested in theGuidance for Understanding AI ethics and safety. This provides a basis to integrate norms, values, and principles informing procedures and protocols that define the project workflow. TheAlan Turing Institutecalls it a 'PBG (Process-Based Governance) Framework' that will provide your team with an overview of: Enable end-to-end auditability by implementing process logs that gather the data across the modelling, training, testing, verifying, and implementation phases of the project lifecycle. Such a log should allow for the variable accessibility and presentation of information with different users in mind to achieve interpretable and justifiable AI. Testing the model on an ongoing basis is necessary to maintain its accuracy. An inaccurate model can result in erroneous decisions that negatively impact citizens. Therefore, establish with the supplier how the efficacy of the model will be monitored once deployed. The National Cyber Security CentreGuidance for assessing intelligent tools for cyber securityalso highlights the importance of these considerations. Evaluate the thoroughness and logic of the knowledge transfer plan to ensure that teams internally will be able to use the tool appropriately on their own once the project is finalised. If this cannot be guaranteed, ensure in-house capacity is established through hiring and retaining or further maintenance contracts. Operational or service staff must have enough knowledge of or training on the AI system to understand how to use it and act on its outputs. Address the need for staff training and support to avoid the misuse of AI applications with the AI supplier. The application must have an easy way to report any suspected unauthorised behaviour to relevant authorities within or outside the organisation. Consider what the end-of-life processes for your AI system and the data should look like. Auditable methods of data cleaning and collection are key for you to effectively apply the guidelines. Defining end-of-contract roles and processes for both the contracting authority and the supplier is important. Ensure the contract includes such considerations, and test if your contract management processes are sufficiently robust to adequately support the end-of-life for the AI system, as part of the wider process it is embedded in.",2023
govuk_015,govuk,Algorithm Transparency 0,"The Government Digital Service (GDS) is helping public sector organisations provide clear information about how and why they are using algorithmic tools. The Algorithmic Transparency Recording Standard (ATRS) establishes a standardised way for public sector organisations to publish information about how and why they are using algorithmic tools. Algorithmic transparency means being open about how algorithmic tools support decisions. This includes providing information on algorithmic tools and algorithm-assisted decisions in a complete, open, understandable, easily-accessible, and free format. The Hub is made up of The ATRS was developed by teams now situated in the Government Digital Service (GDS). Its design and development was underpinned by extensive collaboration with public sector, industry and academic stakeholders as well as a public engagement study. It was piloted with a variety of public sector organisations. The ATRS is mandatory for all government departments, and for arm's-length bodies (ALBs) which deliver public or frontline services, or directly interact with the general public. Thescope and exemptions policy, published in December 2024, sets out the organisations and algorithmic tools for which the ATRS is a requirement in more detail. The ATRS remains recommended by theData Standards Authorityfor use in the broader public sector. TheBlueprint for Modern Digital Governmentpublished in January 2025 stated that government would 'enhance transparency by building on the Algorithmic Transparency Recording Standard to increase openness about AI usage'. We look forward to continuing to enhance and iterate the ATRS and encourage its use across the public sector. Get in touch with us atalgorithmic-transparency@dsit.gov.uk. Updated the ATRS template and associated guidance for public sector bodies, and updated references to Government Digital Service (GDS). Added the 'Scope and exemptions policy for mandatory use of the Algorithmic Transparency Recording Standard'. This page has been updated with a new version of the ATRS template and a link to a new GOV.UK search page for ATRS published records. First published.",2023
govuk_020,govuk,Automated Decision-Making 0,"Updated 29 November 2023 (c) Crown copyright 2023 This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visitnationalarchives.gov.uk/doc/open-government-licence/version/3or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:psi@nationalarchives.gov.uk. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned. This publication is available at https://www.gov.uk/government/publications/ethics-transparency-and-accountability-framework-for-automated-decision-making/ethics-transparency-and-accountability-framework-for-automated-decision-making The ethical considerations of artificial intelligence and automated systems is at the centre of technological advancement. According to a recentEU surveyand aBritish Computer Society surveyin the UK, there is a distinct distrust in the regulation of advanced technology. Areview by the Committee on Standards in Public Lifefound that the government should produce clearer guidance on using artificial intelligence ethically in the public sector. Current guidance can be lengthy, complex and sometimes overly abstract. This is not just a Digital, Data and Technology issue. We need to improve the general literacy of automated or algorithmic decision-making with clear information and practical steps for civil servants and ministers to support the agenda and provide appropriate challenge. Decision-makers should not assume that automated or algorithmic decision-making is a 'fix-all' solution, particularly for the most complex problems. This 7 point framework will help government departments with the safe, sustainable and ethical use of automated or algorithmic decision-making systems. It has been developed in line with guidance from government (such as theData Ethics Framework) and industry, as well as relevant legislation. It supports the priorities of the Central Digital and Data Office, and aligns with wider cross- government strategies in the digital, data and technology space. Departments should use the framework with existing organisational guidance and processes. Automated decision-making refers to both solely automated decisions (no human judgement) and automated assisted decision-making (assisting human judgement). There are some different legal requirements for the two forms of automated decision-making. This framework should be applied in its entirety for both forms to ensure best practice. Solely automated decision-making means decisions that are fully automated with no human judgement. This will likely be used in a scenario that is often repetitive and routine in nature. Automated assisted decision-making is when automated or algorithmic systems assist human judgement and decision-making. These are more complex, often with more serious implications for citizens. Example of solely automated decision-makingA worker's pay is linked to their productivity, which is monitored using an automated system. The decision for how much pay the worker receives for each shift they work is made automatically by referring to the data collected about their productivity. (Source: Information Commissioner's Office) Example of automated decision-making assisting human judgementAn employee is issued with a warning about late attendance. The warning was issued because the employer's automated clock-in system highlighted that the employee had been late on a number of occasions. The actual decision to issue a warning was then taken by the employer's manager after being informed by the automated system. (Source: Information Commissioner's Office) Article 22 of the General Data Protection Regulation (GDPR) states that when a solely automated decision is made, resulting in a legal or similarly significant event, individuals have the right to not be subjected to it. You can only make a solely automated decision that has legal or similar significant effects on an individual when it is any of the following: Before using this framework, you should consider whether using an automated or algorithmic system is appropriate in your context. Scrutiny should be applied to all automated and algorithmic decision-making. They should not be the go-to solution to resolve the most complex and difficult issues because of the high-risk associated with them.Read more information about risks. When using an automated or algorithmic decision-making system, the risks are different and this should be taken into account. Senior owners should conduct a thorough risk assessment, exploring all options. You should be confident that the policy intent, specification or outcome will be best achieved through an automated or algorithmic decision-making system. Algorithmic risks include, but are not exclusive to: This framework is for all civil servants, specifically: Everyone must ensure that data is used responsibly. TheData Ethics Frameworkand data protection law must be followed. Review the quality and limitations of datasets used. For example, is it accurate and representative? Has it been assessed for potential bias and discrimination? When datasets are used for decision-making processes they were not intended for - like proxy or generalised social datasets - caution, human oversight and intervention are required. When working with, or dependent on, third parties the framework should be adhered to. This requires early engagement with commercial expertise to ensure that the framework is embedded into any commercial arrangements. When you use automated decision-making in a service, you should: Prototype and test your algorithm or system so that it is fully understood, robust, sustainable and that it delivers the intended policy outcomes (and unintended consequences are identified). Algorithmic and automated decision-making should not be seen as the solution to all issues, particularly for complex and challenging policy areas. Rigorous, controlled and staged testing should take place before going live. Throughout prototype and testing, human expertise and oversight is required to ensure technically resilient and secure, as well as accurate and reliable, systems. Ensuring that it does not cause unintentional harm, particularly when human life or safety are dependent on it. More sustainable solutions should be prioritised to ensure the delivery of intended policy outcomes in an ever evolving and changing technology landscape. Involve a multidisciplinary and diverse team in the development of the algorithm or system to spot and counter prejudices, bias and discrimination. Algorithms can be used to identify the inherent biases associated with human judgement, but they can also inherit human and societal biases and forms of prejudice, particularly those related to sensitive characteristics such as: race, ethnicity, gender, nationality, income, sexual orientation, ability, and political or religious belief. It should be presumed that the algorithm or system that you are developing is capable of causing harm and injustice. Human judgement, engagement, and testing with diverse and representative stakeholders is required to avoid the unjust impacts on the individual and for the collective. This is integral to the policy specification development phase. Throughout the entire lifecycle of your algorithm or system, draw on and be led by the expertise from other disciplines (for example policy and operational delivery), to ensure that your algorithm or system has integrity and is developed with an inclusive and collective approach at its core, addressing and answering the desired policy intent. A multidisciplinary team with diverse roles and skills will help contribute to reducing bias and producing more accurate outcomes. Work on the assumption that every significant automated decision should be agreed by a minister and all major processes and services, subject to automation consideration, should have a senior owner. The algorithm or system should be designed to be fully answerable and auditable. Responsibility and accountability for algorithms and automation, and their associated outcomes should be made clear. Organisations and individuals should be held accountable to ensure the proper functioning of artificial intelligence. In the public sector there are stricter rules for a decision made or advised to be made by a machine. Officials make decisions on a daily basis on behalf of their Secretary of State, who is ultimately the one accountable for all decision-making in their department. Every process or service that involves an algorithm or system making a significant decision is required to get ministerial sign-off. This should be done in alignment with theMinisterial Code. Ensure that the algorithm or system adequately protects and handles data safely, and is fully compliant with Data Protection legislation. The public sector has a responsibility to lead the way in citizen data handling. Good data can give us insights that help intelligent decision-making. Poor use of data, particularly in algorithmic or automated decision-making, can be damaging. Implementation should align with theData Ethics Framework, and by default, the design of the algorithm and system should keep data secure, and comply withdata protection law. In particular, when datasets are used for decision-making purposes they were not intended for, such as proxy datasets and generalised social datasets (for example individual decisions based on regional location data from the Census), additional caution and robust human oversight is required. To build trust, individuals accountable for the risk management and compliance of the algorithm and automated system should create or build on data governance processes that handle and protect data safely while maintaining the quality of the data used. Work on the basis of a 'presumption of publication' for all algorithms that enable automated decision-making, notifying citizens when a process or service has automated decision-making with plain English explanations (all exceptions to that rule agreed with government legal advisors before ministerial authorisation). Context is essential to the explainability of an automated decision. Under data protection law, for fully automated processes, you are required to give individuals specific information about the process. Process owners need to introduce simple ways for the impacted person(s) to request human intervention or challenge a decision. When automated or algorithmic systems assist a decision made by an accountable officer, you should be able to explain how the system reached that decision or suggested decision in plain English. Traceability mechanisms should be in place to help explain. There are different approaches to explain how a decision has been made so you need to understand what explanations are possible. The explanation needs to be appropriate for your audience, expert or non-expert, and should be scrutinised and iterated by a multidisciplinary and diverse team (including end users) to avoid bias and group speak. Ensure that your algorithm or system adheres to the necessary legislation and has full legal sign-off from relevant government legal advisors. Relevant laws include data protection law (covered in point 5) and the Equality Act (covered in point 2). Government legal advisors need to be involved from the start, to help you to understand what is possible. They will also be able to advise on any upcoming or current legislation that affects your specific policy intent or automated system. Human rights and democratic values should also be at the heart of your approach. Continuously monitor the algorithm or system, institute formal review points (recommended at least quarterly), and end user challenge to ensure it delivers the intended outcomes and mitigates against unintended consequences that may develop over time (referring to points 1 to 6 throughout). Automated systems are tools that can be used to drive a sustainable and inclusive society. There are significant opportunities but they should be approached with awareness, caution and an understanding of the trade-offs. Testing should continue past the initial development stage with datasets regularly reviewed and evaluated by an established governance mechanism of diverse representation, working on the presumption that your algorithm or system is capable of causing harm to a person(s). You should test and apply all of the points in this framework throughout the lifecycle of the system and make sure that your system still aligns with the intended policy outcome (which could change over time). These case studies from the public sector and private sector, both in the UK and abroad, should help teams recognise if their project involves automated assisted decision-making. Avon and Somerset Constabulary use a tool called Qlik Sense that connects internal databases with local authority databases. UsingAIpredictive modelling, it produces individual risk assessment and intelligent profiles to assist the decision-making of officers, handling offenders according to their perceived risk level. The predictive models are regularly validated on a quarterly basis to ensure quality and accuracy. Source:Centre for Data Ethics and Innovation (CDEI), 'Review into bias in algorithmic decision-making' Many Fintech companies useAIto enhance services, for example predicting whether people are able to repay personal loans - enabling a decision to be made. Source:The Alan Turing Institute, 'Artificial intelligence in finance' Many insurance companies use machine learning and predictive models in the claims process, assessing information to make a judgement impacting customers. Source:CDEI, 'Review into bias in algorithmic decision-making' AIcan be used in mammogram screening. Currently, two radiologists are required to examine the mammogram, with a third radiologist sometimes required to provide unanimity - this is a constraint on resources.AIsystems can perform the second mammogram reading, indicating whether a radiologist assessment is needed. This reduces radiologists' workload. The automated decision here has potentially life-changing implications. Source:Karin Dembrower, 'Effect of artificial intelligence-based triaging of breast cancer screening mammograms on cancer detection and radiologist workload: a retrospective simulation study' Canada introduced automated decision-making in its immigration and refugee system. Predictive analytic systems automate activities previously conducted by immigration officials and support the evaluation of some immigrant and visitor applications - this results in a more efficient service, but has life changing implications. The Canadian government has put in place a series of mitigations to reduce risk such as algorithmic impact assessments and bias testing. Source:Roxana Akhmetova, 'HowAIis Being Used in Canada's Immigration Decision-making' A set of step-by-step instructions. In artificial intelligence, the algorithm tells the machine how to go about finding answers to a question or solutions to a problem. Source: Matthew Hutson (2017), 'AIGlossary: Artificial intelligence in so many word' There is no single agreed definition of artificial intelligence, but broadly artificial intelligence is the use of digital technology to create systems capable of performing tasks commonly thought to require intelligence. Source: Government Digital Service, Office for Artificial Intelligence (2019), 'A guide to using artificial intelligence in the public sector' The creation and implementation of technology that automatically processes data with the purpose of processing large amounts of data efficiently with minimal human contact. In this context it is internal decision-making and would not lead to a 'decision' in a public policy context. Source:OECD(2013), 'Automated Data Processing' An emerging branch of applied ethics that studies and evaluates moral problems and describes the value judgements related to data (including generation, recording, curation, processing, dissemination, sharing and use), algorithms, and corresponding practices, in order to formulate and support morally good solutions. Source: Luciano Floridi, Mariarosaria Taddeo (2016), 'What is data ethics?' The term data protection law is used to encompass all required data-related legislation that must be followed throughout the entire life-cycle of your algorithm or automated system. This includes the Data Protection Act (2018) and EU General Data Protection Regulations (GDPR) and its UK successor (UKGDPR). Source: GOV.UK 'Data protection' Red team testing is a structured effort to find flaws and vulnerabilities in a technical system, often performed by dedicated 'red teams'. To anticipate the potential risks associated with artificial intelligence system's, a diverse team should seek out these flaws and vulnerabilities of the system. Source: Miles Brundage et al. (2020) 'Toward TrustworthyAIDevelopment: Mechanisms for Supporting Verifiable Claims' Actions, processes and data are made open to inspection by publishing information about the project in a complete, open, understandable, easily-accessible and free format. Source: Government Digital Service (2020) 'Data Ethics Framework' A person(s) who is impacted by the service or automated decision. This includes, but is not exclusive to, civil servants, citizens and intermediaries.",2023
govuk_007,govuk,Ai Fairness 2,"FairNow's AI governance platform serves as a single source of truth for managing AI governance, risk, and compliance. Many organisations today are leveraging AI in a distributed way across many different teams and departments. FairNow's AI governance platform is an organisation's AI governance command centre, serving as a single source of truth for managing AI governance, risk, and compliance. Risk, legal, data, technology, and business leaders can review their AI risks, track usage, monitor dependencies, and ensure compliance all within a single platform. Core features of FairNow's platform include AI inventory management; management of governance workflows, roles, and accountabilities; risk assessments; testing and ongoing monitoring; documentation and audit trails; vendor AI risk management; and regulatory compliance tracking. Organisations integrate the platform into their day-to-day governance activities, leveraging FairNow's built-in functionality to automate governance tasks, simplify their compliance tracking and reporting, and centralise their oversight of AI risk. Organisations using the platform today leverage a combination of off-the-shelf capabilities and configurable features. For example, many organisations adopt FairNow's risk, regulatory, and compliance intelligence offerings to stay informed of potential risks for each of their AI applications, as well as in-scope laws and regulations; they also add their own incremental risk assessment questions, policies, and controls to ensure that AI usage is adhering to requirements that are specific to their own organisations. More information on the AI White Paper Regulatory Principles By tracking an organisation's AI inventory and usage, the FairNow AI governance platform helps organisations determine the appropriate safety and reliability checks. These checks are logged on the FairNow platform to ensure that these safeguards are applied throughout the AI lifecycle and that a record of compliance is documented and maintained. This is done in two ways. First, through tracking which regulations apply to the organisation's AI and which voluntary standards the organisation follows, we explain which safety, reliability and robustness checks should be followed as part of these frameworks. The platform tracks these various checks. Second, FairNow's proprietary risk assessment logic identifies scenarios in which an AI system poses certain classes of risks related to safety, security, and robustness. The platform explains the risk and recommends potential mitigations to manage the risk. By providing a single source of truth for overall AI governance, organisations can establish a transparent AI governance program. Multiple governance controls help developers and deployers of AI to provide the right level of transparency, notification and explainability to stakeholders; and documentation/approvals stored centrally on the platform ensures organisations maintain robust audit trails. FairNow has built in several model interpretability features to the platform that can help organisations understand the factors that drive their AI. The platform automates multiple types of bias testing in alignment with regulatory expectations and best practices. The first is a disparate impact assessment analysis, which is automated by the platform and reports differences in selection rate or model scoring rate across demographic groups. The second is an explainability analysis which helps organisations understand the drivers behind model decisions, which can help determine the extent to which the model bases its decisions on demographic information versus valid and application-relevant criteria. The third is a chatbot bias assessment, which evaluates chatbots for differences in quality of responses between different demographic groups. Additionally, the platform tracks AI-related laws and regulations to determine which apply to the organisation's AI, so that they can adhere to the appropriate fairness requirements for which they are in scope. Through a set of roles and responsibilities, workflows, and controls, the platform ensures that accountability is established. Organisations can establish multiple roles, set multi-step approval authority, and leverage FairNow's platform to automate notifications and alerts in the event that gaps are identified for a particular AI application or at the organisational level. As a company with decades of combined experience in model risk management and AI governance, we understand how challenging AI governance can be - and how cumbersome it can be without the right tools and automation. Establishing workflows based on industry best practice and in alignment with standards like NIST and ISO while allowing organisations to configure key parts of their AI governance program ensures the platform can serve organisations of all sizes, industries, and maturity levels. With the many different ways that AI can be used across a wide organsation, tracking risk in a centralised manner can be cumbersome and time-consuming. Organisations without the right tools in place can end up tracking their AI inventory and risk across spreadsheets and shared drives. This results in hours spent manually managing processes, increasing the risk of human error and the possibility that something important may fall through the cracks. FairNow's AI governance platform is designed to help organisations track their AI systems and associated risks, whether managing an inventory of five or 500 applications. AI risk assessments allow organisations to evaluate each application's specific components and risk factors, assigning appropriate risk levels. The platform's testing and monitoring features support compliance with global AI laws and regulations, ensuring that AI systems are effective, safe, and fair. Documentation and audit trails provide transparency for internal stakeholders and impacted populations. Vendor AI risk management helps companies ensure that the AI they procure meets international standards. Additionally, regulatory and compliance tracking is essential for those who fall under the jurisdiction of any of the growing number of AI regulations that are in effect across the globe. AI governance platforms and technologies are designed to simplify, streamline, and automate various aspects of the governance process. However, human oversight--both of individual AI applications and the overall governance program--is essential for effectively managing existing and emerging AI risks within an organisation. Platforms like FairNow should be used to facilitate and enhance, rather than replace, human oversight in identifying, managing, and mitigating AI risks in line with internal and external requirements.",2023
